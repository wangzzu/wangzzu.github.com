<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matt&#39;s Blog</title>
  <subtitle>柳年思水</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matt33.com/"/>
  <updated>2020-10-25T15:59:12.835Z</updated>
  <id>http://matt33.com/</id>
  
  <author>
    <name>Matt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink StateFun 2.0 浅谈</title>
    <link href="http://matt33.com/2020/10/25/statefun-introduce/"/>
    <id>http://matt33.com/2020/10/25/statefun-introduce/</id>
    <published>2020-10-25T14:57:05.000Z</published>
    <updated>2020-10-25T15:59:12.835Z</updated>
    
    <content type="html"><![CDATA[<p>Stateful Function（简称 StateFun）从 2019 正式对外宣布之后，今年 4 月份已经发了 2.0 版（并且是作为 Apache Flink 项目中的一部分发布），7 月份也发布了 2.1.0 版。在 2.0 的架构中 Function 已经从 JVM 中解耦出来，只需要通过 HTTP/gRPC 来调用即可，新的架构可以充分利用 FAAS 的能力。本篇文章就来简单看下 StateFun 的架构及应用示例，后面还会有陆续有两篇文章来深入剖析一些其内部实现。</p>
<h2 id="StateFun-的架构"><a href="#StateFun-的架构" class="headerlink" title="StateFun 的架构"></a>StateFun 的架构</h2><p>StateFun 2.0 的架构与 1.0 做了非常大的变化，Function 部分已经完全与 JVM 部分解耦，Function 部分可以单独进行部署，直接部署在 FAAS 上或者直接使用 Kubernetes 启动相应的 HTTP/RPC 服务都是可以的，如下图所示：</p>
<p><img src="/images/statefun/framework-1.png" alt="Flink StateFun 2.0 Architecture"></p>
<p>Flink TaskManagers 从 Ingress 系统（如：kafka、kinesis 等）中接收数据，并且将它们发送给对应的 StateFul Functions 中，经过 Function 计算完后，再发送回 TM，TM 再根据 target address 信息将其发送给其他的 Function 或 Egress 系统（如：kafka、kinesis 等）。</p>
<p>这里先看下 StateFun 框架的几个概念：</p>
<ol>
<li>Ingress：StateFun 的事件输入源，它可以是 queue、logs 或者 HTTP servers，当前 StateFun 内部已经支持的是 Kafka 和 Kinesis，类似于 Flink Streaming Job 中的 Source Operator；</li>
<li>Egress：StateFun 的事件输出源，与 Ingress 类似，在一个 StateFun Application 中，Egress 并不是必需的、是可选的，当前内部支持的是 Kafka 和 Kinesis，类似于 Flink Streaming Job 中 Sink Operator；</li>
<li>Stateful Functions：它就类似与 FAAS 中的 Function，是应用真正做计算的地方，在 StateFun 中流转的每一条 Event，都需要指定其 target address 来表明它需要发向哪个 Function 或 Egress。</li>
</ol>
<p>在这套架构中，Flink Cluster 主要是做 state 一致性保证及 event 路由转发的功能，FAAS 专注于其计算（无需 care 状态存储及一致性的问题）。实际上，在这套系统下，Flink 相当于去掉了传统数据库的角色，因为 Flink 更适合用于 event 驱动的函数和服务，通过集成状态存储，保证了函数或服务间传递消息的有状态性。</p>
<h3 id="Event-driven-Database-vs-Request-Response-Database"><a href="#Event-driven-Database-vs-Request-Response-Database" class="headerlink" title="Event-driven Database vs. Request/Response Database"></a>Event-driven Database vs. Request/Response Database</h3><p>在传统的数据库或者 Key/Value 存储（这里称之为 Request/Response Database）中，应用需主动发送一个查询到数据库（如 SQL via JDBC、GET/PUT via HTTP）。然而，在 StateFun 这类事件驱动数据库中，这个关系被反转了：数据库根据到达的消息来调用函数或服务。这个特性非常适合 FaaS 或者事件驱动架构的应用。</p>
<p><img src="/images/statefun/database.png" alt="Stateful Functions 2.0 inverts the relationship between database and application"></p>
<p>基于请求/响应数据库的应用中，数据库只负责保存状态。函数或服务间的通讯通常一个独立的服务层进行处理。相反，事件驱动数据库以紧密集成的方式既保存了状态的存储，又承担了消息的传输。</p>
<p>另外 StateFun 的架构还有两个优势：</p>
<ol>
<li>借助 Flink 的 Checkpoint 来实现 Exactly once，而如果使用数据库的话这些都需要业务自己来做，业务比较难道整个链路的 Exactly once；</li>
<li>数据库一般都会有从库，在向数据库发送一个读请求时，有可能读取的不是最新的数据，而在 StateFun 中，数据只会存储在一个 StateFun Cluster Partition 中（后续文章会介绍），就不会有这个问题。</li>
</ol>
<h2 id="StateFun-核心组件"><a href="#StateFun-核心组件" class="headerlink" title="StateFun 核心组件"></a>StateFun 核心组件</h2><p>StateFun 2.0 中，一个 StateFun 应用所涉及的核心组件如下图所示：</p>
<p><img src="/images/statefun/arch_components.png" alt="Flink StateFun 2.0 Architecture Components"></p>
<p>在上图中，也可以看到 Flink TaskManagers 中它的主要作用就是接收消息、管理状态、将 event 转发到不同的 Function 以及将数据通过 Egress 发送出去。</p>
<p>在这里，要说明的是，Function 之间并不是直接交流的，数据路由发送都有是由 TM 来操作，TM 将一条 Event 发送给一个 Function，它处理后，会将结果及 target adress 发送回 TM，再由 TM 根据 target address 发送到下游。这些 Function 所使用到的持久化状态都是在 TM 中维护，本身依赖了 Flink 的 StateBackend 及 Checkpoint 机制。</p>
<blockquote>
<p>上图中的 Function Dispatcher 表示的是 Function 的部署方式，图中使用的是 Remote Function。</p>
</blockquote>
<h2 id="StateFun-三种部署方式"><a href="#StateFun-三种部署方式" class="headerlink" title="StateFun 三种部署方式"></a>StateFun 三种部署方式</h2><p>在前面 StateFun 所涉及的核心组件图中，Function Dispatcher 在调用函数时，函数是可以有多种部署选择的。</p>
<h3 id="Remote-Functions"><a href="#Remote-Functions" class="headerlink" title="Remote Functions"></a>Remote Functions</h3><p>2.0 架构中，一个比较大的 Feature 就是支持了 Remote Function，它完全与底层 Flink 集群解耦，通过 HTTP/gRPC 与 Flink TaskManager 进行交互，如下图所示：</p>
<p><img src="/images/statefun/remote-function.png" alt="Remote Functions"></p>
<p>简单来说，Remote Functions 的意思就是函数是独立部署的，从物理上和 Flink Cluster 是分开的。Flink Task Managers 和函数之间的沟通是通过 HTTP/gRPC 请求来完成的。</p>
<h3 id="Co-located-Functions"><a href="#Co-located-Functions" class="headerlink" title="Co-located Functions"></a>Co-located Functions</h3><p>其架构如下图所示：</p>
<p><img src="/images/statefun/co-located-function.png" alt="Co-located Functions"></p>
<p>这种部署方式就是将函数和 TaskManager 的进程部署在一个实例（Pod 或者机器）上，用不同的容器或者进程隔离开来，例如 K8S 中的 sidecar 这种模式。TaskManager 就可以和函数直接在本地通信，但也是去了 FAAS 独立扩缩的能力。</p>
<h3 id="Embedded-Functions"><a href="#Embedded-Functions" class="headerlink" title="Embedded Functions"></a>Embedded Functions</h3><p><img src="/images/statefun/embedded-function.png" alt="Embedded Functions"></p>
<p>这种部署模式更加直接，函数和 TaskManagers 直接在同一个容器内，像 Stateful Functions 1.0 就是这种模式，用高的耦合度换取了高的性能，但损失了灵活性和扩展性，它本质上就完全类似于一个 Flink Streaming Job。</p>
<h2 id="StateFun-示例"><a href="#StateFun-示例" class="headerlink" title="StateFun 示例"></a>StateFun 示例</h2><p>在介绍 StateFun 示例之前，还有两个概念，需要简单看下，那就是 <code>Router</code> 和 <code>Module</code>，它 StateFun API 中比较核心的抽象（针对 Java SDK 而言，Python SDK 抽象得更简单）。</p>
<h3 id="Router"><a href="#Router" class="headerlink" title="Router"></a>Router</h3><p>Router 的含义，这里可以从两个方面来理解：</p>
<ol>
<li>从 StateFun 的角度，它为 Ingress 指定了其要发送的 Function；</li>
<li>从 Flink 的角度，它有两个作用：一是指定下游的 FunctionType（要发送的 Function），二是指定的了其 keyBy shuffle 时使用的 key（StateFun 的状态深度使用了 Flink 中 <code>keyby</code> 操作，这里会再后面的文章详细介绍）。</li>
</ol>
<p>举一个 Java 的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">AddToCartRouter</span> <span class="keyword">implements</span> <span class="title">Router</span>&lt;<span class="title">ProtobufMessages</span>.<span class="title">AddToCart</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">route</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      ProtobufMessages.AddToCart message, Downstream&lt;ProtobufMessages.AddToCart&gt; downstream)</span> </span>&#123;</span><br><span class="line">    downstream.forward(Identifiers.CART, message.getUserId(), message);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// forward 的方法说明</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Forwards the message as an input to a downstream function, addressed by a specified &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * FunctionType&#125; and the functions unique id within its type.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> functionType the target function's type.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> id the target function's unique id.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> message the message being forwarded.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">forward</span><span class="params">(FunctionType functionType, String id, T message)</span> </span>&#123;</span><br><span class="line">  forward(<span class="keyword">new</span> Address(functionType, id), message);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的 <code>Address</code> 就是前面说的 target address，它唯一表示了一个 Function，表示要发送的  Function，由两部分组成：<code>FunctionType</code> 指明了具体的 Function，<code>id</code> 表示在 Flink keyby shuffle 时的 <code>key</code> 值。而如果这里要发送的是 Egress 的话，直接使用 <code>EgressIdentifier</code> 来区分而不需要再设置 <code>id</code>。 </p>
<p>在上面的示例中，这个 Router 就指明了 Ingress 数据要发送的下游 Function 信息。</p>
<h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>在 StateFun 中，Module 是一个用于添加核心模块的一个入口，它把 Ingress、Egress、Routers 及 Stateful Function bind 在一起。一个简单 Java 示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@AutoService</span>(StatefulFunctionModule.class)</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">GreetingModule</span> <span class="keyword">implements</span> <span class="title">StatefulFunctionModule</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KAFKA_KEY = <span class="string">"kafka-address"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_KAFKA_ADDRESS = <span class="string">"kafka-broker:9092"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, String&gt; globalConfiguration, Binder binder)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// pull the configured kafka broker address, or default if none was passed.</span></span><br><span class="line">    String kafkaAddress = globalConfiguration.getOrDefault(KAFKA_KEY, DEFAULT_KAFKA_ADDRESS);</span><br><span class="line">    GreetingIO ioModule = <span class="keyword">new</span> GreetingIO(kafkaAddress);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// bind an ingress to the system along with the router</span></span><br><span class="line">    binder.bindIngress(ioModule.getIngressSpec());</span><br><span class="line">    binder.bindIngressRouter(GreetingIO.GREETING_INGRESS_ID, <span class="keyword">new</span> GreetRouter());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// bind an egress to the system</span></span><br><span class="line">    binder.bindEgress(ioModule.getEgressSpec());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// bind a function provider to a function type</span></span><br><span class="line">    <span class="comment">// note: provider 可以决定这个 function 交互方式，可以使 HTTP 或 GRPC 的形式</span></span><br><span class="line">    binder.bindFunctionProvider(GreetStatefulFunction.TYPE, unused -&gt; <span class="keyword">new</span> GreetStatefulFunction());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于一个 Module 实现，首先需要实现 <code>StatefulFunctionModule</code> 相关的接口，并且用 <code>@AutoService(StatefulFunctionModule.class)</code> 来修饰，这里使用了 Java SPI 的技术（不展开讨论），在 <code>configure()</code> 方法中，将这个 StateFun 应用的需要绑定的组件定义出来，组件的顺序是没有要求的（与 DataStream API 不同），内部在解析时是通过 Target Address 来确定下游的。</p>
<p>在一个 StateFun 应用中可以有多个 Module，用于绑定不同的组件，可以方便团队协同开发（举个例子：一个 Module 绑定一个组件模块，由不同的同学开发不同的组件模块），不过在一个 StateFun 中，只会有一个 Binder，也就是说，多个 module 最终都会被一个 Binder 连接起来。 </p>
<h3 id="Java-SDK-示例"><a href="#Java-SDK-示例" class="headerlink" title="Java SDK 示例"></a>Java SDK 示例</h3><p>在官方仓库中有一个 Java 的示例 —— <a href="https://github.com/apache/flink-statefun/tree/release-2.0/statefun-examples/statefun-greeter-example" target="_blank" rel="external">The Greeter Example</a>，这个示例比较简单，从 kafka 中接收 event 数据（这里可以认为是 user name），在 Function 中会记录每个 event（user）出现的次数，根据出现的次数返回相应的结果，最后将结果写出到一个 Kafka Topic 中，先来看下其 Module 的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@AutoService</span>(StatefulFunctionModule.class)</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">GreetingModule</span> <span class="keyword">implements</span> <span class="title">StatefulFunctionModule</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// kafka 集群的配置</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KAFKA_KEY = <span class="string">"kafka-address"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// kafka 集群的配置</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_KAFKA_ADDRESS = <span class="string">"kafka-broker:9092"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, String&gt; globalConfiguration, Binder binder)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// IO 模块的初始化，这里初始化了 Ingree 和 Egress 部分</span></span><br><span class="line">    <span class="comment">// pull the configured kafka broker address, or default if none was passed.</span></span><br><span class="line">    String kafkaAddress = globalConfiguration.getOrDefault(KAFKA_KEY, DEFAULT_KAFKA_ADDRESS);</span><br><span class="line">    GreetingIO ioModule = <span class="keyword">new</span> GreetingIO(kafkaAddress);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 绑定 Ingress 模块，并设置相应的 Router，为 Ingress 数据源指定下游 Function 信息</span></span><br><span class="line">    <span class="comment">// bind an ingress to the system along with the router</span></span><br><span class="line">    binder.bindIngress(ioModule.getIngressSpec());</span><br><span class="line">    binder.bindIngressRouter(GreetingIO.GREETING_INGRESS_ID, <span class="keyword">new</span> GreetRouter());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 绑定一个 Egress</span></span><br><span class="line">    <span class="comment">// bind an egress to the system</span></span><br><span class="line">    binder.bindEgress(ioModule.getEgressSpec());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 绑定相应的 Function，并指明这个 Function 的交互方式</span></span><br><span class="line">    <span class="comment">// bind a function provider to a function type</span></span><br><span class="line">    binder.bindFunctionProvider(GreetStatefulFunction.TYPE, unused -&gt; <span class="keyword">new</span> GreetStatefulFunction());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StateFun 中比较核心的地方是 State 的使用，下面来看下这个示例中 Function 的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">GreetStatefulFunction</span> <span class="keyword">implements</span> <span class="title">StatefulFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The function type is the unique identifier that identifies this type of function. The type, in</span></span><br><span class="line"><span class="comment">   * conjunction with an identifier, is how routers and other functions can use to reference a</span></span><br><span class="line"><span class="comment">   * particular instance of a greeter function.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * &lt;p&gt;If this was a multi-module application, the function type could be in different package so</span></span><br><span class="line"><span class="comment">   * functions in other modules could message the greeter without a direct dependency on this class.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="comment">// 定义这个 Function 的 FunctionType</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> FunctionType TYPE = <span class="keyword">new</span> FunctionType(<span class="string">"apache"</span>, <span class="string">"greeter"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The persisted value for maintaining state about a particular user. The value returned by this</span></span><br><span class="line"><span class="comment">   * field is always scoped to the current user. seenCount is the number of times the user has been</span></span><br><span class="line"><span class="comment">   * greeted.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="comment">// 声明持久化状态信息</span></span><br><span class="line">  <span class="meta">@Persisted</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> PersistedValue&lt;Integer&gt; seenCount = PersistedValue.of(<span class="string">"seen-count"</span>, Integer.class);</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Context context, Object input)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Function 真正的处理逻辑：得到处理之后的 response 后，再为其指定 target address，这里的 target address 就是 Egress 的信息</span></span><br><span class="line">    GreetRequest greetMessage = (GreetRequest) input;</span><br><span class="line">    GreetResponse response = computePersonalizedGreeting(greetMessage);</span><br><span class="line">    context.send(GreetingIO.GREETING_EGRESS_ID, response);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> GreetResponse <span class="title">computePersonalizedGreeting</span><span class="params">(GreetRequest greetMessage)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String name = greetMessage.getWho();</span><br><span class="line">    <span class="comment">// 获取当前的状态</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> seen = seenCount.getOrDefault(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 更新相应的状态</span></span><br><span class="line">    seenCount.set(seen + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    String greeting = greetText(name, seen);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> GreetResponse.newBuilder().setWho(name).setGreeting(greeting).build();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">greetText</span><span class="params">(String name, <span class="keyword">int</span> seen)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (seen) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> String.format(<span class="string">"Hello %s ! \uD83D\uDE0E"</span>, name);</span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> String.format(<span class="string">"Hello again %s ! \uD83E\uDD17"</span>, name);</span><br><span class="line">      <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> String.format(<span class="string">"Third time is a charm! %s! \uD83E\uDD73"</span>, name);</span><br><span class="line">      <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> String.format(<span class="string">"Happy to see you once again %s ! \uD83D\uDE32"</span>, name);</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">return</span> String.format(<span class="string">"Hello at the %d-th time %s \uD83D\uDE4C"</span>, seen + <span class="number">1</span>, name);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StateFun API 是非常简洁的，在使用 State 时，只需要通过 <code>Persisted</code> 注解修饰即可，否则不会保存到 Flink State 中，也就不会进行容错，在底层的实现上，它通过反射来找到一个 Function 中声明的变量信息，并将其注册到 Flink State 中，如果不通过注解修饰，就无法获取这个 State 变量。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>StateFun 2.0 发布之后，其生产性可用提高很多，它已经可以完全与 JVM 解耦，并且可以很好地利用 FAAS 的扩展能力，但是底层的 state 及数据转发依然受限于 Flink Job 的限制，无法完全做到自动伸缩，在大规模数据量的场景下，其可用性及可靠性有待验证，不过 StateFun 现在还在发展中，未来也不是没有机会。</p>
<hr>
<p>参考</p>
<ol>
<li><a href="https://github.com/apache/flink-statefun" target="_blank" rel="external">Github Flink-StateFun</a>；</li>
<li><a href="https://ci.apache.org/projects/flink/flink-statefun-docs-release-2.0/" target="_blank" rel="external">Stateful Functions Documentation</a>；</li>
<li><a href="https://flink.apache.org/news/2020/06/09/release-statefun-2.1.0.html" target="_blank" rel="external">Stateful Functions 2.1.0 Release Announcement</a>；</li>
<li><a href="https://flink.apache.org/news/2020/04/07/release-statefun-2.0.0.html" target="_blank" rel="external">Stateful Functions 2.0 - An Event-driven Database on Apache Flink</a>；</li>
<li><a href="https://flink.apache.org/news/2020/10/13/stateful-serverless-internals.html" target="_blank" rel="external">Stateful Functions Internals: Behind the scenes of Stateful Serverless</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stateful Function（简称 StateFun）从 2019 正式对外宣布之后，今年 4 月份已经发了 2.0 版（并且是作为 Apache Flink 项目中的一部分发布），7 月份也发布了 2.1.0 版。在 2.0 的架构中 Function 已经从 JV
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Kubenetes 之新手入门篇</title>
    <link href="http://matt33.com/2020/08/02/kubernetes-start/"/>
    <id>http://matt33.com/2020/08/02/kubernetes-start/</id>
    <published>2020-08-02T04:10:33.000Z</published>
    <updated>2020-08-02T10:50:40.101Z</updated>
    
    <content type="html"><![CDATA[<p>近几年来，随着以 Docker 为代表的容器技术的出现，终结了之前 DevOps 中交付和部署环节因环境、配置及程序本身的不同而造成的动辄几种甚至几十种部署配置的困境，将它们统一在容器镜像上。但 Docker 更适用于管理单个容器，一旦开始使用越来越多的容器封装和运行应用程序，必将会导致其管理和编排变得越来越困难。最终，用户不得不对容器实施分组，以便跨所有容器提供网络、安全、监控等服务。于是，以 Kubernetes 为代表的容器编排系统应运而生。本文就是对 Kubernetes 做一个简单的总结，主要从 Kubernetes 架构、组件和核心概念来简单讲述，是一篇关于 Kubernetes 的入门文章。</p>
<h2 id="什么是-Kubernetes"><a href="#什么是-Kubernetes" class="headerlink" title="什么是 Kubernetes"></a>什么是 Kubernetes</h2><p>Kubernetes（因为首尾字母中间有 8 个字符，所以被简写成 K8s），它是一个是用于自动部署、扩展和管理容器化应用程序的工业级容器编排平台，尽管公开面世不过短短数年，但 Kubernetes 已经成为容器编排领域事实上的标准。</p>
<p>Kubernetes（来自希腊语，意为 “舵手” 或 “飞行员”）是由 Joe Beda、Brendan Burns 和 Craig McLuckie 创立，而后 Google 的其他几位工程师，包括 Brian Grant 和 Tim Hockin 等加盟共同研发，并由 Google 在 2014 年首次对外宣布。Kubernetes 的开发和设计都深受 Google 内部系统 Borg 的影响，事实上，它的许多顶级贡献者之前也是 Borg 系统的开发者。</p>
<blockquote>
<p>2015年4月，Borg 论文<a href="https://dl.acm.org/doi/pdf/10.1145/2741948.2741964" target="_blank" rel="external">《Large-scale cluster management at Google with Borg》</a> 首次公开，有兴趣的同学可以看一下。</p>
</blockquote>
<p>Kubernetes 的发展历程如下图所示（图片来自 <a href="https://www.slideshare.net/egg9/kubernetes-introduction" target="_blank" rel="external">Kubernetes Introduction</a>），</p>
<p><img src="/images/k8s/k8s-history.png" alt="Kubernetes History"></p>
<h3 id="Kubernetes-的特性"><a href="#Kubernetes-的特性" class="headerlink" title="Kubernetes 的特性"></a>Kubernetes 的特性</h3><p>Kubernetes 本质上是底层资源与容器间的一个抽象层，如果和单机架构类比，有点类似于分布式时代的 Linux，它旨在提供一个可预测性、可扩展性与高可用性的方法来完全管理容器化应用程序和服务的生命周期的平。简单总结起来，它具有以下几个重要特性：</p>
<ol>
<li>自动装箱（<strong>调度</strong>）：建构于容器之上，基于资源依赖及其他约束自动完成容器部署且不影响其可用性，并通过调度机制混合关键型应用和非关键型应用的工作负载于同一节点以<strong>提升资源利用率</strong>；</li>
<li>自我修复（<strong>自愈</strong>）：支持容器故障后自动重启、节点故障后重新调度容器，以及其他可用节点、健康状态检查失败后关闭容器并重新创建等自我修复机制；</li>
<li>水平扩展（<strong>可扩展</strong>）：支持通过简单命令或 UI 手动水平扩展，以及基于 CPU 等资源负载率的自动水平扩展机制；</li>
<li><strong>服务发现和负载均衡</strong>：Kubernetes 通过其附加组件之一的 KubeDNS（或 CoreDNS）为系统内置了服务发现功能，它会为每个 Service 配置 DNS 名称，并允许集群内的客户端直接使用此名称发出访问请求，而 Service 则通过 <code>iptables</code> 或 <code>ipvs</code> 内建了负载均衡机制；</li>
<li><strong>自动发布和回滚</strong>：Kubernetes 支持 <strong>灰度</strong> 更新应用程序或其配置信息，它会监控更新过程中应用程序的健康状态，以确保它不会在同一时刻杀掉所有实例，而此过程中一旦有故障发生，就会立即自动执行回滚操作；</li>
<li><strong>密钥和配置管理</strong>：Kubernetes 的 <strong>ConfigMap</strong> 实现了配置数据与 Docker 镜像解耦，当需要时，仅对配置做出变更而无须重新构建 Docker 镜像，这为应用开发部署带来了很大的灵活性。此外，对于应用所依赖的一些敏感数据，如用户名和密码、令牌、密钥等信息，Kubernetes 专门提供了 Secret 对象为其解耦，既便利了应用的快速开发和交付，又提供了一定程度上安全保障；</li>
<li><strong>存储编排</strong>：Kubernetes 支持 Pod 对象按需自动挂载不同类型的存储系统，这包括节点本地存储、公有云服务商的云存储（如 AWS 和 GCP 等），以及网络存储系统（例如，NFS、iSCSI、GlusterFS、Ceph、Cinder 和 Flocker 等）；</li>
<li>批量处理执行：除了服务型应用，Kubernetes 还支持批处理作业及 CI（持续集成），如果需要，一样可以实现容器故障后恢复。</li>
</ol>
<h2 id="Kubernetes-架构"><a href="#Kubernetes-架构" class="headerlink" title="Kubernetes 架构"></a>Kubernetes 架构</h2><p>这里我们先来看下 Kubernetes 的架构图，如下图所示（图片来自 <a href="https://www.slideshare.net/egg9/kubernetes-introduction" target="_blank" rel="external">Kubernetes Introduction</a>）：</p>
<p><img src="/images/k8s/k8s-framework.png" alt="Kubernetes 架构图"></p>
<p>可以看出，Kubernetes 架构是一个比较典型的二层架构和 server-client 架构：</p>
<ol>
<li><strong>Master</strong>: 作为中央的管控节点，会去与 Node 进行一个连接。所有 UI 的、clients、这些 user 侧的组件，只会和 Master 进行连接，把希望的状态或者想执行的命令下发给 Master，Master 会把这些命令或者状态下发给相应的节点，进行最终的执行；</li>
<li><strong>Node</strong>: Node 的职责是运行容器应用，Node 由 Master 管理，Node 负责监控并汇报容器的状态，同时根据 Master 的请求管理容器的生命周期。</li>
</ol>
<p>下面分别来看下 Master 和 Node 组件内部的一些核心服务。</p>
<h3 id="Kubernetes-组件-–-Master"><a href="#Kubernetes-组件-–-Master" class="headerlink" title="Kubernetes 组件 – Master"></a>Kubernetes 组件 – Master</h3><p>Master 节点主要由 API Server、Controller Manager 和 Scheduler 三个组件，以及一个用于集群状态存储的 etcd 存储服务组成:</p>
<ol>
<li><strong>API Server</strong>: 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；</li>
<li><strong>Controller Manager</strong>: 负责维护集群的状态，比如：故障检测、自动扩展、滚动更新等；</li>
<li><strong>Scheduler</strong>: 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上（K8s 还支持用户自定义调度器）；</li>
<li><strong>etcd</strong>: 集群的所有状态信息都需要持久存储于存储系统 etcd 中，不过，etcd 是由 CoreOS 基于 Raft 协议开发的分布式键值存储，可用于服务发现、共享配置以及一致性保障（如数据库主节点选择、分布式锁等）。</li>
</ol>
<h3 id="Kubernetes-组件-–-Node"><a href="#Kubernetes-组件-–-Node" class="headerlink" title="Kubernetes 组件 – Node"></a>Kubernetes 组件 – Node</h3><p>Node 负责提供运行容器的各种依赖环境，并接受 Master 的管理。每个 Node 主要由以下几个组件构成： </p>
<ol>
<li><strong>kubelet</strong>: 它是运行于工作节点之上的守护进程，负责容器的生命周期，也负责 Volume 和 网络的管理，它从 API Server 接收关于 Pod 对象的配置信息并确保它们处于期望的状态，kubelet 会在 API Server 上注册当前工作节点，定期向 Master 汇报节点资源使用情况，并通过 <code>cAdvisor</code> 监控容器和节点的资源占用状况；</li>
<li><strong>Container Runtime</strong>: 负责镜像下载、管理以及 Pod 和容器的真正运行；</li>
<li><strong>kube-proxy</strong>: 负责为 Service 提供 Cluster 内部的服务发现和负载均衡。</li>
</ol>
<h3 id="Kubernetes-核心附件"><a href="#Kubernetes-核心附件" class="headerlink" title="Kubernetes 核心附件"></a>Kubernetes 核心附件</h3><p>Kubernetes 集群还依赖于一组称为 ”附件”（add-ons）的组件以提供完整的功能，它们通常是由第三方提供的特定应用程序，且托管运行于 Kubernetes 集群之上，如下图所示（图片来自 <a href="https://item.jd.com/12477105.html" target="_blank" rel="external">《Kubernetes 进阶实战》</a>）：</p>
<p><img src="/images/k8s/k8s-content.png" alt="Kubernetes 组件图"></p>
<p>下面列出的几个附件各自为集群从不同角度引用了所需的核心功能：</p>
<ol>
<li><strong>KubeDNS</strong>: 在集群中调度运行提供 DNS 服务的 Pod，同一集群中的其他 Pod 可使用此 DNS 服务解决主机名；</li>
<li><strong>Kubernetes Dashboard</strong>: 集群的全部功能都要基于 Web 的 UI 来管理集群中的应用设置是集群自身；</li>
<li><strong>Heapster</strong>: 容器和节点的性能监控与分析系统，它收集并解析多种指标数据，如资源利用率、生命周期事件等；</li>
<li><strong>Ingress Controller</strong>: Service 是一种工作与传统层的负载均衡器，而 Ingress 是在应用层实现的 HTTP(s) 负载均衡机制，Ingress 只是一组路由规则的集合，这些规则需要通过 Ingress Controller 发挥作用。</li>
</ol>
<h2 id="Kubernetes-的核心概念"><a href="#Kubernetes-的核心概念" class="headerlink" title="Kubernetes 的核心概念"></a>Kubernetes 的核心概念</h2><p>前面已经了解了 Kubernetes 的架构及组件信息，这里我们来总结一下 Kubernetes 生态下一些核心概念，只有了解并理解这些概念，才能更好地使用 Kubernetes。</p>
<h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>Kubernetes 并不直接运行容器，而是使用一个抽象的资源对象来封装一个或者多个容器，这个抽象即为 Pod，它也是 Kubernetes 的最小调度单元（可以参考 <a href="https://feisky.gitbooks.io/kubernetes/content/concepts/pod.html" target="_blank" rel="external">Kubernetes 指南之 POD</a>）。用户可以通过 Kubernetes 的 Pod API 生产一个 Pod，让 Kubernetes 对这个 Pod 进行调度，也就是把它放在某一个 Kubernetes 管理的节点上运行起来。一个 Pod 简单来说是对一组容器的抽象，它里面会包含一个或多个容器。</p>
<p><img src="/images/k8s/k8s-pod.png" alt="Kubernetes 之 POD"></p>
<p>特点：</p>
<ol>
<li>Pod 资源对象是一种集合了一到多个应用容器、存储资源、专用 IP 及支撑容器运行的其他选项的逻辑组件；</li>
<li>所有 Pod 内部的容器可以访问共享的 Volume 和共享数据；</li>
</ol>
<h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>我们知道容器的数据都是非持久化的，在容器消亡以后数据也跟着丢失，所以 Docker 提供了 Volume 机制以便将数据持久化存储。Volume 本身就是卷的概念，它是用来管理 Kubernetes 存储的，是用来声明在 Pod 中的容器可以访问的文件目录的，一个卷可以被挂载在 Pod 中一个或者多个容器的指定路径下面。</p>
<p>而 Volume 本身是一个抽象的概念，一个 Volume 可以去支持多种的后端的存储。比如说 Kubernetes 的 Volume 就支持了很多存储插件，它可以支持本地的存储，可以支持分布式的存储，比如说像 ceph，GlusterFS ；它也可以支持云存储，比如说阿里云上的云盘、AWS 上的云盘、Google 上的云盘等等（在资源描述文件的配置方式参考 <a href="https://feisky.gitbooks.io/kubernetes/content/concepts/volume.html" target="_blank" rel="external">Kubernetes 指南之 Volume</a>）。</p>
<h3 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h3><p>ReplicaSet（也简称为 RS，K8s 之前的版本这个功能叫做 Replication Controller）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而异常多出来的容器也会自动回收（这些都是由 Master 端的 Controller Manager 来做的）。ReplicaSet 的典型应用场景包括确保健康 Pod 的数量、弹性伸缩、滚动升级以及应用多版本发布跟踪等。</p>
<p>资源配置文件的使用示例，参考 <a href="https://feisky.gitbooks.io/kubernetes/content/concepts/replicaset.html#replicaset-%E7%A4%BA%E4%BE%8B" target="_blank" rel="external">Kubernetes 指南之 ReplicaSet 示例</a>。</p>
<h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p>Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义 (declarative) 方法，用来替代以前的 ReplicationController 或 ReplicaSet 来更方便的管理应用。</p>
<p>Deployment 是在 Pod 这个抽象上更为上层的一个抽象，它可以定义一组 Pod 的副本数目、以及这个 Pod 的版本，一般大家用 Deployment 这个抽象来做应用的真正的管理，而 Pod 是组成 Deployment 最小的单元。</p>
<p>比如说我可以定义一个 Deployment，这个 Deployment 里面需要两个 Pod，当一个 Pod 失败的时候，控制器就会监测到，它重新把 Deployment 中的 Pod 数目从一个恢复到两个，通过再去新生成一个 Pod。通过控制器，我们也会帮助完成发布的策略。比如说进行滚动升级，进行重新生成的升级，或者进行版本的回滚。</p>
<p>Deployment 的资源配置声明及相关的操作命令参考：<a href="https://feisky.gitbooks.io/kubernetes/content/concepts/deployment.html" target="_blank" rel="external">Kubernetes 指南之 Deployment</a>。</p>
<p>一个简单的、3 副本的 nginx 应用的资源配置文件可以定义为：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">nginx:1.7.9</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>
<h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>Service 是对一组提供相同功能的 Pods 的抽象，并为它们提供一个统一的入口，借助 Service，应用可以方便的<strong>实现服务发现与负载均衡，并实现应用的零宕机升级</strong>，Service 通过标签来选取服务后端，一般配合 ReplicaSet（简称 RS）或者 Deployment 来保证后端容器的正常运行。这些匹配标签的 Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上，如下图所示（图片来自 <a href="https://coreos.com/kubernetes/docs/latest/services.html" target="_blank" rel="external">Overview of a Service</a>）：</p>
<p><img src="/images/k8s/k8s-service.png" alt="Kubernetes Service"></p>
<p>关于 Service，个人的理解是，它只是一种抽象，通过 label（资源标签）绑定到对应的 RC 和 Deployment 上，它是不会创建 Pod 的，Pod 还是由 RS 或 Deployment 创建的。下面是一个示例，这个 Service 将服务的 80 端口转发到 default namespace 中带有标签 <code>run=nginx</code> 的 Pod 的 80 端口上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    run:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    run:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  sessionAffinity:</span> <span class="string">None</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">ClusterIP</span></span><br></pre></td></tr></table></figure>
<h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>Namespace 是对一组资源和对象的抽象集合（<a href="https://feisky.gitbooks.io/kubernetes/content/concepts/namespace.html" target="_blank" rel="external">Kubernetes 指南之 Namespace</a>），比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 Pod, service, Replication Controller 和 Deployment 等都是属于某一个 namespace 的（默认是 default），而 node, persistent volume，namespace 等资源则不属于任何 namespace。</p>
<p>Namespace 常用来<strong>隔离不同的用户</strong>，比如 Kubernetes 自带的服务一般运行在 kube-system namespace 中。</p>
<p>常用的命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询 K8s 的 namespace 信息</span></span><br><span class="line">kubectl get namespaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 命令行直接创建 namespace</span></span><br><span class="line">kubectl create namespace new-namespace</span><br><span class="line"><span class="comment"># 通过资源描述文件来创建 namespace</span></span><br><span class="line">kubectl create -f ./my-namespace.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 namespace</span></span><br><span class="line">kubectl delete namespaces new-namespace</span><br></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>其他还是有一些比较重要的资源对象，只不过这些没有上面这些常用，大家可以参考 <a href="https://feisky.gitbooks.io/kubernetes/content/concepts/objects.html" target="_blank" rel="external">Kubernetes 指南之资源对象</a>。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这篇文章主要是对 Kubernetes 的架构、组件及核心的概念做了一下梳理，并没有涉及特别深入的内容，正如文章标题所述，算是一篇入门的文章介绍，以后如果有机会、有时间个人计划是好好研究一下 Kubernetes，更新一些稍微深入的内容。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://kubernetes.io" target="_blank" rel="external">Kubernetes 官方文档</a>；</li>
<li><a href="https://item.jd.com/12477105.html" target="_blank" rel="external">Kubernetes 进阶实战</a>；</li>
<li><a href="https://legacy.gitbook.com/book/feisky/kubernetes" target="_blank" rel="external">Kubernetes 指南</a>；</li>
<li><a href="https://developer.aliyun.com/article/718798" target="_blank" rel="external">从零开始入门 K8s| 阿里技术专家详解 K8s 核心概念</a>；</li>
<li><a href="https://timzhouyes.github.io/2019/11/18/K8s-tutorial/" target="_blank" rel="external">Kubernetes入门简明教程</a>；</li>
<li><a href="https://draveness.me/system-design-scheduler/" target="_blank" rel="external">https://draveness.me/system-design-scheduler/</a>；</li>
<li><a href="https://juejin.im/post/6844903666789384200" target="_blank" rel="external">从0到1使用Kubernetes系列——Kubernetes入门</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近几年来，随着以 Docker 为代表的容器技术的出现，终结了之前 DevOps 中交付和部署环节因环境、配置及程序本身的不同而造成的动辄几种甚至几十种部署配置的困境，将它们统一在容器镜像上。但 Docker 更适用于管理单个容器，一旦开始使用越来越多的容器封装和运行应用程
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kubernetes" scheme="http://matt33.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>浅谈 CPU 分支预测技术</title>
    <link href="http://matt33.com/2020/04/16/cpu-branch-predictor/"/>
    <id>http://matt33.com/2020/04/16/cpu-branch-predictor/</id>
    <published>2020-04-15T16:33:56.000Z</published>
    <updated>2020-06-23T14:13:17.227Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看 SQL 优化之 Code Generation 相关的内容，我们知道 Code Generation 是 SQL 优化的大杀器之一，不管是在 Apache Spark 还是 Apache Flink 中都有比较深入的应用（特别是在 Spark 中），Code Generation 最开始是在数据库中应用的，Spark 将其引入到 Spark SQL 的中优化，后来的 Flink 也借鉴了这一思想。Code Generation 是要解决什么问题呢？相信大部分人应该有所了解，简单来说就是减少虚函数调用、尽可能利用 CPU 分支预测的能力（会在 Code Generation 部分详细介绍，这里只需要了解这一背景即可），那么什么是 CPU 分支预测（<a href="https://en.wikipedia.org/wiki/Branch_predictor" target="_blank" rel="external">Wikipedia: CPU Branch Predictor</a>）呢？为什么虚函数调用会极大消耗 CPU 性能呢？这就是本文将要给大家介绍的内容。</p>
<h2 id="CPU-Instruction-pipelining"><a href="#CPU-Instruction-pipelining" class="headerlink" title="CPU Instruction pipelining"></a>CPU Instruction pipelining</h2><p>在介绍 CPU 分支预测机制之前，先来看下 CPU 的流水线机制（<a href="https://en.wikipedia.org/wiki/Instruction_pipelining" target="_blank" rel="external">Wikipedia: CPU Instruction pipelining</a>）。</p>
<p>关于流水线（pipeline），这里举一个生活中的例子，比如在洗车时，当前面一辆车清洗完成进入擦洗阶段后，下一辆车就可以进入喷水阶段了，这就是一个典型的流水线场景（如下如所示），它不是说非要前面一辆车把清洗、擦洗全部完成后，下一辆车才能开始。</p>
<p><img src="/images/system/cpu-pipeline2.png" alt="洗车中流水线"></p>
<p>从这里也可以看出，流水线机制一个重要的特性就是 <strong>提高了系统的吞吐量</strong>，也就是单位时间内服务的总数，<strong>不过它会有一个轻微的延迟</strong>，对于上面的例子就是，一辆汽车在洗完之后需要开到擦洗的地方擦洗。在 CPU 的设计中，也有类似流水线化的机制，这个汽车就是指令，每个阶段完成执行执行的一部分。</p>
<h3 id="CPU-中计算的流水线化"><a href="#CPU-中计算的流水线化" class="headerlink" title="CPU 中计算的流水线化"></a>CPU 中计算的流水线化</h3><p>下面举一个示例，这里将系统执行分为三个阶段（A、B 和 C），如下图所示，每个阶段需要 100ps（picosecond，皮秒，也就是微微秒，即 $10^{-12}$），中间加载寄存器（也可以叫做流水线寄存器，pipeline register）需要 20ps。对于图 b，时间从左往右流动，对于指令 I1，三个方框分别代表三个阶段（图片来自 <a href="https://book.douban.com/subject/26912767/" target="_blank" rel="external">《深入理解计算机系统 第三版》</a> 中插图）。</p>
<p><img src="/images/system/cpu-pipeline3.png" alt="三阶段流水线化的计算硬件"></p>
<p>这样，每条指令都会按照三步经过这个系统，从头到尾需要三个完整的时钟周期，如上图所示，只要 <code>I1</code> 从 A 进入 B，就可以让 <code>I2</code> 进入 A 阶段了，以此类推。在稳定状态下，三个阶段都应该是活动的，每个时钟周期，一条指令离开系统，一条新的指令进入。在这个系统中，时钟周期设为 <code>100+20=120ps</code>，得到的吞吐量大约为 <code>8.33GIPS</code>，但是因为处理一条指令需要 3 个时钟周期，所以这条流水线的延迟就是 <code>3*120=360ps</code>，它相当于 一阶段 的系统，吞吐量提高了 2.67 倍，代价是增加了一些硬件以及延迟的增加（寄存器变多带来的延迟）。</p>
<h3 id="流水线的局限"><a href="#流水线的局限" class="headerlink" title="流水线的局限"></a>流水线的局限</h3><p>在上面的三阶段系统中，它是一个比较理想的情况，在这个系统中，我们可以将计算分成三个独立的阶段，每个阶段需要的时间是原来逻辑需要时间的三分之一，但是在实际生产中，会出现一些其他的因素，降低流水线的效率。</p>
<h4 id="情况-1：阶段不一致的划分"><a href="#情况-1：阶段不一致的划分" class="headerlink" title="情况 1：阶段不一致的划分"></a>情况 1：阶段不一致的划分</h4><p>在前面的例子划分的阶段中，每个阶段执行都是 100ps，但是实际中并不一定是这样的，假如 A 阶段是 50ps，B 阶段是 150ps，C 阶段是 100ps，在这种情况下，系统必须将时钟周期设置为 170ps（由最慢的来决定），这样的话，其吞吐量就变成了 <code>5.88GIPS</code>，由于时钟减慢，也导致了延迟增加到了 510ps。</p>
<p>因此，在 CPU 硬件设计时，将系统计算设计分为一组具有相同延迟的阶段将是一个严峻的挑战。</p>
<h4 id="情况-2：流水线过深，收益反而下降"><a href="#情况-2：流水线过深，收益反而下降" class="headerlink" title="情况 2：流水线过深，收益反而下降"></a>情况 2：流水线过深，收益反而下降</h4><p>如果流水线过深，中间使用到的寄存器将会变多，寄存器使用带来的延迟在指令运行总延迟中的比重将会增大。一方面，在设计时，为了提高时钟频率，现代处理器会采用很深的流水线，另一方面，由于流水线过深，指令运行延迟会变长。所以，在实际设计时，电路设计师如何设计流水线寄存器，使其延迟尽可能减少，是高速微处理器面临重大挑战之一。</p>
<h2 id="CPU-Branch-Predictor"><a href="#CPU-Branch-Predictor" class="headerlink" title="CPU Branch Predictor"></a>CPU Branch Predictor</h2><p>在开始介绍 CPU 分支预测技术之前，可以先看下 StackOverflow 上一个非常有名的问题（现在有 3w+ 人认同第一个回答） —— <a href="https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array#11227902" target="_blank" rel="external">Why is processing a sorted array faster than processing an unsorted array?</a>，问题的大概是，对一个数组中的每个元素，先做判断，如果大于某个值，就做累加，就是这样一个简单的操作，发现一个有意思的现象，如果用 C++ 写这段代码，对于有序数组和无序数组分别做这个操作，性能大概相差五倍多，在 Java 中，差距小一点，大概是 1 倍。为什么会出现这个问题呢？</p>
<p>背后的原因就是 <strong>CPU 流水线下，CPU 采用分支预测技术</strong>，对于有序数组可以很好地 CPU 这一特性，而无序数组会使得分支预测手足无措。</p>
<h3 id="什么是分支预测"><a href="#什么是分支预测" class="headerlink" title="什么是分支预测"></a>什么是分支预测</h3><p>在前面，我们了解到 CPU 为了提高吞吐量采用了流水线机制，比如下图中的 4 级流水线（图片来自 <a href="https://en.wikipedia.org/wiki/Instruction_pipelining" target="_blank" rel="external">Wikipedia: CPU Instruction pipelining</a>）：</p>
<p><img src="/images/system/cpu-pipeline1.png" alt="CPU 4 级流水线"></p>
<p>上图中的 CPU pipeline 有四个执行阶段:</p>
<ol>
<li>读取指令(Fetch)；</li>
<li>指令解码(Decode)；</li>
<li>运行指令(Execute)；</li>
<li>回写(Write-back)。</li>
</ol>
<p>假设有三条指令，在上面这个四级流水线构架下（每个阶段都会花费一个时钟周期），pipeline 执行流程如下图所示：</p>
<p><img src="/images/system/cpu-pipeline4.png" alt="4 级流水线示例"></p>
<p>我们知道：如果没有流水线机制，一条指令大概会花费 4 个时钟周期，而如果采用流水线机制，当第一条指令完成<code>Fetch</code>后，第二条指令就可以进行<code>Fetch</code>了，极大提高了指令的执行效率。</p>
<p>上面是我们的期待的理想情况，而在现实环境中，如果遇到的指令是 <strong>条件跳转指令</strong>，只要当前面的指令运行到执行阶段，才能知道要选择的分支，显然这种 <strong>停顿</strong> 对于 CPU 的 pipeline 机制是非常不友好的。而 <strong>分支预测技术</strong> 正是为了解决上述问题而诞生的，CPU 会根据分支预测的结果，选择下一条指令进入流水线。待跳转指令执行完成，如果预测正确，则流水线继续执行，不会受到跳转指令的影响。如果分支预测失败，那么便需要清空流水线，重新加载正确的分支（实际上目前市面上所有处理器都采用了类似的技术）。</p>
<h3 id="分支预测技术"><a href="#分支预测技术" class="headerlink" title="分支预测技术"></a>分支预测技术</h3><p>这里看下常见的分支预测技术，主要有：静态分支预测、动态分支预测 和 协同分支预测 三种，有兴趣的可以看下下面的几篇文章：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/22469702" target="_blank" rel="external">深入理解CPU的分支预测(Branch Prediction)模型</a>；</li>
<li><a href="http://djs66256.github.io/2019/01/29/2019-01-29-CPU%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B/" target="_blank" rel="external">CPU分支预测</a>；</li>
</ol>
<p>关于这三种技术，这里就不再展开了，简单总结一下。</p>
<ol>
<li>静态分支预测：实现起来很简单、成本低，而且在生产中，这种预测正确率的波动范围很大；</li>
<li>动态分支预测：根据指令的不同及历史信息（存储在一张分支历史表中 —— Branch History Table）作出相应的预测，常见的有 1-bit/n-bit 动态预测；</li>
<li>协同分支预测：利用代码中分支跳转指令之间的关联关系，提高分支预测的准确率。</li>
</ol>
<h2 id="Java-中的虚函数调用"><a href="#Java-中的虚函数调用" class="headerlink" title="Java 中的虚函数调用"></a>Java 中的虚函数调用</h2><p>Java 本身没有虚函数的概念，它在 C++ 中是最常见的。在 C++ 中，虚函数通过 <code>virtual</code> 关键字定义，实现在类的继承当中，编译器通过判断对象的类型，在调用函数时，执行对应的函数。Java 中并没有显式去定义虚函数的概念，Java 中实际上每个函数都默认是一个虚函数（声明 <code>final</code> 关键字的函数除外），比如下面示例中 <code>eat()</code> 方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span> </span>&#123; System.out.println(<span class="string">"I eat like a generic Animal."</span>); </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Dog</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span> </span>&#123; System.out.println(<span class="string">"I eat like a dog!"</span>); &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cat</span> <span class="keyword">extends</span> <span class="title">Animal</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eat</span><span class="params">()</span> </span>&#123; System.out.println(<span class="string">"I eat like a cat!"</span>); &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">      List&lt;Animal&gt; animals = <span class="keyword">new</span> LinkedList&lt;Animal&gt;();</span><br><span class="line"></span><br><span class="line">      animals.add(<span class="keyword">new</span> Animal());</span><br><span class="line">      animals.add(<span class="keyword">new</span> Dog());</span><br><span class="line">      animals.add(<span class="keyword">new</span> Cat());</span><br><span class="line">      <span class="keyword">for</span> (Animal currentAnimal : animals) &#123;</span><br><span class="line">         currentAnimal.eat();</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>虚函数存在的意义就是为了实现多态，Java 通过 <strong>动态绑定</strong>，不仅实现了虚函数的功能，也使得代码逻辑更为简洁。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>到这里，相信大家已经对 CPU 的流水线机制及 CPU 的分支预测技术有了一定的了解。回到 code<br>上，如果代码里充满着各种不可预知的条件跳转指令，将会极大影响 CPU 的执行效率，数据库中采用的 Volcano-style execution engine（火山执行引擎）在代码中充满着各种虚函数调用（详细机制在后面内容中再介绍），在编译器中，虚函数需要调用查找虚函数表，并且虚函数调用是一个非直接跳转逻辑，在这个逻辑中，最大的代价是可能导致错误的 CPU 分支预测，一次错误的分支预测会导致需要 10 几个周期的系统开销。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Branch_predictor" target="_blank" rel="external">Wikipedia: CPU Branch predictor</a>；</li>
<li><a href="https://en.wikipedia.org/wiki/Instruction_pipelining" target="_blank" rel="external">Wikipedia: CPU Instruction pipelining</a>；</li>
<li><a href="https://www.computerhope.com/jargon/b/branch-prediction.htm" target="_blank" rel="external">CPU Branch prediction</a>；</li>
<li><a href="https://danluu.com/branch-prediction/" target="_blank" rel="external">Branch prediction</a>；</li>
<li><a href="https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array#11227902" target="_blank" rel="external">Why is processing a sorted array faster than processing an unsorted array?</a>；</li>
<li><a href="https://book.douban.com/subject/26912767/" target="_blank" rel="external">《深入理解计算机系统 第三版》</a>；</li>
<li><a href="http://hengyunabc.github.io/optimization-tip-if-vs-switch/" target="_blank" rel="external">优化技巧：提前if判断帮助CPU分支预测</a>；</li>
<li><a href="https://ericfu.me/code-gen-of-expression/" target="_blank" rel="external">JIT 代码生成技术（一）表达式编译</a>；</li>
<li><a href="https://ericfu.me/code-gen-of-query/" target="_blank" rel="external">JIT 代码生成技术（二）查询编译执行</a>；</li>
<li><a href="http://irootlee.com/juicer_vtable/" target="_blank" rel="external">C++性能榨汁机之虚函数的开销</a>；</li>
<li><a href="https://www.techbelife.com/post/Talk-about-Java-virtual-functions.html#toc_2" target="_blank" rel="external">浅谈Java中的”虚函数”</a>；</li>
<li><a href="https://juejin.im/post/5c983bd46fb9a0710a1bd3e1" target="_blank" rel="external">Java 和操作系统交互细节</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看 SQL 优化之 Code Generation 相关的内容，我们知道 Code Generation 是 SQL 优化的大杀器之一，不管是在 Apache Spark 还是 Apache Flink 中都有比较深入的应用（特别是在 Spark 中），Code Ge
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="system" scheme="http://matt33.com/tags/system/"/>
    
  </entry>
  
  <entry>
    <title>Flink 基于 MailBox 实现的 StreamTask 线程模型</title>
    <link href="http://matt33.com/2020/03/20/flink-task-mailbox/"/>
    <id>http://matt33.com/2020/03/20/flink-task-mailbox/</id>
    <published>2020-03-20T15:46:37.000Z</published>
    <updated>2020-06-23T14:13:17.226Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第八篇，在介绍 TaskManager 第二部分之前，先来给介绍一下目前 StreamTask 中基于 MailBox 实现的线程模型，这个模型从 1.9 开始实现，在目前发布的 1.10 版本中，基本上已经改造完成，具体 issue 见 <a href="https://issues.apache.org/jira/browse/FLINK-12477" target="_blank" rel="external">FLINK-12477: Change threading-model in StreamTask to a mailbox-based approach</a>，其设计文档见 <a href="https://docs.google.com/document/d/1eDpsUKv2FqwZiS1Pm6gYO5eFHScBHfULKmH1-ZEWB4g/edit" target="_blank" rel="external">Change threading-model in StreamTask to a mailbox-based approach</a>，去年，vinoyang 也写了一篇关于它的介绍，见 <a href="https://mp.weixin.qq.com/s/MrQZS7-dEuNr442lzw3xYg" target="_blank" rel="external">重磅！Flink 将重构其核心线程模型</a>。因为 Flink 1.10 已经发布，本篇关于 MailBox 实现的介绍会基于 1.10 最新的代码来讲述（系列的其他篇，没有说明的话，默认还是以 1.9 的代码为例），这个功能在 1.9 中还并没有完全完成，所以本文以 1.10 代码为例讲述。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>先来看下这个改造/改进最初的动机，在之前 Flink 的线程模型中，会有多个潜在的线程去并发访问其内部的状态，比如 event-processing 和 checkpoint triggering，它们都是通过一个全局锁（checkpoint lock）来保证线程安全，这种实现方案带来的问题是：</p>
<ol>
<li>锁对象会在多个类中传递，代码的可读性比较差；</li>
<li>而且锁对象还暴露给了面向用户的 API（见 <code>SourceFunction#getCheckpointLock()</code>）；</li>
<li>在使用时，如果没有获取锁，可能会造成很多问题，使得问题难以定位；</li>
</ol>
<p>基于上面的这些问题，关于线程模型，提出了一个全新的解决方案 —— <strong>MailBox 模型</strong>，它可以让 StreamTask 中所有状态的改变都会像在单线程中实现得一样简单。方案借鉴了 Actor 模型的 MailbBox 设计理念，它会让这些 action 操作（需要获取 checkpoint lock 的操作）先加入到一个 <strong>阻塞队列</strong>，然后主线程再从队列取相应的 <strong>mail task</strong> 去执行。</p>
<h2 id="设计方案"><a href="#设计方案" class="headerlink" title="设计方案"></a>设计方案</h2><p>这里先看下，之前的实现方案中，StreamTask 中 checkpoint lock 都主要用在什么地方：</p>
<ol>
<li><code>Event-processing</code>: events、watermarks、barriers、latency markers 等的发送和处理；</li>
<li><code>Checkpoints</code>: 通过 RPC 向 TaskExecutor 发送 Checkpoint trigger 和 completeness 的通知，以及 Checkpoint 的 trigger 和 cancel 在 event 处理期间也可以通过 barrier 接收到；</li>
<li><code>Processing Time Timers</code>: 目前 <code>SystemProcessingTimeService</code> 是使用 <code>ScheduledExecutor</code> 异步地处理 processing time timer（而 event time timer 依赖于 Watermark 的处理，并且它同步触发的）。</li>
</ol>
<p>另外，设计方案不但要能达到排它锁的效果，还要对一些核心环节（比如：event processing）能够做到原子性处理。</p>
<p>下面来看下 <strong>MailBox 模型</strong> 最初设计文档中的设计（方案方案见：<a href="https://docs.google.com/document/d/1eDpsUKv2FqwZiS1Pm6gYO5eFHScBHfULKmH1-ZEWB4g/edit" target="_blank" rel="external">Change threading-model in StreamTask to a mailbox-based approach</a>）。</p>
<h3 id="StreamTask-中要做的改变"><a href="#StreamTask-中要做的改变" class="headerlink" title="StreamTask 中要做的改变"></a>StreamTask 中要做的改变</h3><p>这里会在 StreamTask 中引入一个 <strong>MailBox</strong> 变量，最初的一个想法是将 <strong>MailBox</strong> 设计为一个 <code>ArrayBlockingQueue</code>（实际上在 1.9 的实现中，使用的是一个 <code>ring buffer</code>，1.10 对这部分又做了重构，后面会介绍）。<strong>MailBox</strong> 将会取代 <code>StreamTask#run()</code> 方法的角色，而且它还可以处理 Checkpoint event 和 processing timer event，这些 event 都会被封装为一个 task 添加到 MailBox 的队列中，而 MailBox 的主线程（单线程）将会消费这个队列中的 task 进行顺序处理。StreamTask 实现的伪代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">BlockingQueue&lt;Runnable&gt; mailbox = ...</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">runMailboxProcessing</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">//<span class="doctag">TODO:</span> can become a cancel-event through mailbox eventually</span></span><br><span class="line">    Runnable letter;</span><br><span class="line">    <span class="keyword">while</span> (isRunning()) &#123;</span><br><span class="line">        <span class="keyword">while</span> ((letter = mailbox.poll()) != <span class="keyword">null</span>) &#123; letter.run();</span><br><span class="line">            letter.run();</span><br><span class="line">        &#125;</span><br><span class="line">        defaultAction();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">defaultAction</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// e.g. event-processing from an input</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码实现只是核心代码大概实现，在真正的实现中还可以做很多优化，队列的公平性也是我们考虑的一个点，之前的抢锁操作是完全没有任何公平性而言的。</p>
<h3 id="client-代码需要做的改变"><a href="#client-代码需要做的改变" class="headerlink" title="client 代码需要做的改变"></a>client 代码需要做的改变</h3><p>之前的实现中，Checkpoint lock 通过 <code>getter</code> 暴露给相关的 actor（Checkpoint、processing timer、event processing），而在 MailBox 的实现中，将会把 mailbox 隐藏在 queue 接口后面，仅仅向上层暴露 queue 的 <code>getter</code> 接口。</p>
<h3 id="event-的产生与处理"><a href="#event-的产生与处理" class="headerlink" title="event 的产生与处理"></a>event 的产生与处理</h3><p><strong>MailBox</strong> 的实现将会极大简化代码的实现，<strong>MailBox</strong> 模型可以确保这些改变都是由单线程来操作，之前很多需要加锁的代码在新的实现中可以被移除。而为了实现<strong>MailBox</strong> 模型，需要将之前 <code>run()</code> 方法中 event processing 循环调用处理改为一个 event 有界流处理，举个例子：</p>
<p><code>One/TwoInputStreamTask</code> 中的下面代码 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (running &amp;&amp; inputProcessor.processInput())</span><br></pre></td></tr></table></figure>
<p>可以修改为</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputProcessor.processInput() <span class="comment">// 每次触发，都相当于处理一个有限流</span></span><br></pre></td></tr></table></figure>
<p>在实现中，会先检查 MailBox 有没有 <code>mail</code>（即加入到队列里的 task 任务）需要处理，有的话，就进行处理，如果没有的话，就执行上面的操作，进行 event processing。</p>
<p>这里有一个问题：就是 SourceStreamTask，会有一个兼容性的问题，因为在流的 source 端，它的 <code>event prcessing</code> 是来专门产生一个无限流数据，在这个处理中，并不能穿插 MailBox 中的 <code>mail</code> 检测，也就是说，如果只有一个 MailBox 线程处理的话，当这个线程去产生数据的话，它一直运行下去，就无法再去检测 MailBox 中是否有新的 mail 到来（在 Source 未来的版本中，可以完美兼容 MailBox 线程设计，见 FLIP-27，但现在的版本还不兼容）。</p>
<p>为了兼容 Source 端，目前的解决方案是：两个线程操作，一个专门用产生无限流，另一个是 MailBox 线程（处理 Checkpoint、timer 等），这两个线程为了保证线程安全，还是使用 Checkpoint Lock 做排它锁，如下图所示（图片来自设计文档）：</p>
<p><img src="/images/flink/source-stream-task-mailbox.png" alt="Source StreamTask Mailbox 实现"></p>
<h3 id="Checkpoint-和-timer-的-trigger"><a href="#Checkpoint-和-timer-的-trigger" class="headerlink" title="Checkpoint 和 timer 的 trigger"></a>Checkpoint 和 timer 的 trigger</h3><p>对于 Checkpoint 和 timer 的 trigger，这里会发现，目前的这个设计是完全可以满足需求的，Checkpoint 和 Timer 的触发事件都会以一个 <code>Runnable</code> 的形式添加到 MailBox 的队列中，等待 MailBox 主线程去处理。</p>
<h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>介绍完其设计方案，这里注重看下在 Apache Flink 1.10 的代码中，基于 <strong>MailBox 模型</strong> 的 StreamTask 是如何实现的。</p>
<h3 id="StreamTask-处理流程"><a href="#StreamTask-处理流程" class="headerlink" title="StreamTask 处理流程"></a>StreamTask 处理流程</h3><p>在 Flink 中，当一个作业被调度起来后，对于流计算来说，作业中的 Task 最终会以 StreamTask 的形式去执行，在 1.10 的实现中，一个 StreamTask 的核心处理流程如下：</p>
<p><img src="/images/flink/mailbox-method.png" alt="StreamTask MailBox 模型下核心处理流程"></p>
<p>StreamTask 中 <code>invoke()</code> 和 <code>runMailboxLoop()</code> 方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.flink.streaming.runtime.tasks.StreamTask</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        beforeInvoke();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// final check to exit early before starting to run</span></span><br><span class="line">        <span class="keyword">if</span> (canceled) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CancelTaskException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// let the task do its work</span></span><br><span class="line">        isRunning = <span class="keyword">true</span>;</span><br><span class="line">        runMailboxLoop();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// if this left the run() method cleanly despite the fact that this was canceled,</span></span><br><span class="line">        <span class="comment">// make sure the "clean shutdown" is not attempted</span></span><br><span class="line">        <span class="keyword">if</span> (canceled) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CancelTaskException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        afterInvoke();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">finally</span> &#123;</span><br><span class="line">        cleanUpInvoke();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">runMailboxLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">//note: mailbox 处理</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        mailboxProcessor.runMailboxLoop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        Optional&lt;InterruptedException&gt; interruption = ExceptionUtils.findThrowable(e, InterruptedException.class);</span><br><span class="line">        <span class="keyword">if</span> (interruption.isPresent()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!canceled) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">                <span class="keyword">throw</span> interruption.get();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (canceled) &#123;</span><br><span class="line">            LOG.warn(<span class="string">"Error while canceling task."</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后真正执行的是 MailboxProcessor 中的 <code>runMailboxLoop()</code> 方法，也就是上面说的 MailBox 主线程，StreamTask 运行的核心流程也是在这个方法中，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Runs the mailbox processing loop. This is where the main work is done.</span></span><br><span class="line"><span class="comment"> * note: mailbox 处理核心流程</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runMailboxLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> TaskMailbox localMailbox = mailbox;</span><br><span class="line"></span><br><span class="line">    Preconditions.checkState(</span><br><span class="line">        localMailbox.isMailboxThread(),</span><br><span class="line">        <span class="string">"Method must be executed by declared mailbox thread!"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: MailBox 的状态必须是 OPEN，才能继续循环</span></span><br><span class="line">    <span class="keyword">assert</span> localMailbox.getState() == TaskMailbox.State.OPEN : <span class="string">"Mailbox must be opened!"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> MailboxController defaultActionContext = <span class="keyword">new</span> MailboxController(<span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (processMail(localMailbox)) &#123; <span class="comment">//note: 如果有 mail 需要处理，这里会进行相应的处理，处理完才会进行下面的 event processing</span></span><br><span class="line">        <span class="comment">//note: 进行 task 的 default action，也就是调用 processInput()</span></span><br><span class="line">        mailboxDefaultAction.runDefaultAction(defaultActionContext); <span class="comment">// lock is acquired inside default action as needed</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的方法中，最关键的有两个地方：</p>
<ol>
<li><code>processMail()</code>: 它会检测 MailBox 中是否有 <code>mail</code> 需要处理，如果有的话，就做相应的处理，一直将全部的 mail 处理完才会返回，只要 loop 还在进行，这里就会返回 true，否则会返回 false；</li>
<li><code>runDefaultAction()</code>: 这个最终调用的 StreamTask 的 <code>processInput()</code> 方法，event-processing 的处理就是在这个方法中进行的。</li>
</ol>
<h4 id="event-processing-处理"><a href="#event-processing-处理" class="headerlink" title="event-processing 处理"></a>event-processing 处理</h4><p>对于 StreamTask 来说，event-processing 现在是在 <code>processInput()</code> 方法中实现的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.streaming.runtime.tasks.StreamTask</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This method implements the default action of the task (e.g. processing one event from the input). Implementations</span></span><br><span class="line"><span class="comment"> * should (in general) be non-blocking.</span></span><br><span class="line"><span class="comment"> * note: 这个方法执行这个 task 默认的 action</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> controller controller object for collaborative interaction between the action and the stream task.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception on any problems in the action.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processInput</span><span class="params">(MailboxDefaultAction.Controller controller)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    InputStatus status = inputProcessor.processInput(); <span class="comment">//note: event 处理</span></span><br><span class="line">    <span class="keyword">if</span> (status == InputStatus.MORE_AVAILABLE &amp;&amp; recordWriter.isAvailable()) &#123;</span><br><span class="line">        <span class="comment">//note: 如果输入还有数据，并且 writer 是可用的，这里就直接返回了</span></span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (status == InputStatus.END_OF_INPUT) &#123;</span><br><span class="line">        <span class="comment">//note: 输入已经处理完了，会调用这个方法</span></span><br><span class="line">        controller.allActionsCompleted();</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    CompletableFuture&lt;?&gt; jointFuture = getInputOutputJointFuture(status);</span><br><span class="line">    <span class="comment">//note: 告诉 MailBox 先暂停 loop</span></span><br><span class="line">    MailboxDefaultAction.Suspension suspendedDefaultAction = controller.suspendDefaultAction();</span><br><span class="line">    <span class="comment">//note: 等待 future 完成后，继续 mailbox loop（等待 input 和 output 可用后，才会继续）</span></span><br><span class="line">    jointFuture.thenRun(suspendedDefaultAction::resume);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再结合 MailboxProcessor 中的 <code>runMailboxLoop()</code> 实现一起看，其操作的流程是：</p>
<ol>
<li>首先通过 <code>processMail()</code> 方法处理 MailBox 中的 <code>mail</code>：<ul>
<li>如果没有 <code>mail</code> 要处理，这里直接返回；</li>
<li>先将 MailBox 中当前现存的 <code>mail</code> 全部处理完；</li>
<li>通过 <code>isDefaultActionUnavailable()</code> 做一个状态检查（目的是提供一个接口方便上层控制调用，这里把这个看作一个状态检查方便讲述），如果是 true 的话，会在这里一直处理 mail 事件，不会返回，除非状态改变；</li>
</ul>
</li>
<li>然后再调用 StreamTask 的 <code>processInput()</code> 方法来处理 event:<ul>
<li>先调用 InputProcessor 的 <code>processInput()</code> 方法来处理 event；</li>
<li>如果上面处理结果返回的状态是 <code>MORE_AVAILABLE</code>（表示还有可用的数据等待处理）并且 <code>recordWriter</code> 可用（之前的异步操作已经处理完成），就会立马返回；</li>
<li>如果上面处理结果返回的状态是 <code>END_OF_INPUT</code>，它表示数据处理完成，这里就会告诉 MailBox 数据已经处理完成了；</li>
<li>否则的话，这里会等待，直到有可用的数据到来及 <code>recordWriter</code> 可用。</li>
</ul>
</li>
</ol>
<h4 id="checkpoint-trigger-处理"><a href="#checkpoint-trigger-处理" class="headerlink" title="checkpoint trigger 处理"></a>checkpoint trigger 处理</h4><p>接着来看下 Checkpoint Trigger 是怎么处理的，要先看下 Streamtask 的 <code>triggerCheckpointAsync()</code> 实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;Boolean&gt; <span class="title">triggerCheckpointAsync</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointMetaData checkpointMetaData,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> advanceToEndOfEventTime)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: checkpoint 触发时，提交相应的 task</span></span><br><span class="line">    <span class="keyword">return</span> mailboxProcessor.getMainMailboxExecutor().submit(</span><br><span class="line">            () -&gt; triggerCheckpoint(checkpointMetaData, checkpointOptions, advanceToEndOfEventTime),</span><br><span class="line">            <span class="string">"checkpoint %s with %s"</span>,</span><br><span class="line">        checkpointMetaData,</span><br><span class="line">        checkpointOptions);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里可以看到，其实现跟方案设计中的是一致，Checkpoint trigger 这里的操作就是向 MailBox 提交一个 Task，等待 MailBox 去处理。</p>
<h3 id="SourceStreamTask-如何兼容"><a href="#SourceStreamTask-如何兼容" class="headerlink" title="SourceStreamTask 如何兼容"></a>SourceStreamTask 如何兼容</h3><p>在设计文档中，有个重要的、特别要注意的点就是 SourceStreamTask 的兼容问题，开始的设计方案是在 SourceStreamTask 中专门启动两个线程来保持兼容性问题，而且虽然使用了 MailBox 模型，但还是会继续使用 checkpoint lock 来保证线程安全，这里看下其是如何实现的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.streaming.runtime.tasks.SourceStreamTask</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">processInput</span><span class="params">(MailboxDefaultAction.Controller controller)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 告诉 MailBox 先暂停 loop</span></span><br><span class="line">    controller.suspendDefaultAction();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Against the usual contract of this method, this implementation is not step-wise but blocking instead for</span></span><br><span class="line">    <span class="comment">// compatibility reasons with the current source interface (source functions run as a loop, not in steps).</span></span><br><span class="line">    sourceThread.setTaskDescription(getName());</span><br><span class="line">    sourceThread.start();</span><br><span class="line">    sourceThread.getCompletionFuture().whenComplete((Void ignore, Throwable sourceThreadThrowable) -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (sourceThreadThrowable == <span class="keyword">null</span> || isFinished) &#123;</span><br><span class="line">            <span class="comment">//note: sourceThread 完成后，没有抛出异常或 task 完成的情况下</span></span><br><span class="line">            mailboxProcessor.allActionsCompleted();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//note: 没有完成但结束了或者抛出异常的情况下</span></span><br><span class="line">            mailboxProcessor.reportThrowable(sourceThreadThrowable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Runnable that executes the the source function in the head operator.</span></span><br><span class="line"><span class="comment"> * note: source 产生 data 的一个线程</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">LegacySourceFunctionThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CompletableFuture&lt;Void&gt; completionFuture;</span><br><span class="line"></span><br><span class="line">    LegacySourceFunctionThread() &#123;</span><br><span class="line">        <span class="keyword">this</span>.completionFuture = <span class="keyword">new</span> CompletableFuture&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//note: 调用 source Operator 的 run</span></span><br><span class="line">            headOperator.run(getCheckpointLock(), getStreamStatusMaintainer(), operatorChain);</span><br><span class="line">            completionFuture.complete(<span class="keyword">null</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// Note, t can be also an InterruptedException</span></span><br><span class="line">            completionFuture.completeExceptionally(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTaskDescription</span><span class="params">(<span class="keyword">final</span> String taskDescription)</span> </span>&#123;</span><br><span class="line">        setName(<span class="string">"Legacy Source Thread - "</span> + taskDescription);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">CompletableFuture&lt;Void&gt; <span class="title">getCompletionFuture</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> completionFuture;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到：</p>
<ol>
<li>LegacySourceFunctionThread 线程在启动时，会先通知一下 MailBox，这个就是上面说的那个状态检查，收到这个信号之后，MailBox 就会在 <code>processMail()</code> 中一直等待并且处理 <code>mail</code>，不会返回（也就是 MailBox 主线程一直在处理 <code>mail</code> 事件）；</li>
<li>LegacySourceFunctionThread 线程就是专门生产数据的，跟 MailBox 这两个线程都在运行。</li>
</ol>
<p>那么两个线程如何保证线程安全呢？如果仔细看上面的代码就会发现，在 SourceStreamTask 中还继续使用了 <code>getCheckpointLock()</code>，虽然这个方法现在已经被标注了将要被废弃，但 Source 没有改造完成之前，Source 的实现还是会继续依赖 checkpoint lock。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里，总结一下 Flink 1.10 中 MailBox 模型的核心设计，如下图所示：</p>
<p><img src="/images/flink/mail-box-core.png" alt="MailBox 模型核心设计"></p>
<ol>
<li><code>MailboxExecutor</code>: 它负责向 MailBox 提交 task 任务；</li>
<li><code>TaskMailbox</code>: 负责存储相应 task 任务（也就是 <code>mail</code>），它支持多写单读，单线程读取并处理；</li>
<li><code>MailboxProcessor</code>: MailBox 的核心处理线程，<code>MailboxDefaultAction</code> 是其默认的 action 实现，可以理解为 StreamTask 的 event 处理逻辑就是基于 <code>MailboxDefaultAction</code> 接口实现的。</li>
</ol>
<p>Flink MailBox 这块的设计还是非常不错的，无论是从代码的可读性上还是后续维护性上都是要比之前的设计好很多，也值得我们学习借鉴。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://issues.apache.org/jira/browse/FLINK-12477" target="_blank" rel="external">FLINK-12477: Change threading-model in StreamTask to a mailbox-based approach</a>；</li>
<li><a href="https://docs.google.com/document/d/1eDpsUKv2FqwZiS1Pm6gYO5eFHScBHfULKmH1-ZEWB4g/edit" target="_blank" rel="external">Change threading-model in StreamTask to a mailbox-based approach</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/MrQZS7-dEuNr442lzw3xYg" target="_blank" rel="external">重磅！Flink 将重构其核心线程模型</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第八篇，在介绍 TaskManager 第二部分之前，先来给介绍一下目前 StreamTask 中基于 MailBox 实现的线程模型，这个模型从 1.9 开始实现，在目前发布的 1.10 版本中，基本上已经
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink TaskManager 详解（一）</title>
    <link href="http://matt33.com/2020/03/15/flink-taskmanager-7/"/>
    <id>http://matt33.com/2020/03/15/flink-taskmanager-7/</id>
    <published>2020-03-15T03:46:37.000Z</published>
    <updated>2020-06-23T14:13:17.226Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第七篇，这篇文章主要会讲述 Flink 中的 TaskManager 的一些内容，TaskManager 是 Flink 的 worker 节点，它负责 Flink 中本机 slot 资源的管理以及具体 task 的执行。TaskManager 上的基本资源单位是 slot，一个作业的 task 最终会部署在一个 TM 的 slot 上运行，TM 会负责维护本地的 slot 资源列表，并来与 Flink Master 和 JobManager 通信，预计将会通过两篇左右的文章来向大家揭秘 TaskManager 内部的实现原理。另外，本篇将采用先提出问题，然后再根据源码实现去解答这些问题的形式叙述，如果大家有其他建议，欢迎（博客/公众号）留言反馈。</p>
<p>对于 TaskManager 的内容，这里将会聚焦下面几个问题上，下面的文章将会逐个去分析这些问题（因为内容较多，会分为两篇文章讲述，本篇注重聚焦在前五个问题上）：</p>
<ol>
<li>TaskManager 启动流程？</li>
<li>TaskManager 提供了哪些能力/功能？</li>
<li>TaskManager 怎么发现 RM leader（在使用 ZK 做 HA 的情况下）？</li>
<li>TM 如何维护 JobManager 的关系，如果 JobManager 挂掉，TM 会如何处理？</li>
<li>TM Slot 资源是如何管理的？</li>
<li>TM 如何处理提交过来的 Task；</li>
<li>TM 如何处理 Task 之间 Shuffle 的需求？</li>
</ol>
<h2 id="TaskManager-启动流程"><a href="#TaskManager-启动流程" class="headerlink" title="TaskManager 启动流程"></a>TaskManager 启动流程</h2><p>与 JobManager 类似，TaskManager 的启动类是 <code>TaskManagerRunner</code>，大概的流程如下图所示：</p>
<p><img src="/images/flink/TaskManager-1.png" alt="TaskManager 启动流程"></p>
<p>TaskManager 启动的入口方法是 <code>runTaskManager()</code>，它会首先初始化 TaskManager 一些相关的服务，比如：初始化 RpcService、初始化 HighAvailabilityServices 等等，这些都是为 TaskManager 服务的启动做相应的准备工作。其实 TaskManager 初始化主要分为下面两大块：</p>
<ol>
<li>TaskManager 相关 service 的初始化：比如：内存管理器、IO 管理器、TaskSlotTable（TaskSlot 的管理是在这里进行的）等，这里也包括 TaskExecutor 的初始化，注意这里对于一些需要启动的服务在这一步并没有启动；</li>
<li>TaskExecutor 的启动：它会启动 TM 上相关的服务，Task 的提交和运行也是在 TaskExecutor 中处理的，上一步 TM 初始化的那些服务也是在 TaskExecutor 中使用的。</li>
</ol>
<p>TM 的服务真正 Run 起来之后，核心流程还是在 <code>TaskExecutor</code> 中。</p>
<h3 id="TaskManager-相关服务的初始化"><a href="#TaskManager-相关服务的初始化" class="headerlink" title="TaskManager 相关服务的初始化"></a>TaskManager 相关服务的初始化</h3><p>这里，先从 TaskManager 的入口 <code>runTaskManager()</code> 来看 TaskManager 相关服务的初始化流程，总结来看流程如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  1. 入口方法</span></span><br><span class="line">runTaskManager()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 创建 TaskManagerRunner 对象</span></span><br><span class="line">TaskManagerRunner taskManagerRunner = <span class="keyword">new</span> TaskManagerRunner(configuration, resourceId);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 启动 TaskManager 服务</span></span><br><span class="line">startTaskManager()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 初始化相关的服务</span></span><br><span class="line">TaskManagerServices.fromConfiguration()</span><br></pre></td></tr></table></figure>
<p>首先看下具体的代码实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskManagerRunner.java</span></span><br><span class="line"><span class="comment">//note: 启动 TaskManagerRunner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">runTaskManager</span><span class="params">(Configuration configuration, ResourceID resourceId)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> TaskManagerRunner taskManagerRunner = <span class="keyword">new</span> TaskManagerRunner(configuration, resourceId);</span><br><span class="line"></span><br><span class="line">    taskManagerRunner.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 初始化 TaskManagerRunner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">TaskManagerRunner</span><span class="params">(Configuration configuration, ResourceID resourceId)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.configuration = checkNotNull(configuration);</span><br><span class="line">    <span class="keyword">this</span>.resourceId = checkNotNull(resourceId);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: akka 超时设置</span></span><br><span class="line">    timeout = AkkaUtils.getTimeoutAsTime(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.executor = java.util.concurrent.Executors.newScheduledThreadPool(</span><br><span class="line">        Hardware.getNumberCPUCores(),</span><br><span class="line">        <span class="keyword">new</span> ExecutorThreadFactory(<span class="string">"taskmanager-future"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: HA 的配置及服务初始化</span></span><br><span class="line">    highAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(</span><br><span class="line">        configuration,</span><br><span class="line">        executor,</span><br><span class="line">        HighAvailabilityServicesUtils.AddressResolution.TRY_ADDRESS_RESOLUTION);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: create rpc service</span></span><br><span class="line">    rpcService = createRpcService(configuration, highAvailabilityServices);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 初始化心跳服务</span></span><br><span class="line">    HeartbeatServices heartbeatServices = HeartbeatServices.fromConfiguration(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: metrics 服务</span></span><br><span class="line">    metricRegistry = <span class="keyword">new</span> MetricRegistryImpl(</span><br><span class="line">        MetricRegistryConfiguration.fromConfiguration(configuration),</span><br><span class="line">        ReporterSetup.fromConfiguration(configuration));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 启动相应的 metrics 服务</span></span><br><span class="line">    <span class="keyword">final</span> RpcService metricQueryServiceRpcService = MetricUtils.startMetricsRpcService(configuration, rpcService.getAddress());</span><br><span class="line">    metricRegistry.startQueryService(metricQueryServiceRpcService, resourceId);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 初始化 blob 服务</span></span><br><span class="line">    blobCacheService = <span class="keyword">new</span> BlobCacheService(</span><br><span class="line">        configuration, highAvailabilityServices.createBlobStore(), <span class="keyword">null</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 启动 TaskManager 服务及创建 TaskExecutor 对象</span></span><br><span class="line">    taskManager = startTaskManager(</span><br><span class="line">        <span class="keyword">this</span>.configuration,</span><br><span class="line">        <span class="keyword">this</span>.resourceId,</span><br><span class="line">        rpcService,</span><br><span class="line">        highAvailabilityServices,</span><br><span class="line">        heartbeatServices,</span><br><span class="line">        metricRegistry,</span><br><span class="line">        blobCacheService,</span><br><span class="line">        <span class="keyword">false</span>,</span><br><span class="line">        <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.terminationFuture = <span class="keyword">new</span> CompletableFuture&lt;&gt;();</span><br><span class="line">    <span class="keyword">this</span>.shutdown = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 周期性地输出内存相关的日志信息，直到 terminationFuture complete</span></span><br><span class="line">    MemoryLogger.startIfConfigured(LOG, configuration, terminationFuture);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的流程中，初始化了一些最基本的服务，比如：rpc 服务，在方法的最后调用了 <code>startTaskManager()</code> 启动 TaskManager，其代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskManagerRunner.java</span></span><br><span class="line"><span class="comment">//note: 创建并初始化 TaskExecutor 对象</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TaskExecutor <span class="title">startTaskManager</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Configuration configuration,</span></span></span><br><span class="line"><span class="function"><span class="params">        ResourceID resourceID,</span></span></span><br><span class="line"><span class="function"><span class="params">        RpcService rpcService,</span></span></span><br><span class="line"><span class="function"><span class="params">        HighAvailabilityServices highAvailabilityServices,</span></span></span><br><span class="line"><span class="function"><span class="params">        HeartbeatServices heartbeatServices,</span></span></span><br><span class="line"><span class="function"><span class="params">        MetricRegistry metricRegistry,</span></span></span><br><span class="line"><span class="function"><span class="params">        BlobCacheService blobCacheService,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> localCommunicationOnly,</span></span></span><br><span class="line"><span class="function"><span class="params">        FatalErrorHandler fatalErrorHandler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    checkNotNull(configuration);</span><br><span class="line">    checkNotNull(resourceID);</span><br><span class="line">    checkNotNull(rpcService);</span><br><span class="line">    checkNotNull(highAvailabilityServices);</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"Starting TaskManager with ResourceID: &#123;&#125;"</span>, resourceID);</span><br><span class="line"></span><br><span class="line">    InetAddress remoteAddress = InetAddress.getByName(rpcService.getAddress());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: TM 服务相关的配置都维护在这个对象中，这里会把使用的相关参数解析并维护起来</span></span><br><span class="line">    TaskManagerServicesConfiguration taskManagerServicesConfiguration =</span><br><span class="line">        TaskManagerServicesConfiguration.fromConfiguration(</span><br><span class="line">            configuration,</span><br><span class="line">            resourceID,</span><br><span class="line">            remoteAddress,</span><br><span class="line">            EnvironmentInformation.getSizeOfFreeHeapMemoryWithDefrag(),</span><br><span class="line">            EnvironmentInformation.getMaxJvmHeapMemory(),</span><br><span class="line">            localCommunicationOnly);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 初始化 TM 的 TaskManagerMetricGroup，并相应地初始化 TM 的基本状态（内存、CPU 等）监控</span></span><br><span class="line">    Tuple2&lt;TaskManagerMetricGroup, MetricGroup&gt; taskManagerMetricGroup = MetricUtils.instantiateTaskManagerMetricGroup(</span><br><span class="line">        metricRegistry,</span><br><span class="line">        TaskManagerLocation.getHostName(remoteAddress),</span><br><span class="line">        resourceID,</span><br><span class="line">        taskManagerServicesConfiguration.getSystemResourceMetricsProbingInterval());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 初始化 TaskManagerServices（TM 相关服务的初始化都在这里）</span></span><br><span class="line">    TaskManagerServices taskManagerServices = TaskManagerServices.fromConfiguration(</span><br><span class="line">        taskManagerServicesConfiguration,</span><br><span class="line">        taskManagerMetricGroup.f1,</span><br><span class="line">        rpcService.getExecutor()); <span class="comment">// TODO replace this later with some dedicated executor for io.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: TaskManager 相关的配置，主要用于 TaskExecutor 的初始化</span></span><br><span class="line">    TaskManagerConfiguration taskManagerConfiguration = TaskManagerConfiguration.fromConfiguration(configuration);</span><br><span class="line"></span><br><span class="line">    String metricQueryServiceAddress = metricRegistry.getMetricQueryServiceGatewayRpcAddress();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 最后创建 TaskExecutor 对象</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> TaskExecutor(</span><br><span class="line">        rpcService,</span><br><span class="line">        taskManagerConfiguration,</span><br><span class="line">        highAvailabilityServices,</span><br><span class="line">        taskManagerServices,</span><br><span class="line">        heartbeatServices,</span><br><span class="line">        taskManagerMetricGroup.f0,</span><br><span class="line">        metricQueryServiceAddress,</span><br><span class="line">        blobCacheService,</span><br><span class="line">        fatalErrorHandler,</span><br><span class="line">        <span class="keyword">new</span> PartitionTable&lt;&gt;());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里，来着重看一下 <code>TaskManagerServices.fromConfiguration()</code> 这个方法，在这个方法初始了很多 TM 的服务，从下面的具体实现中也可以看出：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskManagerServices.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates and returns the task manager services.</span></span><br><span class="line"><span class="comment"> * note：根据创建 TM 服务</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> taskManagerServicesConfiguration task manager configuration</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> taskManagerMetricGroup metric group of the task manager</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> taskIOExecutor executor for async IO operations</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> task manager components</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> TaskManagerServices <span class="title">fromConfiguration</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        TaskManagerServicesConfiguration taskManagerServicesConfiguration,</span></span></span><br><span class="line"><span class="function"><span class="params">        MetricGroup taskManagerMetricGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">        Executor taskIOExecutor)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// pre-start checks</span></span><br><span class="line">    checkTempDirs(taskManagerServicesConfiguration.getTmpDirPaths());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建 taskEventDispatcher</span></span><br><span class="line">    <span class="keyword">final</span> TaskEventDispatcher taskEventDispatcher = <span class="keyword">new</span> TaskEventDispatcher();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// start the I/O manager, it will create some temp directories.</span></span><br><span class="line">    <span class="comment">//note: 创建 IO 管理器</span></span><br><span class="line">    <span class="keyword">final</span> IOManager ioManager = <span class="keyword">new</span> IOManagerAsync(taskManagerServicesConfiguration.getTmpDirPaths());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建 ShuffleEnvironment 对象(默认是 NettyShuffleEnvironment)</span></span><br><span class="line">    <span class="keyword">final</span> ShuffleEnvironment&lt;?, ?&gt; shuffleEnvironment = createShuffleEnvironment(</span><br><span class="line">        taskManagerServicesConfiguration,</span><br><span class="line">        taskEventDispatcher,</span><br><span class="line">        taskManagerMetricGroup);</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> dataPort = shuffleEnvironment.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建 KvStateService 实例并启动</span></span><br><span class="line">    <span class="keyword">final</span> KvStateService kvStateService = KvStateService.fromConfiguration(taskManagerServicesConfiguration);</span><br><span class="line">    kvStateService.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 初始化 taskManagerLocation，记录 connection 信息</span></span><br><span class="line">    <span class="keyword">final</span> TaskManagerLocation taskManagerLocation = <span class="keyword">new</span> TaskManagerLocation(</span><br><span class="line">        taskManagerServicesConfiguration.getResourceID(),</span><br><span class="line">        taskManagerServicesConfiguration.getTaskManagerAddress(),</span><br><span class="line">        dataPort);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// this call has to happen strictly after the network stack has been initialized</span></span><br><span class="line">    <span class="comment">//note: 初始化 MemoryManager</span></span><br><span class="line">    <span class="keyword">final</span> MemoryManager memoryManager = createMemoryManager(taskManagerServicesConfiguration);</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> managedMemorySize = memoryManager.getMemorySize();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 初始化 BroadcastVariableManager 对象</span></span><br><span class="line">    <span class="keyword">final</span> BroadcastVariableManager broadcastVariableManager = <span class="keyword">new</span> BroadcastVariableManager();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 当前 TM 拥有的 slot 及每个 slot 的资源信息</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> numOfSlots = taskManagerServicesConfiguration.getNumberOfSlots();</span><br><span class="line">    <span class="keyword">final</span> List&lt;ResourceProfile&gt; resourceProfiles =</span><br><span class="line">        Collections.nCopies(numOfSlots, computeSlotResourceProfile(numOfSlots, managedMemorySize));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 注册一个超时（AKKA 超时设置）服务（在 TaskSlotTable 用于监控 slot 分配是否超时）</span></span><br><span class="line">    <span class="keyword">final</span> TimerService&lt;AllocationID&gt; timerService = <span class="keyword">new</span> TimerService&lt;&gt;(</span><br><span class="line">        <span class="keyword">new</span> ScheduledThreadPoolExecutor(<span class="number">1</span>),</span><br><span class="line">        taskManagerServicesConfiguration.getTimerServiceShutdownTimeout());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 这里会维护 slot 相关列表</span></span><br><span class="line">    <span class="keyword">final</span> TaskSlotTable taskSlotTable = <span class="keyword">new</span> TaskSlotTable(resourceProfiles, timerService);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 维护 jobId 与 JobManager connection 之间的关系</span></span><br><span class="line">    <span class="keyword">final</span> JobManagerTable jobManagerTable = <span class="keyword">new</span> JobManagerTable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 监控注册的 job 的 JobManger leader 信息</span></span><br><span class="line">    <span class="keyword">final</span> JobLeaderService jobLeaderService = <span class="keyword">new</span> JobLeaderService(taskManagerLocation, taskManagerServicesConfiguration.getRetryingRegistrationConfiguration());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> String[] stateRootDirectoryStrings = taskManagerServicesConfiguration.getLocalRecoveryStateRootDirectories();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> File[] stateRootDirectoryFiles = <span class="keyword">new</span> File[stateRootDirectoryStrings.length];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; stateRootDirectoryStrings.length; ++i) &#123;</span><br><span class="line">        stateRootDirectoryFiles[i] = <span class="keyword">new</span> File(stateRootDirectoryStrings[i], LOCAL_STATE_SUB_DIRECTORY_ROOT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建 TaskExecutorLocalStateStoresManager 对象：维护状态信息</span></span><br><span class="line">    <span class="keyword">final</span> TaskExecutorLocalStateStoresManager taskStateManager = <span class="keyword">new</span> TaskExecutorLocalStateStoresManager(</span><br><span class="line">        taskManagerServicesConfiguration.isLocalRecoveryEnabled(),</span><br><span class="line">        stateRootDirectoryFiles,</span><br><span class="line">        taskIOExecutor);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 将上面初始化的这些服务，封装到一个 TaskManagerServices 对象中</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> TaskManagerServices(</span><br><span class="line">        taskManagerLocation,</span><br><span class="line">        memoryManager,</span><br><span class="line">        ioManager,</span><br><span class="line">        shuffleEnvironment,</span><br><span class="line">        kvStateService,</span><br><span class="line">        broadcastVariableManager,</span><br><span class="line">        taskSlotTable,</span><br><span class="line">        jobManagerTable,</span><br><span class="line">        jobLeaderService,</span><br><span class="line">        taskStateManager,</span><br><span class="line">        taskEventDispatcher);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看到这里，是否有点懵圈了，是不是感觉 TaskManager 实现还挺复杂的，但与 TaskManager 要做的功能相比，上面的实现还不够，真正在 TaskManager 中处理复杂繁琐工作的组件是 <strong>TaskExecutor</strong>，这个才是 TaskManager 的核心。</p>
<h3 id="TaskExecutor-的启动"><a href="#TaskExecutor-的启动" class="headerlink" title="TaskExecutor 的启动"></a>TaskExecutor 的启动</h3><p>回顾一下文章最开始的流程图，TaskManagerRunner 调用 <code>run()</code> 方法之后，真正要启动的是 TaskExecutor 服务，其 <code>onStart()</code> 具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 启动服务</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 启动 TM 的相关服务</span></span><br><span class="line">        startTaskExecutorServices();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">final</span> TaskManagerException exception = <span class="keyword">new</span> TaskManagerException(String.format(<span class="string">"Could not start the TaskExecutor %s"</span>, getAddress()), e);</span><br><span class="line">        onFatalError(exception);</span><br><span class="line">        <span class="keyword">throw</span> exception;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 注册超时检测，如果超时还未注册完成，就抛出错误，启动失败</span></span><br><span class="line">    startRegistrationTimeout();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里，主要分为两个部分：</p>
<ol>
<li><code>startTaskExecutorServices()</code>: 启动 TaskManager 相关的服务，结合流程图主要是四大块：<ul>
<li>启动心跳服务；</li>
<li>向 Flink Master 的 ResourceManager 注册 TaskManager；</li>
<li>启动 TaskSlotTable 服务（TaskSlot 的维护主要在这个服务中）；</li>
<li>启动 JobLeaderService 服务，它主要是监控各个作业 JobManager leader 的变化；</li>
</ul>
</li>
<li><code>startRegistrationTimeout()</code>: 启动注册超时的检测，默认是5 min，如果超过这个时间还没注册完成，就会抛出异常退出进程，启动失败。</li>
</ol>
<p>TaskExecutor 启动的核心实现是在 <code>startTaskExecutorServices()</code> 中，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startTaskExecutorServices</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 启动心跳服务</span></span><br><span class="line">        startHeartbeatServices();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 与集群的 ResourceManager 建立连接（并创建一个 listener）</span></span><br><span class="line">        <span class="comment">// start by connecting to the ResourceManager</span></span><br><span class="line">        resourceManagerLeaderRetriever.start(<span class="keyword">new</span> ResourceManagerLeaderListener());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// tell the task slot table who's responsible for the task slot actions</span></span><br><span class="line">        <span class="comment">//note: taskSlotTable 启动</span></span><br><span class="line">        taskSlotTable.start(<span class="keyword">new</span> SlotActionsImpl());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// start the job leader service</span></span><br><span class="line">        <span class="comment">//note: 启动 job leader 服务</span></span><br><span class="line">        jobLeaderService.start(getAddress(), getRpcService(), haServices, <span class="keyword">new</span> JobLeaderListenerImpl());</span><br><span class="line"></span><br><span class="line">        fileCache = <span class="keyword">new</span> FileCache(taskManagerConfiguration.getTmpDirectories(), blobCacheService.getPermanentBlobService());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        handleStartTaskExecutorServicesException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来，详细这块的实现。</p>
<h4 id="1-启动心跳服务"><a href="#1-启动心跳服务" class="headerlink" title="1. 启动心跳服务"></a>1. 启动心跳服务</h4><p>TaskExecutor 启动的第一个服务就是 HeartbeatManager，这里会启动两个：</p>
<ol>
<li><code>jobManagerHeartbeatManager</code>: 用于与 JobManager（如果 Job 有 task 在这个 TM 上，这个 Job 的 JobManager 就与 TaskManager 有心跳通信）之间的心跳通信管理，如果 timeout，这里会重连；</li>
<li><code>resourceManagerHeartbeatManager</code>:用于与 ResourceManager 之间的通信管理，如果 timeout，这里也会重连。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor.java</span></span><br><span class="line"><span class="comment">//note: 启动心跳服务</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startHeartbeatServices</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();</span><br><span class="line">    <span class="comment">//note: 创建一个与 JM 通信的心跳管理器</span></span><br><span class="line">    jobManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(</span><br><span class="line">        resourceId,</span><br><span class="line">        <span class="keyword">new</span> JobManagerHeartbeatListener(),</span><br><span class="line">        getMainThreadExecutor(),</span><br><span class="line">        log);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建一个与 RM 通信的心跳管理器</span></span><br><span class="line">    resourceManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(</span><br><span class="line">        resourceId,</span><br><span class="line">        <span class="keyword">new</span> ResourceManagerHeartbeatListener(),</span><br><span class="line">        getMainThreadExecutor(),</span><br><span class="line">        log);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-向-RM-注册-TM"><a href="#2-向-RM-注册-TM" class="headerlink" title="2. 向 RM 注册 TM"></a>2. 向 RM 注册 TM</h4><p>TaskManger 向 ResourceManager 注册是通过 <code>ResourceManagerLeaderListener</code> 来完成的，它会监控 ResourceManager 的 leader 变化，如果有新的 leader 被选举出来，将会调用 <code>notifyLeaderAddress()</code> 方法去触发与 ResourceManager 的重连，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The listener for leader changes of the resource manager.</span></span><br><span class="line"><span class="comment"> * note：监控 ResourceManager leader 变化的 listener</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ResourceManagerLeaderListener</span> <span class="keyword">implements</span> <span class="title">LeaderRetrievalListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 如果 leader 被选举处理（包括挂掉之后重新选举），将会调用这个方法通知 TM</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">notifyLeaderAddress</span><span class="params">(<span class="keyword">final</span> String leaderAddress, <span class="keyword">final</span> UUID leaderSessionID)</span> </span>&#123;</span><br><span class="line">        runAsync(</span><br><span class="line">            () -&gt; notifyOfNewResourceManagerLeader(</span><br><span class="line">                leaderAddress,</span><br><span class="line">                ResourceManagerId.fromUuidOrNull(leaderSessionID)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleError</span><span class="params">(Exception exception)</span> </span>&#123;</span><br><span class="line">        onFatalError(exception);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 如果 RM 的 new leader 选举出来了，这里会新创建一个 ResourceManagerAddress 对象，并重新建立连接</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">notifyOfNewResourceManagerLeader</span><span class="params">(String newLeaderAddress, ResourceManagerId newResourceManagerId)</span> </span>&#123;</span><br><span class="line">    resourceManagerAddress = createResourceManagerAddress(newLeaderAddress, newResourceManagerId);</span><br><span class="line">    reconnectToResourceManager(<span class="keyword">new</span> FlinkException(String.format(<span class="string">"ResourceManager leader changed to new address %s"</span>, resourceManagerAddress)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 重新与 ResourceManager 连接（可能是 RM leader 切换）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">reconnectToResourceManager</span><span class="params">(Exception cause)</span> </span>&#123;</span><br><span class="line">    closeResourceManagerConnection(cause);</span><br><span class="line">    <span class="comment">//note: 注册超时检测，如果 timeout 还没注册成功，这里就会 failed</span></span><br><span class="line">    startRegistrationTimeout();</span><br><span class="line">    <span class="comment">//note: 与 RM 重新建立连接</span></span><br><span class="line">    tryConnectToResourceManager();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 建立与 ResourceManager 的连接</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">tryConnectToResourceManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (resourceManagerAddress != <span class="keyword">null</span>) &#123;</span><br><span class="line">        connectToResourceManager();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 与 ResourceManager 建立连接</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">connectToResourceManager</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">assert</span>(resourceManagerAddress != <span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">assert</span>(establishedResourceManagerConnection == <span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">assert</span>(resourceManagerConnection == <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">    log.info(<span class="string">"Connecting to ResourceManager &#123;&#125;."</span>, resourceManagerAddress);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 与 RM 建立连接</span></span><br><span class="line">    resourceManagerConnection =</span><br><span class="line">        <span class="keyword">new</span> TaskExecutorToResourceManagerConnection(</span><br><span class="line">            log,</span><br><span class="line">            getRpcService(),</span><br><span class="line">            getAddress(),</span><br><span class="line">            getResourceID(),</span><br><span class="line">            taskManagerConfiguration.getRetryingRegistrationConfiguration(),</span><br><span class="line">            taskManagerLocation.dataPort(),</span><br><span class="line">            hardwareDescription,</span><br><span class="line">            resourceManagerAddress.getAddress(),</span><br><span class="line">            resourceManagerAddress.getResourceManagerId(),</span><br><span class="line">            getMainThreadExecutor(),</span><br><span class="line">            <span class="keyword">new</span> ResourceManagerRegistrationListener());</span><br><span class="line">    resourceManagerConnection.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的最后一步，创建了 <code>TaskExecutorToResourceManagerConnection</code> 对象，它启动后，会向 ResourceManager 注册 TM，具体的方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutorToResourceManagerConnection.java</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> CompletableFuture&lt;RegistrationResponse&gt; <span class="title">invokeRegistration</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ResourceManagerGateway resourceManager, ResourceManagerId fencingToken, <span class="keyword">long</span> timeoutMillis)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    Time timeout = Time.milliseconds(timeoutMillis);</span><br><span class="line">    <span class="keyword">return</span> resourceManager.registerTaskExecutor(</span><br><span class="line">        taskExecutorAddress,</span><br><span class="line">        resourceID,</span><br><span class="line">        dataPort,</span><br><span class="line">        hardwareDescription,</span><br><span class="line">        timeout);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ResourceManager 在收到这个请求，会做相应的处理，主要要做的事情就是：先从缓存里移除旧的 TM 注册信息（如果之前存在的话），然后再更新缓存，并增加心跳监控，只有这些工作完成之后，TM 的注册才会被认为是成功的。</p>
<h4 id="3-启动-TaskSlotTable-服务"><a href="#3-启动-TaskSlotTable-服务" class="headerlink" title="3. 启动 TaskSlotTable 服务"></a>3. 启动 TaskSlotTable 服务</h4><p>TaskSlotTable 从名字也可以看出，它主要是为 TaskSlot 服务的，它主要的功能有以下三点：</p>
<ol>
<li>维护这个 TM 上所有 TaskSlot 与 Task、及 Job 的关系；</li>
<li>维护这个 TM 上所有 TaskSlot 的状态；</li>
<li>TaskSlot 在进行 allocate/free 操作，通过 <strong>TimeService</strong> 做超时检测。</li>
</ol>
<p>先看下 TaskSlotTable 是如何初始化的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskManagerServices.java</span></span><br><span class="line"><span class="comment">//note: 当前 TM 拥有的 slot 及每个 slot 的资源信息</span></span><br><span class="line"><span class="comment">//note: TM 的 slot 数由 taskmanager.numberOfTaskSlots 决定，默认是 1</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> numOfSlots = taskManagerServicesConfiguration.getNumberOfSlots();</span><br><span class="line"><span class="keyword">final</span> List&lt;ResourceProfile&gt; resourceProfiles =</span><br><span class="line">    Collections.nCopies(numOfSlots, computeSlotResourceProfile(numOfSlots, managedMemorySize));</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 注册一个超时（AKKA 超时设置）服务（在 TaskSlotTable 用于监控 slot 分配是否超时）</span></span><br><span class="line"><span class="comment">//note: 超时参数由 akka.ask.timeout 控制，默认是 10s</span></span><br><span class="line"><span class="keyword">final</span> TimerService&lt;AllocationID&gt; timerService = <span class="keyword">new</span> TimerService&lt;&gt;(</span><br><span class="line">    <span class="keyword">new</span> ScheduledThreadPoolExecutor(<span class="number">1</span>),</span><br><span class="line">    taskManagerServicesConfiguration.getTimerServiceShutdownTimeout());</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 这里会维护 slot 相关列表</span></span><br><span class="line"><span class="keyword">final</span> TaskSlotTable taskSlotTable = <span class="keyword">new</span> TaskSlotTable(resourceProfiles, timerService);</span><br></pre></td></tr></table></figure>
<p>TaskSlotTable 的初始化，只需要两个变量：</p>
<ol>
<li><code>resourceProfiles</code>: TM 上每个 Slot 的资源信息；</li>
<li><code>timerService</code>: 超时检测服务，来保证操作超时时做相应的处理。</li>
</ol>
<p>TaskSlotTable 的启动流程如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor.java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// tell the task slot table who's responsible for the task slot actions</span></span><br><span class="line"><span class="comment">//note: taskSlotTable 启动</span></span><br><span class="line">taskSlotTable.start(<span class="keyword">new</span> SlotActionsImpl());</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: SlotActions 相关方法的实现</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">SlotActionsImpl</span> <span class="keyword">implements</span> <span class="title">SlotActions</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 释放 slot 资源</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">freeSlot</span><span class="params">(<span class="keyword">final</span> AllocationID allocationId)</span> </span>&#123;</span><br><span class="line">        runAsync(() -&gt;</span><br><span class="line">            freeSlotInternal(</span><br><span class="line">                allocationId,</span><br><span class="line">                <span class="keyword">new</span> FlinkException(<span class="string">"TaskSlotTable requested freeing the TaskSlot "</span> + allocationId + <span class="string">'.'</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 如果 slot 相关的操作（分配/释放）失败，这里将会调用这个方法</span></span><br><span class="line">    <span class="comment">//note: 监控的手段是：操作前先注册一个 timeout 监控，操作完成后再取消这个监控，如果在这个期间 timeout 了，就会调用这个方法</span></span><br><span class="line">    <span class="comment">//note: TimeService 的 key 是 AllocationID</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">timeoutSlot</span><span class="params">(<span class="keyword">final</span> AllocationID allocationId, <span class="keyword">final</span> UUID ticket)</span> </span>&#123;</span><br><span class="line">        runAsync(() -&gt; TaskExecutor.<span class="keyword">this</span>.timeoutSlot(allocationId, ticket));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-启动-JobLeaderService-服务"><a href="#4-启动-JobLeaderService-服务" class="headerlink" title="4. 启动 JobLeaderService 服务"></a>4. 启动 JobLeaderService 服务</h4><p>TaskExecutor 启动的最后一步是，启动 JobLeader 服务，这个服务通过 <code>JobLeaderListenerImpl</code> 监控 Job 的 JobManager leader 的变化，如果 leader 被选举出来之后，这里将会与新的 JobManager leader 建立通信连接。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor.java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// start the job leader service</span></span><br><span class="line"><span class="comment">//note: 启动 job leader 服务</span></span><br><span class="line">jobLeaderService.start(getAddress(), getRpcService(), haServices, <span class="keyword">new</span> JobLeaderListenerImpl());</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: JobLeaderListener 的实现</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">JobLeaderListenerImpl</span> <span class="keyword">implements</span> <span class="title">JobLeaderListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">jobManagerGainedLeadership</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobID jobId,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobMasterGateway jobManagerGateway,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JMTMRegistrationSuccess registrationMessage)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//note: 建立与 JobManager 的连接</span></span><br><span class="line">        runAsync(</span><br><span class="line">            () -&gt;</span><br><span class="line">                establishJobManagerConnection(</span><br><span class="line">                    jobId,</span><br><span class="line">                    jobManagerGateway,</span><br><span class="line">                    registrationMessage));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">jobManagerLostLeadership</span><span class="params">(<span class="keyword">final</span> JobID jobId, <span class="keyword">final</span> JobMasterId jobMasterId)</span> </span>&#123;</span><br><span class="line">        log.info(<span class="string">"JobManager for job &#123;&#125; with leader id &#123;&#125; lost leadership."</span>, jobId, jobMasterId);</span><br><span class="line"></span><br><span class="line">        runAsync(() -&gt;</span><br><span class="line">            closeJobManagerConnection(</span><br><span class="line">                jobId,</span><br><span class="line">                <span class="keyword">new</span> Exception(<span class="string">"Job leader for job id "</span> + jobId + <span class="string">" lost leadership."</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleError</span><span class="params">(Throwable throwable)</span> </span>&#123;</span><br><span class="line">        onFatalError(throwable);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里，TaskManager 的启动流程就梳理完了，TaskManager 在实现上整体的复杂度还是比较高的，毕竟它要做的事情是非常多的，下面的几个问题，将会进一步分析 TaskManager 内部的实现机制。</p>
<h2 id="TaskManager-提供了哪些能力-功能？"><a href="#TaskManager-提供了哪些能力-功能？" class="headerlink" title="TaskManager 提供了哪些能力/功能？"></a>TaskManager 提供了哪些能力/功能？</h2><p>要想知道 TaskManager 提供了哪些能力，个人认为有一个最简单有效的方法就是查看其对外提供的 API 接口，它向上层暴露哪些 API，这些 API 背后都是 TaskManager 能力的体现，TaskManager 对外的包括的 API 列表如下：</p>
<ol>
<li><code>requestSlot()</code>: RM 向 TM 请求一个 slot 资源；</li>
<li><code>requestStackTraceSample()</code>: 请求某个 task 在执行过程中的一个 stack trace 抽样；</li>
<li><code>submitTask()</code>: JobManager 向 TM 提交 task；</li>
<li><code>updatePartitions()</code>: 更新这个 task 对应的 Partition 信息；</li>
<li><code>releasePartitions()</code>: 释放这个 job 的所有中间结果，比如 close 的时候触发；</li>
<li><code>triggerCheckpoint()</code>: Checkpoint Coordinator 触发 task 的 checkpoint；</li>
<li><code>confirmCheckpoint()</code>: Checkpoint Coordinator 通知 task 这个 checkpoint 完成；</li>
<li><code>cancelTask()</code>: task 取消；</li>
<li><code>heartbeatFromJobManager()</code>: 接收来自 JobManager 的心跳请求；</li>
<li><code>heartbeatFromResourceManager()</code>: 接收来自 ResourceManager 的心跳请求；</li>
<li><code>disconnectJobManager()</code>；</li>
<li><code>disconnectResourceManager()</code>；</li>
<li><code>freeSlot()</code>: JobManager 释放 Slot；</li>
<li><code>requestFileUpload()</code>: 一些文件（log 等）的上传请求；</li>
<li><code>requestMetricQueryServiceAddress()</code>: 请求 TM 的 metric query service 地址；</li>
<li><code>canBeReleased()</code>: 检查 TM 是否可以被 realease；</li>
</ol>
<p>把上面的 API 列表分分类，大概有以下几块：</p>
<ol>
<li>slot 的资源管理：slot 的分配/释放；</li>
<li>task 运行：接收来自 JobManager 的 task 提交、也包括该 task 对应的 Partition（中间结果）信息；</li>
<li>checkpoint 相关的处理；</li>
<li>心跳监控、连接建立等。</li>
</ol>
<p>通常，可以任务 TaskManager 提供的功能主要是前三点，如下图所示：</p>
<p><img src="/images/flink/task-manager-function.png" alt="TaskManager 提供的功能"></p>
<h2 id="TaskManager-怎么发现-RM-leader（在使用-ZK-做-HA-的情况下）？"><a href="#TaskManager-怎么发现-RM-leader（在使用-ZK-做-HA-的情况下）？" class="headerlink" title="TaskManager 怎么发现 RM leader（在使用 ZK 做 HA 的情况下）？"></a>TaskManager 怎么发现 RM leader（在使用 ZK 做 HA 的情况下）？</h2><p>这个是 Flink HA 内容，Flink HA 机制是有一套统一的框架，它跟这个问题（<strong>TM 如何维护 JobManager 的关系，如果 JobManager 挂掉，TM 会如何处理？</strong> ）的原理是一样的，这里以 ResourceManager Leader 的发现为例简单介一下。</p>
<p>这里，我们以使用 Zookeeper 模式的情况来讲述，ZooKeeper 做 HA 是业内最常用的方案，Flink 在实现并没有使用 <code>ZkClient</code> 这个包，而是使用 <code>curator</code> 来做的（有兴趣可以看下这篇文章 <a href="https://colobu.com/2014/12/15/zookeeper-recipes-by-example-5/" target="_blank" rel="external">跟着实例学习ZooKeeper的用法： 缓存</a>）。</p>
<p>关于 Flink HA 的使用，可以参考官方文档——<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/jobmanager_high_availability.html" target="_blank" rel="external">JobManager High Availability (HA)</a>。这里 TaskExecutor 在注册完 <code>ResourceManagerLeaderListener</code> 后，如果 leader 被选举出来或者有节点有变化，就通过它的 <code>notifyLeaderAddress()</code> 方法来通知 TaskExecutor，核心还是利用了 ZK 的 watcher 机制。同理， JobManager leader 的处理也是一样。</p>
<h2 id="TM-Slot-资源是如何管理的？"><a href="#TM-Slot-资源是如何管理的？" class="headerlink" title="TM Slot 资源是如何管理的？"></a>TM Slot 资源是如何管理的？</h2><p>TaskManager Slot 资源的管理主要是在 TaskSlotTable 中处理的，slot 资源的申请与释放都通过 它处理的，相关的流程如下图所示（图中只描述了主要逻辑，相关的异常处理没有展示在图中）：</p>
<p><img src="/images/flink/tm-slot-manager.png" alt="TaskManager slot 的分配与释放"></p>
<h3 id="slot-的申请"><a href="#slot-的申请" class="headerlink" title="slot 的申请"></a>slot 的申请</h3><p>这里先看下 slot 资源请求的处理，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor.java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: slot 请求</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">requestSlot</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> SlotID slotId,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> JobID jobId,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> AllocationID allocationId,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> String targetAddress,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> ResourceManagerId resourceManagerId,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> Time timeout)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Filter invalid requests from the resource manager by using the instance/registration Id</span></span><br><span class="line"></span><br><span class="line">    log.info(<span class="string">"Receive slot request &#123;&#125; for job &#123;&#125; from resource manager with leader id &#123;&#125;."</span>,</span><br><span class="line">        allocationId, jobId, resourceManagerId);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (!isConnectedToResourceManager(resourceManagerId)) &#123;</span><br><span class="line">            <span class="comment">//note: 如果 TM 并没有跟这个 RM 通信，就抛出异常</span></span><br><span class="line">            <span class="keyword">final</span> String message = String.format(<span class="string">"TaskManager is not connected to the resource manager %s."</span>, resourceManagerId);</span><br><span class="line">            log.debug(message);</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TaskManagerException(message);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (taskSlotTable.isSlotFree(slotId.getSlotNumber())) &#123;</span><br><span class="line">            <span class="comment">//note: Slot 状态是 free，还未分配出去</span></span><br><span class="line">            <span class="keyword">if</span> (taskSlotTable.allocateSlot(slotId.getSlotNumber(), jobId, allocationId, taskManagerConfiguration.getTimeout())) &#123;</span><br><span class="line">                log.info(<span class="string">"Allocated slot for &#123;&#125;."</span>, allocationId);</span><br><span class="line">                <span class="comment">//note: allcate 成功</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.info(<span class="string">"Could not allocate slot for &#123;&#125;."</span>, allocationId);</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> SlotAllocationException(<span class="string">"Could not allocate slot."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!taskSlotTable.isAllocated(slotId.getSlotNumber(), jobId, allocationId)) &#123;</span><br><span class="line">            <span class="comment">//note: slot 已经分配出去，但分配的并不是当前这个作业</span></span><br><span class="line">            <span class="keyword">final</span> String message = <span class="string">"The slot "</span> + slotId + <span class="string">" has already been allocated for a different job."</span>;</span><br><span class="line"></span><br><span class="line">            log.info(message);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> AllocationID allocationID = taskSlotTable.getCurrentAllocation(slotId.getSlotNumber());</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SlotOccupiedException(message, allocationID, taskSlotTable.getOwningJob(allocationID));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (jobManagerTable.contains(jobId)) &#123;</span><br><span class="line">            <span class="comment">//note: 如果 TM 已经有这个 JobManager 的 meta，这里会将这个 job 的 slot 分配再汇报给 JobManager 一次</span></span><br><span class="line">            offerSlotsToJobManager(jobId);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">//note: 监控这个作业 JobManager 的 leader 变化</span></span><br><span class="line">                jobLeaderService.addJob(jobId, targetAddress);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                <span class="comment">// free the allocated slot</span></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    taskSlotTable.freeSlot(allocationId);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (SlotNotFoundException slotNotFoundException) &#123;</span><br><span class="line">                    <span class="comment">// slot no longer existent, this should actually never happen, because we've</span></span><br><span class="line">                    <span class="comment">// just allocated the slot. So let's fail hard in this case!</span></span><br><span class="line">                    onFatalError(slotNotFoundException);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// release local state under the allocation id.</span></span><br><span class="line">                localStateStoresManager.releaseLocalStateForAllocationId(allocationId);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// sanity check</span></span><br><span class="line">                <span class="keyword">if</span> (!taskSlotTable.isSlotFree(slotId.getSlotNumber())) &#123;</span><br><span class="line">                    onFatalError(<span class="keyword">new</span> Exception(<span class="string">"Could not free slot "</span> + slotId));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> SlotAllocationException(<span class="string">"Could not add job to job leader service."</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TaskManagerException taskManagerException) &#123;</span><br><span class="line">        <span class="keyword">return</span> FutureUtils.completedExceptionally(taskManagerException);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> CompletableFuture.completedFuture(Acknowledge.get());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>相应的处理逻辑如下：</p>
<ol>
<li>首先检测这个这个 RM 是否当前建立连接的 RM，如果不是，就抛出相应的异常，需要等到 TM 连接上 RM 之后才能处理 RM 上的 slot 请求；</li>
<li>判断这个 slot 是否可以分配<ul>
<li>如果 slot 是 <code>FREE</code> 状态，就进行分配（调用 TaskSlotTable 的 <code>allocateSlot()</code> 方法），如果分配失败，就抛出相应的异常；</li>
<li>如果 slot 已经分配，检查分配的是不是当前作业的 AllocationId，如果不是，也会抛出相应的异常，告诉 RM 这个 Slot 已经分配出去了；</li>
</ul>
</li>
<li>如果 TM 已经有了这个 JobManager 的 meta，这里会将这个 job 在这个 TM 上的 slot 分配再重新汇报给 JobManager 一次；</li>
</ol>
<p>而 TaskSlotTable 在处理 slot 的分配时，主要是根据内部缓存的信息做相应的检查，其 <code>allocateSlot()</code> 的方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskSlotTable.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">allocateSlot</span><span class="params">(<span class="keyword">int</span> index, JobID jobId, AllocationID allocationId, Time slotTimeout)</span> </span>&#123;</span><br><span class="line">    checkInit();</span><br><span class="line"></span><br><span class="line">    TaskSlot taskSlot = taskSlots.get(index);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 分配这个 TaskSlot</span></span><br><span class="line">    <span class="keyword">boolean</span> result = taskSlot.allocate(jobId, allocationId);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (result) &#123;</span><br><span class="line">        <span class="comment">//note: 分配成功，记录到缓存中</span></span><br><span class="line">        <span class="comment">// update the allocation id to task slot map</span></span><br><span class="line">        allocationIDTaskSlotMap.put(allocationId, taskSlot);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// register a timeout for this slot since it's in state allocated</span></span><br><span class="line">        timerService.registerTimeout(allocationId, slotTimeout.getSize(), slotTimeout.getUnit());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// add this slot to the set of job slots</span></span><br><span class="line">        Set&lt;AllocationID&gt; slots = slotsPerJob.get(jobId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (slots == <span class="keyword">null</span>) &#123;</span><br><span class="line">            slots = <span class="keyword">new</span> HashSet&lt;&gt;(<span class="number">4</span>);</span><br><span class="line">            slotsPerJob.put(jobId, slots);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        slots.add(allocationId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="slot-的释放"><a href="#slot-的释放" class="headerlink" title="slot 的释放"></a>slot 的释放</h3><p>这里再看下 Slot 的资源是如何释放的，代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor.java</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 释放这个 slot 资源</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">freeSlot</span><span class="params">(AllocationID allocationId, Throwable cause, Time timeout)</span> </span>&#123;</span><br><span class="line">    freeSlotInternal(allocationId, cause);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> CompletableFuture.completedFuture(Acknowledge.get());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 将本地分配的 slot 释放掉（free the slot）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">freeSlotInternal</span><span class="params">(AllocationID allocationId, Throwable cause)</span> </span>&#123;</span><br><span class="line">    checkNotNull(allocationId);</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Free slot with allocation id &#123;&#125; because: &#123;&#125;"</span>, allocationId, cause.getMessage());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> JobID jobId = taskSlotTable.getOwningJob(allocationId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 释放这个 slot</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> slotIndex = taskSlotTable.freeSlot(allocationId, cause);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (slotIndex != -<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="comment">//note: 成功释放掉的情况下</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isConnectedToResourceManager()) &#123;</span><br><span class="line">                <span class="comment">//note: 通知 ResourceManager 这个 slot 因为被释放了，所以可以变可用了</span></span><br><span class="line">                <span class="comment">// the slot was freed. Tell the RM about it</span></span><br><span class="line">                ResourceManagerGateway resourceManagerGateway = establishedResourceManagerConnection.getResourceManagerGateway();</span><br><span class="line"></span><br><span class="line">                resourceManagerGateway.notifySlotAvailable(</span><br><span class="line">                    establishedResourceManagerConnection.getTaskExecutorRegistrationId(),</span><br><span class="line">                    <span class="keyword">new</span> SlotID(getResourceID(), slotIndex),</span><br><span class="line">                    allocationId);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (jobId != <span class="keyword">null</span>) &#123;</span><br><span class="line">                closeJobManagerConnectionIfNoAllocatedResources(jobId);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (SlotNotFoundException e) &#123;</span><br><span class="line">        log.debug(<span class="string">"Could not free slot for allocation id &#123;&#125;."</span>, allocationId, e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 释放这个 allocationId 的相应状态信息</span></span><br><span class="line">    localStateStoresManager.releaseLocalStateForAllocationId(allocationId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结一下，TaskExecutor 在处理 slot 释放请求的理逻辑如下：</p>
<ol>
<li>先调用 TaskSlotTable 的 <code>freeSlot()</code> 方法，尝试释放这个 slot：<ul>
<li>如果这个 slot 没有 task 在运行，那么 slot 是可以释放的（状态更新为 <code>FREE</code>）;</li>
<li>先将 slot 状态更新为 <code>RELEASING</code>，然后再遍历这个 slot 上的 task，逐个将其标记为 failed；</li>
</ul>
</li>
<li>如果 slot 被成功释放（状态是 <code>FREE</code>），这里将会通知 RM 这个 slot 现在又可用了；</li>
<li>更新缓存信息。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇文章主要把 TaskManager 的启动流程及资源管理做了相应的讲述，正如文章中所述，TaskManager 主要有三大功能：slot 资源管理、task 的提交与运行以及 checkpoint 处理，在下篇文章中将会着重在 Task 的提交与运行上，checkpoint 处理部分将会 checkpoint 的文章中一起介绍。</p>
<p>最后，说一些个人的感想吧，我个人在看开源项目的源码时，慢慢开始感受到阅读优秀的开源代码对个人技术能力的提升是非常有帮助的，它不但会增加你对这个项目的熟悉程度，还会让你看到一些设计或方案在代码里是如何落地或实现的，如果换做是你，你会怎么设计或实现，经常看看这些优秀代码，多多思考（如果能把其中的设计或实现应用到自己的工作上那就更好不过了），这对自己工程能力的提升是有帮助的。</p>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/runtime.html" target="_blank" rel="external">Distributed Runtime Environment</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/TBzzGTNFTzVLjFQdzz-LuQ" target="_blank" rel="external">Apache Flink 进阶（一）：Runtime 核心机制剖析 ​</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第七篇，这篇文章主要会讲述 Flink 中的 TaskManager 的一些内容，TaskManager 是 Flink 的 worker 节点，它负责 Flink 中本机 slot 资源的管理以及具体 ta
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink JobManager 详解</title>
    <link href="http://matt33.com/2019/12/27/flink-jobmanager-6/"/>
    <id>http://matt33.com/2019/12/27/flink-jobmanager-6/</id>
    <published>2019-12-27T02:16:26.000Z</published>
    <updated>2020-06-23T14:13:17.226Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第六篇，紧接着上篇文章，本篇主要讲述 Flink Master 中另一个组件 —— JobManager（在源码中对应的实现类是 <code>JobMaster</code>）。每个作业在启动后，Dispatcher 都会为这个作业创建一个 JobManager 对象，用来做这个作业相关的协调工作，比如：调度这个作业的 task、触发 Checkpoint 以及作业的容错恢复等。另外，本篇文章也将会看下一个作业在生成 ExecutionGraph 之后是如何在集群中调度起来的。</p>
<p>从之前文章的介绍中，我们已经知道 JobManager 其实就是一个作业的 master 服务，主要负责自己作业相关的协调工作，包括：向 ResourceManager 申请 Slot 资源来调度相应的 task 任务、定时触发作业的 checkpoint 和手动 savepoint 的触发、以及作业的容错恢复，这些流程将会在后面的系列文章中介绍（这些流程涉及到的组件比较多，需要等待后面把 TaskManager 及 Flink 的调度模型讲述完再回头来看），本文会从 JobManager 是如何初始化的、JobManager 有哪些组件以及分别提供了哪些功能这两块来讲述。</p>
<h2 id="JobManager-简介"><a href="#JobManager-简介" class="headerlink" title="JobManager 简介"></a>JobManager 简介</h2><p>当用户向 Flink 集群提交一个作业后，Dispatcher 在收到 Client 端提交的 JobGraph 后，会为这个作业创建一个 JobManager 对象（对应的是 JobMaster 类），如下图所示：</p>
<p><img src="/images/flink/4-job-start.png" alt="一个新作业提交后的处理流程"></p>
<p>JobManager 在初始化时，会创建 <code>LegacyScheduler</code> 对象，而 <code>LegacyScheduler</code> 在初始化时会将这个作业的 JobGraph 转化为 ExecutionGraph。在JobManager 启动后，就会开始给这个作业的 task 申请相应的资源、开始调度执行这个作业。</p>
<h2 id="JobManager-详解"><a href="#JobManager-详解" class="headerlink" title="JobManager 详解"></a>JobManager 详解</h2><p>JobMaster 在实现中，也依赖了很多的服务，其中最重要的是 <code>SchedulerNG</code> 和 <code>SlotPool</code>，JobMaster 对外提供的接口实现中大都是使用前面这两个服务的方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JobMaster.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMaster</span> <span class="keyword">extends</span> <span class="title">FencedRpcEndpoint</span>&lt;<span class="title">JobMasterId</span>&gt; <span class="keyword">implements</span> <span class="title">JobMasterGateway</span>, <span class="title">JobMasterService</span> </span>&#123;</span><br><span class="line">    <span class="comment">// LegacyScheduler: 用于调度作业的 ExecutionGraph</span></span><br><span class="line">    <span class="keyword">private</span> SchedulerNG schedulerNG;</span><br><span class="line">    <span class="comment">// SlotPoolImpl: 从名字也能看出它主要处理 slot 相关的内容，在 JM 这边的一个抽象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> SlotPool slotPool;</span><br><span class="line">    <span class="comment">// HA 服务，这里主要用于监控 RM leader，如果 RM Leader 有变化，这里会与新的 leader 建立连接</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> HighAvailabilityServices highAvailabilityServices;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 下面这些都是创建上面 SchedulerNG（即 LegacyScheduler）需要使用到的服务</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="comment">// 用于将数据上传到 BlobServer，这里上传的主要是 JobInformation 和 TaskInformation</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> BlobWriter blobWriter;</span><br><span class="line">    <span class="comment">// 作业的 JobGraph 信息</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> JobGraph jobGraph;</span><br><span class="line">    <span class="comment">// SchedulerImpl: 它也是一个调度器，将 slot 分配给对应的 task，它会调用 SlotPool 的相关接口（它里面有一个 slotSelectionStrategy 对象，用来决定一个 slot 分配的最佳算法）</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Scheduler scheduler;</span><br><span class="line">    <span class="comment">// 用于注册 Intermediate result partition，在作业调度的时候会用到</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ShuffleMaster&lt;?&gt; shuffleMaster;</span><br><span class="line">    <span class="comment">// 用于追踪 Intermediate result partition 的服务</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> PartitionTracker partitionTracker;</span><br><span class="line">    <span class="comment">// --------- BackPressure --------</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> BackPressureStatsTracker backPressureStatsTracker;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>JobMaster 中涉及到重要组件如下图所示：</p>
<p><img src="/images/flink/6-job-manager.png" alt="JobMaster 中的组件组成"></p>
<p>JobMaster 主要有两个服务:</p>
<ol>
<li><code>LegacyScheduler</code>: ExecutionGraph 相关的调度都是在这里实现的，它类似更深层的抽象，封装了 ExecutionGraph 和 BackPressureStatsTracker，JobMaster 不直接去调用 ExecutionGraph 和 BackPressureStatsTracker 的相关方法，都是通过 <code>LegacyScheduler</code> 间接去调用；</li>
<li><code>SlotPool</code>: 它是 JobMaster 管理其 slot 的服务，它负责向 RM 申请/释放 slot 资源，并维护其相应的 slot 信息。</li>
</ol>
<p>从前面的图中可以看出，如果 <code>LegacyScheduler</code> 想调用 <code>CheckpointCoordinator</code> 的方法，比如 <code>LegacyScheduler</code> 的 <code>triggerSavepoint()</code> 方法，它是需要先通过 <code>executionGraph</code> 的 <code>getCheckpointCoordinator()</code> 方法拿到 <code>CheckpointCoordinator</code>，然后再调用 <code>CheckpointCoordinator</code> 的 <code>triggerSavepoint()</code> 方法来触发这个作业的 savepoint。</p>
<h3 id="JobMaster-的-API-概述"><a href="#JobMaster-的-API-概述" class="headerlink" title="JobMaster 的 API 概述"></a>JobMaster 的 API 概述</h3><p>目前 JobMaster 对外提供的 API 列表如下（主要还是 <code>JobMasterGateway</code> 接口对应的实现）：</p>
<ol>
<li><code>cancel()</code>: 取消当前正在执行的作业，如果作业还在调度，会执行停止，如果作业正在运行的话，它会向对应的 TM 发送取消 task 的请求（<code>cancelTask()</code> 请求）；</li>
<li><code>updateTaskExecutionState()</code>: 更新某个 task 的状态信息，这个是 TM 主动向 JM 发送的更新请求；</li>
<li><code>requestNextInputSplit()</code>: Source ExecutionJobVertex 请求 next InputSlipt，这个一般是针对批处理读取而言，有兴趣的可以看下 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface" target="_blank" rel="external">FLIP-27: Refactor Source Interface</a>，这里是社区计划对 Source 做的改进，未来会将批和流统一到一起；</li>
<li><code>requestPartitionState()</code>: 获取指定 Result Partition 对应生产者 JobVertex 的执行状态；</li>
<li><code>scheduleOrUpdateConsumers()</code>: TM 通知 JM 对应的 Result Partition 的数据已经可用，每个 ExecutionVertex 的每个 ResultPartition 都会调用一次这个方法（可能是在第一次生产数据时调用或者所有数据已经就绪时调用）；</li>
<li><code>disconnectTaskManager()</code>: TM 心跳超时或者作业取消时，会调用这个方法，JM 会释放这个 TM 上的所有 slot 资源；</li>
<li><code>acknowledgeCheckpoint()</code>: 当一个 Task 做完 snapshot 后，通过这个接口通知 JM，JM 再做相应的处理，如果这个 checkpoint 所有的 task 都已经 ack 了，那就意味着这个 checkpoint 完成了；</li>
<li><code>declineCheckpoint()</code>: TM 向 JM 发送这个消息，告诉 JM 的 Checkpoint Coordinator 这个 checkpoint request 没有响应，比如：TM 触发 checkpoint 失败，然后 Checkpoint Coordinator 就会知道这个 checkpoint 处理失败了，再做相应的处理；</li>
<li><code>requestKvStateLocation()</code>: 请求某个注册过 registrationName 对应的 KvState 的位置信息；</li>
<li><code>notifyKvStateRegistered()</code>: 当注册一个 KvState 的时候，会调用这个方法，一些 operator 在初始化的时候会调用这个方法注册一个 KvState；</li>
<li><code>notifyKvStateUnregistered()</code>: 取消一个 KVState 的注册，这里是在 operator 关闭 state backend 时调用的（比如：operator 的生命周期结束了，就会调用这个方法）；</li>
<li><code>offerSlots()</code>: TM 通知 JM 其上分配到的 slot 列表；</li>
<li><code>failSlot()</code>: 如果 TM 分配 slot 失败（情况可能很多，比如：slot 分配时状态转移失败等），将会通过这个接口告知 JM；</li>
<li><code>registerTaskManager()</code>: 向这个 JM 注册 TM，JM 会将 TM 注册到 SlotPool 中（只有注册过的 TM 的 Slot 才被认为是有效的，才可以做相应的分配），并且会通过心跳监控对应的 TM；</li>
<li><code>disconnectResourceManager()</code>: 与 ResourceManager 断开连接，这个是有三种情况会触发，JM 与 ResourceManager 心跳超时、作业取消、重连 RM 时会断开连接（比如：RM leader 切换、RM 的心跳超时）；</li>
<li><code>heartbeatFromTaskManager()</code>: TM 向 JM 发送心跳信息；</li>
<li><code>heartbeatFromResourceManager()</code>: JM 向 ResourceManager 发送一个心跳信息，ResourceManager 只会监听 JM 是否超时；</li>
<li><code>requestJobDetails()</code>: 请求这个作业的 <code>JobDetails</code>（作业的概况信息，比如：作业执行了多长时间、作业状态等）；</li>
<li><code>requestJobStatus()</code>: 请求这个作业的执行状态 <code>JobStatus</code>；</li>
<li><code>requestJob()</code>: 请求这个作业的 <code>ArchivedExecutionGraph</code>（它是 <code>ExecutionGraph</code> 序列化之后的结果）；</li>
<li><code>triggerSavepoint()</code>: 对这个作业触发一次 savepoint；</li>
<li><code>stopWithSavepoint()</code>: 停止作业前触发一次 savepoint（触发情况是：用户手动停止作业时指定一个 savepoint 路径，这样的话，会在停止前做一次 savepoint）；</li>
<li><code>requestOperatorBackPressureStats()</code>: 汇报某个 operator 反压的情况；</li>
<li><code>notifyAllocationFailure()</code>: 如果 RM 分配 slot 失败的话，将会通过这个接口通知 JM；</li>
</ol>
<p>这里可以看到有部分接口的方法是在跟 RM 通信使用的，所以在 RM 的接口中也可以看到对应的方法。另外，JobMaster 上面这些方法在实现时基本都是在调用 <code>LegacyScheduler</code> 或 <code>SlotPool</code> 的具体实现方法来实现的。</p>
<h3 id="SlotPool"><a href="#SlotPool" class="headerlink" title="SlotPool"></a>SlotPool</h3><p>SlotPool 是为当前作业的 slot 请求而服务的，它会向 ResourceManager 请求 slot 资源；SlotPool 会维护请求到的 slot 列表信息（即使 ResourceManager 挂掉了，SlotPool 也可以使用当前作业空闲的 slot 资源进行分配），而如果一个 slot 不再使用的话，即使作业在运行，也是可以释放掉的（所有的 slot 都是通过 <code>AllocationID</code> 来区分的）。</p>
<p>目前 SlotPool 提供的 API 列表如下：</p>
<ol>
<li><code>connectToResourceManager()</code>: SlotPool 与 ResourceManager 建立连接，之后 SlotPool 就可以向 ResourceManager 请求 slot 资源了；</li>
<li><code>disconnectResourceManage()</code>: SlotPool 与 ResourceManager 断开连接，这个方法被调用后，SlotPool 就不能从 ResourceManager 请求 slot 资源了，并且所有正在排队等待的 Slot Request 都被取消；</li>
<li><code>allocateAvailableSlot()</code>: 将指定的 Slot Request 分配到指定的 slot 上，这里只是记录其对应关系（哪个 slot 对应哪个 slot 请求）；</li>
<li><code>releaseSlot()</code>: 释放一个 slot； </li>
<li><code>requestNewAllocatedSlot()</code>: 从 RM 请求一个新的 slot 资源分配，申请到的 slot 之后也会添加到 SlotPool 中；</li>
<li><code>requestNewAllocatedBatchSlot()</code>: 上面的方法是 Stream 类型，这里是 batch 类型，但向 RM 申请的时候，这里并没有区别，只是为了做相应的标识；</li>
<li><code>getAvailableSlotsInformation()</code>: 获取当前可用的 slot 列表；</li>
<li><code>failAllocation()</code>: 分配失败，并释放相应的 slot，可能是因为请求超时由 JM 触发或者 TM 分配失败；</li>
<li><code>registerTaskManager()</code>: 注册 TM，这里会记录一下注册过来的 TM，只能向注册过来的 TM 分配 slot；</li>
<li><code>releaseTaskManager()</code>: 注销 TM，这个 TM 相关的 slot 都会被释放，task 将会被取消，SlotPool 会通知相应的 TM 释放其 slot；</li>
<li><code>createAllocatedSlotReport()</code>: 汇报指定 TM 上的 slot 分配情况；</li>
</ol>
<p>通过上面 SlotPool 对外提供的 API 列表，可以看到其相关方法都是跟 Slot 相关的，整体可以分为下面几部分：</p>
<ol>
<li>与 ResourceManager 建立/取消 连接；</li>
<li>注册/注销 TM，这里只是记录注册过 TM 列表，只有是注册过的 TM 才允许使用其上面的 slot 资源；</li>
<li>向 ResourceManager 请求 slot 资源；</li>
<li>分配/释放 slot，这里只是更新其状态信息，并不做实质的操作。</li>
</ol>
<p>SlotPool 这里，更多只是维护一个状态信息，以及与 ResourceManager（请求 slot 资源）和 TM（释放对应的 slot）做一些交互工作，它对这些功能做了相应的封装，方便 JobMaster 来调用。</p>
<h3 id="LegacyScheduler"><a href="#LegacyScheduler" class="headerlink" title="LegacyScheduler"></a>LegacyScheduler</h3><p>如前面所述，LegacyScheduler 其实是对 <code>ExecutionGraph</code> 和 <code>BackPressureStatsTracker</code> 方法的一个抽象，它还负责为作业创建对应的 ExecutionGraph 以及对这个作业进行调度。关于 LegacyScheduler 提供的 API 这里就不再展开，有兴趣的可以直接看下源码，它提供的大部分 API 都是在 JobMaster 的 API 列表中，因为 JobMaster 的很多方法实现本身就是调用 LegacyScheduler 对应的方法。</p>
<h2 id="作业调度的详细流程"><a href="#作业调度的详细流程" class="headerlink" title="作业调度的详细流程"></a>作业调度的详细流程</h2><p>有了前面的讲述，这里看下一个新提交的作业，JobMaster 是如何调度起来的。当 JobMaster 调用 LegacyScheduler 的 <code>startScheduling()</code> 方法后，就会开始对这个作业进行相应的调度，申请对应的 slot，并部署 task，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// LegacyScheduler.java</span></span><br><span class="line"><span class="comment">//note: ExecutionGraph 开始调度</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startScheduling</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//note: 启动这个线程</span></span><br><span class="line">    mainThreadExecutor.assertRunningInMainThread();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 调度这个 graph</span></span><br><span class="line">        executionGraph.scheduleForExecution();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        executionGraph.failGlobal(t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>一个作业开始调度后详细流程如下图所示（其中比较核心方法已经标成黄颜色）：</p>
<p><img src="/images/flink/6-job-schedule.png" alt="一个作业调度的详细流程"></p>
<p>ExecutionGraph 通过 <code>scheduleForExecution()</code> 方法对这个作业调度执行，其方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">/note: 把 CREATED 状态转换为 RUNNING 状态，并做相应的调度，如果有异常这里会抛出</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">scheduleForExecution</span><span class="params">()</span> <span class="keyword">throws</span> JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    assertRunningInJobMasterMainThread();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> currentGlobalModVersion = globalModVersion;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 先将作业状态转移为 RUNNING</span></span><br><span class="line">    <span class="keyword">if</span> (transitionState(JobStatus.CREATED, JobStatus.RUNNING)) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 这里会真正调度相应的 Execution Graph</span></span><br><span class="line">        <span class="keyword">final</span> CompletableFuture&lt;Void&gt; newSchedulingFuture = SchedulingUtils.schedule(</span><br><span class="line">            scheduleMode,</span><br><span class="line">            getAllExecutionVertices(),</span><br><span class="line">            <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (state == JobStatus.RUNNING &amp;&amp; currentGlobalModVersion == globalModVersion) &#123;</span><br><span class="line">            schedulingFuture = newSchedulingFuture;</span><br><span class="line">            <span class="comment">//note: 前面调度完成后，如果最后的结果有异常，这里会做相应的处理</span></span><br><span class="line">            newSchedulingFuture.whenComplete(</span><br><span class="line">                (Void ignored, Throwable throwable) -&gt; &#123;</span><br><span class="line">                    <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">final</span> Throwable strippedThrowable = ExceptionUtils.stripCompletionException(throwable);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (!(strippedThrowable <span class="keyword">instanceof</span> CancellationException)) &#123;</span><br><span class="line">                            <span class="comment">// only fail if the scheduling future was not canceled</span></span><br><span class="line">                            failGlobal(strippedThrowable);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            newSchedulingFuture.cancel(<span class="keyword">false</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Job may only be scheduled from state "</span> + JobStatus.CREATED);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>配合前面图中的流程，接下来，看下这个作业在 SchedulingUtils 中是如何调度的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SchedulingUtils.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CompletableFuture&lt;Void&gt; <span class="title">schedule</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ScheduleMode scheduleMode,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Iterable&lt;ExecutionVertex&gt; vertices,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> ExecutionGraph executionGraph)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (scheduleMode) &#123;</span><br><span class="line">        <span class="comment">// LAZY 的意思是：是有上游数据就绪后，下游的 task 才能调度，这个主要是批场景会用到，流不能走这个模式</span></span><br><span class="line">        <span class="keyword">case</span> LAZY_FROM_SOURCES:</span><br><span class="line">        <span class="keyword">case</span> LAZY_FROM_SOURCES_WITH_BATCH_SLOT_REQUEST:</span><br><span class="line">            <span class="keyword">return</span> scheduleLazy(vertices, executionGraph);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 流默认的是这个调度模式</span></span><br><span class="line">        <span class="keyword">case</span> EAGER:</span><br><span class="line">            <span class="keyword">return</span> scheduleEager(vertices, executionGraph);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(String.format(<span class="string">"Schedule mode %s is invalid."</span>, scheduleMode));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Schedule vertices eagerly. That means all vertices will be scheduled at once.</span></span><br><span class="line"><span class="comment"> * note: 所有的节点会被同时调度</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> vertices Topologically sorted vertices to schedule.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> executionGraph The graph the given vertices belong to.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> CompletableFuture&lt;Void&gt; <span class="title">scheduleEager</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Iterable&lt;ExecutionVertex&gt; vertices,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> ExecutionGraph executionGraph)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    executionGraph.assertRunningInJobMasterMainThread();</span><br><span class="line"></span><br><span class="line">    checkState(executionGraph.getState() == JobStatus.RUNNING, <span class="string">"job is not running currently"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Important: reserve all the space we need up front.</span></span><br><span class="line">    <span class="comment">// that way we do not have any operation that can fail between allocating the slots</span></span><br><span class="line">    <span class="comment">// and adding them to the list. If we had a failure in between there, that would</span></span><br><span class="line">    <span class="comment">// cause the slots to get lost</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// collecting all the slots may resize and fail in that operation without slots getting lost</span></span><br><span class="line">    <span class="keyword">final</span> ArrayList&lt;CompletableFuture&lt;Execution&gt;&gt; allAllocationFutures = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> SlotProviderStrategy slotProviderStrategy = executionGraph.getSlotProviderStrategy();</span><br><span class="line">    <span class="keyword">final</span> Set&lt;AllocationID&gt; allPreviousAllocationIds = Collections.unmodifiableSet(</span><br><span class="line">        computePriorAllocationIdsIfRequiredByScheduling(vertices, slotProviderStrategy.asSlotProvider()));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// allocate the slots (obtain all their futures)</span></span><br><span class="line">    <span class="keyword">for</span> (ExecutionVertex ev : vertices) &#123;</span><br><span class="line">        <span class="comment">// these calls are not blocking, they only return futures</span></span><br><span class="line">        <span class="comment">//note: 给每个 Execution 分配相应的资源</span></span><br><span class="line">        CompletableFuture&lt;Execution&gt; allocationFuture = ev.getCurrentExecutionAttempt().allocateResourcesForExecution(</span><br><span class="line">            slotProviderStrategy,</span><br><span class="line">            LocationPreferenceConstraint.ALL,</span><br><span class="line">            allPreviousAllocationIds);</span><br><span class="line"></span><br><span class="line">        allAllocationFutures.add(allocationFuture);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// this future is complete once all slot futures are complete.</span></span><br><span class="line">    <span class="comment">// the future fails once one slot future fails.</span></span><br><span class="line">    <span class="keyword">final</span> ConjunctFuture&lt;Collection&lt;Execution&gt;&gt; allAllocationsFuture = FutureUtils.combineAll(allAllocationFutures);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> allAllocationsFuture.thenAccept(</span><br><span class="line">        (Collection&lt;Execution&gt; executionsToDeploy) -&gt; &#123;</span><br><span class="line">            <span class="keyword">for</span> (Execution execution : executionsToDeploy) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">//note: 部署每个 Execution</span></span><br><span class="line">                    execution.deploy();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(</span><br><span class="line">                        <span class="keyword">new</span> FlinkException(</span><br><span class="line">                            String.format(<span class="string">"Could not deploy execution %s."</span>, execution),</span><br><span class="line">                            t));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment">// Generate a more specific failure message for the eager scheduling</span></span><br><span class="line">        .exceptionally(</span><br><span class="line">            <span class="comment">//...</span></span><br><span class="line">        );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于对于流作业来说，它默认的调度模式（<code>ScheduleMode</code>）是 <code>ScheduleMode.EAGER</code>，也就是说，所有 task 会同时调度起来，上面的代码里也可以看到调度的时候有两个主要方法：</p>
<ol>
<li><code>allocateResourcesForExecution()</code>: 它的作用是给这个 Execution 分配资源，获取要分配的 slot（它还会向 ShuffleMaster 注册 produced partition，这个 shuffle 部分内容后面文章再讲述，这里就不展开了）；</li>
<li><code>deploy()</code>: 这个方法会直接向 TM 提交这个 task 任务；</li>
</ol>
<p>这里，主要展开一下 <code>allocateResourcesForExecution()</code> 方法的实现，<code>deploy()</code> 的实现将会在后面 TaskManager 这篇文章中讲述。</p>
<h3 id="如何给-ExecutionVertex-分配-slot"><a href="#如何给-ExecutionVertex-分配-slot" class="headerlink" title="如何给 ExecutionVertex 分配 slot"></a>如何给 ExecutionVertex 分配 slot</h3><p>通过前面的代码，我们知道，<code>allocateResourcesForExecution()</code> 方法会给每一个 ExecutionVertex 分配一个 slot，而它具体是如何分配的，这个流程是在 Execution 的 <code>allocateAndAssignSlotForExecution()</code> 方法中实现的，代码如下如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Allocates and assigns a slot obtained from the slot provider to the execution.</span></span><br><span class="line"><span class="comment"> * note: 从 slot provider 获取一个 slot，将任务分配到这个 slot 上</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> slotProviderStrategy to obtain a new slot from</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> locationPreferenceConstraint constraint for the location preferences</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> allPreviousExecutionGraphAllocationIds set with all previous allocation ids in the job graph.</span></span><br><span class="line"><span class="comment"> *                                                 Can be empty if the allocation ids are not required for scheduling.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Future which is completed with the allocated slot once it has been assigned</span></span><br><span class="line"><span class="comment"> *          or with an exception if an error occurred.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;LogicalSlot&gt; <span class="title">allocateAndAssignSlotForExecution</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        SlotProviderStrategy slotProviderStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">        LocationPreferenceConstraint locationPreferenceConstraint,</span></span></span><br><span class="line"><span class="function"><span class="params">        @Nonnull Set&lt;AllocationID&gt; allPreviousExecutionGraphAllocationIds)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    checkNotNull(slotProviderStrategy);</span><br><span class="line"></span><br><span class="line">    assertRunningInJobMasterMainThread();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 获取这个 vertex 的相关信息</span></span><br><span class="line">    <span class="keyword">final</span> SlotSharingGroup sharingGroup = vertex.getJobVertex().getSlotSharingGroup();</span><br><span class="line">    <span class="keyword">final</span> CoLocationConstraint locationConstraint = vertex.getLocationConstraint();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sanity check</span></span><br><span class="line">    <span class="comment">//note: 做相应的检查</span></span><br><span class="line">    <span class="keyword">if</span> (locationConstraint != <span class="keyword">null</span> &amp;&amp; sharingGroup == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line">                <span class="string">"Trying to schedule with co-location constraint but without slot sharing allowed."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// this method only works if the execution is in the state 'CREATED'</span></span><br><span class="line">    <span class="comment">//note: 这个只会在 CREATED 下工作</span></span><br><span class="line">    <span class="keyword">if</span> (transitionState(CREATED, SCHEDULED)) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> SlotSharingGroupId slotSharingGroupId = sharingGroup != <span class="keyword">null</span> ? sharingGroup.getSlotSharingGroupId() : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建一个 ScheduledUnit 对象（跟 sharingGroup/locationConstraint 都有关系）</span></span><br><span class="line">        ScheduledUnit toSchedule = locationConstraint == <span class="keyword">null</span> ?</span><br><span class="line">                <span class="keyword">new</span> ScheduledUnit(<span class="keyword">this</span>, slotSharingGroupId) :</span><br><span class="line">                <span class="keyword">new</span> ScheduledUnit(<span class="keyword">this</span>, slotSharingGroupId, locationConstraint);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// try to extract previous allocation ids, if applicable, so that we can reschedule to the same slot</span></span><br><span class="line">        <span class="comment">//note: 如果能找到之前调度的 AllocationID，会尽量先重新调度在同一个 slot 上</span></span><br><span class="line">        ExecutionVertex executionVertex = getVertex();</span><br><span class="line">        AllocationID lastAllocation = executionVertex.getLatestPriorAllocation();</span><br><span class="line"></span><br><span class="line">        Collection&lt;AllocationID&gt; previousAllocationIDs =</span><br><span class="line">            lastAllocation != <span class="keyword">null</span> ? Collections.singletonList(lastAllocation) : Collections.emptyList();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// calculate the preferred locations</span></span><br><span class="line">        <span class="comment">//note: 这里先根据 state 和上游数据的输入节点获取这个 Task Execution 的最佳 TM location</span></span><br><span class="line">        <span class="keyword">final</span> CompletableFuture&lt;Collection&lt;TaskManagerLocation&gt;&gt; preferredLocationsFuture =</span><br><span class="line">            calculatePreferredLocations(locationPreferenceConstraint);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> SlotRequestId slotRequestId = <span class="keyword">new</span> SlotRequestId();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 根据指定的需求分配这个 slot</span></span><br><span class="line">        <span class="keyword">final</span> CompletableFuture&lt;LogicalSlot&gt; logicalSlotFuture =</span><br><span class="line">            preferredLocationsFuture.thenCompose(</span><br><span class="line">                (Collection&lt;TaskManagerLocation&gt; preferredLocations) -&gt;</span><br><span class="line">                    slotProviderStrategy.allocateSlot(</span><br><span class="line">                        slotRequestId,</span><br><span class="line">                        toSchedule,</span><br><span class="line">                        <span class="keyword">new</span> SlotProfile(</span><br><span class="line">                            vertex.getResourceProfile(),</span><br><span class="line">                            preferredLocations,</span><br><span class="line">                            previousAllocationIDs,</span><br><span class="line">                            allPreviousExecutionGraphAllocationIds)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// register call back to cancel slot request in case that the execution gets canceled</span></span><br><span class="line">        releaseFuture.whenComplete(</span><br><span class="line">            (Object ignored, Throwable throwable) -&gt; &#123;</span><br><span class="line">                <span class="keyword">if</span> (logicalSlotFuture.cancel(<span class="keyword">false</span>)) &#123;</span><br><span class="line">                    slotProviderStrategy.cancelSlotRequest(</span><br><span class="line">                        slotRequestId,</span><br><span class="line">                        slotSharingGroupId,</span><br><span class="line">                        <span class="keyword">new</span> FlinkException(<span class="string">"Execution "</span> + <span class="keyword">this</span> + <span class="string">" was released."</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// This forces calls to the slot pool back into the main thread, for normal and exceptional completion</span></span><br><span class="line">        <span class="comment">//note: 返回 LogicalSlot</span></span><br><span class="line">        <span class="keyword">return</span> logicalSlotFuture.handle(</span><br><span class="line">            (LogicalSlot logicalSlot, Throwable failure) -&gt; &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (failure != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(failure);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (tryAssignResource(logicalSlot)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> logicalSlot;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// release the slot</span></span><br><span class="line">                    logicalSlot.releaseSlot(<span class="keyword">new</span> FlinkException(<span class="string">"Could not assign logical slot to execution "</span> + <span class="keyword">this</span> + <span class="string">'.'</span>));</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(</span><br><span class="line">                        <span class="keyword">new</span> FlinkException(</span><br><span class="line">                            <span class="string">"Could not assign slot "</span> + logicalSlot + <span class="string">" to execution "</span> + <span class="keyword">this</span> + <span class="string">" because it has already been assigned "</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// call race, already deployed, or already done</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalExecutionStateException(<span class="keyword">this</span>, CREATED, state);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里，简单总结一下上面这个方法的流程：</p>
<ol>
<li>状态转换，将这个 Execution 的状态（<code>ExecutionState</code>）从 <code>CREATED</code> 转为 <code>SCHEDULED</code> 状态；</li>
<li>根据是否是一个有状态的 operator 以及它上游输入节点位置，来计算一个最佳的 TM 位置列表（<code>TaskManagerLocation</code>）列表；</li>
<li>如果这个 Execution 之前有调度记录，也就是说，这次由 failover 导致的重启，这里会拿到上次调度的 TM 位置信息；</li>
<li>根据 2、3 拿到 TM 位置信息，去调用 SlotProviderStrategy 的 <code>allocateSlot()</code> 获取要分配的 slot。</li>
</ol>
<p>在 SchedulerImpl 去分配 slot 的时候，其实是会分两种情况的：</p>
<ol>
<li><code>allocateSingleSlot()</code>: 如果对应的 task 节点没有设置 SlotSharingGroup，会直接走这个方法，就不会考虑 share group 的情况，直接给这个 task 分配对应的 slot；</li>
<li><code>allocateSharedSlot()</code>: 如果对应的 task 节点有设置 SlotSharingGroup，就会走到这个方法，在分配 slot 的时候，考虑的因素就会多一些。</li>
</ol>
<h3 id="分配时如何选择最优的-TM-列表"><a href="#分配时如何选择最优的-TM-列表" class="headerlink" title="分配时如何选择最优的 TM 列表"></a>分配时如何选择最优的 TM 列表</h3><p>这里，我们先来看下如何给这个 slot 选择一个最佳的 TM 列表，具体的方法实现是在 <code>Execution</code> 中的 <code>calculatePreferredLocations()</code> 方法中实现的，其具体的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Execution.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Calculates the preferred locations based on the location preference constraint.</span></span><br><span class="line"><span class="comment"> * note: 根据 LocationPreferenceConstraint 策略计算前置输入节点的 TaskManagerLocation</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> locationPreferenceConstraint constraint for the location preference</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Future containing the collection of preferred locations. This might not be completed if not all inputs</span></span><br><span class="line"><span class="comment"> *      have been a resource assigned.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="keyword">public</span> CompletableFuture&lt;Collection&lt;TaskManagerLocation&gt;&gt; calculatePreferredLocations(LocationPreferenceConstraint locationPreferenceConstraint) &#123;</span><br><span class="line">    <span class="comment">//note: 获取一个最佳分配的 TM location 集合</span></span><br><span class="line">    <span class="keyword">final</span> Collection&lt;CompletableFuture&lt;TaskManagerLocation&gt;&gt; preferredLocationFutures = getVertex().getPreferredLocations();</span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Collection&lt;TaskManagerLocation&gt;&gt; preferredLocationsFuture;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span>(locationPreferenceConstraint) &#123;</span><br><span class="line">        <span class="keyword">case</span> ALL:</span><br><span class="line">            <span class="comment">//note: 默认是 ALL，就是前面拿到的列表，这里都可以使用</span></span><br><span class="line">            preferredLocationsFuture = FutureUtils.combineAll(preferredLocationFutures);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> ANY:</span><br><span class="line">            <span class="comment">//note: 遍历所有 input，先获取已经完成 assign 的 input 列表</span></span><br><span class="line">            <span class="keyword">final</span> ArrayList&lt;TaskManagerLocation&gt; completedTaskManagerLocations = <span class="keyword">new</span> ArrayList&lt;&gt;(preferredLocationFutures.size());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (CompletableFuture&lt;TaskManagerLocation&gt; preferredLocationFuture : preferredLocationFutures) &#123;</span><br><span class="line">                <span class="keyword">if</span> (preferredLocationFuture.isDone() &amp;&amp; !preferredLocationFuture.isCompletedExceptionally()) &#123;</span><br><span class="line">                    <span class="comment">//note: 在这个 future 完成（没有异常的情况下），这里会使用这个 taskManagerLocation 对象</span></span><br><span class="line">                    <span class="keyword">final</span> TaskManagerLocation taskManagerLocation = preferredLocationFuture.getNow(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (taskManagerLocation == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> FlinkRuntimeException(<span class="string">"TaskManagerLocationFuture was completed with null. This indicates a programming bug."</span>);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    completedTaskManagerLocations.add(taskManagerLocation);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            preferredLocationsFuture = CompletableFuture.completedFuture(completedTaskManagerLocations);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Unknown LocationPreferenceConstraint "</span> + locationPreferenceConstraint + <span class="string">'.'</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> preferredLocationsFuture;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的实现可以看出，这里是先通过 <code>ExecutionVertex</code> 的 <code>getPreferredLocations()</code> 方法获取一个 TaskManagerLocation 列表，然后再根据 <code>LocationPreferenceConstraint</code> 的模式做过滤，如果是 <code>ALL</code>，那么前面拿到的所有列表都会直接返回，而如果是 <code>ANY</code>，只会把那些已经分配好的 input 节点的 <code>TaskManagerLocation</code> 返回。</p>
<p>这里，看下 <code>ExecutionVertex</code> 的 <code>getPreferredLocations()</code> 方法的实现逻辑：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExecutionVertex.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Gets the overall preferred execution location for this vertex's current execution.</span></span><br><span class="line"><span class="comment"> * The preference is determined as follows:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;If the task execution has state to load (from a checkpoint), then the location preference</span></span><br><span class="line"><span class="comment"> *         is the location of the previous execution (if there is a previous execution attempt).</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;If the task execution has no state or no previous location, then the location preference</span></span><br><span class="line"><span class="comment"> *         is based on the task's inputs.</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;</span></span><br><span class="line"><span class="comment"> * note: 如果这个 task Execution 是从 checkpoint 加载的状态，那么这个 location preference 就是之前执行的状态；</span></span><br><span class="line"><span class="comment"> * note: 如果这个 task Execution 没有状态信息或之前的 location 记录，这个 location preference 依赖于 task 的输入；</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;These rules should result in the following behavior:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note: 1. 无状态 task 总是基于与输入共享的方式调度；</span></span><br><span class="line"><span class="comment"> * note: 2. 有状态 task 基于与输入共享的方式来初始化他们最开始的调度；</span></span><br><span class="line"><span class="comment"> * note: 3. 有状态 task 的重复执行会尽量与他们的 state 共享执行；</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Stateless tasks are always scheduled based on co-location with inputs.</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Stateful tasks are on their initial attempt executed based on co-location with inputs.</span></span><br><span class="line"><span class="comment"> *     &lt;li&gt;Repeated executions of stateful tasks try to co-locate the execution with its state.</span></span><br><span class="line"><span class="comment"> * &lt;/ul&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Collection&lt;CompletableFuture&lt;TaskManagerLocation&gt;&gt; getPreferredLocations() &#123;</span><br><span class="line">    Collection&lt;CompletableFuture&lt;TaskManagerLocation&gt;&gt; basedOnState = getPreferredLocationsBasedOnState();</span><br><span class="line">    <span class="keyword">return</span> basedOnState != <span class="keyword">null</span> ? basedOnState : getPreferredLocationsBasedOnInputs();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Gets the preferred location to execute the current task execution attempt, based on the state that the execution attempt will resume.</span></span><br><span class="line"><span class="comment"> * note: 根据这个 Execution 试图恢复的状态来获取当前 task execution 的首选位置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Collection&lt;CompletableFuture&lt;TaskManagerLocation&gt;&gt; getPreferredLocationsBasedOnState() &#123;</span><br><span class="line">    TaskManagerLocation priorLocation;</span><br><span class="line">    <span class="keyword">if</span> (currentExecution.getTaskRestore() != <span class="keyword">null</span> &amp;&amp; (priorLocation = getLatestPriorLocation()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.singleton(CompletableFuture.completedFuture(priorLocation));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里简单介绍一下其处理逻辑：</p>
<ol>
<li>如果这个作业是从 Checkpoint 恢复的话，这里会根据它之前的状态信息获取上次的位置信息，直接返回这个位置信息；</li>
<li>另一种情况是，根据这个 ExecutionVertex 的 <code>inputEdges</code>，获取其上游 ExecutionVertex 的位置信息列表，但是如果这个列表的数目超过阈值（默认是 8），就会直接返回 null（上游过于分散，再根据 input 位置信息去分配就没有太大意义了）。</li>
</ol>
<p>可以看出，在选取最优的 TaskManagerLocation 列表时，主要是根据 state 和 input 的位置信息来判断，会优先选择 state，也就是上次 checkpoint 中记录的位置。</p>
<h3 id="最优的-slot-分配算法"><a href="#最优的-slot-分配算法" class="headerlink" title="最优的 slot 分配算法"></a>最优的 slot 分配算法</h3><p>在上面选择了最优的 TaskManagerLocation 列表后，这里来看下如何给 task 选择具体的 slot，这个是在 <code>SlotSelectionStrategy</code> 中的 <code>selectBestSlotForProfile()</code> 方法中做的，目前 <code>SlotSelectionStrategy</code> 有两个实现类：<code>PreviousAllocationSlotSelectionStrategy</code> 和 <code>LocationPreferenceSlotSelectionStrategy</code>，这个是在 <code>state.backend.local-recovery</code> 参数中配置的，默认是 false，选择的是 <code>PreviousAllocationSlotSelectionStrategy</code>，如果配置为 true，那么就会选择 <code>PreviousAllocationSlotSelectionStrategy</code>，这部分的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DefaultSchedulerFactory.java</span></span><br><span class="line"><span class="meta">@Nonnull</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> SlotSelectionStrategy <span class="title">selectSlotSelectionStrategy</span><span class="params">(@Nonnull Configuration configuration)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 根据 state.backend.local-recover 配置选择</span></span><br><span class="line">    <span class="keyword">if</span> (configuration.getBoolean(CheckpointingOptions.LOCAL_RECOVERY)) &#123;</span><br><span class="line">        <span class="keyword">return</span> PreviousAllocationSlotSelectionStrategy.INSTANCE;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> LocationPreferenceSlotSelectionStrategy.INSTANCE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里分别看下这两个实现类的 <code>selectBestSlotForProfile()</code> 的实现逻辑：</p>
<ol>
<li><code>PreviousAllocationSlotSelectionStrategy</code>: 它会根据上次的分配记录，如果这个位置刚好在 SlotPool 的可用列表里，这里就会直接选这个 slot，否则会走到 <code>LocationPreferenceSlotSelectionStrategy</code> 的处理逻辑；</li>
<li><code>LocationPreferenceSlotSelectionStrategy</code>: 这个是对可用的 slot 列表做打分，选择分数最高的（分数相同的话，会选择第一个），如果 slot 在前面得到的最优 <code>TaskManagerLocation</code> 列表中，分数就会比较高。</li>
</ol>
<h3 id="allocateSharedSlot-VS-allocateSingleSlot"><a href="#allocateSharedSlot-VS-allocateSingleSlot" class="headerlink" title="allocateSharedSlot VS allocateSingleSlot"></a>allocateSharedSlot VS allocateSingleSlot</h3><p>在分配 slot 时，这里分为两种情况：</p>
<ol>
<li><code>allocateSingleSlot()</code>: 如果没有设置 SlotSharingGroup 将会走到这个方法，直接给这个 SlotRequestId 分配一个 slot，具体选择哪个 slot 就是上面的逻辑；</li>
<li><code>allocateSharedSlot()</code>: 而如果设置了 SlotSharingGroup 就会走到这里，先根据 <code>SlotSharingGroupId</code> 获取或创建对应的 <code>SlotSharingManager</code>，然后创建（或者根据 <code>SlotSharingGroup</code> 获取）一个的 <code>MultiTaskSlot</code>（每个 <code>SlotSharingGroup</code> 会对应一个 <code>MultiTaskSlot</code> 对象），这里再将这个 task 分配到这个 <code>MultiTaskSlot</code> 上（这个只是简单介绍，后面在调度模型文章中，将会详细讲述）。</li>
</ol>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>到这里，Flink JobManager 的大部分内容已经讲述完了，还有一些小点会在后面的系列文章中再给大家讲述。这里总结一下，JobManager 主要是为一个具体的作业而服务的，它负责这个作业每个 task 的调度、checkpoint/savepoint（后面 checkpoint 的文章中会详述其流程）的触发以及容错恢复，它有两个非常重点的服务组件 —— <code>LegacyScheduler</code> 和 <code>SlotPool</code>，其中：</p>
<ol>
<li><code>LegacyScheduler</code>: 它封装了作业的 <code>ExecutionGraph</code> 以及 <code>BackPressureStatsTracker</code> 中的接口，它会负责这个作业具体调度、savepoint 触发等工作；</li>
<li><code>SlotPool</code>: 它主要负责这个作业 slot 相关的内容，像与 ResourceManager 通信、分配或释放 slot 资源等工作。</li>
</ol>
<p>文章的后半部分，又总结了一个作业是如何调度起来的，首先是分配 slot，最后是通过 <code>deploy()</code> 接口向 TM 提交这个 task，本文着重关注了 slot 的分配，task 的部署将会在下节的 TaskManager 详解中给大家介绍。</p>
<hr>
<p>参考</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/job_scheduling.html" target="_blank" rel="external">Jobs and Scheduling</a>；</li>
<li><a href="https://www.cnblogs.com/andyhe/p/10633692.html" target="_blank" rel="external">Flink架构分析之资源分配</a>；</li>
<li><a href="http://chenyuzhao.me/2017/02/08/jobmanager%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6/" target="_blank" rel="external">Flink JobManager 基本组件</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/TBzzGTNFTzVLjFQdzz-LuQ" target="_blank" rel="external">Apache Flink 进阶（一）：Runtime 核心机制剖析</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第六篇，紧接着上篇文章，本篇主要讲述 Flink Master 中另一个组件 —— JobManager（在源码中对应的实现类是 &lt;code&gt;JobMaster&lt;/code&gt;）。每个作业在启动后，Dispat
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink Master 详解</title>
    <link href="http://matt33.com/2019/12/23/flink-master-5/"/>
    <id>http://matt33.com/2019/12/23/flink-master-5/</id>
    <published>2019-12-23T15:50:50.000Z</published>
    <updated>2020-06-23T14:13:17.225Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第五篇，从这篇开始会向大家介绍一下 Flink Runtime 中涉及到的分布式调度相关的内容。Flink 本身也是 Master/Slave 架构（当前的架构是在 <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc</a> 中实现的），这个 Master 节点就类似于 Storm 中 Nimbus 节点，它负责整个集群的一些协调工作，Flink 中 Master 节点主要包含三大组件：Flink Resource Manager、Flink Dispatcher 以及为每个运行的 Job 创建一个 JobManager 服务，本篇文章主要给大家介绍一下 Flink 中 Master 节点相关内容。</p>
<p>这里要说明的一点是：通常我们认为 Flink 集群的 master 节点就是 JobManager，slave 节点就是 TaskManager 或者 TaskExecutor（见：<a href="https://ci.apache.org/projects/flink/flink-docs-master/concepts/runtime.html" target="_blank" rel="external">Distributed Runtime Environment</a>），这本身是没有什么问题的。但这里需要强调一下，在本文中集群的 Master 节点暂时就叫做 Master 节点，而负责每个作业调度的服务，这里叫做 JobManager/JobMaster（现在源码的实现中对应的类是 JobMaster）。集群的 Master 节点的工作范围与 JobManager 的工作范围还是有所不同的，而且 Master 节点的其中一项工作职责就是为每个提交的作业创建一个 JobManager 对象，用来处理这个作业相关协调工作，比如：task 的调度、Checkpoint 的触发及失败恢复等，JobManager 的内容将会在下篇文章单独讲述，本文主要聚焦 Master 节点除 JobManager 之外的工作。</p>
<h2 id="Flink-Master-简介"><a href="#Flink-Master-简介" class="headerlink" title="Flink Master 简介"></a>Flink Master 简介</h2><p>Flink 的 Master 节点包含了三个组件: Dispatcher、ResourceManager 和 JobManager。其中:</p>
<ol>
<li><strong>Dispatcher</strong>: 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 服务；</li>
<li><strong>ResourceManager</strong>: 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager，资源相关的内容都由这个服务负责；</li>
<li><strong>JobManager</strong>: 负责管理具体某个作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都会有自己的 JobManager 服务。</li>
</ol>
<p><img src="/images/flink/1-3.png" alt="Flink 的架构图（来自官网）"></p>
<p>根据上面的 Flink 的架构图（等把 runtime 的内容介绍完，届时会画一张更细的 Flink 的架构图，现在先以官方的图来看），当用户开始提交一个作业，首先会将用户编写的代码转化为一个 JobGraph（参考这个系列前面的文章），在这个过程中，它会进行一些检查或优化相关的工作（比如：检查配置，把可以 Chain 在一起算子 Chain 在一起）。然后，Client 再将生成的 JobGraph 提交到集群中执行。此时有两种情况（对于两种不同类型的集群）：</p>
<ol>
<li>类似于 Standalone 这种 Session 模式（对于 YARN 模式来说），这种情况下 Client 可以直接与 Dispatcher 建立连接并提交作业；</li>
<li>是 Per-Job 模式，这种情况下 Client 首先向资源管理系统 （如 Yarn）申请资源来启动 ApplicationMaster，然后再向 ApplicationMaster 中的 Dispatcher 提交作业。</li>
</ol>
<p>当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 服务，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。ResourceManager 选择到空闲的 Slot （<a href="http://matt33.com/2019/11/23/flink-learn-start-1/#Flink-%E6%9E%B6%E6%9E%84">Flink 架构-基本概念</a>）之后，就会通知相应的 TM 将该 Slot 分配给指定的 JobManager。</p>
<h2 id="Master-启动整体流程"><a href="#Master-启动整体流程" class="headerlink" title="Master 启动整体流程"></a>Master 启动整体流程</h2><p>Flink 集群 Master 节点在初始化时，会先调用 ClusterEntrypoint 的 <code>runClusterEntrypoint()</code> 方法启动集群，其整体流程如下图所示：</p>
<p><img src="/images/flink/5-flink-master.png" alt="Flink Master 启动的整体流程"></p>
<p>上图流程中 <code>runCluster()</code> 方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ClusterEntrypoint.java</span></span><br><span class="line"><span class="comment">//note: run cluster real start-point</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">runCluster</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">        <span class="comment">//note: 首先会初始化相关的服务(这里会涉及到一系列的服务)</span></span><br><span class="line">        initializeServices(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// write host information into configuration</span></span><br><span class="line">        configuration.setString(JobManagerOptions.ADDRESS, commonRpcService.getAddress());</span><br><span class="line">        configuration.setInteger(JobManagerOptions.PORT, commonRpcService.getPort());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> DispatcherResourceManagerComponentFactory&lt;?&gt; dispatcherResourceManagerComponentFactory = createDispatcherResourceManagerComponentFactory(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建 DispatcherResourceManagerComponent 对象（前面初始化的服务，都在这里使用了）</span></span><br><span class="line">        clusterComponent = dispatcherResourceManagerComponentFactory.create(</span><br><span class="line">            configuration,</span><br><span class="line">            commonRpcService,</span><br><span class="line">            haServices,</span><br><span class="line">            blobServer,</span><br><span class="line">            heartbeatServices,</span><br><span class="line">            metricRegistry,</span><br><span class="line">            archivedExecutionGraphStore,</span><br><span class="line">            <span class="keyword">new</span> RpcMetricQueryServiceRetriever(metricRegistry.getMetricQueryServiceRpcService()),</span><br><span class="line">            <span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">        clusterComponent.getShutDownFuture().whenComplete(</span><br><span class="line">            (ApplicationStatus applicationStatus, Throwable throwable) -&gt; &#123;</span><br><span class="line">                <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">//note: 抛出异常的情况下</span></span><br><span class="line">                    shutDownAsync(</span><br><span class="line">                        ApplicationStatus.UNKNOWN,</span><br><span class="line">                        ExceptionUtils.stringifyException(throwable),</span><br><span class="line">                        <span class="keyword">false</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// This is the general shutdown path. If a separate more specific shutdown was</span></span><br><span class="line">                    <span class="comment">// already triggered, this will do nothing</span></span><br><span class="line">                    shutDownAsync(</span><br><span class="line">                        applicationStatus,</span><br><span class="line">                        <span class="keyword">null</span>,</span><br><span class="line">                        <span class="keyword">true</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法主要分为下面两个步骤：</p>
<ol>
<li><code>initializeServices()</code>: 初始化相关的服务，都是 Master 节点将会使用到的一些服务；</li>
<li><code>create DispatcherResourceManagerComponent</code>: 这里会创建一个 <code>DispatcherResourceManagerComponent</code> 对象，这个对象在创建的时候会启动 <code>Dispatcher</code> 和 <code>ResourceManager</code> 服务。</li>
</ol>
<p>下面来详细看下具体实现。</p>
<h3 id="initializeServices"><a href="#initializeServices" class="headerlink" title="initializeServices"></a>initializeServices</h3><p><code>initializeServices()</code> 初始化一些基本的服务，具体的代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ClusterEntrypoint.java</span></span><br><span class="line"><span class="comment">//note: 初始化相关的服务</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initializeServices</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    LOG.info(<span class="string">"Initializing cluster services."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">        <span class="keyword">final</span> String bindAddress = configuration.getString(JobManagerOptions.ADDRESS);</span><br><span class="line">        <span class="keyword">final</span> String portRange = getRPCPortRange(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建 RPC 服务</span></span><br><span class="line">        commonRpcService = createRpcService(configuration, bindAddress, portRange);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the configuration used to create the high availability services</span></span><br><span class="line">        <span class="comment">//note: 根据当前创建的 RPC 服务信息做相关的配置（之前设置的端口可能是一个 range）</span></span><br><span class="line">        configuration.setString(JobManagerOptions.ADDRESS, commonRpcService.getAddress());</span><br><span class="line">        configuration.setInteger(JobManagerOptions.PORT, commonRpcService.getPort());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 用于 IO 的线程池</span></span><br><span class="line">        ioExecutor = Executors.newFixedThreadPool(</span><br><span class="line">            Hardware.getNumberCPUCores(),</span><br><span class="line">            <span class="keyword">new</span> ExecutorThreadFactory(<span class="string">"cluster-io"</span>));</span><br><span class="line">        <span class="comment">//note: HA service（跟用户配置有关，可以是 NONE、ZooKeeper 也可以自定义的类）</span></span><br><span class="line">        haServices = createHaServices(configuration, ioExecutor);</span><br><span class="line">        <span class="comment">//note: 初始化 Blob Server</span></span><br><span class="line">        blobServer = <span class="keyword">new</span> BlobServer(configuration, haServices.createBlobStore());</span><br><span class="line">        blobServer.start();</span><br><span class="line">        <span class="comment">//note: heartbeat service</span></span><br><span class="line">        heartbeatServices = createHeartbeatServices(configuration);</span><br><span class="line">        <span class="comment">//note: metrics reporter</span></span><br><span class="line">        metricRegistry = createMetricRegistry(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建了一个 Flink 内部的 metrics rpc service</span></span><br><span class="line">        <span class="keyword">final</span> RpcService metricQueryServiceRpcService = MetricUtils.startMetricsRpcService(configuration, bindAddress);</span><br><span class="line">        <span class="comment">//note: start MetricQueryService</span></span><br><span class="line">        metricRegistry.startQueryService(metricQueryServiceRpcService, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建一个 ArchivedExecutionGraphStore 对象，用于存储用户作业的物理 graph</span></span><br><span class="line">        archivedExecutionGraphStore = createSerializableExecutionGraphStore(configuration, commonRpcService.getScheduledExecutor());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述流程涉及到服务有：</p>
<ol>
<li>RpcService: 创建一个 rpc 服务；</li>
<li>HighAvailabilityServices: HA service 相关的实现，它的作用有很多，比如：处理 ResourceManager 的 leader 选举、JobManager leader 的选举等；</li>
<li>BlobServer: 主要管理一些大文件的上传等，比如用户作业的 jar 包、TM 上传 log 文件等（Blob 是指二进制大对象也就是英文 Binary Large Object 的缩写）；</li>
<li>HeartbeatServices: 初始化一个心跳服务；</li>
<li>MetricRegistryImpl: metrics 相关的服务；</li>
<li>ArchivedExecutionGraphStore: 存储 execution graph 的服务，默认有两种实现，<code>MemoryArchivedExecutionGraphStore</code> 主要是在内存中缓存，<code>FileArchivedExecutionGraphStore</code> 会持久化到文件系统，也会在内存中缓存。</li>
</ol>
<p>这些服务都会在前面第二步创建 <code>DispatcherResourceManagerComponent</code> 对象时使用到。</p>
<h3 id="create-DispatcherResourceManagerComponent"><a href="#create-DispatcherResourceManagerComponent" class="headerlink" title="create DispatcherResourceManagerComponent"></a>create DispatcherResourceManagerComponent</h3><p>创建 <code>DispatcherResourceManagerComponent</code> 对象的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AbstractDispatcherResourceManagerComponentFactory.java</span></span><br><span class="line"><span class="comment">//note: 创建 DispatcherResourceManagerComponent 对象</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DispatcherResourceManagerComponent&lt;T&gt; <span class="title">create</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Configuration configuration,</span></span></span><br><span class="line"><span class="function"><span class="params">        RpcService rpcService,</span></span></span><br><span class="line"><span class="function"><span class="params">        HighAvailabilityServices highAvailabilityServices,</span></span></span><br><span class="line"><span class="function"><span class="params">        BlobServer blobServer,</span></span></span><br><span class="line"><span class="function"><span class="params">        HeartbeatServices heartbeatServices,</span></span></span><br><span class="line"><span class="function"><span class="params">        MetricRegistry metricRegistry,</span></span></span><br><span class="line"><span class="function"><span class="params">        ArchivedExecutionGraphStore archivedExecutionGraphStore,</span></span></span><br><span class="line"><span class="function"><span class="params">        MetricQueryServiceRetriever metricQueryServiceRetriever,</span></span></span><br><span class="line"><span class="function"><span class="params">        FatalErrorHandler fatalErrorHandler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    LeaderRetrievalService dispatcherLeaderRetrievalService = <span class="keyword">null</span>;</span><br><span class="line">    LeaderRetrievalService resourceManagerRetrievalService = <span class="keyword">null</span>;</span><br><span class="line">    WebMonitorEndpoint&lt;U&gt; webMonitorEndpoint = <span class="keyword">null</span>;</span><br><span class="line">    ResourceManager&lt;?&gt; resourceManager = <span class="keyword">null</span>;</span><br><span class="line">    JobManagerMetricGroup jobManagerMetricGroup = <span class="keyword">null</span>;</span><br><span class="line">    T dispatcher = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 用于 Dispatcher leader 选举</span></span><br><span class="line">        dispatcherLeaderRetrievalService = highAvailabilityServices.getDispatcherLeaderRetriever();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 用于 Resource Manager leader 选举（对于使用 zk 的 HA 模式来说，与上面的区别是使用的路径不同）</span></span><br><span class="line">        resourceManagerRetrievalService = highAvailabilityServices.getResourceManagerLeaderRetriever();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: Dispatcher 的 Gateway</span></span><br><span class="line">        <span class="keyword">final</span> LeaderGatewayRetriever&lt;DispatcherGateway&gt; dispatcherGatewayRetriever = <span class="keyword">new</span> RpcGatewayRetriever&lt;&gt;(</span><br><span class="line">            rpcService,</span><br><span class="line">            DispatcherGateway.class,</span><br><span class="line">            DispatcherId::fromUuid,</span><br><span class="line">            <span class="number">10</span>,</span><br><span class="line">            Time.milliseconds(<span class="number">50L</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: ResourceManager 的 Gateway</span></span><br><span class="line">        <span class="keyword">final</span> LeaderGatewayRetriever&lt;ResourceManagerGateway&gt; resourceManagerGatewayRetriever = <span class="keyword">new</span> RpcGatewayRetriever&lt;&gt;(</span><br><span class="line">            rpcService,</span><br><span class="line">            ResourceManagerGateway.class,</span><br><span class="line">            ResourceManagerId::fromUuid,</span><br><span class="line">            <span class="number">10</span>,</span><br><span class="line">            Time.milliseconds(<span class="number">50L</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 它主要使用 web 前端的 rest 接口调用</span></span><br><span class="line">        <span class="keyword">final</span> ExecutorService executor = WebMonitorEndpoint.createExecutorService(</span><br><span class="line">            configuration.getInteger(RestOptions.SERVER_NUM_THREADS),</span><br><span class="line">            configuration.getInteger(RestOptions.SERVER_THREAD_PRIORITY),</span><br><span class="line">            <span class="string">"DispatcherRestEndpoint"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: metrics Fetcher</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> updateInterval = configuration.getLong(MetricOptions.METRIC_FETCHER_UPDATE_INTERVAL);</span><br><span class="line">        <span class="keyword">final</span> MetricFetcher metricFetcher = updateInterval == <span class="number">0</span></span><br><span class="line">            ? VoidMetricFetcher.INSTANCE</span><br><span class="line">            : MetricFetcherImpl.fromConfiguration(</span><br><span class="line">                configuration,</span><br><span class="line">                metricQueryServiceRetriever,</span><br><span class="line">                dispatcherGatewayRetriever,</span><br><span class="line">                executor);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: standalone 模式下，这里创建的是 DispatcherRestEndpoint 对象</span></span><br><span class="line">        webMonitorEndpoint = restEndpointFactory.createRestEndpoint(</span><br><span class="line">            configuration,</span><br><span class="line">            dispatcherGatewayRetriever,</span><br><span class="line">            resourceManagerGatewayRetriever,</span><br><span class="line">            blobServer,</span><br><span class="line">            executor,</span><br><span class="line">            metricFetcher,</span><br><span class="line">            highAvailabilityServices.getWebMonitorLeaderElectionService(),</span><br><span class="line">            fatalErrorHandler);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 启动 DispatcherRestEndpoint</span></span><br><span class="line">        log.debug(<span class="string">"Sarting Dispatcher REST endptoint."</span>);</span><br><span class="line">        webMonitorEndpoint.start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> String hostname = getHostname(rpcService);</span><br><span class="line"></span><br><span class="line">        jobManagerMetricGroup = MetricUtils.instantiateJobManagerMetricGroup(</span><br><span class="line">            metricRegistry,</span><br><span class="line">            hostname,</span><br><span class="line">            ConfigurationUtils.getSystemResourceMetricsProbingInterval(configuration));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建 ResourceManager 对象（StandAlone 模式，这里创建的是 StandaloneResourceManager 对象）</span></span><br><span class="line">        resourceManager = resourceManagerFactory.createResourceManager(</span><br><span class="line">            configuration,</span><br><span class="line">            ResourceID.generate(),</span><br><span class="line">            rpcService,</span><br><span class="line">            highAvailabilityServices,</span><br><span class="line">            heartbeatServices,</span><br><span class="line">            metricRegistry,</span><br><span class="line">            fatalErrorHandler,</span><br><span class="line">            <span class="keyword">new</span> ClusterInformation(hostname, blobServer.getPort()),</span><br><span class="line">            webMonitorEndpoint.getRestBaseUrl(),</span><br><span class="line">            jobManagerMetricGroup);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> HistoryServerArchivist historyServerArchivist = HistoryServerArchivist.createHistoryServerArchivist(configuration, webMonitorEndpoint);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建 dispatcher 对象（StandAlone 模式下，创建的是 StandaloneDispatcher 对象）</span></span><br><span class="line">        dispatcher = dispatcherFactory.createDispatcher(</span><br><span class="line">            configuration,</span><br><span class="line">            rpcService,</span><br><span class="line">            highAvailabilityServices,</span><br><span class="line">            resourceManagerGatewayRetriever,</span><br><span class="line">            blobServer,</span><br><span class="line">            heartbeatServices,</span><br><span class="line">            jobManagerMetricGroup,</span><br><span class="line">            metricRegistry.getMetricQueryServiceGatewayRpcAddress(),</span><br><span class="line">            archivedExecutionGraphStore,</span><br><span class="line">            fatalErrorHandler,</span><br><span class="line">            historyServerArchivist);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 启动 ResourceManager</span></span><br><span class="line">        log.debug(<span class="string">"Starting ResourceManager."</span>);</span><br><span class="line">        resourceManager.start();</span><br><span class="line">        resourceManagerRetrievalService.start(resourceManagerGatewayRetriever); <span class="comment">//note: 监听 leader</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 启动 Dispatcher</span></span><br><span class="line">        log.debug(<span class="string">"Starting Dispatcher."</span>);</span><br><span class="line">        dispatcher.start();</span><br><span class="line">        dispatcherLeaderRetrievalService.start(dispatcherGatewayRetriever);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> createDispatcherResourceManagerComponent(</span><br><span class="line">            dispatcher,</span><br><span class="line">            resourceManager,</span><br><span class="line">            dispatcherLeaderRetrievalService,</span><br><span class="line">            resourceManagerRetrievalService,</span><br><span class="line">            webMonitorEndpoint,</span><br><span class="line">            jobManagerMetricGroup);</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">        <span class="comment">//note: 清除前面启动的所有服务的组件</span></span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的方法实现中，Master 中的两个重要服务就是在这里初始化并启动的：</p>
<ol>
<li><code>Dispatcher</code>: 初始化并启动这个服务，如果 JM 启动了 HA 模式，这里会竞选 leader，只有是 leader 的 <code>Dispatcher</code> 才会真正对外提供服务（参考前面图中的流程）；</li>
<li><code>ResourceManager</code>: 这个跟 <code>Dispatcher</code> 有点类似。</li>
</ol>
<h2 id="Master-各个服务详解"><a href="#Master-各个服务详解" class="headerlink" title="Master 各个服务详解"></a>Master 各个服务详解</h2><p>这里，我们来详细看下 Master 使用到各个服务组件，并做下详细的介绍。</p>
<h3 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h3><p><strong>Dispatcher</strong> 主要是用于作业的提交、并把它们持久化、为作业创建对应的 JobManager 等，Client 端提交的 JobGraph 就是提交给了 Dispatcher 服务，这里先看一下一个 Dispatcher 对象被选举为 leader 后是如何初始化的，如果当前的 Dispatcher 被选举为 leader，则会调用其 <code>grantLeadership()</code> 方法，该方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dispatcher.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Callback method when current resourceManager is granted leadership.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note: 如果当前的 dispatcher 是 leader 的情况下</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> newLeaderSessionID unique leadershipID</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">grantLeadership</span><span class="params">(<span class="keyword">final</span> UUID newLeaderSessionID)</span> </span>&#123;</span><br><span class="line">    runAsyncWithoutFencing(</span><br><span class="line">        () -&gt; &#123;</span><br><span class="line">            log.info(<span class="string">"Dispatcher &#123;&#125; was granted leadership with fencing token &#123;&#125;"</span>, getAddress(), newLeaderSessionID);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//note: 通过 recoverJobs() 方法先从 job graph store 中恢复所有的 jobs</span></span><br><span class="line">            <span class="keyword">final</span> CompletableFuture&lt;Collection&lt;JobGraph&gt;&gt; recoveredJobsFuture = recoveryOperation.thenApplyAsync(</span><br><span class="line">                FunctionUtils.uncheckedFunction(ignored -&gt; recoverJobs()),</span><br><span class="line">                getRpcService().getExecutor());</span><br><span class="line"></span><br><span class="line">            <span class="comment">//note: 通过 tryAcceptLeadershipAndRunJobs() 调用 runJob 启动前面的每一个 job</span></span><br><span class="line">            <span class="keyword">final</span> CompletableFuture&lt;Boolean&gt; fencingTokenFuture = recoveredJobsFuture.thenComposeAsync(</span><br><span class="line">                (Collection&lt;JobGraph&gt; recoveredJobs) -&gt; tryAcceptLeadershipAndRunJobs(newLeaderSessionID, recoveredJobs),</span><br><span class="line">                getUnfencedMainThreadExecutor());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> CompletableFuture&lt;Void&gt; confirmationFuture = fencingTokenFuture.thenCombineAsync(</span><br><span class="line">                recoveredJobsFuture,</span><br><span class="line">                BiFunctionWithException.unchecked((Boolean confirmLeadership, Collection&lt;JobGraph&gt; recoveredJobs) -&gt; &#123;</span><br><span class="line">                    <span class="keyword">if</span> (confirmLeadership) &#123;</span><br><span class="line">                        <span class="comment">//note: 如果是 leader，并且前面两步都完成的话，就会走到这里</span></span><br><span class="line">                        leaderElectionService.confirmLeaderSessionID(newLeaderSessionID);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">for</span> (JobGraph recoveredJob : recoveredJobs) &#123;</span><br><span class="line">                            <span class="comment">//note: 从 job graph store 中删除这些作业相关的 state</span></span><br><span class="line">                            submittedJobGraphStore.releaseJobGraph(recoveredJob.getJobID());</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                &#125;),</span><br><span class="line">                getRpcService().getExecutor());</span><br><span class="line"></span><br><span class="line">            confirmationFuture.whenComplete(</span><br><span class="line">                (Void ignored, Throwable throwable) -&gt; &#123;</span><br><span class="line">                    <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        onFatalError(</span><br><span class="line">                            <span class="keyword">new</span> DispatcherException(</span><br><span class="line">                                String.format(<span class="string">"Failed to take leadership with session id %s."</span>, newLeaderSessionID),</span><br><span class="line">                                (ExceptionUtils.stripCompletionException(throwable))));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">            recoveryOperation = confirmationFuture;</span><br><span class="line">        &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Dispatcher 被选举为 leader 后，它主要的操作步骤如下：</p>
<ol>
<li><code>recoverJobs()</code>: 先从 job graph store 恢复所有作业的 JobGraph；</li>
<li><code>tryAcceptLeadershipAndRunJobs()</code>: 启动前面恢复的每个作业，这里要说明的是，目前看到的 1.9 的实现，这里会将前面所有的作业都会重启，我在看的时候是有点懵逼的，这个 HA 有点伪 HA，相当于 leader 切换之后，作业就必须要得重启恢复，这个代价是有点大的，不过也看到社区有改进的计划（<a href="https://issues.apache.org/jira/browse/FLINK-10333" target="_blank" rel="external">FLINK-10333</a> 这个进度有点慢）；</li>
</ol>
<p>我们这里再详细看下 Dispatcher 对外提供了哪些 API 实现（这些接口主要还是 <code>DispatcherGateway</code> 中必须要实现的接口），通过这些 API，其实就很容易看出它到底对外提供了哪些功能，提供的 API 有：</p>
<ol>
<li><code>listJobs()</code>: 列出当前提交的作业列表；</li>
<li><code>submitJob()</code>: 向集群提交作业；</li>
<li><code>getBlobServerPort()</code>: 返回 blob server 的端口；</li>
<li><code>requestJob()</code>: 根据 jobId 请求一个作业的 ArchivedExecutionGraph（它是这个作业  ExecutionGraph 序列化后的形式）；</li>
<li><code>disposeSavepoint()</code>: 清理指定路径的 savepoint 状态信息；</li>
<li><code>cancelJob()</code>: 取消一个指定的作业；</li>
<li><code>requestClusterOverview()</code>: 请求这个集群的全局信息，比如：集群有多少个 slot，有多少可用的 slot，有多少个作业等等；</li>
<li><code>requestMultipleJobDetails()</code>: 返回当前集群正在执行的作业详情，返回对象是 JobDetails 列表；</li>
<li><code>requestJobStatus()</code>: 请求一个作业的作业状态（返回的类型是 <code>JobStatus</code>）； </li>
<li><code>requestOperatorBackPressureStats()</code>: 请求一个 Operator 的反压情况； </li>
<li><code>requestJobResult()</code>: 请求一个 job 的 <code>JobResult</code>；</li>
<li><code>requestMetricQueryServiceAddresses()</code>: 请求 MetricQueryService 的地址；</li>
<li><code>requestTaskManagerMetricQueryServiceAddresses()</code>: 请求 TaskManager 的 MetricQueryService 的地址；</li>
<li><code>triggerSavepoint()</code>: 使用指定的目录触发一个 savepoint；</li>
<li><code>stopWithSavepoint()</code>: 停止当前的作业，并在停止前做一次 savepoint；</li>
<li><code>shutDownCluster()</code>: 关闭集群；</li>
</ol>
<p>通过 Dispatcher 提供的 API 可以看出，Dispatcher 服务主要有功能有：</p>
<ol>
<li>提交/取消作业；</li>
<li>触发/取消/清理 一个作业的 savepoint；</li>
<li>作业状态/列表查询；</li>
</ol>
<p>Dispatcher 这里主要处理的还是 Job 相关的请求，对外提供了统一的接口。</p>
<h3 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h3><p>ResourceManager 从名字就可以看出，它主要是资源管理相关的服务，如果其被选举为 leader，实现如下，它会清除缓存中的数据，然后启动 SlotManager 服务：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ResourceManager.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Callback method when current resourceManager is granted leadership.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：如果当前的 resourceManager 被选举为 leader 的话，就执行这个方法</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> newLeaderSessionID unique leadershipID</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">grantLeadership</span><span class="params">(<span class="keyword">final</span> UUID newLeaderSessionID)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//note: tryAcceptLeadership() 清除之前 leader 的信息，这里会重新初始化 leader 相关的信息，并启动 SlotManager 服务</span></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Boolean&gt; acceptLeadershipFuture = clearStateFuture</span><br><span class="line">        .thenComposeAsync((ignored) -&gt; tryAcceptLeadership(newLeaderSessionID), getUnfencedMainThreadExecutor());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Void&gt; confirmationFuture = acceptLeadershipFuture.thenAcceptAsync(</span><br><span class="line">        (acceptLeadership) -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (acceptLeadership) &#123;</span><br><span class="line">                <span class="comment">// confirming the leader session ID might be blocking,</span></span><br><span class="line">                leaderElectionService.confirmLeaderSessionID(newLeaderSessionID);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        getRpcService().getExecutor());</span><br><span class="line"></span><br><span class="line">    confirmationFuture.whenComplete(</span><br><span class="line">        (Void ignored, Throwable throwable) -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">                onFatalError(ExceptionUtils.stripCompletionException(throwable));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;Boolean&gt; <span class="title">tryAcceptLeadership</span><span class="params">(<span class="keyword">final</span> UUID newLeaderSessionID)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (leaderElectionService.hasLeadership(newLeaderSessionID)) &#123;</span><br><span class="line">        <span class="keyword">final</span> ResourceManagerId newResourceManagerId = ResourceManagerId.fromUuid(newLeaderSessionID);</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">"ResourceManager &#123;&#125; was granted leadership with fencing token &#123;&#125;"</span>, getAddress(), newResourceManagerId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// clear the state if we've been the leader before</span></span><br><span class="line">        <span class="comment">//note: 清除之前的状态</span></span><br><span class="line">        <span class="keyword">if</span> (getFencingToken() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            clearStateInternal();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        setFencingToken(newResourceManagerId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 本节点启动 leader 服务</span></span><br><span class="line">        startServicesOnLeadership();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prepareLeadershipAsync().thenApply(ignored -&gt; <span class="keyword">true</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> CompletableFuture.completedFuture(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">startServicesOnLeadership</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//note: 启动心跳服务</span></span><br><span class="line">    startHeartbeatServices();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 启动 slotManager</span></span><br><span class="line">    slotManager.start(getFencingToken(), getMainThreadExecutor(), <span class="keyword">new</span> ResourceActionsImpl());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里也来看下 ResourceManager 对外提供的 API（<code>ResourceManagerGateway</code> 相关方法的实现）：</p>
<ol>
<li><code>registerJobManager()</code>: 在 ResourceManager 中注册一个 <code>JobManager</code> 对象，一个作业启动后，JobManager 初始化后会调用这个方法；</li>
<li><code>registerTaskExecutor()</code>: 在 ResourceManager 中注册一个 <code>TaskExecutor</code>（<code>TaskExecutor</code> 实际上就是一个 TaskManager），当一个 TaskManager 启动后，会主动向 ResourceManager 注册；</li>
<li><code>sendSlotReport()</code>: TM 向 ResourceManager 发送 <code>SlotReport</code>（<code>SlotReport</code> 包含了这个 TaskExecutor 的所有 slot 状态信息，比如：哪些 slot 是可用的、哪些 slot 是已经被分配的、被分配的 slot 分配到哪些 Job 上了等）；</li>
<li><code>heartbeatFromTaskManager()</code>: 向 ResourceManager 发送来自 TM 的心跳信息；</li>
<li><code>heartbeatFromJobManager()</code>: 向 ResourceManager 发送来自 JM 的心跳信息；</li>
<li><code>disconnectTaskManager()</code>: TM 向 ResourceManager 发送一个断开连接的请求；</li>
<li><code>disconnectJobManager()</code>: JM 向 ResourceManager 发送一个断开连接的请求；</li>
<li><code>requestSlot()</code>: JM 向 ResourceManager 请求 slot 资源；</li>
<li><code>cancelSlotRequest()</code>: JM 向 ResourceManager 发送一个取消 slot 申请的请求；</li>
<li><code>notifySlotAvailable()</code>: TM 向 ResourceManager 发送一个请求，通知 ResourceManager 某个 slot 现在可用了（TM 端某个 slot 的资源被释放，可以再进行分配了）；</li>
<li><code>deregisterApplication()</code>: 向资源管理系统（比如：yarn、mesos）申请关闭当前的 Flink 集群，一般是在关闭集群的时候调用的；</li>
<li><code>requestTaskManagerInfo()</code>: 请求当前注册到 ResourceManager 的 TM 的详细信息（返回的类型是 <code>TaskManagerInfo</code>，可以请求的是全部的 TM 列表，也可以是根据某个 <code>ResourceID</code> 请求某个具体的 TM）；</li>
<li><code>requestResourceOverview()</code>: 向 ResourceManager 请求资源概况，返回的类型是 <code>ResourceOverview</code>，它包括注册的 TM 数量、注册的 slot 数、可用的 slot 数等；</li>
<li><code>requestTaskManagerMetricQueryServiceAddresses()</code>: 请求 TM MetricQueryService 的地址信息；</li>
<li><code>requestTaskManagerFileUpload()</code>: 向 TM 发送一个文件上传的请求，这里上传的是 TM 的 LOG/STDOUT 类型的文件，文件会上传到 Blob Server，这里会拿到一个 BlobKey（Blobkey 实际上是文件名的一部分，通过 BlobKey 可以确定这个文件的物理位置信息）；</li>
</ol>
<p>从上面的 API 列表中，可以看出 ResourceManager 的主要功能是：</p>
<ol>
<li>JobManager/TaskManager 资源的注册/心跳监控/连接断开的处理；</li>
<li>处理/取消 JM 资源（slot）的申请；</li>
<li>提供资源信息查询；</li>
<li>向 TM 发送请求，触发其 LOG/STDOUT 文件上传到 BlobServer；</li>
</ol>
<p>ResourceManager 在启动的时候，也会启动一个 SlotManager 服务，TM 相关的 slot 资源都是在 SlotManager 中维护的。</p>
<h4 id="SlotManager"><a href="#SlotManager" class="headerlink" title="SlotManager"></a>SlotManager</h4><p>SlotManager 会维护所有从 TaskManager 注册过来的 slot（包括它们的分配情况）以及所有 pending 的 SlotRequest（所有的 slot 请求都会先放到 pending 列表中，然后再去判断是否可以满足其资源需求）。只要有新的 slot 注册或者旧的 slot 资源释放，SlotManager 都会检测 pending SlotRequest 列表，检查是否有 SlotRequest 可以满足，如果可以满足，就会将资源分配给这个 SlotRequest；如果没有足够可用的 slot，SlotManager 会尝试着申请新的资源（比如：申请一个 worker 启动）。</p>
<p>当然，为了资源及时释放和避免资源浪费，空转的 task manager（它当前已经分配的 slot 并未使用）和 pending slot request 在 timeout 之后将会分别触发它们的释放和失败（对应的方法实现是 <code>checkTaskManagerTimeouts()</code> 和 <code>checkSlotRequestTimeouts()</code>）。</p>
<p>SlotManager 对外的提供的 API 如下（<code>SlotManager</code> 中必须要实现的接口，实现类是 <code>SlotManagerImpl</code>）：</p>
<ol>
<li><code>getNumberRegisteredSlots()</code>: 获取注册的 slot 的总数量；</li>
<li><code>getNumberRegisteredSlotsOf()</code>: 获取某个 TM 注册的 slot 的数量；</li>
<li><code>getNumberFreeSlots()</code>: 获取当前可用的（还未分配的 slot） slot 的数量；</li>
<li><code>getNumberFreeSlotsOf()</code>: 获取某个 TM 当前可用的 slot 的数量；</li>
<li><code>getNumberPendingTaskManagerSlots()</code>: 获取 <code>pendingSlots</code> 中 slot 的数量（<code>pendingSlots</code> 记录的是 SlotManager 主动去向资源管理系统申请的资源，该系统在一些情况下会新启动 worker 来创建资源，但这些slot 还没有主动汇报过来，就会暂时先放到 <code>pendingSlots</code> 中，如果 TM 过来注册的话，该 slot 就会从 pendingSlots 中移除，存储到其他对象中）；</li>
<li><code>getNumberPendingSlotRequests()</code>: 获取 <code>pendingSlotRequests</code> 列表的数量，这个集合中存储的是收到的、还没分配的 SlotRequest 列表，当一个 SlotRequest 发送过来之后，会先存储到这个集合中，当分配完成后，才会从这个集合中移除；</li>
<li><code>registerSlotRequest()</code>: JM 发送一个 slot 请求（这里是 ResourceManager 通过 <code>requestSlot()</code> 接口调用的）；</li>
<li><code>unregisterSlotRequest()</code>: 取消或移除一个正在排队（可能已经在处理中）的 SlotRequest；</li>
<li><code>registerTaskManager()</code>: 注册一个 TM，这里会将 TM 中所有的 slot 注册过来，等待后面分配；</li>
<li><code>unregisterTaskManager()</code>: 取消一个 TM 的注册（比如：关闭的时候可能会调用），这里会将这个 TM 上所有的 slot 都移除，会先从缓存中移除，然后再通知 JM 这个 slot 分配失败；</li>
<li><code>reportSlotStatus()</code>: TM 汇报当前 slot 分配的情况，SlotManager 会将其更新到自己的缓存中；</li>
<li><code>freeSlot()</code>: 释放一个指定的 slot，如果这个 slot 之前已经被分配出去了，这里会更新其状态，将其状态改为 <code>FREE</code>；</li>
<li><code>setFailUnfulfillableRequest()</code>: 遍历 <code>pendingSlotRequests</code> 列表，如果这些 slot 请求现在还分配不到合适的资源，这里会将其设置为 fail，会通知 JM slot 分配失败。</li>
</ol>
<p>同样，从上面的 API 列表中，总结一下 SlotManager 的功能：</p>
<ol>
<li>提供 slot 相关的信息查询；</li>
<li>处理/取消 JM 发送的 SlotRequest；</li>
<li>注册/取消 一个 TM（该 TM 涉及到的所有 slot 都会被注册或取消）；</li>
<li>Slot 资源的释放；</li>
</ol>
<h3 id="其他服务"><a href="#其他服务" class="headerlink" title="其他服务"></a>其他服务</h3><p>Master 除了上面的服务，还启动了其他的服务，这里简单列一下：</p>
<ol>
<li><code>BlobServer</code>: 它是 Flink 用来管理二进制大文件的服务，Flink JobManager 中启动的 BlobServer 负责监听请求并派发线程去处理（这个将会在下篇文章中讲述）；</li>
<li><code>JobManager</code>: Dispatcher 会为每个作业创建一个 JobManager 对象，它用来处理这个作业相关的协调工作，比如：task 的调度、Checkpoint 的触发及失败恢复等（这个也会在下篇文章中讲述）；</li>
<li><code>HA service</code>: Flink HA 的实现目前是依赖了 ZK，使用 <code>curator</code> 这个包来实现的，有兴趣的可以看下 <a href="https://www.cnblogs.com/francisYoung/p/5464789.html" target="_blank" rel="external">Curator leader 选举(一)</a> 这篇文章。</li>
</ol>
<h2 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h2><p>到这里，终于就把 Flink Master 相关内容的一部分梳理完了，这里简单总结一下：</p>
<ol>
<li><strong>Dispatcher</strong>: 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件，它主要还是处理 Job 相关的请求，对外提供了统一的接口抽象；</li>
<li><strong>ResourceManager</strong>: 负责资源的管理，所有资源相关的请求都是 ResourceManager 中处理的；</li>
<li><strong>JobManager</strong>: 负责管理具体作业的执行；</li>
</ol>
<p>Flink Master 这部分的抽象还是比较好的，三大组件各司其职。当然还有一些需要改善的地方，比如：为什么不抽象一个 Master 类，然后把这些子服务全都放到 Master 类里，这样代码看起来会清晰舒服很多，现在的代码对初学者其实并不友好。</p>
<hr>
<p>参考</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/job_scheduling.html" target="_blank" rel="external">Jobs and Scheduling</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077" target="_blank" rel="external">FLIP-6 - Flink Deployment and Process Model - Standalone, Yarn, Mesos, Kubernetes, etc</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/TBzzGTNFTzVLjFQdzz-LuQ" target="_blank" rel="external">Apache Flink 进阶（一）：Runtime 核心机制剖析</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/89537466?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1052584028930203648" target="_blank" rel="external">FLINK 高可用服务概览与改造</a>;</li>
<li><a href="http://chenyuzhao.me/2017/02/08/jobmanager%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6/" target="_blank" rel="external">Flink JobManager 基本组件</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第五篇，从这篇开始会向大家介绍一下 Flink Runtime 中涉及到的分布式调度相关的内容。Flink 本身也是 Master/Slave 架构（当前的架构是在 &lt;a href=&quot;https://cwik
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 如何生成 ExecutionGraph</title>
    <link href="http://matt33.com/2019/12/20/flink-execution-graph-4/"/>
    <id>http://matt33.com/2019/12/20/flink-execution-graph-4/</id>
    <published>2019-12-20T03:06:33.000Z</published>
    <updated>2020-06-23T14:13:17.225Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第四篇，紧接着前面两篇文章，在前两篇文章中介绍的 StreamGraph 和 JobGraph 都是在 client 端生成的，本文将会讲述 JobGraph 是如何转换成 ExecutionGraph 的。当 JobGraph 从 client 端提交到 JobManager 端后，JobManager 会根据 JobGraph 生成对应的 ExecutionGraph，ExecutionGraph 是 Flink 作业调度时使用到的核心数据结构，它包含每一个并行的 task、每一个 intermediate stream 以及它们之间的关系，本篇将会详细分析一下 JobGraph 转换为 ExecutionGraph 的流程。</p>
<h2 id="Create-ExecutionGraph-的整体流程"><a href="#Create-ExecutionGraph-的整体流程" class="headerlink" title="Create ExecutionGraph 的整体流程"></a>Create ExecutionGraph 的整体流程</h2><p>当用户向一个 Flink 集群提交一个作业后，JobManager 会接收到 Client 相应的请求，JobManager 会先做一些初始化相关的操作（也就是 JobGraph 到 ExecutionGraph 的转化），当这个转换完成后，才会根据 ExecutionGraph 真正在分布式环境中调度当前这个作业，而 JobManager 端处理的整体流程如下：</p>
<p><img src="/images/flink/4-job-start.png" alt="一个新作业提交后的处理流程"></p>
<p>上图是一个作业提交后，在 JobManager 端的处理流程，本篇文章主要聚焦于 ExecutionGraph 的生成过程，也就是图中的红色节点，即 ExecutionGraphBuilder 的 <code>buildGraph()</code> 方法，这个方法就是根据 JobGraph 及相关的配置来创建 ExecutionGraph 对象的核心方法。</p>
<h2 id="具体实现逻辑"><a href="#具体实现逻辑" class="headerlink" title="具体实现逻辑"></a>具体实现逻辑</h2><p>这里将会详细来讲述 ExecutionGraphBuilder <code>buildGraph()</code> 方法的详细实现。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>ExecutionGraph 引入了几个基本概念，先简单介绍一下这些概念，对于理解 ExecutionGraph 有较大帮助：</p>
<ul>
<li><strong>ExecutionJobVertex</strong>: 在 ExecutionGraph 中，节点对应的是 ExecutionJobVertex，它是与 JobGraph 中的 JobVertex 一一对应，实际上每个 ExexutionJobVertex 也都是由一个 JobVertex 生成；</li>
<li><strong>ExecutionVertex</strong>: 在 ExecutionJobVertex 中有一个 <code>taskVertices</code> 变量，它是 ExecutionVertex 类型的数组，数组的大小就是这个 JobVertex 的并发度，在创建 ExecutionJobVertex 对象时，会创建相同并发度梳理的 ExecutionVertex 对象，在真正调度时，一个 ExecutionVertex 实际就是一个 task，它是 ExecutionJobVertex 并行执行的一个子任务；</li>
<li><strong>Execution</strong>: Execution 是对 ExecutionVertex 的一次执行，通过 ExecutionAttemptId 来唯一标识，一个 ExecutionVertex 在某些情况下可能会执行多次，比如遇到失败的情况或者该 task 的数据需要重新计算时；</li>
<li><strong>IntermediateResult</strong>: 在 JobGraph 中用 IntermediateDataSet 表示 JobVertex 的输出 stream，一个 JobGraph 可能会有多个输出 stream，在 ExecutionGraph 中，与之对应的就是 IntermediateResult 对象；</li>
<li><strong>IntermediateResultPartition</strong>: 由于 ExecutionJobVertex 可能有多个并行的子任务，所以每个 IntermediateResult 可能就有多个生产者，每个生产者的在相应的 IntermediateResult 上的输出对应一个 IntermediateResultPartition 对象，IntermediateResultPartition 表示的是 ExecutionVertex 的一个输出分区；</li>
<li><strong>ExecutionEdge</strong>: ExecutionEdge 表示 ExecutionVertex 的输入，通过 ExecutionEdge 将 ExecutionVertex 和 IntermediateResultPartition 连接起来，进而在 ExecutionVertex 和 IntermediateResultPartition 之间建立联系。</li>
</ul>
<p>从这些基本概念中，也可以看出以下几点：</p>
<ol>
<li>由于每个 JobVertex 可能有多个 IntermediateDataSet，所以每个 ExecutionJobVertex 可能有多个 IntermediateResult，因此，每个 ExecutionVertex 也可能会包含多个 IntermediateResultPartition；</li>
<li>ExecutionEdge 这里主要的作用是把 <code>ExecutionVertex</code> 和 <code>IntermediateResultPartition</code> 连接起来，表示它们之间的连接关系。</li>
</ol>
<p>这里先放一张 ExecutionGraph 粗略图，它展示上面这些类之间的关系：</p>
<p><img src="/images/flink/4-excution-graph.png" alt="ExecutionGraph"></p>
<h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>ExecutionGraph 的生成是在 ExecutionGraphBuilder 的 <code>buildGraph()</code> 方法中实现的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExecutionGraphBuilder.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutionGraph <span class="title">buildGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    @Nullable ExecutionGraph prior,</span></span></span><br><span class="line"><span class="function"><span class="params">    JobGraph jobGraph,</span></span></span><br><span class="line"><span class="function"><span class="params">    Configuration jobManagerConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">    ScheduledExecutorService futureExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">    Executor ioExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">    SlotProvider slotProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">    ClassLoader classLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">    CheckpointRecoveryFactory recoveryFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">    Time rpcTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">    RestartStrategy restartStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">    MetricGroup metrics,</span></span></span><br><span class="line"><span class="function"><span class="params">    BlobWriter blobWriter,</span></span></span><br><span class="line"><span class="function"><span class="params">    Time allocationTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">    Logger log,</span></span></span><br><span class="line"><span class="function"><span class="params">    ShuffleMaster&lt;?&gt; shuffleMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">    PartitionTracker partitionTracker,</span></span></span><br><span class="line"><span class="function"><span class="params">    FailoverStrategy.Factory failoverStrategyFactory)</span> <span class="keyword">throws</span> JobExecutionException, JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    checkNotNull(jobGraph, <span class="string">"job graph cannot be null"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> String jobName = jobGraph.getName();</span><br><span class="line">    <span class="keyword">final</span> JobID jobId = jobGraph.getJobID();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: build jobInformation</span></span><br><span class="line">    <span class="keyword">final</span> JobInformation jobInformation = <span class="keyword">new</span> JobInformation(</span><br><span class="line">        jobId,</span><br><span class="line">        jobName,</span><br><span class="line">        jobGraph.getSerializedExecutionConfig(),</span><br><span class="line">        jobGraph.getJobConfiguration(),</span><br><span class="line">        jobGraph.getUserJarBlobKeys(),</span><br><span class="line">        jobGraph.getClasspaths());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: Execution 保留的最大历史数</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> maxPriorAttemptsHistoryLength =</span><br><span class="line">            jobManagerConfig.getInteger(JobManagerOptions.MAX_ATTEMPTS_HISTORY_SIZE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 决定什么时候释放 IntermediateResultPartitions 的策略</span></span><br><span class="line">    <span class="keyword">final</span> PartitionReleaseStrategy.Factory partitionReleaseStrategyFactory =</span><br><span class="line">        PartitionReleaseStrategyFactoryLoader.loadPartitionReleaseStrategyFactory(jobManagerConfig);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create a new execution graph, if none exists so far</span></span><br><span class="line">    <span class="comment">//note: 如果 executionGraph 还不存在，就创建一个新的对象</span></span><br><span class="line">    <span class="keyword">final</span> ExecutionGraph executionGraph;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        executionGraph = (prior != <span class="keyword">null</span>) ? prior :</span><br><span class="line">            <span class="keyword">new</span> ExecutionGraph(</span><br><span class="line">                jobInformation,</span><br><span class="line">                futureExecutor,</span><br><span class="line">                ioExecutor,</span><br><span class="line">                rpcTimeout,</span><br><span class="line">                restartStrategy,</span><br><span class="line">                maxPriorAttemptsHistoryLength,</span><br><span class="line">                failoverStrategyFactory,</span><br><span class="line">                slotProvider,</span><br><span class="line">                classLoader,</span><br><span class="line">                blobWriter,</span><br><span class="line">                allocationTimeout,</span><br><span class="line">                partitionReleaseStrategyFactory,</span><br><span class="line">                shuffleMaster,</span><br><span class="line">                partitionTracker,</span><br><span class="line">                jobGraph.getScheduleMode(),</span><br><span class="line">                jobGraph.getAllowQueuedScheduling());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">"Could not create the ExecutionGraph."</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set the basic properties</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 以 json 的形式记录 JobGraph</span></span><br><span class="line">        executionGraph.setJsonPlan(JsonPlanGenerator.generatePlan(jobGraph));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        log.warn(<span class="string">"Cannot create JSON plan for job"</span>, t);</span><br><span class="line">        <span class="comment">// give the graph an empty plan</span></span><br><span class="line">        executionGraph.setJsonPlan(<span class="string">"&#123;&#125;"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize the vertices that have a master initialization hook</span></span><br><span class="line">    <span class="comment">// file output formats create directories here, input formats create splits</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> initMasterStart = System.nanoTime();</span><br><span class="line">    log.info(<span class="string">"Running initialization on master for job &#123;&#125; (&#123;&#125;)."</span>, jobName, jobId);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (JobVertex vertex : jobGraph.getVertices()) &#123;</span><br><span class="line">        String executableClass = vertex.getInvokableClassName();</span><br><span class="line">        <span class="keyword">if</span> (executableClass == <span class="keyword">null</span> || executableClass.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobSubmissionException(jobId,</span><br><span class="line">                    <span class="string">"The vertex "</span> + vertex.getID() + <span class="string">" ("</span> + vertex.getName() + <span class="string">") has no invokable class."</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//note: 对于 InputOutputFormatVertex 类型的对象，会在 master 节点做一些额外的初始化操作</span></span><br><span class="line">            vertex.initializeOnMaster(classLoader);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId,</span><br><span class="line">                        <span class="string">"Cannot initialize task '"</span> + vertex.getName() + <span class="string">"': "</span> + t.getMessage(), t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.info(<span class="string">"Successfully ran initialization on master in &#123;&#125; ms."</span>,</span><br><span class="line">            (System.nanoTime() - initMasterStart) / <span class="number">1_000_000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// topologically sort the job vertices and attach the graph to the existing one</span></span><br><span class="line">    <span class="comment">//note: 这里会先做一个排序，source 会放在最前面，接着开始遍历，必须保证当前添加到集合的节点的前置节点都已经添加进去了</span></span><br><span class="line">    List&lt;JobVertex&gt; sortedTopology = jobGraph.getVerticesSortedTopologicallyFromSources();</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">        log.debug(<span class="string">"Adding &#123;&#125; vertices from job graph &#123;&#125; (&#123;&#125;)."</span>, sortedTopology.size(), jobName, jobId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 处理的重点：生成具体的 Execution Plan</span></span><br><span class="line">    executionGraph.attachJobGraph(sortedTopology);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">        log.debug(<span class="string">"Successfully created execution graph from job graph &#123;&#125; (&#123;&#125;)."</span>, jobName, jobId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: cp 相关的配置</span></span><br><span class="line">    <span class="comment">// configure the state CheckPointing</span></span><br><span class="line">    JobCheckpointingSettings snapshotSettings = jobGraph.getCheckpointingSettings();</span><br><span class="line">    <span class="keyword">if</span> (snapshotSettings != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">//note: cp 时，需要 trigger（插入 barrier）的 JobVertex，这里指的是 source 节点</span></span><br><span class="line">        List&lt;ExecutionJobVertex&gt; triggerVertices =</span><br><span class="line">                idToVertex(snapshotSettings.getVerticesToTrigger(), executionGraph);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: cp 时，需要向 master 返回 ack 信息的 JobVertex 节点的集合</span></span><br><span class="line">        List&lt;ExecutionJobVertex&gt; ackVertices =</span><br><span class="line">                idToVertex(snapshotSettings.getVerticesToAcknowledge(), executionGraph);</span><br><span class="line"></span><br><span class="line">        List&lt;ExecutionJobVertex&gt; confirmVertices =</span><br><span class="line">                idToVertex(snapshotSettings.getVerticesToConfirm(), executionGraph);</span><br><span class="line"></span><br><span class="line">        CompletedCheckpointStore completedCheckpoints;</span><br><span class="line">        CheckpointIDCounter checkpointIdCounter;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> maxNumberOfCheckpointsToRetain = jobManagerConfig.getInteger(</span><br><span class="line">                    CheckpointingOptions.MAX_RETAINED_CHECKPOINTS);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (maxNumberOfCheckpointsToRetain &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// warning and use 1 as the default value if the setting in</span></span><br><span class="line">                <span class="comment">// state.checkpoints.max-retained-checkpoints is not greater than 0.</span></span><br><span class="line">                log.warn(<span class="string">"The setting for '&#123;&#125; : &#123;&#125;' is invalid. Using default value of &#123;&#125;"</span>,</span><br><span class="line">                        CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.key(),</span><br><span class="line">                        maxNumberOfCheckpointsToRetain,</span><br><span class="line">                        CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.defaultValue());</span><br><span class="line"></span><br><span class="line">                maxNumberOfCheckpointsToRetain = CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.defaultValue();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            completedCheckpoints = recoveryFactory.createCheckpointStore(jobId, maxNumberOfCheckpointsToRetain, classLoader);</span><br><span class="line">            checkpointIdCounter = recoveryFactory.createCheckpointIDCounter(jobId);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId, <span class="string">"Failed to initialize high-availability checkpoint handler"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Maximum number of remembered checkpoints</span></span><br><span class="line">        <span class="comment">//note: cp 保存的最多数量</span></span><br><span class="line">        <span class="keyword">int</span> historySize = jobManagerConfig.getInteger(WebOptions.CHECKPOINTS_HISTORY_SIZE);</span><br><span class="line"></span><br><span class="line">        CheckpointStatsTracker checkpointStatsTracker = <span class="keyword">new</span> CheckpointStatsTracker(</span><br><span class="line">                historySize,</span><br><span class="line">                ackVertices,</span><br><span class="line">                snapshotSettings.getCheckpointCoordinatorConfiguration(),</span><br><span class="line">                metrics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load the state backend from the application settings</span></span><br><span class="line">        <span class="keyword">final</span> StateBackend applicationConfiguredBackend;</span><br><span class="line">        <span class="keyword">final</span> SerializedValue&lt;StateBackend&gt; serializedAppConfigured = snapshotSettings.getDefaultStateBackend();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (serializedAppConfigured == <span class="keyword">null</span>) &#123;</span><br><span class="line">            applicationConfiguredBackend = <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                applicationConfiguredBackend = serializedAppConfigured.deserializeValue(classLoader);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException | ClassNotFoundException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId,</span><br><span class="line">                        <span class="string">"Could not deserialize application-defined state backend."</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: state backend</span></span><br><span class="line">        <span class="keyword">final</span> StateBackend rootBackend;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            rootBackend = StateBackendLoader.fromApplicationOrConfigOrDefault(</span><br><span class="line">                    applicationConfiguredBackend, jobManagerConfig, classLoader, log);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (IllegalConfigurationException | IOException | DynamicCodeLoadingException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId, <span class="string">"Could not instantiate configured state backend"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// instantiate the user-defined checkpoint hooks</span></span><br><span class="line">        <span class="comment">//note: 实例话用户自定义的 cp hook</span></span><br><span class="line">        <span class="keyword">final</span> SerializedValue&lt;MasterTriggerRestoreHook.Factory[]&gt; serializedHooks = snapshotSettings.getMasterHooks();</span><br><span class="line">        <span class="keyword">final</span> List&lt;MasterTriggerRestoreHook&lt;?&gt;&gt; hooks;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (serializedHooks == <span class="keyword">null</span>) &#123;</span><br><span class="line">            hooks = Collections.emptyList();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> MasterTriggerRestoreHook.Factory[] hookFactories;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                hookFactories = serializedHooks.deserializeValue(classLoader);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">catch</span> (IOException | ClassNotFoundException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId, <span class="string">"Could not instantiate user-defined checkpoint hooks"</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> Thread thread = Thread.currentThread();</span><br><span class="line">            <span class="keyword">final</span> ClassLoader originalClassLoader = thread.getContextClassLoader();</span><br><span class="line">            thread.setContextClassLoader(classLoader);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                hooks = <span class="keyword">new</span> ArrayList&lt;&gt;(hookFactories.length);</span><br><span class="line">                <span class="keyword">for</span> (MasterTriggerRestoreHook.Factory factory : hookFactories) &#123;</span><br><span class="line">                    hooks.add(MasterHooks.wrapHook(factory.create(), classLoader));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">finally</span> &#123;</span><br><span class="line">                thread.setContextClassLoader(originalClassLoader);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> CheckpointCoordinatorConfiguration chkConfig = snapshotSettings.getCheckpointCoordinatorConfiguration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 创建 CheckpointCoordinator 对象</span></span><br><span class="line">        executionGraph.enableCheckpointing(</span><br><span class="line">            chkConfig,</span><br><span class="line">            triggerVertices,</span><br><span class="line">            ackVertices,</span><br><span class="line">            confirmVertices,</span><br><span class="line">            hooks,</span><br><span class="line">            checkpointIdCounter,</span><br><span class="line">            completedCheckpoints,</span><br><span class="line">            rootBackend,</span><br><span class="line">            checkpointStatsTracker);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create all the metrics for the Execution Graph</span></span><br><span class="line"></span><br><span class="line">    metrics.gauge(RestartTimeGauge.METRIC_NAME, <span class="keyword">new</span> RestartTimeGauge(executionGraph));</span><br><span class="line">    metrics.gauge(DownTimeGauge.METRIC_NAME, <span class="keyword">new</span> DownTimeGauge(executionGraph));</span><br><span class="line">    metrics.gauge(UpTimeGauge.METRIC_NAME, <span class="keyword">new</span> UpTimeGauge(executionGraph));</span><br><span class="line">    metrics.gauge(NumberOfFullRestartsGauge.METRIC_NAME, <span class="keyword">new</span> NumberOfFullRestartsGauge(executionGraph));</span><br><span class="line"></span><br><span class="line">    executionGraph.getFailoverStrategy().registerMetrics(metrics);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> executionGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法里，会先创建一个 ExecutionGraph 对象，然后对 JobGraph 中的 JobVertex 列表做一下排序（先把有 source 节点的 JobVertex 放在最前面，然后开始遍历，只有当前 JobVertex 的前置节点都已经添加到集合后才能把当前 JobVertex 节点添加到集合中），最后通过 <code>attachJobGraph()</code> 方法生成具体的 Execution Plan。</p>
<p>ExecutionGraph 的 <code>attachJobGraph()</code> 方法会将这个作业的 ExecutionGraph 构建出来，它会根据 JobGraph 创建相应的 ExecutionJobVertex、IntermediateResult、ExecutionVertex、ExecutionEdge、IntermediateResultPartition，其详细的执行逻辑如下图所示：</p>
<p><img src="/images/flink/4-execution-job-vertex.png" alt="ExecutionGraph"></p>
<p>上面的图还是有些凌乱，要配合本文的第二张图来看，接下来看下具体的方法实现。</p>
<h4 id="创建-ExecutionJobVertex-对象"><a href="#创建-ExecutionJobVertex-对象" class="headerlink" title="创建 ExecutionJobVertex 对象"></a>创建 ExecutionJobVertex 对象</h4><p>先来看下创建 ExecutionJobVertex 对象的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ExecutionJobVertex</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ExecutionGraph graph,</span></span></span><br><span class="line"><span class="function"><span class="params">        JobVertex jobVertex,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> defaultParallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> maxPriorAttemptsHistoryLength,</span></span></span><br><span class="line"><span class="function"><span class="params">        Time timeout,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> initialGlobalModVersion,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> createTimestamp)</span> <span class="keyword">throws</span> JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (graph == <span class="keyword">null</span> || jobVertex == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.graph = graph;</span><br><span class="line">    <span class="keyword">this</span>.jobVertex = jobVertex;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 并发度</span></span><br><span class="line">    <span class="keyword">int</span> vertexParallelism = jobVertex.getParallelism();</span><br><span class="line">    <span class="keyword">int</span> numTaskVertices = vertexParallelism &gt; <span class="number">0</span> ? vertexParallelism : defaultParallelism;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> configuredMaxParallelism = jobVertex.getMaxParallelism();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.maxParallelismConfigured = (VALUE_NOT_SET != configuredMaxParallelism);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if no max parallelism was configured by the user, we calculate and set a default</span></span><br><span class="line">    setMaxParallelismInternal(maxParallelismConfigured ?</span><br><span class="line">            configuredMaxParallelism : KeyGroupRangeAssignment.computeDefaultMaxParallelism(numTaskVertices));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// verify that our parallelism is not higher than the maximum parallelism</span></span><br><span class="line">    <span class="keyword">if</span> (numTaskVertices &gt; maxParallelism) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobException(</span><br><span class="line">            String.format(<span class="string">"Vertex %s's parallelism (%s) is higher than the max parallelism (%s). Please lower the parallelism or increase the max parallelism."</span>,</span><br><span class="line">                jobVertex.getName(),</span><br><span class="line">                numTaskVertices,</span><br><span class="line">                maxParallelism));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.parallelism = numTaskVertices;</span><br><span class="line">    <span class="keyword">this</span>.resourceProfile = ResourceProfile.fromResourceSpec(jobVertex.getMinResources(), <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: taskVertices 记录这个 task 每个并发</span></span><br><span class="line">    <span class="keyword">this</span>.taskVertices = <span class="keyword">new</span> ExecutionVertex[numTaskVertices];</span><br><span class="line">    <span class="keyword">this</span>.operatorIDs = Collections.unmodifiableList(jobVertex.getOperatorIDs());</span><br><span class="line">    <span class="keyword">this</span>.userDefinedOperatorIds = Collections.unmodifiableList(jobVertex.getUserDefinedOperatorIDs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 记录输入的 IntermediateResult 列表</span></span><br><span class="line">    <span class="keyword">this</span>.inputs = <span class="keyword">new</span> ArrayList&lt;&gt;(jobVertex.getInputs().size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// take the sharing group</span></span><br><span class="line">    <span class="keyword">this</span>.slotSharingGroup = jobVertex.getSlotSharingGroup();</span><br><span class="line">    <span class="keyword">this</span>.coLocationGroup = jobVertex.getCoLocationGroup();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// setup the coLocation group</span></span><br><span class="line">    <span class="keyword">if</span> (coLocationGroup != <span class="keyword">null</span> &amp;&amp; slotSharingGroup == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">"Vertex uses a co-location constraint without using slot sharing"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the intermediate results</span></span><br><span class="line">    <span class="comment">//note: 创建 IntermediateResult 对象数组（根据 JobVertex 的 targets 来确定）</span></span><br><span class="line">    <span class="keyword">this</span>.producedDataSets = <span class="keyword">new</span> IntermediateResult[jobVertex.getNumberOfProducedIntermediateDataSets()];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; jobVertex.getProducedDataSets().size(); i++) &#123;</span><br><span class="line">        <span class="comment">//note: JobGraph 中 IntermediateDataSet 这里会转换为 IntermediateResult 对象</span></span><br><span class="line">        <span class="keyword">final</span> IntermediateDataSet result = jobVertex.getProducedDataSets().get(i);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 这里一个 IntermediateDataSet 会对应一个 IntermediateResult</span></span><br><span class="line">        <span class="keyword">this</span>.producedDataSets[i] = <span class="keyword">new</span> IntermediateResult(</span><br><span class="line">                result.getId(),</span><br><span class="line">                <span class="keyword">this</span>,</span><br><span class="line">                numTaskVertices,</span><br><span class="line">                result.getResultType());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create all task vertices</span></span><br><span class="line">    <span class="comment">//note: task vertices 创建</span></span><br><span class="line">    <span class="comment">//note: 一个 JobVertex/ExecutionJobVertex 代表的是一个operator chain，而具体的 ExecutionVertex 则代表了每一个 Task</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numTaskVertices; i++) &#123;</span><br><span class="line">        ExecutionVertex vertex = <span class="keyword">new</span> ExecutionVertex(</span><br><span class="line">                <span class="keyword">this</span>,</span><br><span class="line">                i,</span><br><span class="line">                producedDataSets,</span><br><span class="line">                timeout,</span><br><span class="line">                initialGlobalModVersion,</span><br><span class="line">                createTimestamp,</span><br><span class="line">                maxPriorAttemptsHistoryLength);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.taskVertices[i] = vertex;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sanity check for the double referencing between intermediate result partitions and execution vertices</span></span><br><span class="line">    <span class="keyword">for</span> (IntermediateResult ir : <span class="keyword">this</span>.producedDataSets) &#123;</span><br><span class="line">        <span class="keyword">if</span> (ir.getNumberOfAssignedPartitions() != parallelism) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"The intermediate result's partitions were not correctly assigned."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它主要做了一下工作：</p>
<ol>
<li>根据这个 JobVertex 的 <code>results</code>（<code>IntermediateDataSet</code> 列表）来创建相应的 <code>IntermediateResult</code> 对象，每个 <code>IntermediateDataSet</code> 都会对应的一个 <code>IntermediateResult</code>；</li>
<li>再根据这个 JobVertex 的并发度，来创建相同数量的 <code>ExecutionVertex</code> 对象，每个 <code>ExecutionVertex</code> 对象在调度时实际上就是一个 task 任务；</li>
<li>在创建 <code>IntermediateResult</code> 和 <code>ExecutionVertex</code> 对象时都会记录它们之间的关系，它们之间的关系可以参考本文的图二。</li>
</ol>
<h4 id="创建-ExecutionVertex-对象"><a href="#创建-ExecutionVertex-对象" class="headerlink" title="创建 ExecutionVertex 对象"></a>创建 ExecutionVertex 对象</h4><p>创建 ExecutionVertex 对象的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ExecutionVertex</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ExecutionJobVertex jobVertex,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> subTaskIndex,</span></span></span><br><span class="line"><span class="function"><span class="params">        IntermediateResult[] producedDataSets,</span></span></span><br><span class="line"><span class="function"><span class="params">        Time timeout,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> initialGlobalModVersion,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> createTimestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> maxPriorExecutionHistoryLength)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.jobVertex = jobVertex;</span><br><span class="line">    <span class="keyword">this</span>.subTaskIndex = subTaskIndex;</span><br><span class="line">    <span class="keyword">this</span>.executionVertexId = <span class="keyword">new</span> ExecutionVertexID(jobVertex.getJobVertexId(), subTaskIndex);</span><br><span class="line">    <span class="keyword">this</span>.taskNameWithSubtask = String.format(<span class="string">"%s (%d/%d)"</span>,</span><br><span class="line">            jobVertex.getJobVertex().getName(), subTaskIndex + <span class="number">1</span>, jobVertex.getParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.resultPartitions = <span class="keyword">new</span> LinkedHashMap&lt;&gt;(producedDataSets.length, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 新建 IntermediateResultPartition 对象，并更新到缓存中</span></span><br><span class="line">    <span class="keyword">for</span> (IntermediateResult result : producedDataSets) &#123;</span><br><span class="line">        IntermediateResultPartition irp = <span class="keyword">new</span> IntermediateResultPartition(result, <span class="keyword">this</span>, subTaskIndex);</span><br><span class="line">        <span class="comment">//note: 记录 IntermediateResult 与 IntermediateResultPartition 之间的关系</span></span><br><span class="line">        result.setPartition(subTaskIndex, irp);</span><br><span class="line"></span><br><span class="line">        resultPartitions.put(irp.getPartitionId(), irp);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建 input ExecutionEdge 列表，记录输入的 ExecutionEdge 列表</span></span><br><span class="line">    <span class="keyword">this</span>.inputEdges = <span class="keyword">new</span> ExecutionEdge[jobVertex.getJobVertex().getInputs().size()][];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.priorExecutions = <span class="keyword">new</span> EvictingBoundedList&lt;&gt;(maxPriorExecutionHistoryLength);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建对应的 Execution 对象，初始化时 attemptNumber 为 0，如果后面重新调度这个 task，它会自增加 1</span></span><br><span class="line">    <span class="keyword">this</span>.currentExecution = <span class="keyword">new</span> Execution(</span><br><span class="line">        getExecutionGraph().getFutureExecutor(),</span><br><span class="line">        <span class="keyword">this</span>,</span><br><span class="line">        <span class="number">0</span>,</span><br><span class="line">        initialGlobalModVersion,</span><br><span class="line">        createTimestamp,</span><br><span class="line">        timeout);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create a co-location scheduling hint, if necessary</span></span><br><span class="line">    CoLocationGroup clg = jobVertex.getCoLocationGroup();</span><br><span class="line">    <span class="keyword">if</span> (clg != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.locationConstraint = clg.getLocationConstraint(subTaskIndex);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.locationConstraint = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    getExecutionGraph().registerExecution(currentExecution);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.timeout = timeout;</span><br><span class="line">    <span class="keyword">this</span>.inputSplits = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ExecutionVertex 创建时，主要做了下面这三件事：</p>
<ol>
<li>根据这个 ExecutionJobVertex 的 <code>producedDataSets</code>（IntermediateResult 类型的数组），给每个 ExecutionVertex 创建相应的 IntermediateResultPartition 对象，它代表了一个 IntermediateResult 分区；</li>
<li>调用 IntermediateResult 的 <code>setPartition()</code> 方法，记录 IntermediateResult 与 IntermediateResultPartition 之间的关系；</li>
<li>给这个 ExecutionVertex 创建一个 Execution 对象，如果这个 ExecutionVertex 重新调度（失败重新恢复等情况），那么 Execution 对应的 <code>attemptNumber</code> 将会自增加 1，这里初始化的时候其值为 0。</li>
</ol>
<h4 id="创建-ExecutionEdge"><a href="#创建-ExecutionEdge" class="headerlink" title="创建 ExecutionEdge"></a>创建 ExecutionEdge</h4><p>根据前面的流程图，接下来，看下 ExecutionJobVertex 的 <code>connectToPredecessors()</code> 方法。在这个方法中，主要做的工作是创建对应的 ExecutionEdge 对象，并使用这个对象将 ExecutionVertex 与 IntermediateResultPartition 连接起来，ExecutionEdge 的成员变量比较简单，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExecutionEdge.java</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExecutionEdge</span> </span>&#123;</span><br><span class="line">    <span class="comment">// source 节点</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntermediateResultPartition source;</span><br><span class="line">    <span class="comment">// target 节点</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ExecutionVertex target;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> inputNum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ExecutionEdge 的创建是在 ExecutionVertex 中 <code>connectSource()</code> 方法中实现的，代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExecutionVertex.java</span></span><br><span class="line"><span class="comment">//note: 与上游节点连在一起</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connectSource</span><span class="params">(<span class="keyword">int</span> inputNumber, IntermediateResult source, JobEdge edge, <span class="keyword">int</span> consumerNumber)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> DistributionPattern pattern = edge.getDistributionPattern();</span><br><span class="line">    <span class="keyword">final</span> IntermediateResultPartition[] sourcePartitions = source.getPartitions();</span><br><span class="line"></span><br><span class="line">    ExecutionEdge[] edges;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 只有 forward/RESCALE 的方式的情况下，pattern 才是 POINTWISE 的，否则均为 ALL_TO_ALL</span></span><br><span class="line">    <span class="keyword">switch</span> (pattern) &#123;</span><br><span class="line">        <span class="keyword">case</span> POINTWISE:</span><br><span class="line">            edges = connectPointwise(sourcePartitions, inputNumber);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> ALL_TO_ALL:</span><br><span class="line">            <span class="comment">//note: 它会连接上游所有的 IntermediateResultPartition</span></span><br><span class="line">            edges = connectAllToAll(sourcePartitions, inputNumber);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Unrecognized distribution pattern."</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    inputEdges[inputNumber] = edges;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the consumers to the source</span></span><br><span class="line">    <span class="comment">// for now (until the receiver initiated handshake is in place), we need to register the</span></span><br><span class="line">    <span class="comment">// edges as the execution graph</span></span><br><span class="line">    <span class="comment">//note: 之前已经为 IntermediateResult 添加了 consumer，这里为 IntermediateResultPartition 添加 consumer，即关联到 ExecutionEdge 上</span></span><br><span class="line">    <span class="keyword">for</span> (ExecutionEdge ee : edges) &#123;</span><br><span class="line">        ee.getSource().addConsumer(ee, consumerNumber);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在创建 ExecutionEdge 时，会根据这个 JobEdge 的 <code>DistributionPattern</code> 选择不同的实现，这里主要分两种情况，<code>DistributionPattern</code> 是跟 Partitioner 的配置有关（<a href="http://matt33.com/2019/12/09/flink-job-graph-3/#Partitioner">Partitioner 详解</a>）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// StreamingJobGraphGenerator.java</span></span><br><span class="line"><span class="comment">//note: 创建 JobEdge（它会连接上下游的 node）</span></span><br><span class="line">JobEdge jobEdge;</span><br><span class="line"><span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner || partitioner <span class="keyword">instanceof</span> RescalePartitioner) &#123;</span><br><span class="line">    jobEdge = downStreamVertex.connectNewDataSetAsInput( <span class="comment">//note: 这个方法会创建 IntermediateDataSet 对象</span></span><br><span class="line">        headVertex,</span><br><span class="line">        DistributionPattern.POINTWISE, <span class="comment">//note: 上游与下游的消费模式，（每个生产任务的 sub-task 会连接到消费任务的一个或多个 sub-task）</span></span><br><span class="line">        resultPartitionType);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">            headVertex,</span><br><span class="line">            DistributionPattern.ALL_TO_ALL, <span class="comment">//note: 每个生产任务的 sub-task 都会连接到每个消费任务的 sub-task</span></span><br><span class="line">            resultPartitionType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果 DistributionPattern 是 <code>ALL_TO_ALL</code> 模式，这个 ExecutionVertex 会与 IntermediateResult 对应的所有 IntermediateResultPartition 连接起来，而如果是 <code>POINTWISE</code> 模式，ExecutionVertex 只会与部分的 IntermediateResultPartition 连接起来。<code>POINTWISE</code> 模式下 IntermediateResultPartition 与 ExecutionVertex 之间的分配关系如下图所示，具体的分配机制是跟 IntermediateResultPartition 数与 ExecutionVertex 数有很大关系的，具体细节实现可以看下相应代码，这里只是举了几个示例。</p>
<p><img src="/images/flink/4-partitioner.png" alt="POINTWISE 模式下的分配机制"></p>
<p>到这里，这个作业的 ExecutionGraph 就创建完成了，有了 ExecutionGraph，JobManager 才能对这个作业做相应的调度。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文详细介绍了 JobGraph 如何转换为 ExecutionGraph 的过程。到这里，StreamGraph、 JobGraph 和 ExecutionGraph 的生成过程，在最近的三篇文章中已经详细讲述完了，后面将会给大家逐步介绍 runtime 的其他内容。</p>
<p>简单总结一下：</p>
<ol>
<li>streamGraph 是最原始的用户逻辑，是一个没有做任何优化的 DataFlow；</li>
<li>JobGraph 对 StreamGraph 做了一些优化，主要是将能够 Chain 在一起的算子 Chain 在一起，这一样可以减少网络 shuffle 的开销；</li>
<li>ExecutionGraph 则是作业运行是用来调度的执行图，可以看作是并行化版本的 JobGraph，将 DAG 拆分到基本的调度单元。</li>
</ol>
<hr>
<p>参考</p>
<ul>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/glossary.html#physical-graph" target="_blank" rel="external">Glossary</a>；</li>
<li><a href="http://chenyuzhao.me/2016/12/03/Flink%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6%E5%92%8C%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92/" target="_blank" rel="external">Flink 集群构建 &amp; 逻辑计划生成</a>；</li>
<li><a href="http://chenyuzhao.me/2017/02/06/flink%E7%89%A9%E7%90%86%E8%AE%A1%E5%88%92%E7%94%9F%E6%88%90/" target="_blank" rel="external">Flink 物理计划生成</a>；</li>
<li><a href="https://yq.aliyun.com/articles/225618" target="_blank" rel="external">Flink原理与实现：如何生成ExecutionGraph及物理执行图</a>；</li>
<li><a href="https://blog.jrwang.me/2019/flink-source-code-executiongraph/" target="_blank" rel="external">Flink 源码阅读笔记（3）- ExecutionGraph 的生成</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/22736103" target="_blank" rel="external">Flink源码解析-从API到JobGraph</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第四篇，紧接着前面两篇文章，在前两篇文章中介绍的 StreamGraph 和 JobGraph 都是在 client 端生成的，本文将会讲述 JobGraph 是如何转换成 ExecutionGraph 的。
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink Streaming 作业如何转化为 JobGraph</title>
    <link href="http://matt33.com/2019/12/09/flink-job-graph-3/"/>
    <id>http://matt33.com/2019/12/09/flink-job-graph-3/</id>
    <published>2019-12-09T01:34:07.000Z</published>
    <updated>2020-06-23T14:13:17.225Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第三篇，紧接着上一篇文章，本文主要讲述 StreamGraph 是如何转换成 JobGraph 的，在前面的文章中，我们知道 StreamGraph 是根据用户作业的处理逻生成初始的逻辑计划，它并没有做任何的优化，而 JobGraph 将会在原来的基础上做相应的优化（主要是算子的 Chain 操作，Chain 在一起的算子将会在同一个 task 上运行，会极大减少 shuffle 的开销）。刚开始接触的同学可能会有一个疑问，为什么要有 StreamGraph 和 JobGraph 两层的 Graph，这里最主要的原因是为兼容 batch process，Streaming process 最初产生的是 StreamGraph，而 batch process 产生的则是 OptimizedPlan，但是它们最后都会转换为 JobGraph，本文主要是以 Streaming 作业的 StreamGraph 转换为 JobGraph 的处理流程来介绍。</p>
<h2 id="生成-JobGraph-的整体流程"><a href="#生成-JobGraph-的整体流程" class="headerlink" title="生成 JobGraph 的整体流程"></a>生成 JobGraph 的整体流程</h2><p>这里我们先看下 FlinkPlan 的实现，它主要有两个实现类：StreamGraph 和 OptimizedPlan，分别对应 Streaming 和 Batch process，不管是哪种类型最后可以转换为 JobGraph：</p>
<p><img src="/images/flink/3-FlinkPlan.png" alt="FlinkPlan 的实现"></p>
<p>OptimizedPlan 可以通过 JobGraphGenerator 的 <code>compileJobGraph()</code> 方法来转换为 JobGraph，而 StreamGraph 则可以通过 StreamingJobGraphGenerator 的 <code>createJobGraph()</code> 方法来转换为相应的 JobGraph。其中，StreamGraph 的整体转换流程如下图所示（下图主要展示了这个流程涉及到主要方法调用，比较核心的方法图中也加了颜色，也是本文会着重讲述的方法）：</p>
<p><img src="/images/flink/3-create-job-graph.png" alt="生成 JobGraph 的流程"></p>
<h2 id="具体实现流程"><a href="#具体实现流程" class="headerlink" title="具体实现流程"></a>具体实现流程</h2><p>StreamingJobGraphGenerator 的 <code>createJobGraph()</code> 的方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 根据 StreamGraph 生成 JobGraph</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> JobGraph <span class="title">createJobGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// make sure that all vertices start immediately</span></span><br><span class="line">    <span class="comment">//note: 设置调度模式</span></span><br><span class="line">    jobGraph.setScheduleMode(streamGraph.getScheduleMode());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Generate deterministic hashes for the nodes in order to identify them across</span></span><br><span class="line">    <span class="comment">// submission iff they didn't change.</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * note: 为每个 SteamNode 生成一个确定的 hash id，如果提交的拓扑没有改变，则每次生成的 hash id 都是一样的</span></span><br><span class="line"><span class="comment">     * note: 这里只要保证 source 的顺序是确定的，就可以保证最后生产的 hash id 不变</span></span><br><span class="line"><span class="comment">     * note: 它是利用 input 节点的 hash 值及该节点在 map 中位置（实际上是 map.size 算的）来计算确定的</span></span><br><span class="line"><span class="comment">     * note: 实现逻辑见 &#123;<span class="doctag">@link</span> StreamGraphHasherV2#traverseStreamGraphAndGenerateHashes(StreamGraph)&#125;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Generate legacy version hashes for backwards compatibility</span></span><br><span class="line">    <span class="comment">//note: 这个设置主要是为了防止 hash 机制变化时出现不兼容的情况</span></span><br><span class="line">    List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes = <span class="keyword">new</span> ArrayList&lt;&gt;(legacyStreamGraphHashers.size());</span><br><span class="line">    <span class="keyword">for</span> (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123;</span><br><span class="line">        legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 最重要的函数，生成 JobVertex/JobEdge 等，并尽可能地将多个节点 chain 在一起</span></span><br><span class="line">    setChaining(hashes, legacyHashes, chainedOperatorHashes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 将每个 JobVertex 的入边集合也序列化到该 JobVertex 的 StreamConfig 中 (出边集合已经在 setChaining 的时候写入了)</span></span><br><span class="line">    setPhysicalEdges();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 为每个 JobVertex 指定所属的 SlotSharingGroup 以及设置 CoLocationGroup</span></span><br><span class="line">    setSlotSharingAndCoLocation();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: checkpoint相关的配置</span></span><br><span class="line">    configureCheckpointing();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 用户的第三方依赖包就是在这里（cacheFile）传给 JobGraph</span></span><br><span class="line">    JobGraphGenerator.addUserArtifactEntries(streamGraph.getUserArtifacts(), jobGraph);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set the ExecutionConfig last when it has been finalized</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 将 StreamGraph 的 ExecutionConfig 序列化到 JobGraph 的配置中</span></span><br><span class="line">        jobGraph.setExecutionConfig(streamGraph.getExecutionConfig());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalConfigurationException(<span class="string">"Could not serialize the ExecutionConfig."</span> +</span><br><span class="line">                <span class="string">"This indicates that non-serializable types (like custom serializers) were registered"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>核心步骤如下：</p>
<ol>
<li>先给每个 StreamNode 生成一个唯一确定的 hash id；</li>
<li><code>setChaining()</code> 方法将可以 Chain 到一起的 StreamNode Chain 在一起，这里会生成相应的 JobVertex 、JobEdge 、 IntermediateDataSet 对象，JobGraph 的 Graph 在这一步就已经完全构建出来了；</li>
<li><code>setPhysicalEdges()</code> 方法会将每个 JobVertex 的入边集合也序列化到该 JobVertex 的 StreamConfig 中 (出边集合已经在 setChaining 的时候写入了)；</li>
<li><code>setSlotSharingAndCoLocation()</code> 方法主要是 JobVertex 的 SlotSharingGroup 和 CoLocationGroup 设置；</li>
<li><code>configureCheckpointing()</code> 方法主要是 checkpoint 相关的设置。</li>
</ol>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>JobGraph 又引入了几个概念，这里先简单介绍一下。</p>
<ol>
<li><strong>StreamConfig</strong>: 它会记录一个 StreamOperator 的配置信息，它保存了这个 StreamOperator 的基本信息，在这里它会将 StreamGraph 中的 StreamNode 的详细信息同步到它对应的 StreamConfig 对象中；</li>
<li><strong>JobVertex</strong>: JobVertex 相当于是 JobGraph 的顶点，跟 StreamNode 的区别是，它是 Operator Chain 之后的顶点，会包含多个 StreamNode；</li>
<li><strong>IntermediateDataSet</strong>: 它是由一个 Operator（可能是 source，也可能是某个中间算子）产生的一个中间数据集；</li>
<li><strong>JobEdge</strong>: 它相当于是 JobGraph 中的边（连接通道），这个边连接的是一个 IntermediateDataSet 跟一个要消费的 JobVertex。</li>
</ol>
<p>如果跟前面的 StreamGraph 做对比，JobGraph 这里不但会对算子做 Chain 操作，还多抽象了一个概念 —— IntermediateDataSet，IntermediateDataSet 的抽象主要是为了后面 ExecutionGraph 的生成。</p>
<h3 id="算子是如何-Chain-到一起的"><a href="#算子是如何-Chain-到一起的" class="headerlink" title="算子是如何 Chain 到一起的"></a>算子是如何 Chain 到一起的</h3><p>这里，我们来介绍一下生成的 JobGraph 过程中最核心一步，算子如何 Chain 到一起，先看一下示例，示例与前面两篇文章的示例是一样的（这里因为图片大小限制，去掉了 filter 算子），StreamGraph 及转换后的 JobGraph 如何下图所示：</p>
<p><img src="/images/flink/3-operator-chain.png" alt="Operator Chain 的示例"></p>
<p>StreamGraph 转换为 JobGraph 的处理过程主要是在 <code>setChaining()</code> 中完成，先看下这个方法的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Sets up task chains from the source &#123;<span class="doctag">@link</span> StreamNode&#125; instances.</span></span><br><span class="line"><span class="comment"> * note：从 Source StreamNode 实例开始设置 task chain，它将会递归地创建所有的 JobVertex 实例</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;This will recursively create all &#123;<span class="doctag">@link</span> JobVertex&#125; instances.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setChaining</span><span class="params">(Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes, List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes, Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Integer sourceNodeId : streamGraph.getSourceIDs()) &#123;</span><br><span class="line">        <span class="comment">//note: 处理每个 Source StreamNode</span></span><br><span class="line">        createChain(sourceNodeId, sourceNodeId, hashes, legacyHashes, <span class="number">0</span>, chainedOperatorHashes);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> List&lt;StreamEdge&gt; <span class="title">createChain</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        Integer startNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">        Integer currentNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes,</span></span></span><br><span class="line"><span class="function"><span class="params">        List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> chainIndex,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!builtVertices.contains(startNodeId)) &#123;</span><br><span class="line"></span><br><span class="line">        List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 以 Edge 的粒度，记录上下游算子能 chain 在一起的 Edge</span></span><br><span class="line">        List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">        List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 当前要处理的 StreamNode</span></span><br><span class="line">        StreamNode currentNode = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 遍历当前的输出节点，判断是否可以 chain 在一起</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge outEdge : currentNode.getOutEdges()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isChainable(outEdge, streamGraph)) &#123; <span class="comment">//note: 如果可以 chain 在一起的话</span></span><br><span class="line">                chainableOutputs.add(outEdge);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                nonChainableOutputs.add(outEdge);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 递归调用</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">            <span class="comment">//note: 如果可以 chain 在一起的话，这里的 chainIndex 会加 1</span></span><br><span class="line">            transitiveOutEdges.addAll(</span><br><span class="line">                    createChain(startNodeId, chainable.getTargetId(), hashes, legacyHashes, chainIndex + <span class="number">1</span>, chainedOperatorHashes));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.add(nonChainable);</span><br><span class="line">            <span class="comment">//note: 不能 chain 一起的话，这里的 chainIndex 是从 0 开始算的，后面也肯定会走到 createJobVertex 的逻辑</span></span><br><span class="line">            createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes, legacyHashes, <span class="number">0</span>, chainedOperatorHashes);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 记录每个 startNodeId 的 hash id（主要是 legacyHashes 中记录的）</span></span><br><span class="line">        List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; operatorHashes =</span><br><span class="line">            chainedOperatorHashes.computeIfAbsent(startNodeId, k -&gt; <span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span>[] primaryHashBytes = hashes.get(currentNodeId);</span><br><span class="line">        <span class="comment">//note: OperatorID</span></span><br><span class="line">        OperatorID currentOperatorId = <span class="keyword">new</span> OperatorID(primaryHashBytes);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map&lt;Integer, <span class="keyword">byte</span>[]&gt; legacyHash : legacyHashes) &#123;</span><br><span class="line">            operatorHashes.add(<span class="keyword">new</span> Tuple2&lt;&gt;(primaryHashBytes, legacyHash.get(currentNodeId)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 记录 chainedName</span></span><br><span class="line">        chainedNames.put(currentNodeId, createChainedName(currentNodeId, chainableOutputs));</span><br><span class="line">        <span class="comment">//note: 计算 Chain 之后 node 的 minResources</span></span><br><span class="line">        chainedMinResources.put(currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs));</span><br><span class="line">        <span class="comment">//note: 计算 Chain 之后 node 的资源上限</span></span><br><span class="line">        chainedPreferredResources.put(currentNodeId, createChainedPreferredResources(currentNodeId, chainableOutputs));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: InputFormat &amp; OutputFormat 的处理</span></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getInputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId).addInputFormat(currentOperatorId, currentNode.getInputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getOutputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId).addOutputFormat(currentOperatorId, currentNode.getOutputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 如果当前节点是 chain 的起始节点, 则直接创建 JobVertex 并返回 StreamConfig, 否则先创建一个空的 StreamConfig</span></span><br><span class="line">        <span class="comment">//note: 这里实际上，如果节点不能 chain 在一起，那么 currentNodeId 跟 startNodeId 肯定是不相等的</span></span><br><span class="line">        <span class="comment">//note: createJobVertex 函数就是根据 StreamNode 创建对应的 JobVertex, 并返回了空的 StreamConfig</span></span><br><span class="line">        StreamConfig config = currentNodeId.equals(startNodeId)</span><br><span class="line">                ? createJobVertex(startNodeId, hashes, legacyHashes, chainedOperatorHashes)<span class="comment">//note: chain 的起始 StreamNode</span></span><br><span class="line">                : <span class="keyword">new</span> StreamConfig(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 设置 JobVertex 的 StreamConfig, 基本上是将 StreamNode 中的配置设置到 StreamConfig 中</span></span><br><span class="line">        setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123; <span class="comment">//note: 如果走到这里，证明这个 chain 已经完成</span></span><br><span class="line">            <span class="comment">//note: chain 中起始 StreamNode</span></span><br><span class="line">            config.setChainStart();</span><br><span class="line">            config.setChainIndex(<span class="number">0</span>);</span><br><span class="line">            config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName());</span><br><span class="line">            <span class="comment">//note: Config 中也会记录这个 chain 的出边</span></span><br><span class="line">            config.setOutEdgesInOrder(transitiveOutEdges);</span><br><span class="line">            config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges());</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</span><br><span class="line">                <span class="comment">//note: 构建 graph</span></span><br><span class="line">                connect(startNodeId, edge);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//note: 将 chain 中所有子节点的 StreamConfig 写入到 headOfChain 节点的 CHAINED_TASK_CONFIG 配置中</span></span><br><span class="line">            config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId));</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">//note: 如果是 chain 中子节点</span></span><br><span class="line">            chainedConfigs.computeIfAbsent(startNodeId, k -&gt; <span class="keyword">new</span> HashMap&lt;Integer, StreamConfig&gt;());</span><br><span class="line"></span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            StreamNode node = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line">            config.setOperatorName(node.getOperatorName());</span><br><span class="line">            <span class="comment">//note: 将当前 StreamNode 的 config 记录到该 chain 的 config 集合中</span></span><br><span class="line">            chainedConfigs.get(startNodeId).put(currentNodeId, config);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        config.setOperatorID(currentOperatorId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (chainableOutputs.isEmpty()) &#123;</span><br><span class="line">            config.setChainEnd();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> transitiveOutEdges;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码处理完成后，整个 JobGraph 就构建完成了，它首先从会遍历这个 StreamGraph 的 source 节点，然后选择从 source 节点开始执行 <code>createChain()</code> 方法，在具体的实现里，主要逻辑如下（需要配合前面的代码去看，这里会把多个 StreamNode Chain 在一起的 Node 叫做 ChainNode，方便讲述）：</p>
<ol>
<li><code>createChain()</code> 当前要处理的节点是 <code>currentNodeId</code>，先从 StreamGraph 中拿到这个 StreamNode 的 outEdge（<code>currentNode.getOutEdges()</code>），然后判断这个 outEdge 连接的两个 StreamNode 是否可以 Chain 在一起，判断方法是 <code>isChainable()</code>；</li>
<li>紧接着会有一个递归调用：<ul>
<li>对于可以 Chain 在一起的 StreamEdge（这个 Edge 连接两个 StreamNode 是可以 Chain 在一起），会再次调用 <code>createChain()</code> 方法，并且 <code>createChain()</code> 中的 <code>startNodeId</code> 还是最开始的 <code>startNodeId</code>（这个标识了这个 ChainNode 的开始 NodeId），而 <code>chainIndex</code> 会自增加 1；</li>
<li>而对于不能 Chain 在一起的 StreamEdge，<code>createChain()</code> 中的 <code>startNodeId</code>  变成了这个 StreamEdge 的 target StreamNode（相当于如果 Chain 在一起，ChainNode 中的 startNodeId 会赋值为下一个节点的 NodeId，然后再依次类推），<code>chainIndex</code> 又从 0 开始计；</li>
<li>也就是说：<code>createChain()</code> 中的 <code>startNodeId</code> 表示了当前可以 Chain 之后 Node 的 startId，这里，会一直递归调用，直到达到 Sink 节点。</li>
</ul>
</li>
<li>然后在生成 <code>StreamConfig</code> 对象时，判断当前的 <code>currentNodeId</code> 与 <code>startNodeId</code> 是否相等，如果相等的话，证明当前 Node 就是这个 ChainNode 的 StartNode，这里会调用 <code>createJobVertex()</code> 方法给这个 ChainNode 创建一个 JobVertex 对象，最后会返回一个 StreamConfig 对象，如果前面的 id 不相等的话，这里会直接返回一个 StreamConfig 对象（这个对象主要是记录当前 StreamNode 的一些配置，它会同步 StreamGraph 中相关的配置）；</li>
<li>最后还会分两种情况判断：<ul>
<li>如果 id 相等，相当于这个 ChainNode 已经完成，先做一些相关的配置（比如：标识当前 StreamNode 为这个 JobVertex 的起始 node），最后再通过 <code>connect()</code> 方法创建 JobEdge 和 IntermediateDataSet 对象，把这个 Graph 连接起来；</li>
<li>如果 id 不相等，那么证明当前 StreamNode 只是这个 ChainNode 的一部分，这里只是同步一下信息，并记录到缓存。</li>
</ul>
</li>
</ol>
<p>上面就是这个方法的主要实现逻辑，下面会详细把这个方法展开，重点介绍其中的一些方法实现。</p>
<h4 id="如何判断算子是否可以-Chain-在一起"><a href="#如何判断算子是否可以-Chain-在一起" class="headerlink" title="如何判断算子是否可以 Chain 在一起"></a>如何判断算子是否可以 Chain 在一起</h4><p>两个 StreamNode 是否可以 Chain 到一起，是通过 <code>isChainable()</code> 方法来判断的，这里判断的粒度是 StreamEdge，实际上就是判断 StreamEdge 连接的两个 StreamNode 是否 Chain 在一起：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 是否可以 chain 在一起</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isChainable</span><span class="params">(StreamEdge edge, StreamGraph streamGraph)</span> </span>&#123;</span><br><span class="line">    StreamNode upStreamVertex = streamGraph.getSourceVertex(edge); <span class="comment">//note: edge 的 source node</span></span><br><span class="line">    StreamNode downStreamVertex = streamGraph.getTargetVertex(edge); <span class="comment">//note: edge 的 sink node</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 获取输入和输出的 Operator Factory</span></span><br><span class="line">    StreamOperatorFactory&lt;?&gt; headOperator = upStreamVertex.getOperatorFactory();</span><br><span class="line">    StreamOperatorFactory&lt;?&gt; outOperator = downStreamVertex.getOperatorFactory();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> downStreamVertex.getInEdges().size() == <span class="number">1</span> <span class="comment">//note: 下游 Operator 的 Edge 只有一个（如果是多个合并，是无法 Chain 在一起的）</span></span><br><span class="line">            &amp;&amp; outOperator != <span class="keyword">null</span></span><br><span class="line">            &amp;&amp; headOperator != <span class="keyword">null</span></span><br><span class="line">            &amp;&amp; upStreamVertex.isSameSlotSharingGroup(downStreamVertex) <span class="comment">//note: 对应的 slotSharingGroup 一样</span></span><br><span class="line">            &amp;&amp; outOperator.getChainingStrategy() == ChainingStrategy.ALWAYS <span class="comment">//note: out operator 允许 chain 操作</span></span><br><span class="line">            &amp;&amp; (headOperator.getChainingStrategy() == ChainingStrategy.HEAD || <span class="comment">//note: head Operator 允许跟后面的 chain 在一起</span></span><br><span class="line">                headOperator.getChainingStrategy() == ChainingStrategy.ALWAYS)</span><br><span class="line">            &amp;&amp; (edge.getPartitioner() <span class="keyword">instanceof</span> ForwardPartitioner) <span class="comment">//note: partitioner 是 ForwardPartitioner 类型</span></span><br><span class="line">            &amp;&amp; edge.getShuffleMode() != ShuffleMode.BATCH</span><br><span class="line">            &amp;&amp; upStreamVertex.getParallelism() == downStreamVertex.getParallelism() <span class="comment">//note: 并发相等</span></span><br><span class="line">            &amp;&amp; streamGraph.isChainingEnabled(); <span class="comment">//note: StreamGraph 允许 Chain 在一起</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法判断的指标有很多，具体看上面代码就可以明白，这里着重介绍两个：<code>slotSharingGroup</code> 和 <code>edge.getPartitioner()</code>。</p>
<h5 id="slotSharingGroup"><a href="#slotSharingGroup" class="headerlink" title="slotSharingGroup"></a>slotSharingGroup</h5><p>先看下一个 StreamNode 的 <code>slotSharingGroup</code> 是如何生成的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.flink.streaming.api.graph.StreamGraphGenerator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_SLOT_SHARING_GROUP = <span class="string">"default"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Determines the slot sharing group for an operation based on the slot sharing group set by</span></span><br><span class="line"><span class="comment"> * the user and the slot sharing groups of the inputs.</span></span><br><span class="line"><span class="comment"> * note: 根据这个 operation 设置的 slot sharing group 和 inputs 的 slot sharing group 来确定其 slot sharing group</span></span><br><span class="line"><span class="comment"> * note：1. 如果用户指定了 group name，直接使用这个 name；</span></span><br><span class="line"><span class="comment"> * note：2. 如果所有的 input 都是同一个 group name，使用这个即可；</span></span><br><span class="line"><span class="comment"> * note：3. 否则使用 default group；</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;If the user specifies a group name, this is taken as is. If nothing is specified and</span></span><br><span class="line"><span class="comment"> * the input operations all have the same group name then this name is taken. Otherwise the</span></span><br><span class="line"><span class="comment"> * default group is chosen.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> specifiedGroup The group specified by the user. note: 用户指定的 group name</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> inputIds The IDs of the input operations. note: 输入 operation 的 id 集合</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">determineSlotSharingGroup</span><span class="params">(String specifiedGroup, Collection&lt;Integer&gt; inputIds)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!isSlotSharingEnabled) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (specifiedGroup != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> specifiedGroup;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        String inputGroup = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> id: inputIds) &#123;</span><br><span class="line">            String inputGroupCandidate = streamGraph.getSlotSharingGroup(id);</span><br><span class="line">            <span class="keyword">if</span> (inputGroup == <span class="keyword">null</span>) &#123;</span><br><span class="line">                inputGroup = inputGroupCandidate;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inputGroup.equals(inputGroupCandidate)) &#123;</span><br><span class="line">                <span class="keyword">return</span> DEFAULT_SLOT_SHARING_GROUP;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> inputGroup == <span class="keyword">null</span> ? DEFAULT_SLOT_SHARING_GROUP : inputGroup;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>一个 StreamNode 的 SlotSharingGroup 会按照下面这个逻辑来确定:</p>
<ol>
<li>如果用户指定了 SlotSharingGroup，直接使用这个 SlotSharingGroup name；</li>
<li>如果所有的 input 都是同一个 group name，使用这个即可；</li>
<li>否则使用 default group；</li>
</ol>
<h5 id="Partitioner"><a href="#Partitioner" class="headerlink" title="Partitioner"></a>Partitioner</h5><p>这个 StreamEdge 的属性，在创建 StreamEdge 对象会配置这个属性，先看 Flink 中提供的 Partitioner 有哪几种：</p>
<p><img src="/images/flink/3-StreamPartitioner.png" alt="StreamPartitioner 的实现"></p>
<p>用户可以在自己的代码中调用 DataStream API （比如：<code>broadcast()</code>、<code>shuffle()</code> 等）配置相应的 StreamPartitioner，如果这个没有指定 StreamPartitioner 的话，则会走下面的逻辑创建默认的 StreamPartitioner：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.streaming.api.graph.StreamGraph</span></span><br><span class="line"><span class="comment">//note: 未指定 partitioner 的话，会为其选择 forward（并发设置相同时） 或 rebalance（并发设置不同时）</span></span><br><span class="line"><span class="keyword">if</span> (partitioner == <span class="keyword">null</span> &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">    partitioner = <span class="keyword">new</span> ForwardPartitioner&lt;Object&gt;();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">    partitioner = <span class="keyword">new</span> RebalancePartitioner&lt;Object&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="创建-JobVertex-节点"><a href="#创建-JobVertex-节点" class="headerlink" title="创建 JobVertex 节点"></a>创建 JobVertex 节点</h4><p>JobVertex 对象的创建是在 <code>createJobVertex()</code> 方法中实现的，这个方法实现比较简单，创建相应的 JobVertex 对象，并把相关的配置信息设置到 JobVertex 对象中就完成了，这里就不再展开详细介绍了。</p>
<h4 id="connect-创建-JobEdge-和-IntermediateDataSet-对象"><a href="#connect-创建-JobEdge-和-IntermediateDataSet-对象" class="headerlink" title="connect() 创建 JobEdge 和 IntermediateDataSet 对象"></a><code>connect()</code> 创建 JobEdge 和 IntermediateDataSet 对象</h4><p><code>connect()</code> 方法在执行的时候，它会遍历 <code>transitiveOutEdges</code> 中的 StreamEdge，也就是这个 ChainNode 的 out StreamEdge（这些 StreamEdge 是不能与前面的 ChainNode Chain 在一起）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.flink.streaming.api.graph.StreamGraphGenerator</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">(Integer headOfChain, StreamEdge edge)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 记录 StreamEdge，这个主要是 chain 之间的边</span></span><br><span class="line">    physicalEdgesInOrder.add(edge);</span><br><span class="line"></span><br><span class="line">    Integer downStreamvertexID = edge.getTargetId();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 这里 headVertex 指的是 headOfChain 对应的 JobVertex（也是当前 node 对应的 vertex）</span></span><br><span class="line">    JobVertex headVertex = jobVertices.get(headOfChain);</span><br><span class="line">    JobVertex downStreamVertex = jobVertices.get(downStreamvertexID);</span><br><span class="line"></span><br><span class="line">    StreamConfig downStreamConfig = <span class="keyword">new</span> StreamConfig(downStreamVertex.getConfiguration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 这个节点的输入数增加 1</span></span><br><span class="line">    downStreamConfig.setNumberOfInputs(downStreamConfig.getNumberOfInputs() + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamPartitioner&lt;?&gt; partitioner = edge.getPartitioner();</span><br><span class="line"></span><br><span class="line">    ResultPartitionType resultPartitionType;</span><br><span class="line">    <span class="keyword">switch</span> (edge.getShuffleMode()) &#123;</span><br><span class="line">        <span class="keyword">case</span> PIPELINED:</span><br><span class="line">            resultPartitionType = ResultPartitionType.PIPELINED_BOUNDED;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> BATCH:</span><br><span class="line">            resultPartitionType = ResultPartitionType.BLOCKING;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> UNDEFINED:</span><br><span class="line">            resultPartitionType = streamGraph.isBlockingConnectionsBetweenChains() ?</span><br><span class="line">                    ResultPartitionType.BLOCKING : ResultPartitionType.PIPELINED_BOUNDED;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Data exchange mode "</span> +</span><br><span class="line">                edge.getShuffleMode() + <span class="string">" is not supported yet."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建 JobEdge（它会连接上下游的 node）</span></span><br><span class="line">    JobEdge jobEdge;</span><br><span class="line">    <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner || partitioner <span class="keyword">instanceof</span> RescalePartitioner) &#123;</span><br><span class="line">        jobEdge = downStreamVertex.connectNewDataSetAsInput( <span class="comment">//note: 这个方法会创建 IntermediateDataSet 对象</span></span><br><span class="line">            headVertex,</span><br><span class="line">            DistributionPattern.POINTWISE, <span class="comment">//note: 上游与下游的消费模式，（每个生产任务的 sub-task 会连接到消费任务的一个或多个 sub-task）</span></span><br><span class="line">            resultPartitionType);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        jobEdge = downStreamVertex.connectNewDataSetAsInput(</span><br><span class="line">                headVertex,</span><br><span class="line">                DistributionPattern.ALL_TO_ALL, <span class="comment">//note: 每个生产任务的 sub-task 都会连接到每个消费任务的 sub-task</span></span><br><span class="line">                resultPartitionType);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// set strategy name so that web interface can show it.</span></span><br><span class="line">    <span class="comment">//note: 设置 partitioner</span></span><br><span class="line">    jobEdge.setShipStrategyName(partitioner.toString());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(<span class="string">"CONNECTED: &#123;&#125; - &#123;&#125; -&gt; &#123;&#125;"</span>, partitioner.getClass().getSimpleName(),</span><br><span class="line">                headOfChain, downStreamvertexID);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>真正创建 JobEdge 和 IntermediateDataSet 对象是在 JobVertex 中的 <code>connectNewDataSetAsInput()</code> 方法中，在这里也会把 JobVertex、JobEdge、IntermediateDataSet 三者连接起来（JobGraph 的 graph 就是这样构建的）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.flink.runtime.jobgraph.JobVertex</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobEdge <span class="title">connectNewDataSetAsInput</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        JobVertex input,</span></span></span><br><span class="line"><span class="function"><span class="params">        DistributionPattern distPattern,</span></span></span><br><span class="line"><span class="function"><span class="params">        ResultPartitionType partitionType)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 连接 Vertex 的中间数据集</span></span><br><span class="line">    IntermediateDataSet dataSet = input.createAndAddResultDataSet(partitionType);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建对应的 edge</span></span><br><span class="line">    JobEdge edge = <span class="keyword">new</span> JobEdge(dataSet, <span class="keyword">this</span>, distPattern);</span><br><span class="line">    <span class="keyword">this</span>.inputs.add(edge);</span><br><span class="line">    dataSet.addConsumer(edge);</span><br><span class="line">    <span class="keyword">return</span> edge;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里，<code>createChain()</code> 方法就执行完了，在 JobGraph 总共会涉及到三个对象：JobVertex、JobEdge 和 IntermediateDataSet，最后生成的 JobGraph 大概下面这个样子：</p>
<p><img src="/images/flink/3-JobGraph.png" alt="JobGraph"></p>
<h3 id="JobGraph-的其他配置"><a href="#JobGraph-的其他配置" class="headerlink" title="JobGraph 的其他配置"></a>JobGraph 的其他配置</h3><p>执行完 <code>setChaining()</code> 方法后，下面还有几步操作：</p>
<ol>
<li><code>setPhysicalEdges()</code>: 将每个 JobVertex 的入边集合也序列化到该 JobVertex 的 StreamConfig 中 (出边集合已经在 setChaining 的时候写入了)；</li>
<li><code>setSlotSharingAndCoLocation()</code>: 为每个 JobVertex 指定所属的 SlotSharingGroup 以及设置 CoLocationGroup；</li>
<li><code>configureCheckpointing()</code>: checkpoint相关的配置；</li>
<li><code>JobGraphGenerator.addUserArtifactEntries()</code>: 用户依赖的第三方包就是在这里（cacheFile）传给 JobGraph；</li>
</ol>
<p>这几个方法的实现比较简单，这里简单看下 <code>configureCheckpointing()</code> 这个方法，其他三个就不再叙述了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.flink.streaming.api.graph.StreamGraphGenerator</span></span><br><span class="line"><span class="comment">//note: 主要是 checkpoint 相关的配置</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">configureCheckpointing</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    CheckpointConfig cfg = streamGraph.getCheckpointConfig();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> interval = cfg.getCheckpointInterval();</span><br><span class="line">    <span class="keyword">if</span> (interval &lt; MINIMAL_CHECKPOINT_TIME) &#123;</span><br><span class="line">        <span class="comment">// interval of max value means disable periodic checkpoint</span></span><br><span class="line">        interval = Long.MAX_VALUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  --- configure the participating vertices ---</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 配置 checkpoint 中要参与的 vertices 节点信息</span></span><br><span class="line">    <span class="comment">// collect the vertices that receive "trigger checkpoint" messages.</span></span><br><span class="line">    <span class="comment">// currently, these are all the sources</span></span><br><span class="line">    <span class="comment">//note: 记录接收 trigger checkpoint msg 的 vertices，当前都是 source 的情况</span></span><br><span class="line">    List&lt;JobVertexID&gt; triggerVertices = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// collect the vertices that need to acknowledge the checkpoint</span></span><br><span class="line">    <span class="comment">// currently, these are all vertices</span></span><br><span class="line">    <span class="comment">//note: 记录当前需要向 checkpoint coordinator 发送 ack 的 vertices，当前指的是所有的 vertices</span></span><br><span class="line">    List&lt;JobVertexID&gt; ackVertices = <span class="keyword">new</span> ArrayList&lt;&gt;(jobVertices.size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// collect the vertices that receive "commit checkpoint" messages</span></span><br><span class="line">    <span class="comment">// currently, these are all vertices</span></span><br><span class="line">    <span class="comment">//note: 记录接收 'commit checkpoint' 的 vertices，当前也指的是所有 vertices</span></span><br><span class="line">    List&lt;JobVertexID&gt; commitVertices = <span class="keyword">new</span> ArrayList&lt;&gt;(jobVertices.size());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (JobVertex vertex : jobVertices.values()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (vertex.isInputVertex()) &#123;</span><br><span class="line">            triggerVertices.add(vertex.getID());</span><br><span class="line">        &#125;</span><br><span class="line">        commitVertices.add(vertex.getID());</span><br><span class="line">        ackVertices.add(vertex.getID());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  --- configure options ---</span></span><br><span class="line"></span><br><span class="line">    CheckpointRetentionPolicy retentionAfterTermination;</span><br><span class="line">    <span class="keyword">if</span> (cfg.isExternalizedCheckpointsEnabled()) &#123;</span><br><span class="line">        CheckpointConfig.ExternalizedCheckpointCleanup cleanup = cfg.getExternalizedCheckpointCleanup();</span><br><span class="line">        <span class="comment">// Sanity check</span></span><br><span class="line">        <span class="keyword">if</span> (cleanup == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Externalized checkpoints enabled, but no cleanup mode configured."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        retentionAfterTermination = cleanup.deleteOnCancellation() ?</span><br><span class="line">                CheckpointRetentionPolicy.RETAIN_ON_FAILURE :</span><br><span class="line">                CheckpointRetentionPolicy.RETAIN_ON_CANCELLATION;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//note: 默认是 NEVER_RETAIN_AFTER_TERMINATION，作业只要进入终止 checkpoint 就会删除</span></span><br><span class="line">        retentionAfterTermination = CheckpointRetentionPolicy.NEVER_RETAIN_AFTER_TERMINATION;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 默认是 EXACTLY_ONCE</span></span><br><span class="line">    CheckpointingMode mode = cfg.getCheckpointingMode();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isExactlyOnce;</span><br><span class="line">    <span class="keyword">if</span> (mode == CheckpointingMode.EXACTLY_ONCE) &#123;</span><br><span class="line">        isExactlyOnce = <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (mode == CheckpointingMode.AT_LEAST_ONCE) &#123;</span><br><span class="line">        isExactlyOnce = <span class="keyword">false</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unexpected checkpointing mode. "</span> +</span><br><span class="line">            <span class="string">"Did not expect there to be another checkpointing mode besides "</span> +</span><br><span class="line">            <span class="string">"exactly-once or at-least-once."</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  --- configure the master-side checkpoint hooks ---</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> ArrayList&lt;MasterTriggerRestoreHook.Factory&gt; hooks = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (StreamNode node : streamGraph.getStreamNodes()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (node.getOperatorFactory() <span class="keyword">instanceof</span> UdfStreamOperatorFactory) &#123;</span><br><span class="line">            Function f = ((UdfStreamOperatorFactory) node.getOperatorFactory()).getUserFunction();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (f <span class="keyword">instanceof</span> WithMasterCheckpointHook) &#123;</span><br><span class="line">                <span class="comment">//note: 它会在 CheckpointCoordinator 端在每次 checkpoint 及 restore 时触发一个 'global action'</span></span><br><span class="line">                <span class="comment">//note: 比如这里可以通过这个接口将状态刷到外部存储</span></span><br><span class="line">                hooks.add(<span class="keyword">new</span> FunctionMasterCheckpointHookFactory((WithMasterCheckpointHook&lt;?&gt;) f));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// because the hooks can have user-defined code, they need to be stored as</span></span><br><span class="line">    <span class="comment">// eagerly serialized values</span></span><br><span class="line">    <span class="comment">//note: 这里对 hooks 做一下序列化</span></span><br><span class="line">    <span class="keyword">final</span> SerializedValue&lt;MasterTriggerRestoreHook.Factory[]&gt; serializedHooks;</span><br><span class="line">    <span class="keyword">if</span> (hooks.isEmpty()) &#123;</span><br><span class="line">        serializedHooks = <span class="keyword">null</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            MasterTriggerRestoreHook.Factory[] asArray =</span><br><span class="line">                    hooks.toArray(<span class="keyword">new</span> MasterTriggerRestoreHook.Factory[hooks.size()]);</span><br><span class="line">            serializedHooks = <span class="keyword">new</span> SerializedValue&lt;&gt;(asArray);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> FlinkRuntimeException(<span class="string">"Trigger/restore hook is not serializable"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// because the state backend can have user-defined code, it needs to be stored as</span></span><br><span class="line">    <span class="comment">// eagerly serialized value</span></span><br><span class="line">    <span class="comment">//note: 对 state backend 类做下序列化</span></span><br><span class="line">    <span class="keyword">final</span> SerializedValue&lt;StateBackend&gt; serializedStateBackend;</span><br><span class="line">    <span class="keyword">if</span> (streamGraph.getStateBackend() == <span class="keyword">null</span>) &#123;</span><br><span class="line">        serializedStateBackend = <span class="keyword">null</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            serializedStateBackend =</span><br><span class="line">                <span class="keyword">new</span> SerializedValue&lt;StateBackend&gt;(streamGraph.getStateBackend());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> FlinkRuntimeException(<span class="string">"State backend is not serializable"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  --- done, put it all together ---</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建一个 JobCheckpointingSettings 对象</span></span><br><span class="line">    JobCheckpointingSettings settings = <span class="keyword">new</span> JobCheckpointingSettings(</span><br><span class="line">        triggerVertices,</span><br><span class="line">        ackVertices,</span><br><span class="line">        commitVertices,</span><br><span class="line">        <span class="keyword">new</span> CheckpointCoordinatorConfiguration( <span class="comment">//note: 创建一个 CheckpointCoordinatorConfiguration 对象</span></span><br><span class="line">            interval,</span><br><span class="line">            cfg.getCheckpointTimeout(),</span><br><span class="line">            cfg.getMinPauseBetweenCheckpoints(),</span><br><span class="line">            cfg.getMaxConcurrentCheckpoints(),</span><br><span class="line">            retentionAfterTermination,</span><br><span class="line">            isExactlyOnce,</span><br><span class="line">            cfg.isPreferCheckpointForRecovery(),</span><br><span class="line">            cfg.getTolerableCheckpointFailureNumber()),</span><br><span class="line">        serializedStateBackend,</span><br><span class="line">        serializedHooks);</span><br><span class="line"></span><br><span class="line">    jobGraph.setSnapshotSettings(settings);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里，StreamGraph 转换为 JobGraph 的流程已经梳理完成了，个人感觉这部分还有一些绕的，不过这种开源代码，只要看多几遍，多 debug 看看具体的执行流程，基本都可以搞明白。</p>
<hr>
<p>参考</p>
<ul>
<li><a href="http://asterios.katsifodimos.com/assets/publications/flink-deb.pdf" target="_blank" rel="external">Apache Flink: Stream and Batch Processing in a Single Engine</a>；</li>
<li><a href="http://chenyuzhao.me/2016/12/03/Flink%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6%E5%92%8C%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92/" target="_blank" rel="external">Flink 集群构建 &amp; 逻辑计划生成</a>；</li>
<li><a href="http://wuchong.me/blog/2016/05/10/flink-internals-how-to-build-jobgraph/" target="_blank" rel="external">Flink 原理与实现：如何生成 JobGraph</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/22736103" target="_blank" rel="external">Flink源码解析-从API到JobGraph</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/tmB7q9MTg3c_uhI51ZDAWQ" target="_blank" rel="external">Apache Flink 进阶（六）：Flink 作业执行深度解析</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/l-x3wSxuIvPMgxZzwYxZkA" target="_blank" rel="external">Flink CookBook—Apach Flink核心知识介绍</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第三篇，紧接着上一篇文章，本文主要讲述 StreamGraph 是如何转换成 JobGraph 的，在前面的文章中，我们知道 StreamGraph 是根据用户作业的处理逻生成初始的逻辑计划，它并没有做任何的
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink DataStream API 概述及作业如何转换为 StreamGraph</title>
    <link href="http://matt33.com/2019/12/08/flink-stream-graph-2/"/>
    <id>http://matt33.com/2019/12/08/flink-stream-graph-2/</id>
    <published>2019-12-08T03:03:50.000Z</published>
    <updated>2020-06-23T14:13:17.224Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第二篇，将会给大家讲述一个 Flink 作业（DataStream 高阶 API 为例的作业）是如何转换为 StreamGraph 的, StreamGraph 可以认为是一个还未经过优化处理的逻辑计划，它完全是在 Client 端生成的。StreamGraph 然后再经过优化转换为 JobGraph，Client 端向 JobManager 提交的作业就是以 JobGraph 的形式提交的，也就是说对于 JobManager 来说，它从客户端接收的作业实际上就是一个 JobGraph，然后它再对 JobGraph 做相应处理，生成具体的物理执行计划进行调度。</p>
<p>关于分布式计算中的 Graph，对于很多人来说，最开始接触和理解这个概念应该还是在 Spark 中。Spark 中有个 DAG （Directed Acyclic Graph，有向无环图）的概念，它包括一些边和一些顶点，其中边代表了 RDD（Spark 中对数据的封装和抽象）、顶点代表了 RDD 上的 Operator，在一个作业中，一旦有 Action 被调用，创建的 DAG 就会被提交到 DAG Scheduler，它会将这个 graph 以 task 的形式调度到不同的节点上去执行计算。Spark 在 MapReduce 的基础上提出了 DAG 的概念，带来了很多的好处，比如：更方便对复杂作业（复杂的 DAG）做全局优化、通过 DAG 恢复丢失的 RDD 等等。Apache Flink 在设计实现中，也借鉴了这个设计，Flink 中的每个作业在调度时都是一个 Graph（Flink 一般叫 DataFlow Graph，Spark 中一般叫作 DAG）。另外，Google 的 Beam 也是类似的概念，Collection 和 Transformation 对数据和操作的最基本抽象，Graph 由 Collection 和 Transformation 构成。</p>
<p>一个 Flink 作业（Steaming 作业），从 Client 端提交到最后真正调度执行，其 Graph 的转换会经过下面三个阶段（第四个阶段是作业真正执行时的状态，都是以 task 的形式在 TM 中运行）：</p>
<ol>
<li>StreamGraph：根据编写的代码生成最初的 Graph，它表示最初的拓扑结构；</li>
<li>JobGraph：这里会对前面生成的 Graph，做一些优化操作（比如: operator chain 等），最后会提交给 JobManager；</li>
<li>ExecutionGraph：JobManager 根据 JobGraph 生成 ExecutionGraph，是 Flink 调度时依赖的核心数据结构；</li>
<li>物理执行图：JobManager 根据生成的 ExecutionGraph 对 Job 进行调度后，在各个 TM 上部署 Task 后形成的一张虚拟图。</li>
</ol>
<p>这整个转换的内容还是比较多的，也考虑到单篇文章的篇幅问题，这里会先给大家讲述第一部分的转换，也就是 StreamGraph 的转换，同时也会给大家把基本的概念理清楚，便于后面的讲解。</p>
<h2 id="DataSteam-API"><a href="#DataSteam-API" class="headerlink" title="DataSteam API"></a>DataSteam API</h2><p>如果想对后面的内容理解更清楚，首先需要对 DataStream API 的基本概念有一定的理解，Apache Flink 自从 1.0 开始推出 DataStream API 后，经过最近几年的演化，这部分的代码已经变得比较复杂了，有些地方个人感觉还是有些冗余的，这里尽量给大家梳理清楚。</p>
<h3 id="DataStream"><a href="#DataStream" class="headerlink" title="DataStream"></a>DataStream</h3><p>A DataStream represents a stream of elements of the same type. A DataStream can be transformed into another DataStream by applying a transformation.</p>
<p>上面是 DataStream 的定义，从这个叙述中，可以看出，DataStream 实际上就是对相同类型数据流做的封装，它的主要作用就是可以用通过 Transformation 操作将其转换为另一个 DataStream，DataStream 向用户提供非常简单的 API 操作，比如 <code>map()</code>、<code>filter()</code>、<code>flatMap()</code> 等，目前 Flink 1.9 的代码里提供的 DataStream 实现如下：</p>
<p><img src="/images/flink/2-DataStream.png" alt="DataStream 实现"> </p>
<h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>A Transformation represents the operation that creates a DataStream。Transformation 代表创建 DataStream 的一个 operation，这里举一个示例，看一下下面的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// source 节点，随机产生一行一行的英文语句</span></span><br><span class="line">DataStream&lt;String&gt; inputStream = env.addSource(<span class="keyword">new</span> RandomWordCount.RandomStringSource());</span><br><span class="line"><span class="comment">// wordCount 里的第一步，将单词拆分</span></span><br><span class="line">inputStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">for</span> (String word : value.split(<span class="string">"\\s"</span>)) &#123;</span><br><span class="line">                        out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br></pre></td></tr></table></figure>
<p>这段代码首先会执行 <code>addSource()</code> 操作，它会创建一个 DataStreamSource 节点， 只有创建了 Source 的 DataStream 节点，后面才能对这个 DataStream 做相应的 Transformation 操作（实际上 DataStreamSource 节点也会有一个对应的 SourceTransformation 对象）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;OUT&gt; <span class="function">DataStreamSource&lt;OUT&gt; <span class="title">addSource</span><span class="params">(SourceFunction&lt;OUT&gt; function)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> addSource(function, <span class="string">"Custom Source"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 创建一个 DataStreamSource</span></span><br><span class="line"><span class="keyword">public</span> &lt;OUT&gt; <span class="function">DataStreamSource&lt;OUT&gt; <span class="title">addSource</span><span class="params">(SourceFunction&lt;OUT&gt; function, String sourceName, TypeInformation&lt;OUT&gt; typeInfo)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (function <span class="keyword">instanceof</span> ResultTypeQueryable) &#123;</span><br><span class="line">        typeInfo = ((ResultTypeQueryable&lt;OUT&gt;) function).getProducedType();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 找到相应的 TypeInformation</span></span><br><span class="line">    <span class="keyword">if</span> (typeInfo == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            typeInfo = TypeExtractor.createTypeInfo(</span><br><span class="line">                    SourceFunction.class,</span><br><span class="line">                    function.getClass(), <span class="number">0</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (<span class="keyword">final</span> InvalidTypesException e) &#123;</span><br><span class="line">            typeInfo = (TypeInformation&lt;OUT&gt;) <span class="keyword">new</span> MissingTypeInfo(sourceName, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isParallel = function <span class="keyword">instanceof</span> ParallelSourceFunction;</span><br><span class="line"></span><br><span class="line">    clean(function);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 创建一个 Operator</span></span><br><span class="line">    <span class="keyword">final</span> StreamSource&lt;OUT, ?&gt; sourceOperator = <span class="keyword">new</span> StreamSource&lt;&gt;(function);</span><br><span class="line">    <span class="comment">//note: 创建 DataStreamSource（这里再创建 DataStreamSource 对象时，会创建一个 SourceTransformation 对象）</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DataStreamSource&lt;&gt;(<span class="keyword">this</span>, typeInfo, sourceOperator, isParallel, sourceName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来再看 <code>flatMap()</code> 方法，这个实现其实跟前面的实现有一些类似之处，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">flatMap</span><span class="params">(FlatMapFunction&lt;T, R&gt; flatMapper)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TypeInformation&lt;R&gt; outType = TypeExtractor.getFlatMapReturnTypes(clean(flatMapper),</span><br><span class="line">            getType(), Utils.getCallLocationName(), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transform(<span class="string">"Flat Map"</span>, outType, <span class="keyword">new</span> StreamFlatMap&lt;&gt;(clean(flatMapper)));</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">transform</span><span class="params">(String operatorName, TypeInformation&lt;R&gt; outTypeInfo, OneInputStreamOperator&lt;T, R&gt; operator)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read the output type of the input Transform to coax out errors about MissingTypeInfo</span></span><br><span class="line">    transformation.getOutputType();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note： 新的 transformation 会连接上当前 DataStream 中的 transformation，从而构建成一棵树</span></span><br><span class="line">    OneInputTransformation&lt;T, R&gt; resultTransform = <span class="keyword">new</span> OneInputTransformation&lt;&gt;(</span><br><span class="line">            <span class="keyword">this</span>.transformation, <span class="comment">//note: 记录这个 transformation 的输入  transformation</span></span><br><span class="line">            operatorName,</span><br><span class="line">            operator,</span><br><span class="line">            outTypeInfo,</span><br><span class="line">            environment.getParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"unchecked"</span>, <span class="string">"rawtypes"</span> &#125;)</span><br><span class="line">    SingleOutputStreamOperator&lt;R&gt; returnStream = <span class="keyword">new</span> SingleOutputStreamOperator(environment, resultTransform);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 所有的 transformation 都会存到 env 中</span></span><br><span class="line">    getExecutionEnvironment().addOperator(resultTransform);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> returnStream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析到这里，那么 Transformation 到底是什么呢？这里之所以给大家举这个示例，也是为了让大家对 Transformation 有更深入的了解。这里看下下面这一张图，最开始是一个 SourceTransformation，然后又创建一个 OneInputTransformation 对象（这张图就是这里我们举的示例）：</p>
<p><img src="/images/flink/2-graph-transform.png" alt="Transformation 转换图"> </p>
<p>实际上，一个 Transformation ，它是对 StreamOperator 的一个封装（而 StreamOperator 又是对 Function 的一个封装，真正的处理逻辑是在 Function 实现的，当然并不一定所有的 Operator 都会有 Function，这里为了便于理解，就按照这个来讲述了），并且会记录它前面的 Transformation，只有这样才能把这个 Job 的完整 graph 构建出来。这里也可以看到，所有对 DataStream 的操作，最终都是以 Transformation 体现的，DataStream 仅仅是暴露给用户的一套操作 API，用于简化数据处理的实现。</p>
<h3 id="StreamOperator"><a href="#StreamOperator" class="headerlink" title="StreamOperator"></a>StreamOperator</h3><p>Operator 最基本类的是 StreamOperator，从名字也能看出来，它表示的是对 Stream 的一个 operation，它主要的实现类如下：</p>
<p><img src="/images/flink/2-StreamOperator.png" alt="StreamOperator 的实现"> </p>
<ul>
<li>AbstractUdfStreamOperator：会封装一个 Function，真正的操作是在 Function 中的实现，它主要是在最基础的方法实现上也会相应地调用对应 Function 的实现，比如：<code>open/close</code>方法也会调用 Function 的对应实现等；</li>
<li>OneInputStreamOperator：如果这个 Operator 只有一个输入，实现这个接口即可， 这个 <code>processElement()</code> 方法需要自己去实现；</li>
<li>TwoInputStreamOperator：如果这个 Operator 是一个二元操作符，是对两个流的处理，比如：双流 join，那么实现这个接口即可，用户需要自己去实现 <code>processElement1()</code> 和 <code>processElement2()</code> 方法。</li>
</ul>
<h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><p>Function 是 Transformation 最底层的封装，用户真正的处理逻辑是在这个里面实现的，包括前面示例中实现的 FlatMapFunction 对象。</p>
<p><img src="/images/flink/2-Function.png" alt="Function 的实现"> </p>
<p>到这里，终于把最基本这些概念介绍完了，只有对这些概念有了相应的理解之后，阅读源码时才不至于被绕进去。</p>
<h2 id="如何生成-StreamGraph"><a href="#如何生成-StreamGraph" class="headerlink" title="如何生成 StreamGraph"></a>如何生成 StreamGraph</h2><p>这里在讲述一个作业转换为 StreamGraph 的细节时，依然以上一篇文章中的示例 —— <a href="https://github.com/wangzzu/flink/blob/1.9-note/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/matt/RandomWordCount.java" target="_blank" rel="external">RandomWordCount</a> 来讲述。在执行 <code>env.getStreamGraph().getStreamingPlanAsJSON()</code> 后，这个 StreamGraph 将会以 JSON 的格式输出出来，输出结果如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"nodes"</span>:[&#123;<span class="attr">"id"</span>:<span class="number">1</span>,<span class="attr">"type"</span>:<span class="string">"Source: Custom Source"</span>,<span class="attr">"pact"</span>:<span class="string">"Data Source"</span>,<span class="attr">"contents"</span>:<span class="string">"Source: Custom Source"</span>,<span class="attr">"parallelism"</span>:<span class="number">1</span>&#125;,&#123;<span class="attr">"id"</span>:<span class="number">2</span>,<span class="attr">"type"</span>:<span class="string">"Source: Custom Source"</span>,<span class="attr">"pact"</span>:<span class="string">"Data Source"</span>,<span class="attr">"contents"</span>:<span class="string">"Source: Custom Source"</span>,<span class="attr">"parallelism"</span>:<span class="number">1</span>&#125;,&#123;<span class="attr">"id"</span>:<span class="number">4</span>,<span class="attr">"type"</span>:<span class="string">"Flat Map"</span>,<span class="attr">"pact"</span>:<span class="string">"Operator"</span>,<span class="attr">"contents"</span>:<span class="string">"Flat Map"</span>,<span class="attr">"parallelism"</span>:<span class="number">8</span>,<span class="attr">"predecessors"</span>:[&#123;<span class="attr">"id"</span>:<span class="number">1</span>,<span class="attr">"ship_strategy"</span>:<span class="string">"REBALANCE"</span>,<span class="attr">"side"</span>:<span class="string">"second"</span>&#125;,&#123;<span class="attr">"id"</span>:<span class="number">2</span>,<span class="attr">"ship_strategy"</span>:<span class="string">"REBALANCE"</span>,<span class="attr">"side"</span>:<span class="string">"second"</span>&#125;]&#125;,&#123;<span class="attr">"id"</span>:<span class="number">6</span>,<span class="attr">"type"</span>:<span class="string">"Filter"</span>,<span class="attr">"pact"</span>:<span class="string">"Operator"</span>,<span class="attr">"contents"</span>:<span class="string">"Filter"</span>,<span class="attr">"parallelism"</span>:<span class="number">8</span>,<span class="attr">"predecessors"</span>:[&#123;<span class="attr">"id"</span>:<span class="number">4</span>,<span class="attr">"ship_strategy"</span>:<span class="string">"SHUFFLE"</span>,<span class="attr">"side"</span>:<span class="string">"second"</span>&#125;]&#125;,&#123;<span class="attr">"id"</span>:<span class="number">8</span>,<span class="attr">"type"</span>:<span class="string">"Keyed Aggregation"</span>,<span class="attr">"pact"</span>:<span class="string">"Operator"</span>,<span class="attr">"contents"</span>:<span class="string">"Keyed Aggregation"</span>,<span class="attr">"parallelism"</span>:<span class="number">8</span>,<span class="attr">"predecessors"</span>:[&#123;<span class="attr">"id"</span>:<span class="number">6</span>,<span class="attr">"ship_strategy"</span>:<span class="string">"HASH"</span>,<span class="attr">"side"</span>:<span class="string">"second"</span>&#125;]&#125;,&#123;<span class="attr">"id"</span>:<span class="number">9</span>,<span class="attr">"type"</span>:<span class="string">"Sink: Print to Std. Out"</span>,<span class="attr">"pact"</span>:<span class="string">"Data Sink"</span>,<span class="attr">"contents"</span>:<span class="string">"Sink: Print to Std. Out"</span>,<span class="attr">"parallelism"</span>:<span class="number">2</span>,<span class="attr">"predecessors"</span>:[&#123;<span class="attr">"id"</span>:<span class="number">8</span>,<span class="attr">"ship_strategy"</span>:<span class="string">"REBALANCE"</span>,<span class="attr">"side"</span>:<span class="string">"second"</span>&#125;]&#125;]&#125;</span><br></pre></td></tr></table></figure>
<p>在 <a href="https://flink.apache.org/visualizer/" target="_blank" rel="external">Flink Plan Visualizer</a>中可以看到 StreamGraph 可视化之后 graph（用 Chrome 打开可能会显示不全，可以试下 Firefox），如下如所示：</p>
<p><img src="/images/flink/2-stream-graph.png" alt="StreamGraph 展示"> </p>
<p>接下来，详细介绍一下 StreamGraph 是如何转换的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// StreamExecutionEnvironment</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Getter of the &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.graph.StreamGraph&#125; of the streaming job.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The streamgraph representing the transformations</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">getStreamGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> getStreamGraphGenerator().generate();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> StreamGraphGenerator <span class="title">getStreamGraphGenerator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (transformations.size() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"No operators defined in streaming topology. Cannot execute."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 数据处理操作都在这个 transformations 列表里</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> StreamGraphGenerator(transformations, config, checkpointCfg) <span class="comment">//note: ExecutionConfig/CheckpointConfig</span></span><br><span class="line">        .setStateBackend(defaultStateBackend) <span class="comment">//note: StateBackend = null</span></span><br><span class="line">        .setChaining(isChainingEnabled) <span class="comment">//note: isChainingEnabled = true</span></span><br><span class="line">        .setUserArtifacts(cacheFile)</span><br><span class="line">        .setTimeCharacteristic(timeCharacteristic) <span class="comment">//note: TimeCharacteristic = ProcessingTime</span></span><br><span class="line">        .setDefaultBufferTimeout(bufferTimeout); <span class="comment">//note: default 100</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamGraph 最后是通过 StreamGraphGenerator 的 <code>generate()</code> 方法生成的，那这个方法到底做了什么事情呢？其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 构建 stream graph</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">generate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    streamGraph = <span class="keyword">new</span> StreamGraph(executionConfig, checkpointConfig);</span><br><span class="line">    streamGraph.setStateBackend(stateBackend);</span><br><span class="line">    streamGraph.setChaining(chaining);</span><br><span class="line">    streamGraph.setScheduleMode(scheduleMode);</span><br><span class="line">    streamGraph.setUserArtifacts(userArtifacts);</span><br><span class="line">    streamGraph.setTimeCharacteristic(timeCharacteristic);</span><br><span class="line">    streamGraph.setJobName(jobName);</span><br><span class="line">    streamGraph.setBlockingConnectionsBetweenChains(blockingConnectionsBetweenChains);</span><br><span class="line"></span><br><span class="line">    alreadyTransformed = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 自底向上(先遍历 input transformations)对转换树的每个 transformation 进行转换</span></span><br><span class="line">    <span class="keyword">for</span> (Transformation&lt;?&gt; transformation: transformations) &#123;</span><br><span class="line">        transform(transformation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> StreamGraph builtStreamGraph = streamGraph;</span><br><span class="line"></span><br><span class="line">    alreadyTransformed.clear();</span><br><span class="line">    alreadyTransformed = <span class="keyword">null</span>;</span><br><span class="line">    streamGraph = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> builtStreamGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最关键的还是 <code>transform()</code> 方法的实现，这里会根据 Transformation 的类型对其做相应的转换，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transforms one &#123;<span class="doctag">@code</span> Transformation&#125;.</span></span><br><span class="line"><span class="comment"> * note：对具体的一个 transformation 进行转换，转换成 StreamGraph 中的 StreamNode 和 StreamEdge</span></span><br><span class="line"><span class="comment"> * note：返回值为该 transform 的 id 集合，通常大小为1个（除 FeedbackTransformation）</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;This checks whether we already transformed it and exits early in that case. If not it</span></span><br><span class="line"><span class="comment"> * delegates to one of the transformation specific methods.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(Transformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 已经 Transform 的 Transformation 会放在这个集合中</span></span><br><span class="line">    <span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">        <span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.debug(<span class="string">"Transforming "</span> + transform);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getMaxParallelism() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// if the max parallelism hasn't been set, then first use the job wide max parallelism</span></span><br><span class="line">        <span class="comment">// from the ExecutionConfig.</span></span><br><span class="line">        <span class="comment">//note: 如果 MaxParallelism 没有设置，使用 job 的 MaxParallelism 设置</span></span><br><span class="line">        <span class="keyword">int</span> globalMaxParallelismFromConfig = executionConfig.getMaxParallelism();</span><br><span class="line">        <span class="keyword">if</span> (globalMaxParallelismFromConfig &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// call at least once to trigger exceptions about MissingTypeInfo</span></span><br><span class="line">    <span class="comment">//note: 如果是 MissingTypeInfo 类型（类型不确定），将会触发异常</span></span><br><span class="line">    transform.getOutputType();</span><br><span class="line"></span><br><span class="line">    Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">    <span class="comment">//note: 根据 transform 的类型，做相应不同的转换</span></span><br><span class="line">    <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">        transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">        transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">    <span class="comment">// transforming the feedback edges</span></span><br><span class="line">    <span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">        alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 将这个 Transform 相关的信息记录到 StreamGraph 中</span></span><br><span class="line">    <span class="keyword">if</span> (transform.getBufferTimeout() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        streamGraph.setBufferTimeout(transform.getId(), defaultBufferTimeout);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (transform.getUserProvidedNodeHash() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!streamGraph.getExecutionConfig().hasAutoGeneratedUIDsEnabled()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (transform.getUserProvidedNodeHash() == <span class="keyword">null</span> &amp;&amp; transform.getUid() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Auto generated UIDs have been disabled "</span> +</span><br><span class="line">                <span class="string">"but no UID or hash has been assigned to operator "</span> + transform.getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getMinResources() != <span class="keyword">null</span> &amp;&amp; transform.getPreferredResources() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transformedIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里以 <code>transformOneInputTransform()</code> 的实现来举个相应的例子，它会给这个 Transformation 创建相应的 StreamNode，并且创建 StreamEdge 来连接前后的 StreamNode：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;IN, OUT&gt; <span class="function">Collection&lt;Integer&gt; <span class="title">transformOneInputTransform</span><span class="params">(OneInputTransformation&lt;IN, OUT&gt; transform)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 递归调用，input 的 Transformation 处理完后才能处理后面</span></span><br><span class="line">    Collection&lt;Integer&gt; inputIds = transform(transform.getInput());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the recursive call might have already transformed this</span></span><br><span class="line">    <span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">        <span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 获取 share group</span></span><br><span class="line">    String slotSharingGroup = determineSlotSharingGroup(transform.getSlotSharingGroup(), inputIds);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 添加一个 Operator（streamGraph 端会添加一个 StreamNode）</span></span><br><span class="line">    streamGraph.addOperator(transform.getId(),</span><br><span class="line">            slotSharingGroup,</span><br><span class="line">            transform.getCoLocationGroupKey(),</span><br><span class="line">            transform.getOperatorFactory(),</span><br><span class="line">            transform.getInputType(),</span><br><span class="line">            transform.getOutputType(),</span><br><span class="line">            transform.getName());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getStateKeySelector() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        TypeSerializer&lt;?&gt; keySerializer = transform.getStateKeyType().createSerializer(executionConfig);</span><br><span class="line">        streamGraph.setOneInputStateKey(transform.getId(), transform.getStateKeySelector(), keySerializer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> parallelism = transform.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT ?</span><br><span class="line">        transform.getParallelism() : executionConfig.getParallelism();</span><br><span class="line">    streamGraph.setParallelism(transform.getId(), parallelism);</span><br><span class="line">    streamGraph.setMaxParallelism(transform.getId(), transform.getMaxParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Integer inputId: inputIds) &#123;</span><br><span class="line">        <span class="comment">//note: 根据输入的 id，给这个 node 在 graph 中设置相应的 graph</span></span><br><span class="line">        streamGraph.addEdge(inputId, transform.getId(), <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Collections.singleton(transform.getId());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过上面的 <code>transform()</code> 操作，最后生成的 StreamGraph 样板如下图所示：</p>
<p><img src="/images/flink/2-StreamGraph.png" alt="StreamGraph"> </p>
<p>关于上面的 <code>transform()</code> ，还有一个需要注意的是：这三个实现方法 <code>transformSelect()</code>、<code>transformPartition()</code>、<code>transformSideOutput()</code> 在操作时，并不会创建真正的 StreamNode 节点，它们会创建一个虚拟节点，将相应的配置赋给对应的 StreamEdge 即可。另外对于 <code>transformUnion()</code> 方法，它连虚拟节点也不会创建，原因其实看源码也能明白，它们并不包含具体的处理操作。</p>
<p>到这里，StreamGraph 的创建过程就分析完了，如果理解了 Flink 基本对象的抽象后，再去看这部分代码，实际上并不复杂，这里是对用户的作业逻辑做了一个最简单的转换，并没做什么优化操作，相当于还是原生的用户作业逻辑。</p>
<hr>
<p>参考</p>
<ul>
<li><a href="http://asterios.katsifodimos.com/assets/publications/flink-deb.pdf" target="_blank" rel="external">Apache Flink: Stream and Batch Processing in a Single Engine</a>；</li>
<li><a href="https://data-flair.training/blogs/dag-in-apache-spark/" target="_blank" rel="external">Directed Acyclic Graph DAG in Apache Spark</a>；</li>
<li><a href="http://chenyuzhao.me/2016/12/03/Flink%E5%9F%BA%E6%9C%AC%E7%BB%84%E4%BB%B6%E5%92%8C%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92/" target="_blank" rel="external">Flink 集群构建 &amp; 逻辑计划生成</a>；</li>
<li><a href="http://chenyuzhao.me/2017/02/08/flink%E7%AE%97%E5%AD%90%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/" target="_blank" rel="external">Flink Operator 的生命周期</a>；</li>
<li><a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/" target="_blank" rel="external">Flink 原理与实现：架构和拓扑概览</a>；</li>
<li><a href="http://wuchong.me/blog/2016/05/04/flink-internal-how-to-build-streamgraph/" target="_blank" rel="external">Flink 原理与实现：如何生成 StreamGraph</a>；</li>
<li><a href="http://wuchong.me/blog/2016/05/10/flink-internals-how-to-build-jobgraph/" target="_blank" rel="external">Flink 原理与实现：如何生成 JobGraph</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/22736103" target="_blank" rel="external">Flink源码解析-从API到JobGraph</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/tmB7q9MTg3c_uhI51ZDAWQ" target="_blank" rel="external">Apache Flink 进阶（六）：Flink 作业执行深度解析</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/l-x3wSxuIvPMgxZzwYxZkA" target="_blank" rel="external">Flink CookBook—Apach Flink核心知识介绍</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第二篇，将会给大家讲述一个 Flink 作业（DataStream 高阶 API 为例的作业）是如何转换为 StreamGraph 的, StreamGraph 可以认为是一个还未经过优化处理的逻辑计划，它完
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Apache Flink 初探</title>
    <link href="http://matt33.com/2019/11/23/flink-learn-start-1/"/>
    <id>http://matt33.com/2019/11/23/flink-learn-start-1/</id>
    <published>2019-11-23T10:49:56.000Z</published>
    <updated>2020-06-23T14:13:17.224Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是 <strong>Flink 系列</strong> 的第一篇，最近计划花个一到两个月的时间以最新的 Flink-1.9 代码为例把 Flink 的主要内容梳理一遍，这个系列文章的主要内容见 <a href="https://github.com/wangzzu/awesome/issues/28" target="_blank" rel="external">Flink 源码分析</a>，这个 issue 拖了好几个月，现在终于开动了，不容易。梳理的过程也是个人强化学习的过程，博客中有问题的地方也欢迎各位指正（邮件联系 or disqus 评论都行，我这边都会及时回复）。</p>
<p>本篇的题目是 <strong>Apache Flink 初探</strong>，比较适合对 Flink 不是很了解，想进一步了解的同学，主要会讲述一下流计算的基本知识，以及对 Flink 做了一个简单的介绍，算是这个系列的开胃小菜。</p>
<h2 id="流计算的基础知识"><a href="#流计算的基础知识" class="headerlink" title="流计算的基础知识"></a>流计算的基础知识</h2><p>关于流计算，业内有一本口碑神一般存在的书，那就是大名名鼎鼎的《Streaming Systems》，这本书对流计算领域的问题及技术做了很深的讨论，如果你看过相关的内容，你就会发现 Flink 实际上就是开源届里实现最接近 DataFlow 模型的框架，这里先给大家介绍一下流计算相关的背景知识，对于后面理解 Flink 的设计，特别是高阶 API 的设计（实际上 DataStream API 就是为 DataFlow 模型而开发的）。</p>
<h3 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h3><p>计算的数据源可以有很多种类型，比如：电商的交易数据、用户行为日志、物联网数据等，这些数据集可以分为两类：</p>
<ol>
<li>有边界数据集（Bounded dataset）: A type of dataset that is finite in size;</li>
<li>无边界数据集（Unbounded dataset）: A type of dataset that is infinite in size;</li>
</ol>
<p>从另一个角度来看，无边界数据集更符合现实中数据的产生方式，这样的话，就可以认为有边界数据集是无边界数据集的一个特例或一个子集。</p>
<h3 id="时间域"><a href="#时间域" class="headerlink" title="时间域"></a>时间域</h3><p>在分布式计算中，关于时间域有两种类型：</p>
<ol>
<li>Event Time（事件时间）: This is the time at which events actually occurred；</li>
<li>Processing Time（处理时间）: This is the time at  witch events are observed in the system。</li>
</ol>
<p>简单来说，事件时间是事件真实发生的时间，而处理时间是事件在计算引擎中被处理的时间，理想情况下，两者是相等的，但在实际情况下，它们之间差距的影响因素非常多，可能跟软件、硬件或数据有关，并且这个差距毫无规律可言，如下如图所示：</p>
<p><img src="/images/flink/1-1.png" alt="Event time VS Processing time（图片来自 《Streaming Systems》）"> </p>
<p>上面的问题给流计算带来了很多的问题，而且由于数据的无边界特性，业内通常的做法是将输入数据进行 window 操作（本质上还是按照时间切片），而对于一些关注 Event Time 的应用来说，按照 Processing Time 做 window 是完全无法满足需求的（流计算之前困扰大家最大的问题之一就是这个准确性的问题）。</p>
<h3 id="Window-amp-Watermark"><a href="#Window-amp-Watermark" class="headerlink" title="Window &amp; Watermark"></a>Window &amp; Watermark</h3><p>目前常用的 window 类型有以下几种：</p>
<p><img src="/images/flink/1-2.png" alt="Window Strategies（图片来自 《Streaming Systems》）"> </p>
<ol>
<li>Fixed Window：按时间切成固定大小的 window，是 aligned window 的一种；</li>
<li>Sliding Window：也是一种 Fixed Window，但它有 fixed length 和 fixed period 两个设置；</li>
<li>Sessions：一种 unaligned window，长度是未知的，一种动态的 window，比如分析用户的行为等。</li>
</ol>
<p>Window 在 Event time 和 Processing time 下都是有意义的，只是适用于不同的应用场景而已，而对于 Event time 场景，如何来保证一个窗口数据的完整性呢？而窗口数据的完整性又确定了数据的准确性。</p>
<p>Watermark 就是来就解决这个问题的，它用于界定什么时间(时间戳)认为一个时间窗口内的数据已经全部到齐，之后晚于该 watermark 到达的数据则为迟到数据。</p>
<h3 id="有状态计算"><a href="#有状态计算" class="headerlink" title="有状态计算"></a>有状态计算</h3><p>计算任务可以分为有状态计算和无状态计算：</p>
<ol>
<li>无状态计算：如果处理一个事件（或一条数据）的结果只跟该事件本身有关；</li>
<li>有状态计算：计算结果还和之前处理过的事件有关，比如说基本的聚合计算，就是有状态计算。</li>
</ol>
<p>对于批处理，每次处理的都是全量数据，所以就不用考虑状态这个问题。而流处理，一般会借助外部存储系统实现状态保存（这个对应的 Flink 中 State 模块的内容）。</p>
<h3 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h3><p>流计算另一个难点的是容错恢复，如何保证恢复之后作业状态的一致性，目前业内通用的解决方案采用的是 Chandy-Lamport 算法（有兴趣的可以看下 <a href="http://matt33.com/2019/10/27/paper-chandy-lamport/">Paper 阅读: Distributed Snapshots: Determining Global States of Distributed Systems</a>），包括 Structured Streaming 也采用的这个方案。</p>
<p>到这里，把流计算的基础知识简单过了一下，想了解更多的同学，建议阅读一下 Google DataFlow 那篇论文或者《Streaming Systems》这本书（<a href="https://mp.weixin.qq.com/s/oBmRhRA-52CLRLXp6sZwEw" target="_blank" rel="external">Apache Flink 零基础入门（一）：基础概念解析</a> 这篇讲述得也不错）。</p>
<h2 id="Why-Flink"><a href="#Why-Flink" class="headerlink" title="Why Flink?"></a>Why Flink?</h2><p>流计算领域的开源框架，不可谓不多，但到现在还能让大家记住的（或者对业内产生巨大影响的）其实并不多，通常大家对比的也就是：Storm/Spark Streaming/Flink。在 17 年之前，我们在面临流计算技术选型时还可能会徘徊一下，但如果放在现在，你会发现，几乎没有太大可比性，几个引擎的差距已经很大，简单对比一下（只列出了流计算中重点关注的特性，只是粗略的比较，勿喷）：</p>
<ol>
<li>Storm：没有 SQL 和高阶 API 的支持、无法支持 exactly once；</li>
<li>Spark Streaming：对实时计算来说，微批处理天生是有架构上的缺陷；</li>
<li>Structured Streaming：完全处于初级阶段，没有经过大规模生产业务的验证；</li>
<li>Kafka Streams：目前没有自己的调度框架，不知道未来 on k8s 会不会在架构上做支持，要不然 kafka streams 要想应用大规模业务场景，维护成本太高，社区最好是能给出一套统一的解决方案，但是如果业务规模比较小，其实选 kafka streams 也不错，只维护一套系统，维护成本会低一些；</li>
<li>Flink: 有 Streaming SQL 的支持，支持 exactly once 等等；</li>
</ol>
<p>一圈比较下来，你会发现 Flink 真的是流计算的最佳选择，当然选择 Flink 还有其他很多的原因，可以参考阿里官方给出这两篇文章：</p>
<ol>
<li><a href="https://mp.weixin.qq.com/s/AoSDPDKbTbjH9rviioK-5Q" target="_blank" rel="external">阿里巴巴为什么选择Apache Flink？</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/rt6nmSyU0IBZ8XPtcG9IPw" target="_blank" rel="external">Apache Flink，流计算？不仅仅是流计算！</a>；</li>
</ol>
<h2 id="Flink-架构"><a href="#Flink-架构" class="headerlink" title="Flink 架构"></a>Flink 架构</h2><p>Apache Flink 采用经典的分布式架构设计 —— Master/Slave 架构，Flink 集群的架构图如下图所示，这张图展示了其整体结构，但是很多内部细节并没有展示，我也翻了很多的博客，也没有找到一张特别满意的架构图。</p>
<p><img src="/images/flink/1-3.png" alt="Flink 架构图（图片来自 Flink 官网）"> </p>
<p>一个 Flink 集群，主要包含了两个核心组件：</p>
<ol>
<li>JobManager（master）：它会负责整个任务的协调工作，包括：调度 task、触发协调 Tasks 做 Checkpoint、协调容错恢复等等（HA 模式下，一个集群会启动多个 JobManager，但只会有一个处在 leader 状态，其他处在热备状态 —— standby）；</li>
<li>TaskManager（workers）：负责执行一个 DataFlow Graph 的各个 tasks 以及 data streams 的 buffer 和数据交换。</li>
</ol>
<p>JobManager/TaskManager 都是进程级别，TaskManager 在启动时，会根据配置将其内部的资源分为多个 slot，每个 slot 只会启动一个 Task，Task 是线程级别，从这里可以看出 Flink 是多线程调度模型，一个 TM 中可能会有来自多个任务的 task，从资源利用的角度看，这样的设计是有一些收益的，但是从资源隔离的角度看，这种设计就不是那么好了，不过好在现在业内的使用方式基本都是 On Yarn 的单集群单作业模式，相当于把资源隔离这个问题避过去了，但不可否认，这种设计是有缺陷的。</p>
<h3 id="Flink-部署"><a href="#Flink-部署" class="headerlink" title="Flink 部署"></a>Flink 部署</h3><p>关于 Flink 的部署，这里推荐一下这几篇文章，本文就没有必要再整理了：</p>
<ol>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/cluster_setup.html" target="_blank" rel="external">Standalone Cluster</a>；</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/yarn_setup.html" target="_blank" rel="external">YARN Setup</a>;</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/deployment/kubernetes.html" target="_blank" rel="external">Kubernetes Setup</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/PlcxA0jXgNO7E3xhUpLPTg" target="_blank" rel="external">Flink CookBook-环境准备</a>；</li>
<li><a href="https://www.infoq.cn/article/zbBAGroBgtytDiBs*Xq9" target="_blank" rel="external">Apache Flink 零基础入门（二）：开发环境搭建和应用的配置、部署及运行</a>；</li>
</ol>
<h3 id="编译-Flink-源码"><a href="#编译-Flink-源码" class="headerlink" title="编译 Flink 源码"></a>编译 Flink 源码</h3><p>如果你想自己编译 Flink 安装包的话，可以参考 <a href="https://github.com/apache/flink#building-apache-flink-from-source" target="_blank" rel="external">Flink Readme - Building Apache Flink from Source</a>，这里给了几个不同的编译命令，最终的结果是一样，都可以正常编译出安装包：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除已有的 build，编译 flink binary</span></span><br><span class="line"><span class="comment"># 会执行测试 case，编译速度会比较慢</span></span><br><span class="line">mvn clean install/package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不编译 tests、QA plugins 和 JavaDocs，因此编译要更快一些</span></span><br><span class="line">mvn clean install/package -DskipTests -Dfast</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果你需要使用指定 hadoop 的版本，可以通过指定“-Dhadoop.version”来设置</span></span><br><span class="line">mvn clean instal/packagel -DskipTests -Dhadoop.version=2.6.1</span><br></pre></td></tr></table></figure>
<p>最后生成的安装包就在 <code>flink-dist/target/flink-1.9-SNAPSHOT-bin</code> 下，在 flink 目录下也会生成一个 <code>build-target</code> 的软连。</p>
<h2 id="Flink-示例"><a href="#Flink-示例" class="headerlink" title="Flink 示例"></a>Flink 示例</h2><p>这里有一个示例，见 <a href="https://github.com/wangzzu/flink/blob/1.9-note/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/matt/RandomWordCount.java" target="_blank" rel="external">RandomWordCount</a>（后面的文章也会以这个示例讲述），这个示例比较简单，就是先模拟两个数据源，再对流做 union， 再做过滤，最后再做 WorkCount，这个作业可以在 Flink 工程中直接运行。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RandomWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// get the execution environment</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note 模拟两个数据源，它们会生成一行随机单词组（单词之间是空格分隔）</span></span><br><span class="line">        DataStream&lt;String&gt; inputStream = env.addSource(<span class="keyword">new</span> RandomWordCount.RandomStringSource());</span><br><span class="line">        DataStream&lt;String&gt; inputStream2 = env.addSource(<span class="keyword">new</span> RandomWordCount.RandomStringSource());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 先对流做 union，然后做一个过滤后，做 word-count</span></span><br><span class="line">        inputStream.union(inputStream2)</span><br><span class="line">            .flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">for</span> (String word : value.split(<span class="string">"\\s"</span>)) &#123;</span><br><span class="line">                        out.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .shuffle()</span><br><span class="line">            .filter(<span class="keyword">new</span> FilterFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple2&lt;String, Integer&gt; value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (value.f0.startsWith(<span class="string">"a"</span>)) &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line">            .print()</span><br><span class="line">            .setParallelism(<span class="number">8</span>);</span><br><span class="line">        env.execute(<span class="string">"Random WordCount"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>开胃小菜到这里就结束了，后面会逐步给大家剖析 Flink 的内部实现原理与机制，其实整个 Flink 的代码可以三大块：</p>
<ol>
<li>分布式框架相关的内容，这块的内容，其实你会感觉很多系统有一些相似的地方，但是每套系统又只能自己去开发一遍，像 JobManager/TaskManager 之间的交互、内存管理、IO 管理等都属于这一部分；</li>
<li>Flink 专门去解决 Streaming Process 问题而实现的设计，其实也就是 DataFlow 模型如何在 Flink 上实现的；</li>
<li>SQL：这块比较特殊，算是比较单独的一块。</li>
</ol>
<hr>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ol>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/programming-model.html" target="_blank" rel="external">Dataflow Programming Model</a>；</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/concepts/runtime.html" target="_blank" rel="external">Distributed Runtime Environment</a>；</li>
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/internals/components.html" target="_blank" rel="external">Component Stack</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/TBzzGTNFTzVLjFQdzz-LuQ" target="_blank" rel="external">Apache Flink 进阶（一）：Runtime 核心机制剖析 ​</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/oBmRhRA-52CLRLXp6sZwEw" target="_blank" rel="external">Apache Flink 零基础入门（一）：基础概念解析</a>；</li>
<li><a href="https://www.infoq.cn/article/zbBAGroBgtytDiBs*Xq9" target="_blank" rel="external">Apache Flink 零基础入门（二）：开发环境搭建和应用的配置、部署及运行</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/UeoOYX1n6pnedHh8VcY8OQ" target="_blank" rel="external">Flink SQL 系列 | 5 个 TableEnvironment 我该用哪个？</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/PlcxA0jXgNO7E3xhUpLPTg" target="_blank" rel="external">Flink CookBook-环境准备</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/l-x3wSxuIvPMgxZzwYxZkA" target="_blank" rel="external">Flink CookBook—Apach Flink核心知识介绍</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/FaAHwRV8AQgNFd7SB4BVMQ" target="_blank" rel="external">Flink CookBook—流式计算介绍</a>；</li>
<li><a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/" target="_blank" rel="external">Flink 原理与实现：架构和拓扑概览</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是 &lt;strong&gt;Flink 系列&lt;/strong&gt; 的第一篇，最近计划花个一到两个月的时间以最新的 Flink-1.9 代码为例把 Flink 的主要内容梳理一遍，这个系列文章的主要内容见 &lt;a href=&quot;https://github.com/wangzzu/
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="flink" scheme="http://matt33.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Paper 阅读: Distributed Snapshots: Determining Global States of Distributed Systems</title>
    <link href="http://matt33.com/2019/10/27/paper-chandy-lamport/"/>
    <id>http://matt33.com/2019/10/27/paper-chandy-lamport/</id>
    <published>2019-10-27T15:12:51.000Z</published>
    <updated>2020-06-23T14:13:17.223Z</updated>
    
    <content type="html"><![CDATA[<p>今天对分布式系统领域的一篇经典论文 —— Chandy-Lamport 算法做了一下总结，这篇论文对于分布式快照算法产生了非常巨大的影响，比如：Apache Flink、Apache Spark 的 Structured Streaming、Ray 等分布式计算引擎都是使用的这个算法做快照。这篇论文的其中一位作者 —— Lamport，他也是 Paxos 算法的提出者，2013 年图领奖得主（图领奖是计算机领域的诺贝尔奖，目前只有一位华裔 —— 姚期智院士获得过这个殊荣，没错，就是清华交叉学院姚班的姚院士）。这篇论文发表于 1985 年，算法的由来可以参考下面的小段子：</p>
<blockquote>
<p>The distributed snapshot algorithm described here came about when I visited Chandy, who was then at the University of Texas in Austin. He posed the problem to me over dinner, but we had both had too much wine to think about it right then. The next morning, in the shower, I came up with the solution. When I arrived at Chandy’s office, he was waiting for me with the same solution.</p>
</blockquote>
<p>另外，如果你只是想要明白这个算法是怎么做的，可以直接看这篇文章 —— <a href="https://zhuanlan.zhihu.com/p/53482103" target="_blank" rel="external">分布式快照算法: Chandy-Lamport 算法</a>，它讲得更通俗易懂，本文更多的是论文的角度来讲述，会详细介绍一下这个算法的数学证明。</p>
<h2 id="背景-amp-问题"><a href="#背景-amp-问题" class="headerlink" title="背景 &amp; 问题"></a>背景 &amp; 问题</h2><p>分布式系统的很多问题都可以归结于获取 global states 的问题，比如：</p>
<ol>
<li>stable property detection（系统的一些稳定特性检测），一个 stable property 是不可变的，如：计算停止或完成了（不会自己恢复的）、系统死锁了（不会自己恢复），通过 global states，就可以检测到这些 stable property；</li>
<li>用于 checkpoint。</li>
</ol>
<p>但是获取一个系统的 global states 并不是一件容易的事情，对于一个分布式系统而言，我们需要在同一个时间点记录下这个系统的全局状态，它包括每个 process 的状态以及相关 channel 的状态（一个计算是由有限的 process 和 channel 组成的一个 graph）。这就好比：在一个满是候鸟的天空大场景下，这个场景大到一张照片无法全部覆盖，摄影师不得不拍摄多张照片，然后把它们合并成一张全景，因为多张照片不能同时拍摄、在拍摄过程中候鸟也不会静止不动，所以如何保证合成的全景照片是有意义的（它可能少拍了某些鸟或者多拍了某些鸟）？这个就是分布式快照算法要解决的问题，因为没有全局统一的一把锁，所以不可能保证所有 process 能在同一时刻记录他们的状态信息。</p>
<h2 id="分布式系统模型"><a href="#分布式系统模型" class="headerlink" title="分布式系统模型"></a>分布式系统模型</h2><p>一个分布式系统包含一个有限的 process 集合和有限的 channel 集合，它可以通过一个有向的 graph（顶点代表process、边代表 channel）来描述，如下图所示：</p>
<p><img src="/images/paper/chandy-lamport-1.png" alt="一个分布式系统示例"> </p>
<p><strong>Channel</strong>：这里为了便于解释，文章会假设一个 channel 有一个无限、零错误、有序传输的 buffer（否者还要考虑 buffer 是否 full 的情况），channel 中数据的延迟是任意的并且有限的。一个 channel 的 state 就是它从上游收到的 msg list 减去下游已经接收到的 msg list；</p>
<p><strong>Process</strong>：它是由一组状态、一个初始状态和一组 event 来定义。process <code>p</code> 中的一个 event <code>e</code> 代表一个可能改变 <code>p</code> 本身状态和对应 channel <code>c</code> 状态（<code>c</code> 发送或接收数据都可能会改变其状态）的原子操作。</p>
<p>一个 event <code>e</code> 被定义为 $&lt;p, s, s’, M, c&gt;$，其中：</p>
<ol>
<li>Process <code>p</code> 是 event 产生的地方；</li>
<li>在处理 <code>e</code> 之前 <code>p</code> 的状态是 <code>s</code>；</li>
<li>在处理 <code>e</code> 之后 <code>p</code> 的状态是 $s’$；</li>
<li>Channel <code>c</code> 它的状态会被 <code>e</code> 所改变；</li>
<li>M 是发向或发离 <code>c</code> 的 msg；</li>
</ol>
<p>如果 event 没有改变任何 channel 的状态，那么 M 和 <code>c</code> 则为 null，可能只改变了 <code>p</code> 的状态（这个概念很重要，需要好好理解，是后面论证的基础）。</p>
<h3 id="global-state-模型"><a href="#global-state-模型" class="headerlink" title="global state 模型"></a>global state 模型</h3><p>有了前面的模型抽象，这里我们可以认为一个分布式系统的 global state 就是这批 process state 和 channel state 的集合。初始的 global state 就是每个 process 都是其对应的初始状态以及每个 channel 都是 empty 集合。</p>
<p>一个 event <code>e</code> 可能会改变 global state（这里记为 <code>S</code>），这里定义另外一个函数：$next(S, e)$，它指的是 event <code>e</code> 发生在 global state <code>S</code> 之后的 global state，根据前面介绍的，<code>e</code> 处理后的 global state 变化是：<code>p</code> 的状态由 <code>s</code> 变为 <code>s&#39;</code>，Channel <code>c</code> 的状态是在原来的基础上加上（数据是发向 Channel <code>c</code> 的）或删除（数据是发离 Channel <code>c</code> 的） msg M。</p>
<p>这里再定义一个 $seq = (e_i: 0 \leq i \leq n)$，它代表的是这个分布式系统将要处理的 event 序列，<strong>这个 $seq$ 实际上就是 <code>a computation of the system</code></strong>（这个 event 序列就代表了这个分布式计算），假设在 $e_i$ 处理前，系统的 global state 是 $S_i$（系统的初始状态时 $S_0$），那么可以得到下面公式：</p>
<p>$S_{i+1} = next(S_i, e_i)$, for $0 \leq i \leq n$</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>论文中举了两个示例，这里也介绍一下，对于理解后面的论证是有帮助的。</p>
<h4 id="Exmaple-1：Single-token-Conservation"><a href="#Exmaple-1：Single-token-Conservation" class="headerlink" title="Exmaple 1：Single-token Conservation"></a>Exmaple 1：Single-token Conservation</h4><p>先看一个最简单的计算系统，这个系统有两个 process <code>p</code> 和 <code>q</code>，有两个 Channel <code>c</code> 和 <code>c&#39;</code>（下面第二个示例也是这种基本模型），如下图所示：</p>
<p><img src="/images/paper/chandy-lamport-2.png" alt="Single-token Conservation"> </p>
<p>在这个系统中，有一个 <code>token</code> 它在两个 process 之间传输处理，每个 process 都有两种状态：$s_0$ 和 $s_1$，如果这个 process 不包含 <code>token</code>，它的状态就是 $s_0$，如果包含 <code>token</code> 的话，它的状态就是 $s_1$，<code>p</code> 的初始状态是 $s_1$，<code>q</code> 的初始状态是 $s_0$。而对每个 process 而言，都会有两种 event 类型（这里根据这个例子理解前面 event 的概念，如上面的图中所示）：</p>
<ol>
<li>发送 <code>token</code> 时，process 状态从 $s_1$ 转为 $s_0$；</li>
<li>接收 <code>token</code> 时，process 状态从 $s_0$ 转为 $s_1$。</li>
</ol>
<p>对于 global state 而言，可能会出现四种不同的状态，如下图所示：</p>
<p><img src="/images/paper/chandy-lamport-3.png" alt="Single-token Conservation 系统的 global state 转换"> </p>
<p>在上面图中，四种状态实际是跟 <code>token</code> 所在的位置有关：<code>in-c</code>，<code>in-p</code>、<code>in-q</code>、<code>in-c&#39;</code>。这个示例比较简单，但它跟后面作者提出的算法来源的灵感有关。</p>
<h4 id="Example-2-非确定性计算"><a href="#Example-2-非确定性计算" class="headerlink" title="Example 2: 非确定性计算"></a>Example 2: 非确定性计算</h4><p>这里依然是两个 process <code>p</code> 和 <code>q</code>，它们的状态转移图如下所示：</p>
<p><img src="/images/paper/chandy-lamport-4.png" alt="两个进程的状态转移图"> </p>
<p>Example 1 的示例比较比较简单，在每个 global state 中正好只有一个 event（一个状态转换），但是在真实的系统中，很多情况下是一些非确定性计算（nondeterministic computation），可能同时会有多个 event 一起转换，比如：<code>p</code> 发送 <code>M</code> 和 <code>q</code> 发送 <code>M&#39;</code> 这两个 event 同时发生（下面就是这两个 event 同时发生的情况，如下图的 global state $S_2$），那么得到的 global state 就会与预期的不同。下图是这个系统可能的一个 global state 转移情况：</p>
<p><img src="/images/paper/chandy-lamport-5.png" alt="A computation"> </p>
<p>Notice： 这个示例，我在看的时候，最开始一直没有搞明白，主要在 $S_2$ 这一步没有明白，后来仔细想了几次，算是明白了，这个示例举得的是一个非确定计算的示例，上面也只是系统可能出现中的一种状态，比如：<code>p</code> 在发送 <code>M</code> 之后，<code>M</code> 在 Channel <code>c</code> 中还没有被 <code>q</code> 接收到，<code>q</code> 就发送了 <code>M&#39;</code>。或者换成另一种理解方式，<code>p</code> 发送 <code>M</code> 和 <code>q</code> 发送 <code>M&#39;</code>  同时发送，上图只是把两个拆开了一下展示，于是就有了 global state S1 和 S2，再接着有可能发生的就是 S3 的情况，<code>p</code> 接收到了 <code>M&#39;</code>，状态发生了变化。这里，把这个示例当作一种在现实系统中的非确定性计算就好理解了。</p>
<h2 id="Chandy-Lamport-算法"><a href="#Chandy-Lamport-算法" class="headerlink" title="Chandy-Lamport 算法"></a>Chandy-Lamport 算法</h2><p>下面开始进入到算法的核心部分，这里作者介绍了一下算法的由来，以及在数学上的证明。</p>
<h3 id="算法的动机-由来"><a href="#算法的动机-由来" class="headerlink" title="算法的动机/由来"></a>算法的动机/由来</h3><p>Global state recording 算法工作过程如下：</p>
<ul>
<li>每个 process 记录自己的 state；</li>
<li>process 之间的通道 Channel 也会记录自己的状态；</li>
</ul>
<p>因为没有一个全局的锁，所以我们无法保证，所有的 process 和 Channel 都是在同一时刻记录的。因此，我们需要保证记录的 process 和 Channel 状态能够组成 <strong>一个有意义的 global state</strong>。</p>
<p>这个算法是与跟底层计算嵌套在一起，但是不会对计算产生改变、也不会影响底层的计算。这里通过一个示例来逐步引出我们的算法，假设我们是可以很自然地记录 Channel 的状态，Channel <code>c</code> 是 process <code>p</code> 和 <code>q</code> 的之间的传输通道，下面来分析一下它们之间的状态关系。</p>
<h4 id="p-与-c-状态之间的关系"><a href="#p-与-c-状态之间的关系" class="headerlink" title="p 与 c 状态之间的关系"></a><code>p</code> 与 <code>c</code> 状态之间的关系</h4><p>这里以前面 Single-token Conservation 的示例来分析，假设 process <code>p</code> 的状态记录在 global state <code>in-p</code> 中，<code>p</code> 记录的状态显示 <code>token</code> 是在 <code>p</code> 中。现在假设 Channels <code>c</code> 和 <code>c&#39;</code> 以及 process <code>q</code> 的状态时记录在 global state <code>in-c</code> 中的，同样 <code>c</code> 中记录的状态也显示 <code>token</code> 在 <code>c</code> 中（因为无法保证它们在同一时刻记录，所以每个组件是有可能在不同的时刻记录）。组成的 global state 显示系统中有两个 <code>token</code>，一个是在 <code>p</code> 中、一个是在 <code>c</code>中。但是由于这个系统是 single-token，它是不可能同时出现两个 <code>token</code> 的，所以一定是哪里有问题了，这样组成的 global state 不是有意义的。先定义两个变量：</p>
<ol>
<li>$n$：在 <code>p</code> 的状态记录前，<code>p</code> 发往 Channel <code>c</code> 的 msg 数；</li>
<li>$n’$：在 <code>c</code> 的状态记录前，<code>p</code> 发往 Channel <code>c</code> 的 msg 数；</li>
</ol>
<p>上面出现的情况就是 $n &lt; n’$.</p>
<p>假设另一种情况，<code>c</code> 的状态记录在 <code>in-p</code> 中，而 <code>p</code>、<code>q</code>、<code>c&#39;</code> 的状态记录在 <code>in-c</code>，那么这样组成的 global state 会显示系统没有 <code>token</code>，这个组成的 global state 同样也是没有意义的，这就是 $n &gt; n’$ 的情况。</p>
<p>从前面的分析中，可以得到：这里有个一致的全局状态要求</p>
<p>$n = n’$ </p>
<h4 id="q-与-c-状态之间的关系"><a href="#q-与-c-状态之间的关系" class="headerlink" title="q 与 c 状态之间的关系"></a><code>q</code> 与 <code>c</code> 状态之间的关系</h4><p>这里，再定义另外两个变量：</p>
<ol>
<li>$m$：在 <code>q</code> 的状态记录前，<code>q</code> 从 Channel <code>c</code> 中接收到的 msg 数；</li>
<li>$m’$：在 <code>c</code> 的状态记录前，<code>q</code> 从 Channel <code>c</code> 中接收到的 msg 数；</li>
</ol>
<p>跟前面的分析类似，这里也会有一个一致性的要求：</p>
<p>$m = m’$ </p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>在任何一种状态下，都要求 Channel <code>c</code> 下游接收到的 msg 数不能超过 <code>p</code> 发送给 Channel 的 msg 条数，即：</p>
<p>$n’ \geq m’$  以及  $n \geq m$</p>
<p>现在来分析一下 <strong>Channel 的状态要记录什么数据？</strong> 一个 Channel 要记录的状态是，它 sender 记录自己状态之前它所接收到的 msg 列表，再减去 receiver 记录自己状态之前它已经收到的 msg 列表，减去的之后的数据列表就是还在通道中的数据列表，这个列表是需要 Channel 作为状态记录下来的。而如果 $n’ = m’$，那么 Channel <code>c</code> 中要记录的 msg 列表就是 empty 列表。如果 $n’ &gt; m’$，那么要记录的列表是 $(m’+1)st , … n’$ 对应的 msg 列表。</p>
<p><strong>重点，重点，重点</strong>：分析到这里之后，就有了下面的一个灵感：那么 Channel <code>c</code> 状态要记录的 msg 列表是可以在 <code>q</code> 中记录的。那么具体怎么做的？就是在发送数据中插入一条特殊的数据 —— <code>marker</code> 数据，这条数据不会对计算有任何影响，那么 <code>c</code> 的状态就是 <code>q</code> 在记录自己状态之后并在收到 <code>marker</code> 之前接收到 msg 列表，另一种情况就是 <code>q</code> 收到 <code>marker</code> 之后，就必须要把自己的状态记录下来（伟大的算法就这样诞生了）。 </p>
<h4 id="算法概况"><a href="#算法概况" class="headerlink" title="算法概况"></a>算法概况</h4><p>对于发送者来说：</p>
<ul>
<li>在 <code>p</code> 记录自己的状态之后它先向 Channel <code>c</code> 发送一条 <code>marker</code>，然后才会继续发送数据信息；</li>
</ul>
<p>对于接收者来说：</p>
<ul>
<li>如果 <code>q</code> 已经还没记录自己的 state，在收到 <code>marker</code> 之后，它会记录自己的状态，并且把 <code>c</code> 的状态设置为 empty；</li>
<li>如果 <code>q</code> 已经记录自己的 state，它会把从 <code>c</code> 接收到的数据作为 msg 列表当作 <code>c</code> 的状态记录下来。</li>
</ul>
<h3 id="算法能够在有限时间结束的论证"><a href="#算法能够在有限时间结束的论证" class="headerlink" title="算法能够在有限时间结束的论证"></a>算法能够在有限时间结束的论证</h3><p>关于算法能够在有限时间内结束，是有两个前提的：</p>
<ol>
<li><code>marker</code> 数据不会在 Channel 阻塞永远发不出去的；</li>
<li>Process 在根据一个初始状态记录自己的状态时，能够在有限的时间内完成。</li>
</ol>
<p>有了这两个前提，一个 graph 中每个 process 都会收到相应的 <code>marker</code>，然后都会记录自己的状态，所以这个是完全可以保证能够在有限的时间内完成。</p>
<h2 id="算法证明"><a href="#算法证明" class="headerlink" title="算法证明"></a>算法证明</h2><p>事先说明，这里证明比较烧脑，我尽量描述清楚，最开始看论文也是看了好几遍、想好几遍把整个过程捋顺，当然如果理解有误，欢迎指正。</p>
<p>以前面 Example 2 的示例来讲述，假设在 global state $S_0$ 时，<code>p</code> 记录下了自己的状态（<code>A</code>），然后 <code>p</code> 向 Channel <code>c</code> 发送一条 <code>marker</code> 数据（它是在 <code>M</code> 数据之前），假设这个时候系统在正常运行，已经经历 $S_1$、$S_2$，到了 $S_3$ 阶段，但是 <code>marker</code> 数据在传输中。<code>q</code> 在收到 <code>marker</code> 之后，它记录了一下自己的状态 <code>D</code>（对应 <code>c</code> 的状态为空），然后再发送一条 <code>marker</code> 数据给 Channel <code>c&#39;</code>。<code>p</code> 因为之前已经记录过自己的状态，所以在收到 <code>c&#39;</code> 传过来的 <code>M&#39;</code> 之后（<code>p</code> 先收到 <code>M&#39;</code> 然后才会收到 <code>q</code> 发送的 <code>marker</code> 消息），会把它作为 Channel <code>c&#39;</code> 的状态记录下来。</p>
<p>整个流程下来，组合的 global state 是 $S_*$，如下如所示：</p>
<p><img src="/images/paper/chandy-lamport-6.png" alt="记录的一个 global state"> </p>
<p>可以看到这里算法得到的 global state $S_*$ 与真实环境下的 global state（$S_0$、$S_1$、$S_2$、$S_3$）都不相同。</p>
<p>那么来考虑一个问？题：<strong>如果算法记录的状态，在真实环境中并没有实际存在过？那么这个 global state 有什么用呢？</strong>（或许大家之前都理解了这个算法，但很少有人会去思考深入这个问题）</p>
<h3 id="算法结论"><a href="#算法结论" class="headerlink" title="算法结论"></a>算法结论</h3><p>假设 $seq = (e_i, 0 \leq i)$ 是一个分布式计算（是一个 computation），global state $S_i$ 是在 event $e_i$ 处理前系统当时的全局状态（这个是真实的那个时刻的状态）。假设算法在计算 global state 时是在 $S_t$ 时初始化，并且在 $S_ø$ 前终止的（算法在计算全局状态时会横跨多个 event），也就是 $0 \leq t \leq ø$，那么如果我们能证明下面的结论，基本上回答了上面的问题：</p>
<ol>
<li>$S_*$ 可以由 $S_t$ 得到；</li>
<li>$S_ø$ 可以由 $S_*$ 得到；</li>
</ol>
<p>如果能证明这个，那就说明，算法得到的 global state 是可以由之前的 global state 得到，并且得到后面的 global state，从工程上来理解就是，<strong>算法得到的 global state 是可以完整正确得恢复计算作业的状态信息，让作业继续运行</strong>。</p>
<p>前面的是结论，这里将证明转化为：存在一个序列 $seq’$，它可以满足以下条件：</p>
<ol>
<li>$seq’$ 是 $seq$ 集合的一种变形（元素列表相同、顺序不同）， $S_t$、$S_*$、$S_ø$ 都是发生在这个 $seq’$ 上面的 global state；</li>
<li>$S_t$ 早于 $S_*$ 或者两者相等；</li>
<li>$S_*$ 早于 $S_ø$ 或者两者相等。 </li>
</ol>
<p>一个更加数学化的描述（方便后面证明）：一定存在一个 computation $seq’ = (e’_i, 0 \leq i)$，它满足以下条件：</p>
<ol>
<li>对于所有的 $i$，当 $i &lt; t$ 或者 $i \geq ø$ 时，$e’_i = e_i$；</li>
<li>$e’_i(t \leq i &lt; ø)$ 序列是 $e_i(t \leq i &lt; ø)$ 序列的一种变换（元素相同，顺序可能会有不同）；</li>
<li>对于所有的 $i$，当 $i \leq t$ 或者 $i \geq ø$ 时，$S’_i = S_i$；</li>
<li>并且存在一个 $k, t \leq k \leq ø$，使得 $S_* = S’_k$；</li>
</ol>
<p>这里实际要证明的是，找到这个 $seq’$，并且找到上面第四条要求的 $k$。</p>
<h3 id="结论证明"><a href="#结论证明" class="headerlink" title="结论证明"></a>结论证明</h3><p>为了证明上面的结论，这里引入两个概念：</p>
<ol>
<li><strong>prerecording event</strong>（后面记为 preEvent）：对于 process <code>p</code> 中的 event $e_i$，如果 <code>p</code> 做 snapshot（记录自己的状态）发生在收到 $e_i$ 之后，那么这个 $e_i$ 就是 prerecording event（也就是说：做 snapshot 时这个 $e_i$ 已经处理过了）；</li>
<li><strong>postrecording event</strong>（后面记为 postEvent）：对于 process <code>p</code> 中的 event $e_i$，如果 <code>p</code> 做 snapshot（记录自己的状态）发生在收到 $e_i$ 之前，那么这个 $e_i$ 就是 postrecording event（也就是说：这个 $e_i$ 是在做完 snapshot 后才处理的）；</li>
</ol>
<p>因此，对于 $e_i, (i &lt; t)$，都是 preEvent，对于 $e_i, (i \geq ø)$，都是 postEvent。</p>
<p>对于一个真实的 computation，可能会出现一个 postEvent $e_{j-1}$ （$i &lt; j &lt; ø$）出现在 preEvent $e_j$ 之前，当然这种情况只可能是 $e_{j-1}$ 和 $e_j$ 出现在不同的节点上（大家可以反向思考一下：对于同一个节点来说，event 的处理会保证 FIFO，如果 $e_{j-1}$ 是 postEvent，那么 $e_j$ 必然也是 postEvent）。</p>
<p>接下来，我们证明一下下面的结论：</p>
<p><strong>对于一个 event 序列 $seq’$（$seq$ 序列的变形），在这个序列中，所有的 preEvent 都在 postEvent 之前，下面我们将要证明 $S_*$ 就是 $seq’$ 中处理完所有 preEvent 后的 global state。</strong></p>
<p>这里假设有一个 postEvent $e_{j-1}$ （$i &lt; j &lt; ø$）出现在 preEvent $e_j$ 之前，这里我们将证明 <strong>交换 $e_{j-1}$ 和 $e_j$ 的位置之后得到的新 $seq’$ 序列依然是一个 computation</strong>（与原来的计算是保持一致的，只不过在中间某些时刻它们当时的状态不完全相同）。根据前面的叙述，这里的 event $e_{j-1}$ 和 $e_j$ 肯定是在两个不同的 process 上的。这里假设 $e_{j-1}$ 发生在 <code>p</code> 上，$e_j$ 发生在 <code>q</code> 上。</p>
<p>首先经过分析可以得到：<strong>绝对不可能出现 $e_{j-1}$ 发送一条数据然后在 $e_j$ 中收到</strong>，通过反证法分析：</p>
<ol>
<li>如果当 $e_{j-1}$ 发生时，通过 Channel <code>c</code> 向 <code>q</code> 发送了一条数据，那么在发送数据前一定已经有了 <code>marker</code> 发送过去（因为 $e_{j-1}$ 是 postEvent）；</li>
<li>当 $e_j$ 发生时，如果从 Channel <code>c</code> 中获得了这条数据，那么在这之前一定先收到了 <code>marker</code> 数据，这样的话，$e_j$ 也变成了 postEvent，所以这种情况是不可能存在的（这里不是很容易理解，可以换一种思路理解，它说明了 $e_{j-1}$ 和 $e_j$ 之间是没有因果关系的）。</li>
</ol>
<p>因为 $e_{j-1}$ 是发生在 <code>p</code> 中的，所以当 $e_{j-1}$ 发生时，<code>q</code> 的状态是不会改变的（可以回顾一下前面关于 event 的公式定义）。而假如 event $e_j$ 触发时，<code>q</code> 会从 Channel c 收到一条数据 <code>M</code>，那么 <code>M</code> 一定是在 Channel <code>c</code> 中队列的头部，并且是在 $e_{j-1}$ 之前，因为 $e_{j-1}$ 发出的数据是不可能会在 $e_j$ 中接收到的。因此，$e_j$ 可以出现在 global state $S_{j-1}$ 中（这个可以在回顾一下前面关于 $S_j$ 的定义，实际这段说明了 $e_j$ 可以出现在 $e_{j-1}$ 之前，因此也就有了这个结论）。 </p>
<p>而且在 $e_j$ 发生时，<code>p</code> 的状态并没有改变，因此，$e_{j-1}$ 是发生在 $e_j$ 之后的。那么也就是说明 $e_1, e_2, … , e_{j-2}, e_j, e_{j-1}$ 也是一个 computation，而且在经过这个 $e_1, e_2, …, e_{j-2}, e_j, e_{j-1}$ 计算之后的 global state 也跟 $e_1, e_2, …, e_{j-1}, e_j$ 计算之后的 global state 是一致的（主要还是因为 $e_{j-1}$ 和 $e_j$ 之间是没有因果关系的）。 </p>
<p>假设 $seq^*$ 也是 $seq$ 的序列的一个变形，它只是把交换了 $e_{j-1}$ 和 $e_j$ 的位置，假设 $\overline{S_i}$ 是 $seq^*$ 对应的瞬时（就是当时那一刻系统的真实状态，<code>i</code> 对应的是序列第几个 event）全局状态，则有下面的公式：</p>
<p>$\overline{S_i} = S_i, i \neq j$</p>
<p>如果将前面的 postEvent 与后面紧贴的 preEvent 的位置互换，将会存在一个 $seq’$（$seq$ 序列的一个变形），使得：</p>
<ol>
<li>所有 preEvent 都在 postEvent 前面；</li>
<li>$seq’$ 是一个 computation（这个 computation，我理解它的意思是说这个变换后的序列列表依然是可以运行的计算逻辑）；</li>
<li>当 $i &lt; t$ 或者 $i \geq ø $ 有 $e’_i = e_i$；</li>
<li>当 $i \leq t$ 或者 $i \geq ø $ 有 $S’_i = S_i$；</li>
</ol>
<p>现在我们证明：<strong> 这个 $seq’$ 序列中所有 preEvent 处理完之后的 global state 就是 $S_*$ </strong>，只需要证明下面两点即可：</p>
<ol>
<li>$S_*$ 中的每个 <code>p</code> 的状态是与 <code>p</code> 处理完所有 preEvent 之后的状态相同的，这个并不用证明，因为 perEvent 的概念就是这样来的，它指的就是那些在 snapshot 前要处理的 event 列表；</li>
<li>$S_*$ 中的每个 Channel <code>c</code> 的状态：所有 preEvent 发往 <code>c</code> 的数据列表，减去所有  preEvent 从 <code>c</code> 接收到数据列表。</li>
</ol>
<p>这里看下上面的第二点：假设 <code>c</code> 是 process <code>p</code> 和 <code>q</code> 之前的 Channel，$S_*$ 中关于 Channel <code>c</code> 的状态指的是，<code>q</code> 在记录自己的状态后在收到 <code>marker</code> 前从 <code>c</code> 收到的数据列表。而 <code>c</code> 在收到 <code>marker</code> 前接收到的数据列表都是 preEvent 发送过去的，所以上面第二点也就是完全得证了。</p>
<p>到这里，整个证明就结束了，我在看原论文的时候，这个证明看得云里雾里，看了好几遍才理解这个证明逻辑，这里比较关键的点有两个：</p>
<ol>
<li>前面所说的 $e_{j-1}$ 和 $e_j$ 之间是没有因果关系的，所以他们可以交换位置，并不会对后面的计算产生什么影响；</li>
<li>算法得到的 global state，作者找到了理论上的解释，就是处理完 preEvent 之后系统那个时刻的状态，虽然现实系统并不一定有这个状态，但它是有意义的，它可以完整、正确得恢复系统的状态，让系统继续后面的运行，并且恢复后的系统后面的运行情况跟正常情况是保持一致的。</li>
</ol>
<p>最后，作者给出了前面 Example 2 的解释，前面状态转移图中，发生的 event 事件列表如下：</p>
<ol>
<li>$e_0$：<code>p</code> 发送 <code>M</code>，并且将状态转移成 B（这是一个 postEvent，在这之前 <code>p</code> 已经记录了自己的状态）；</li>
<li>$e_1$：<code>q</code> 发送 <code>M&#39;</code>，并且将状态转移成 D（这是一个 preEvent，因为它发送在 <code>q</code> 记录自己状态之前，根据前面的讲述，因为 <code>q</code> 是 global state $S_3$ 时收到的 <code>marker</code>，当然这里只是其中一种情况，这里就是解释前面的所述的情况）；</li>
<li>$e_2$：<code>p</code> 接收到 <code>M&#39;</code>，并且将状态转移成 A（这是一个 postEvent，以为在这之前 <code>p</code> 已经记录了自己的状态）。</li>
</ol>
<p>根据上面的证明，这里的 $seq’$ 序列就是 $e_1、e_0、e_2$，而前面图中记录的 global state 就是系统在处理完 $e_1$ 之后的结果。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文的思想还是比较容易理解的，比如 <a href="https://zhuanlan.zhihu.com/p/53482103" target="_blank" rel="external">分布式快照算法: Chandy-Lamport 算法</a> 这篇文章介绍得就很简洁清晰，在我前面的文章 <a href="http://matt33.com/2019/10/20/paper-flink-snapshot/">Paper 阅读: Lightweight Asynchronous Snapshots for Distributed Dataflow</a> 中也讲述了 Flink 是如何将这个算法在落地应用的，但是这个算法的证明，并不容易。在看这篇论文之前，我并没有想过这个算法应该怎么证明？因为我潜意识的认为这是一个很容易理解、很正确的算法，甚至感觉完全不需要证明，就像苹果就应该从树上落到地上一样。但是看完这篇论文之后，才不得不佩服 Lamport 大神的牛逼之处，它不但提出了这个算法，还给这个算法找到理论上的证明方法，虽然论文并不是那么容易理解，但看完看明白之后收获很大，再次向 Chandy 和 Lamport 致敬~</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf" target="_blank" rel="external">Distributed Snapshots: Determining Global States of Distributed Systems</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/53482103" target="_blank" rel="external">分布式快照算法: Chandy-Lamport 算法</a>；</li>
<li><a href="http://liumx10.github.io/2016/04/06/Distributed-snapshot/" target="_blank" rel="external">Distributed snapshot</a>；</li>
<li><a href="https://yq.aliyun.com/articles/448900?spm=5176.10695662.1996646101.searchclickresult.6be16eb62Y8vXL&amp;aly_as=Z_bQHupu" target="_blank" rel="external">分布式Snapshot和Flink Checkpointing简介</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天对分布式系统领域的一篇经典论文 —— Chandy-Lamport 算法做了一下总结，这篇论文对于分布式快照算法产生了非常巨大的影响，比如：Apache Flink、Apache Spark 的 Structured Streaming、Ray 等分布式计算引擎都是使用
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="paper" scheme="http://matt33.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Paper 阅读: Lightweight Asynchronous Snapshots for Distributed Dataflow</title>
    <link href="http://matt33.com/2019/10/20/paper-flink-snapshot/"/>
    <id>http://matt33.com/2019/10/20/paper-flink-snapshot/</id>
    <published>2019-10-20T03:41:42.000Z</published>
    <updated>2020-06-23T14:13:17.223Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章是对 <a href="https://arxiv.org/pdf/1506.08603.pdf" target="_blank" rel="external">Lightweight Asynchronous Snapshots for Distributed Dataflow</a> 的一个总结，从文章题目也可以看出文章的主题 —— 分布式 dataflow 的轻量级异步 snapshot 算法，它是 Flink 团队在 2015 年发表的论文，主要讲述了对于 Streaming System 如何做 snapshot 的，它选取的是 Chandy-Lamport 算法（论文见 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf" target="_blank" rel="external"> Distributed Snapshots: Determining Global States of Distributed Systems</a>），关于这个算法后面会单独一篇文章来总结。在 <a href="https://arxiv.org/pdf/1506.08603.pdf" target="_blank" rel="external">Lightweight Asynchronous Snapshots for Distributed Dataflow</a> 这篇论文中，作者更多向我们表达的是 Chandy-Lamport 算法如何在 Flink 中落地的以及如何解决分布式 dataflow 做 snapshot 时遇到的痛点。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>分布式有状态的流处理允许在云上部署和执行大规模的流数据计算，并且要求低延迟和高吞吐。这种模式一个比较大的挑战，就是其容错能力，能够应对潜在的 failure。当前业内的方案都是依赖<strong>周期性地全局状态的 snapshot </strong> 做 failure recovery。但这种方案有两个非常大的缺陷：</p>
<ol>
<li>它们在做 snapshot 时会影响当前计算（目前的算法都是同步 snapshot）；</li>
<li>它们在当前 Operator State 中会把未处理和正在传输过程中的 record 做为 snapshot 的一部分持久化，这会导致 snapshot 非常大，记录了很多其实并不需要的数据。</li>
</ol>
<p>本篇论文中提出了一个新的  global consistent snapshot 算法 —— <strong>Asynchronous Barrier Snapshot（ABS）</strong>，它是一个轻量级的算法，非常适合现代 dataflow 系统，数据存储空间占用也非常小（论文原话是 <code>Our solution provides asynchronous state snapshots with low space costs that contain only operator states in acyclic execution topologies.</code>）。另外，这个算法不会影响作业计算，性能开销比较小。</p>
<h2 id="业内现状"><a href="#业内现状" class="headerlink" title="业内现状"></a>业内现状</h2><p>在过去的几十年中，关于连续处理系统的 recovery 机制，工业界和学术界提出了很多种解决办法，如：<a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf" target="_blank" rel="external"> Distributed Snapshots: Determining Global States of Distributed Systems</a>） 和 <a href="http://sigops.org/s/conferences/sosp/2013/papers/p439-murray.pdf" target="_blank" rel="external">Naiad: A Timely Dataflow System</a>。有一些系统如  Discretized Streams 和 Comet 会把连续处理当作 无状态的分布式批处理计算 来做状态恢复；对于有状态的 dataflow 系统，如：Naiad、SDGs、Piccolo 和 SEEP，它们是我们的主要关注点，它们使用 checkpoint 获取全局一致的 snapshot 来做故障恢复。</p>
<p>关于 <strong>consistent global snapshot</strong> 的问题，自从在 Chandy 和 Lamport 的论文中提出来后，过去二十多年一直在被广泛地研究。全局 snapshot 理论上反映了作业执行的总体状态以及 operator 实例的可能状态。对于全局一致性 snapshot 算法，Naiad 中提出了一个简单但代价非常高昂的实现方案：</p>
<ol>
<li>第一步，先停止计算；</li>
<li>第二步，开始做 snapshot；</li>
<li>第三步，如果 snapshot 完成了，每个 task 再恢复之前的计算。</li>
</ol>
<p>这个实现方案对吞吐和空间占用都有很大的影响，它并不是一个很好的方案。另一个实现方案，就是 Chandy-Lamport 算法，当前它已经应用在很多的系统中，<strong>它是异步地执行快照，并且要求上游数据源可以回溯</strong>（也就是要求数据源能够自己备份）。它是通过在数据流中发送 marker 来实现，marker 会触发 operator 和 state 的 snapshot。但这种算法还需要额外的存储空间用于上游数据量恢复，数据流的重新计算也会导致恢复时间较长（主要还是原生算法会对一些 record 也做相应的 snapshot，这会导致存储空间占用过高以及恢复时间过长）。本论文中提出的方案扩展了原生的 Chandy-Lamport 算法，但对于无环 graph 它不会备份未处理及通道中正在传输的 record，对于有环的 graph，它也只需要很少量的 record 备份。</p>
<h2 id="解决方案：Asynchronous-Barrier-Snapshot（ABS）"><a href="#解决方案：Asynchronous-Barrier-Snapshot（ABS）" class="headerlink" title="解决方案：Asynchronous Barrier Snapshot（ABS）"></a>解决方案：Asynchronous Barrier Snapshot（ABS）</h2><p>因为这个算法的实现本身就是为了解决 <a href="https://flink.apache.org/" target="_blank" rel="external">Apache Flink</a> 的容错问题，论文中的描述也是以 Flink 系统为例，所以想要搞明白这个算法还是需要一些 Flink 的基础，本文中，我们就不再对 Flink 展开了。这里只简单介绍一下，有兴趣的可以看下官网资料，Flink 是一个可用于 Streaming 和 Batch 处理的大数据处理引擎，它本身的设计也是深受 Google DataFlow 模型的影响，可以说 Flink 是开源系统中最接近 DataFlow 思想的一个计算引擎。另外，Flink 的作业，在提交的时候都会被翻译成一个有向无环图（DAG），对于 Flink Master 来说，提交过来的作业都是一个 graph。</p>
<h3 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h3><p>这里，我们这样定义一个 global snapshot（它需要包含所有的状态信息，这样才能保证 failover 之后能够正确恢复状态）：</p>
<p>$G^*=(T^*, E^*)$</p>
<p>它代表一个 execution graph $G = (T, E)$ 的一个全局快照，$T^*$ 代表所有 task 状态的集合，$E^*$ 代表所有 edge 状态的集合。也就是说：</p>
<ol>
<li>$∀t ∈ T, s_t^∗ ∈ T^∗$，$T^*$ 会包含所有 Operator 的状态；</li>
<li>$E^*$ 会包含所有 channel 状态的集合，$e^*$ 会包含 $e$ 中正在传输的所有 record。</li>
</ol>
<p>为了能够保证 recovery 后正确恢复状态信息，对于每个 $G^*$，都需要保证以下两个特性：</p>
<ol>
<li>Termination：snapshot 能够在一定的时间内完成；</li>
<li>Feasibility：它表示这个 snapshot 是有意义的，也就是说在 snapshot 期间尽管计算没有停止，也不会有任何信息丢失。</li>
</ol>
<h3 id="ABS-for-Acyclic-Dataflows：非环-dataflow-中的-ABS-实现"><a href="#ABS-for-Acyclic-Dataflows：非环-dataflow-中的-ABS-实现" class="headerlink" title="ABS for Acyclic Dataflows：非环 dataflow 中的 ABS 实现"></a>ABS for Acyclic Dataflows：非环 dataflow 中的 ABS 实现</h3><p>这里先看下在无环 dataflow 中 ABS 是如何实现的，因为 Flink 只支持有向无环图，所以这个就是 Flink checkpoint 的实现方案。</p>
<p>当把一个作业的执行逻辑划分为多个 stage 时，做 snapshot 不存储 channel 中的 state 是完全有可能的。如果一个 operator 已经完成了对输入的所有计算，并且数据已经完全输出出去，那么只对 operator 的 state 做 snapshot 就可以达到我们的要求。</p>
<p>具体的实现就是：每个阶段的输入数据都会被周期性地插入一些特殊标记 —— <strong>barrier</strong>，这些 barrier 会推送到整个 dataflow 中直到 sink 节点（dataflow 中结束节点，它没有下游输出），每个 task 如果收到输入所有的 barrier 就开始做相应的 snapshot。这个算法的实现是有以下假设的：</p>
<ol>
<li>网络传输是可靠的、可以做到 FIFO 传输，并且可以实现 <code>blocked</code> 和 <code>unblocked</code>，如果一个通道是 <code>blocked</code>，它会把这个通道接收到的所有数据缓存起来先不发送，直接收到 <code>unblocked</code> 的信号才会发送；</li>
<li>Task 可以在其对应的 channel 触发以下三种操作：<code>blocked</code>、<code>unblocked</code> 和 <code>send</code> msgs，<code>Broadcasting</code> msgs 表示的是向下游所有的 channel 发送数据；</li>
<li>对于 source 节点来说，输入节点被抽象为 <code>Nil</code> 输入通道（一个虚拟通道）。</li>
</ol>
<p>这个算法的伪代码如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Algorithm 1 Asynchronous Barrier Snapshotting for Acyclic Execution Graphs</span></span><br><span class="line">1: upon event &lt;Init | input_channels, output_channels, fun, init_statei&gt; <span class="keyword">do</span></span><br><span class="line">2:     state := init_state; blocked_inputs := 0;</span><br><span class="line">3:     inputs := input_channels;</span><br><span class="line">4:     out_puts := output_channels; udf := fun;</span><br><span class="line">5:</span><br><span class="line">6: upon event &lt;receive | input, &lt;barrier&gt;&gt; <span class="keyword">do</span></span><br><span class="line">7:     <span class="keyword">if</span> input != Nil <span class="keyword">then</span> <span class="comment"># 非 source 节点收到 barrier 时，会先阻塞当前 channel</span></span><br><span class="line">8:         blocked_inputs := blocked_inputs ∪</span><br><span class="line">&#123;input&#125;;</span><br><span class="line">9:         trigger &#123;block | input&#125;;</span><br><span class="line">10:     <span class="keyword">if</span> blocked_inputs = inputs <span class="keyword">then</span> <span class="comment"># 只有所有的 input 都收到 barrier</span></span><br><span class="line">11:         blocked_inputs := 0;</span><br><span class="line">12:         broadcast &#123;send | outputs, &lt;barrier&gt;&gt;; <span class="comment"># 把 barrier 广播到下游所有节点</span></span><br><span class="line">13:         trigger &#123;snapshot | state&#125;; <span class="comment"># 触发 snapshot</span></span><br><span class="line">14:         <span class="keyword">for</span> each inputs as input <span class="comment"># unblock 所有 blocked 的通道</span></span><br><span class="line">15:             trigger &lt;unblock | input&gt;;</span><br><span class="line">16:</span><br><span class="line">17:</span><br><span class="line">18: upon event &lt;receive | input, msg&gt; <span class="keyword">do</span> <span class="comment"># 非常 barrier 的数据处理</span></span><br><span class="line">19:     &#123;state1 ,out_records&#125; := udf(msg, state);</span><br><span class="line">20:     state := state1;</span><br><span class="line">21:     <span class="keyword">for</span> each out_records as &#123;out_put, out_record&#125;</span><br><span class="line">22:         trigger &lt;send | output, out record&gt;;</span><br><span class="line">23:</span><br><span class="line">24:</span><br></pre></td></tr></table></figure>
<p>dataflow graph 执行图如下所示：</p>
<p><img src="/images/paper/abs-1.png" alt="Asynchronous barrier snapshots for acyclic graphs"></p>
<p>ABS 算法的执行流程如下：</p>
<ol>
<li>中心协调器周期性地在所有输入端插入 barrier；</li>
<li>当一个 source 节点接收到 barrier 时，它对当前的状态做下 snapshot，并且 broadcast barrier 到所有的下游节点（如上图中的 a 子图）；</li>
<li>当一个非 source 节点从它的输入通道中接收到一个 barrier 时，它会 block 当前的 channel 直到接收该节点所有输入端发送的 barrier（如上图中的 b 子图以及代码第 9 行）；</li>
<li>当从所有输入 channel 都接收到 barrier 之后，这个 task 会对当前状态做一个 snapshot，并且 broadcast 这个 barrier 到所有的输出端（如上图中的 c 子图以及代码第 12-13 行）；</li>
<li>最后，这个 task 会 unblock 它所有的输入 channel，继续进行计算（如上图中的 d 子图以及代码第 15 行）。</li>
</ol>
<p>根据前面所示，我们知道，当前一个完整的 snapshot $G^* = (T^*, E^*)$，其 $E^* =0$，Operator 中的 state 信息就是完整的 snapshot。</p>
<p>对于 Termination 要求，它依赖于 channel 的可靠性以及 graph 的无环性；对于 Feasibility 要求，它依赖于 channel 的 FIFO 特性。只要这些是可以满足的，那么这两个要求就是可以满足的。</p>
<h3 id="ABS-for-Cyclic-Dataflows：有环-dataflow-中的-ABS-实现"><a href="#ABS-for-Cyclic-Dataflows：有环-dataflow-中的-ABS-实现" class="headerlink" title="ABS for Cyclic Dataflows：有环 dataflow 中的 ABS 实现"></a>ABS for Cyclic Dataflows：有环 dataflow 中的 ABS 实现</h3><p>前面分析完无环的情况，接下来再来看看有环的情况，当前的 ABS 算法稍微做些改造也是可以处理有环的情况。根据前面的介绍，有环带来的最大问题是：</p>
<ol>
<li>死锁，一个 task 可能会不断收到 barrier，导致 snapshot 无法在可预期的时间内完成；</li>
<li>当然，有环还会导致另外一个问题，就是数据可能没有记录到 snapshot 中，会导致准确性有误；</li>
</ol>
<p>对于有环的情况，论文是在不引入额外 channel block 的情况下扩展了原来的算法，这里就不再列出伪代码了，有兴趣的可以看下论文，这里以下图为例简单介绍一下：</p>
<ol>
<li>对于有 back-edge 输入的节点（后边节点做输入的情况）来说，一旦它所有正常的输入 channel 都收到了这个 barrier，它会先对本地状态做本地 copy（下图 b）；</li>
<li>从这个时间点开始，这个节点会将从 back-edge channel 接收到的所有数据记录下来直到接收到了相应的 barrier（下图 c），第一步 copy 的状态及第二步记录的数据都会作为 snapshot 的一部分。</li>
</ol>
<p><img src="/images/paper/abs-2.png" alt="Asynchronous barrier snapshots for cyclic graphs"></p>
<p>按照改进后的算法，是可以避免死锁的，这样的话 Termination 的要求是可以满足的；Feasibility 的特性依然是依赖于 channel 的 FIFO 来保证，snapshot 中每个 task state 都会包含该 task 在收到前置节点 barrier 之后的状态，对于有后置节点输入的 task 来说，它会把从后置节点接收到的数据记录下来，只会 copy 非常少量的数据。</p>
<h3 id="Failure-Recovery"><a href="#Failure-Recovery" class="headerlink" title="Failure Recovery"></a>Failure Recovery</h3><p>论文还简单介绍了 Flink 是如何做 failover 恢复的，有了前面的全局一致 snapshot 算法，failover 做起来就简单很多。在 Flink 中，还支持  partial graph recovery，对于失败的 task，只需要恢复它的上游即可，并不需要全局恢复。为了在内部实现 exactly-once，通过给数据进行编号来避免重复数据。</p>
<h3 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h3><p>性能对比了本文提出的 ABS 算法以及 Naiad 中提出的全局同步 snapshot 算法，测试 case 选择了一个有 6 个 operator 的作业，它有三个地方会进行网络 shuffle，这样可以尽量增大 ABS 算法 channel block 带来的影响（如下图 5）。实验中，输入端会模拟 1 百万测试数据，operator 的状态信息主要包括按 key 聚合的中间结果以及 offset 信息，下图的纵坐标是作业运行时间，baseline 表示的是不开启 snapshot 时的性能，在这里做对比使用。</p>
<p><img src="/images/paper/abs-3.png" alt="性能测试结果"></p>
<p>如上图 6，可以看到，当 snapshot 时间间隔非常小，同步的 snapshot 性能非常差，因为它在做 snapshot 会阻塞计算，时间都花费在 snapshot 上了，而 ABS 算法的实验结果就好了很多。如上图 7，集群节点及作业并行度从 5 逐渐增加到 40，可以看到 ABS 算法的性能还很稳定的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文是工业界对 Chandy-Lamport 算法实践后做的改进优化，将 Chandy-Lamport 算法在 Flink 的 global consistent snapshot 中落地，这篇论文还是非常值得读一下的，看下 Flink 在解决这个问题的时候是怎么去做的，论文的优化点其实并不是很大，一是把同步变成异步，二是从尽量减小存储空间占用的点出发，最后发现只存储 operator 状态不存储 edge 状态也是完全可以的，而且实践起来的效果确实证明比当时其他系统的算法要好。</p>
<hr>
<p>参考</p>
<ol>
<li><a href="https://arxiv.org/pdf/1506.08603.pdf" target="_blank" rel="external">Lightweight Asynchronous Snapshots for Distributed Dataflow</a>；</li>
<li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/Determining-Global-States-of-a-Distributed-System.pdf" target="_blank" rel="external"> Distributed Snapshots: Determining Global States of Distributed Systems</a>）；</li>
<li><a href="http://sigops.org/s/conferences/sosp/2013/papers/p439-murray.pdf" target="_blank" rel="external">Naiad: A Timely Dataflow System</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章是对 &lt;a href=&quot;https://arxiv.org/pdf/1506.08603.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Lightweight Asynchronous Snapshots for Distributed D
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="paper" scheme="http://matt33.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Paper 阅读: Real-Time Machine Learning: The Missing Pieces</title>
    <link href="http://matt33.com/2019/10/19/paper-ray1/"/>
    <id>http://matt33.com/2019/10/19/paper-ray1/</id>
    <published>2019-10-19T14:26:20.000Z</published>
    <updated>2020-06-23T14:13:17.223Z</updated>
    
    <content type="html"><![CDATA[<p>这周抽空看了关于 ray 的一篇论文，论文是 2017 年发表的（见：<a href="https://arxiv.org/pdf/1703.03924.pdf" target="_blank" rel="external">Real-Time Machine Learning: The Missing Pieces</a>，他们比较新的论文是 18 年发表的，见：<a href="https://www.usenix.org/system/files/osdi18-moritz.pdf" target="_blank" rel="external">Ray: A Distributed Framework for Emerging AI Applications</a>），虽然论文描述的架构与现在 ray 真正的构架实现已经有了较大的不同，主要也是 ray 这两年发展比较快，架构做了很多的优化，不过本篇论文依然值得仔细阅读学习 一下的，这篇论文也展示了 ray 最初设计实现的出发点。</p>
<blockquote>
<p>本章不会严格按照论文翻译，整体会按照下面的思路来叙述：</p>
<ol>
<li>遇到的问题什么？</li>
<li>当前业内的方案是什么？</li>
<li>论文提出了什么样的解决方案？达到了什么效果？</li>
</ol>
</blockquote>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>现在有越来越多的 ML 应用，不仅仅使用静态模型进行训练预测，它们会使用动态、实时决策的反馈来实时调整应用，这种场景就给计算模型提出了一些新的要求：</p>
<ul>
<li>高吞吐低延迟；</li>
<li>自适应创建任意的 task graph；</li>
<li>针对不同的数据集使用的不同内核（可以理解为融合计算）；</li>
</ul>
<p>这些要求如果单独去实现的话，并不是很难，但是把它们在一套系统里同时实现就非常有挑战性了，而目前业内并没有这样的一套方案（指的是这套系统设计之前，业内还没有）。</p>
<h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h3><p>这里，我们看个示例，下图 a 是一个传统的 ML 应用架构，它主要使用离线数据做训练和预测（ML 中监督式学习），但是现在有个趋势，就是如下图 b 所示的构架越来越多，即 ML 应用与实时反馈的回路紧密集成，它会依赖实时数据做训练和预测。</p>
<p><img src="/images/paper/ray1-1.png" alt="ML 应用实例"></p>
<h3 id="场景对计算模型的要求"><a href="#场景对计算模型的要求" class="headerlink" title="场景对计算模型的要求"></a>场景对计算模型的要求</h3><p>对于前面提出的场景，对模型的灵活性和性能有了新的要求，在满足这些要求的同时，还要保持现代分布式执行模型的优势（比如：应用级别的容错保证等等），挑战性很大，根据之前在 Spark、MPI 和 TensorFlow 开发 ML 和 RL（强化学习）的经历，这些痛点更加明显。当然这些要求也是通用的，并不仅仅使用在 ML 和 RL 中。</p>
<h4 id="性能要求"><a href="#性能要求" class="headerlink" title="性能要求"></a>性能要求</h4><p>这些 ML 应用也是有严格的延迟和吞吐要求：</p>
<ul>
<li>R1：<strong>low latency</strong>，ML 应用的实时性、reactive 和 interactive 特性都是需要端到端执行的毫秒级延迟；</li>
<li>R2：<strong>High throughput</strong>，训练和部署期间的计算都是需要支持每秒几百万 task 执行的高吞吐任务；</li>
</ul>
<h4 id="执行模型要求"><a href="#执行模型要求" class="headerlink" title="执行模型要求"></a>执行模型要求</h4><p>尽管现在业内很多的执行模型已经对常见计算模式的识别和优化取得了很大进展，但 ML 应用还需要更大的灵活性：</p>
<ul>
<li>R3：<strong>dynamic task creation</strong>，诸如蒙卡洛树搜索（Monte Carlo Tree）的 RL 基本算法在执行期间会根据其他 task 执行的结果动态创建新的 task；</li>
<li>R4：<strong>heterogeneous tasks（异构任务）</strong>，深度学习和 RL 在执行时间和资源需求上差异很大，因此对执行异构任务和资源的支持是非常有必要的；</li>
<li>R5：<strong>arbitrary dataflow dependencies</strong>，深度学习和 RL 应用会产生任意且更细粒度的任务依赖；</li>
</ul>
<h4 id="实践要求"><a href="#实践要求" class="headerlink" title="实践要求"></a>实践要求</h4><ul>
<li>R6：<strong>transparent fault tolerance</strong>：容错是一个非常重要和核心的要求，但与高吞吐、非确定性 task 执行放在一起实现就有一定难度了；</li>
<li>R7：<strong>debuggablitity and profiling（调试和性能分析）</strong>：调试和性能分析是编写任何分布式作业最耗时的方面，ML 应用尤其如此，这些应用通常是计算密集和随机的。</li>
</ul>
<p>上面的要求与我们常见的大数据计算系统，如：Flink 和 Spark，最大的区别是 R3~R5，对于 Flink 和 Spark 来说，向集群提交的 dataflow graph 是固定的，提交之后是不能改变的，这种模式在 ML 场景就显得非常不灵活了。</p>
<h2 id="业内现况"><a href="#业内现况" class="headerlink" title="业内现况"></a>业内现况</h2><p><strong>Static dataflow Systems</strong>，它们需要开发者提前设计好 dataflow graph，比如：MR 和 Spark。对于其他的像 Dryad 和 Naiad 的系统，它们是支持复杂的依赖结构（R5）；TensorFlow 和 MXNet，它们对深度学习场景做了很多优化。然而没有一个系统，可以完全支持根据输入数据和 task 执行任意动态扩展 dataflow graph。</p>
<p><strong>Dynamic Dataflow Systems</strong>，像 CIEL 和 Dask 不但支持上面 static dataflow Systems 的很多特性，还支持动态 task 创建（R3），这些模型符合我们 R3~R5 的需要。然而，它们有一些受限的地方，比如：完全中心化的调度，它们会导致我们不得不在吞吐和 latency 之间做取舍。</p>
<p><strong>Other Systems</strong> 像 Open MPI 和基于 actor 模型变体的系统（Orleans 和 Erlang）提供了低延迟（R1）和高吞吐（R2）的分布式计算。尽管这些系统也可以支持我们执行模型的需要（R3-R5，并且已经在 ML 中应用了），但是很多系统 level 的逻辑需求却需要应用程序自己去实现，比如：容错和 task 调度的本地感知。</p>
<p>综上，业内并没有一套可以完全符合我们的需求的系统，所以最好的办法就是重新造轮子，从头开始设计和写一套系统，业内对这块也有了很多的实践，虽然是重头开始设计，但还是可以从业内现有的系统中借鉴很多的经验（毕竟这套系统设计的出发点，也考虑到了通用性，而不仅仅用在 ML 领域）。</p>
<h2 id="解决方案：一套新的架构模型"><a href="#解决方案：一套新的架构模型" class="headerlink" title="解决方案：一套新的架构模型"></a>解决方案：一套新的架构模型</h2><p>论文发表的时候，ray 还处于初期，当时的一些架构设计后来也有了一些变化，但本文依然以论文中的架构为主来介绍。</p>
<h3 id="API-和-执行模型"><a href="#API-和-执行模型" class="headerlink" title="API 和 执行模型"></a>API 和 执行模型</h3><p>新提出的架构与 Flink 和 Spark 最大区别是在 R3~R5，为了支持这三个执行模型要求，这里设计了一套 API，它允许任意的 function 作为远程的 task 执行（并且还是在 dataflow 中的一环）。</p>
<ol>
<li><strong>task 创建是非阻塞的</strong>：当一个 task 创建后，会以 <code>future</code> 做 task 的返回值，task 是在系统中异步执行的；</li>
<li><strong>任意 function 的执行都可以作为远程 task 执行</strong>：为了支持 R4，function 都可以作为远程 task 执行的；task 创建的参数可以是一个 <code>future</code>，这样的话，新创建的 task 就会依赖这个 future 对应的 task，它也就实现了任意的 DAG 依赖（R5）；</li>
<li><strong>任何执行的 task 都可以在不阻塞它们计算的同时创建新的 task</strong>，因此，task 的吞吐不会受到 worker 带宽的限制（R2），并且可以做到动态创建 graph（R3）；</li>
<li>一个 task 的返回值可以通过调用 <code>get</code> 方法得到，它会阻塞直到 task 执行结束；</li>
<li><code>wait</code> 方法可以执行批量任务等待，该方法需要指定一个 future 列表、timeout 参数和要返回的 task number 的数量，这个方法会返回 future 任务的子集，它们是在 timeout 达到或满足数量要求时返回的。</li>
</ol>
<p>这里看这些 API 可能会有一些不太理解，给大家推荐一篇文章：<a href="https://www.cnblogs.com/fanzhidongyzby/p/7901139.html" target="_blank" rel="external">高性能分布式执行框架——Ray</a>，这篇文章对于这些 API 在 ray 上的实现都有详细的示例，有兴趣的可以看下。</p>
<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>这里设计的架构也是 Master/Slave 模型，它包含：运行在每个 node 上的 worker 进程、每个 node 会有一个 local scheduler、一个或多个 global scheduler 以及在 worker 间做数据共享的内存对象存储，如下图所示（大家依然可以看下这篇文章 <a href="https://www.cnblogs.com/fanzhidongyzby/p/7901139.html" target="_blank" rel="external">高性能分布式执行框架——Ray</a>，它介绍了 Ray 的落地实现架构，但论文中更多的还是模型设计）。</p>
<p><img src="/images/paper/ray1-2.png" alt="架构模型"></p>
<p>Master 负责全局协调和状态维护，Slave 执行分布式计算任务，不同与传统计算系统的是，它使用了混合任务调度的思路：</p>
<ul>
<li><strong>Global Scheduler</strong>：Master上启动了一个全局调度器，用于接收本地调度器提交的任务，并将任务分发给合适的本地任务调度器执行；</li>
<li><strong>Control State（db 服务）</strong>：Master 上启动了一到多个 db server 用于保存分布式任务的状态信息，包括对象机器的映射、任务描述、任务debug信息等；</li>
<li><strong>Local Scheduler</strong>：每个 Slave 上启动了一个本地调度器，用于接收本地 worker 提交任务的请求以及提交任务请求到全局调度器；</li>
<li><strong>Worker</strong>：每个 Slave 上可以启动多个 Worker 进程执行分布式任务，并将计算结果存储到 ObjectStore；</li>
<li><strong>Object Store</strong>：每个 Slave 上启动了一个 Object Store 存储只读数据对象，Worker 可以通过共享内存的方式访问这些对象数据，这样可以有效地减少内存拷贝和对象序列化成本（Object Store 底层由 Apache Arrow 实现）。</li>
</ul>
<h4 id="Centralized-Control-State"><a href="#Centralized-Control-State" class="headerlink" title="Centralized Control State"></a>Centralized Control State</h4><p>如前面图中所示，这套架构是依赖一个逻辑中心控制器，为了实现这套架构，设计时使用了一个 database 来做 Control State 的工作，存储系统的状态信息以及用于系统组件间的通信信息。</p>
<p>在这个设计中，除了 Control State，其他组件都是无状态的，所以只要 Control State 具有容错性，系统就可以简单恢复任务中失败的节点（因为 dataflow graph 是不固定，所以真正实现时 recover 的逻辑与 Spark 和 Flink 这类系统是不同的）(R6<br>)。为了实现高吞吐，在写数据库的时候，允许按 key hash 写入（R1-R2）。</p>
<h4 id="Hybrid-Scheduler"><a href="#Hybrid-Scheduler" class="headerlink" title="Hybrid Scheduler"></a>Hybrid Scheduler</h4><p>这套架构采用混度调度器的模式，简单来说，在 task 调度时，实现如下：</p>
<ol>
<li>worker 提交 task 到本地调度器，它会决定是 assign 到本地本机其他 worker 还是上报到 global 调度器，global 调度器会根据全局信息（资源利用率和计算本地化等因素）来决定把 task 分配到哪个节点上；</li>
<li>因为 task 是允许创建其他 task，所以一个集群里的 task 调度请求是可能来自任何的 worker 的；</li>
<li>系统允许本地调度器处理本地的调度工作，可以减少与全局调度器的交互，最大限度减少了通信开销。</li>
</ol>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>ray 在真正实现时，提交给作业是一个更细粒度的 remote function，任务 DAG 依赖关系由函数依赖关系自由控制，像 Flink 和 Spark 系统，提交的是任务的 DAG，一旦提交就不能修改。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>就像在前面文中说的一样，个人感觉这套架构与目前主流大数据计算引擎最大的区别还是 R3~R5，这样设计也是因为业务场景驱动，static dataflow graph 在一些 ML 和图计算的场景下无法很好的满足业务需求，每套计算引擎最开始要解决的问题都不太一样，都有一个自己的切入点，只不过做着做着大家发现自己的场景很有限，都想切入到更多的领域，做一个更加通用的引擎，通用引擎对于一些简单业务场景来说可能会显得特别重、对另一些复杂业务场景来说可能又显得不能完全支持，这也是计算引擎最近几年遍地开花的原因，而且相信未来还会有很大变化。而且现在有个趋势：对于业务来说，大家发现没有必要非要使用什么统一的引擎，引擎（包括存储和计算）是什么我可以完全不 care，面向用户的是统一的 DSL，用什么引擎由平台来帮业务选择，这个或许是一个趋势，但从另一个方面来说维护多套引擎的成本有点高，就像现在公司不会选择在服务器维护很多套操作系统一样，最终会是什么样子，过几年再看。</p>
<h2 id="业内实践"><a href="#业内实践" class="headerlink" title="业内实践"></a>业内实践</h2><p>关于 ray，目前国内看到的是蚂蚁金服有在使用，其他公司好像没有听说过，ray 目前已经在蚂蚁金服的很多业务上落地，这个大家可以参考今年阿里云栖大会上蚂蚁金服的分享（见：<a href="https://developer.aliyun.com/article/721329" target="_blank" rel="external">开放计算架构：蚂蚁金服是如何用一套架构容纳所有计算的？</a>），可以看到 ray 中落地比较好的场景还是 ML 和图计算相关的业务，关于图计算，国内估计也只有蚂蚁和腾讯有这么强烈的业务需求。刚好今天看到一篇文章 —— <a href="https://mp.weixin.qq.com/s/bSpm72WIx061cFMFgkaMlw" target="_blank" rel="external">腾讯开源全栈机器学习平台 Angel 3.0，支持三大类型图计算算法</a>，腾讯这边在图计算这块选择了他们开源的 Angel 平台做图计算，他们有兴趣地的可以深入看下。</p>
<p>最后，说点题外话，笔者本来计划今年每个月都要输出一篇经典论文的阅读笔记的，但不料的是，今年工作实在是太忙太累，很多计划并没有落地执行，后面会多花点工作之外的时间把今年欠的博客补一下，最近也会开始写 Apache Flink1.9 源码分析系列以及 Paper 阅读总结系列，文章会在公众号同步发布，大家多多关注。</p>
<p><img src="/images/wangm92-2.jpg" alt="个人公众号，欢迎关注"></p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1703.03924.pdf" target="_blank" rel="external">Real-Time Machine Learning: The Missing Pieces</a>;</li>
<li><a href="https://www.usenix.org/system/files/osdi18-moritz.pdf" target="_blank" rel="external">Ray: A Distributed Framework for Emerging AI Applications</a>；</li>
<li><a href="https://developer.aliyun.com/article/721329" target="_blank" rel="external">开放计算架构：蚂蚁金服是如何用一套架构容纳所有计算的？</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/bSpm72WIx061cFMFgkaMlw" target="_blank" rel="external">腾讯开源全栈机器学习平台 Angel 3.0，支持三大类型图计算算法</a>；</li>
<li><a href="https://www.cnblogs.com/fanzhidongyzby/p/7901139.html" target="_blank" rel="external">高性能分布式执行框架——Ray</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这周抽空看了关于 ray 的一篇论文，论文是 2017 年发表的（见：&lt;a href=&quot;https://arxiv.org/pdf/1703.03924.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Real-Time Machine Learni
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="paper" scheme="http://matt33.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Apache Calcite 优化器详解（二）</title>
    <link href="http://matt33.com/2019/03/17/apache-calcite-planner/"/>
    <id>http://matt33.com/2019/03/17/apache-calcite-planner/</id>
    <published>2019-03-17T12:41:35.000Z</published>
    <updated>2020-06-23T14:13:17.223Z</updated>
    
    <content type="html"><![CDATA[<p>紧接上篇文章<a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a>，这里是 Calcite 系列文章的第二篇，后面还会有文章讲述 Calcite 的实践（包括：如何开发用于 SQL 优化的 Rule）。本篇文章主要介绍 Apache Calcite 优化器部分的内容，会先简单介绍一下 RBO 和 CBO 模型，之后详细讲述 Calcite 关于这两个优化器的实现 —— HepPlanner 和 VolcanoPlanner，文章内容都是个人的一些理解，由于也是刚接触这块，理解有偏差的地方，欢迎指正。</p>
<h1 id="什么是查询优化器"><a href="#什么是查询优化器" class="headerlink" title="什么是查询优化器"></a>什么是查询优化器</h1><p>查询优化器是传统数据库的核心模块，也是大数据计算引擎的核心模块，开源大数据引擎如 Impala、Presto、Drill、HAWQ、 Spark、Hive 等都有自己的查询优化器。Calcite 就是从 Hive 的优化器演化而来的。</p>
<p>优化器的作用：将解析器生成的关系代数表达式转换成执行计划，供执行引擎执行，在这个过程中，会应用一些规则优化，以帮助生成更高效的执行计划。</p>
<blockquote>
<p>关于 Volcano 模型和 Cascades 模型的内容，建议看下相关的论文，这个是 Calcite 优化器的理论基础，代码只是把这个模型落地实现而已。 </p>
</blockquote>
<h2 id="基于规则优化（RBO）"><a href="#基于规则优化（RBO）" class="headerlink" title="基于规则优化（RBO）"></a>基于规则优化（RBO）</h2><p>基于规则的优化器（Rule-Based Optimizer，RBO）：根据优化规则对关系表达式进行转换，这里的转换是说一个关系表达式经过优化规则后会变成另外一个关系表达式，同时原有表达式会被裁剪掉，经过一系列转换后生成最终的执行计划。</p>
<p>RBO 中包含了一套有着严格顺序的优化规则，同样一条 SQL，无论读取的表中数据是怎么样的，最后生成的执行计划都是一样的。同时，在 RBO 中 SQL 写法的不同很有可能影响最终的执行计划，从而影响执行计划的性能。</p>
<h2 id="基于成本优化（CBO）"><a href="#基于成本优化（CBO）" class="headerlink" title="基于成本优化（CBO）"></a>基于成本优化（CBO）</h2><p>基于代价的优化器(Cost-Based Optimizer，CBO)：根据优化规则对关系表达式进行转换，这里的转换是说一个关系表达式经过优化规则后会生成另外一个关系表达式，同时原有表达式也会保留，经过一系列转换后会生成多个执行计划，然后 CBO 会根据统计信息和代价模型 (Cost Model) 计算每个执行计划的 Cost，从中挑选 Cost 最小的执行计划。</p>
<p>由上可知，CBO 中有两个依赖：统计信息和代价模型。统计信息的准确与否、代价模型的合理与否都会影响 CBO 选择最优计划。 从上述描述可知，CBO 是优于 RBO 的，原因是 RBO 是一种只认规则，对数据不敏感的呆板的优化器，而在实际过程中，数据往往是有变化的，通过 RBO 生成的执行计划很有可能不是最优的。事实上目前各大数据库和大数据计算引擎都倾向于使用 CBO，但是对于流式计算引擎来说，使用 CBO 还是有很大难度的，因为并不能提前预知数据量等信息，这会极大地影响优化效果，CBO 主要还是应用在离线的场景。</p>
<h1 id="优化规则"><a href="#优化规则" class="headerlink" title="优化规则"></a>优化规则</h1><p>无论是 RBO，还是 CBO 都包含了一系列优化规则，这些优化规则可以对关系表达式进行等价转换，常见的优化规则包含：</p>
<ol>
<li>谓词下推 Predicate Pushdown</li>
<li>常量折叠 Constant Folding</li>
<li>列裁剪 Column Pruning</li>
<li>其他</li>
</ol>
<p>在 Calcite 的代码里，有一个测试类（<code>org.apache.calcite.test.RelOptRulesTest</code>）汇集了对目前内置所有 Rules 的测试 case，这个测试类可以方便我们了解各个 Rule 的作用。在这里有下面一条 SQL，通过这条语句来说明一下上面介绍的这三种规则。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="number">10</span> + <span class="number">30</span>, users.name, users.age</span><br><span class="line"><span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">join</span> jobs <span class="keyword">on</span> users.id= user.id</span><br><span class="line"><span class="keyword">where</span> users.age &gt; <span class="number">30</span> <span class="keyword">and</span> jobs.id&gt;<span class="number">10</span></span><br></pre></td></tr></table></figure>
<h2 id="谓词下推（Predicate-Pushdown）"><a href="#谓词下推（Predicate-Pushdown）" class="headerlink" title="谓词下推（Predicate Pushdown）"></a>谓词下推（Predicate Pushdown）</h2><p>关于谓词下推，它主要还是从关系型数据库借鉴而来，关系型数据中将谓词下推到外部数据库用以减少数据传输；属于逻辑优化，优化器将谓词过滤下推到数据源，使物理执行跳过无关数据。最常见的例子就是 join 与 filter 操作一起出现时，提前执行 filter 操作以减少处理的数据量，将 filter 操作下推，以上面例子为例，示意图如下（对应 Calcite 中的 <code>FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN</code> Rule）：</p>
<p><img src="/images/calcite/6-filter-pushdown.png" alt="Filter操作下推前后的对比"></p>
<p>在进行 join 前进行相应的过滤操作，可以极大地减少参加 join 的数据量。</p>
<h2 id="常量折叠（Constant-Folding）"><a href="#常量折叠（Constant-Folding）" class="headerlink" title="常量折叠（Constant Folding）"></a>常量折叠（Constant Folding）</h2><p>常量折叠也是常见的优化策略，这个比较简单、也很好理解，可以看下 <a href="http://blog.caoxudong.info/blog/2013/10/23/compiler_optimizations_constant_folding" target="_blank" rel="external">编译器优化 – 常量折叠</a> 这篇文章，基本不用动脑筋就能理解，对于我们这里的示例，有一个常量表达式 <code>10 + 30</code>，如果不进行常量折叠，那么每行数据都需要进行计算，进行常量折叠后的结果如下图所示（ 对应 Calcite 中的 <code>ReduceExpressionsRule.PROJECT_INSTANCE</code> Rule）：</p>
<p><img src="/images/calcite/7-constant.png" alt="常量折叠前后的对比"></p>
<h2 id="列裁剪（Column-Pruning）"><a href="#列裁剪（Column-Pruning）" class="headerlink" title="列裁剪（Column Pruning）"></a>列裁剪（Column Pruning）</h2><p>列裁剪也是一个经典的优化规则，在本示例中对于jobs 表来说，并不需要扫描它的所有列值，而只需要列值 id，所以在扫描 jobs 之后需要将其他列进行裁剪，只留下列 id。这个优化带来的好处很明显，大幅度减少了网络 IO、内存数据量的消耗。裁剪前后的示意图如下（不过并没有找到 Calcite 对应的 Rule）：</p>
<p><img src="/images/calcite/8-pruning.png" alt="列裁剪前后的对比"></p>
<h1 id="Calcite-中的优化器实现"><a href="#Calcite-中的优化器实现" class="headerlink" title="Calcite 中的优化器实现"></a>Calcite 中的优化器实现</h1><p>有了前面的基础后，这里来看下 Calcite 中优化器的实现，RelOptPlanner 是 Calcite 中优化器的基类，其子类实现如下图所示：</p>
<p><img src="/images/calcite/9-RelOptPlanner.png" alt="RelOptPlanner"></p>
<p>Calcite 中关于优化器提供了两种实现：</p>
<ol>
<li>HepPlanner：就是前面 RBO 的实现，它是一个启发式的优化器，按照规则进行匹配，直到达到次数限制（match 次数限制）或者遍历一遍后不再出现 rule match 的情况才算完成；</li>
<li>VolcanoPlanner：就是前面 CBO 的实现，它会一直迭代 rules，直到找到 cost 最小的 paln。</li>
</ol>
<blockquote>
<p>前面提到过像calcite这类查询优化器最核心的两个问题之一是怎么把优化规则应用到关系代数相关的RelNode Tree上。所以在阅读calicite的代码时就得带着这个问题去看看它的实现过程，然后才能判断它的代码实现得是否优雅。<br>calcite的每种规则实现类(RelOptRule的子类)都会声明自己应用在哪种RelNode子类上，每个RelNode子类其实都可以看成是一种operator(中文常翻译成算子)。<br>VolcanoPlanner就是优化器，用的是动态规划算法，在创建VolcanoPlanner的实例后，通过calcite的标准jdbc接口执行sql时，默认会给这个VolcanoPlanner的实例注册将近90条优化规则(还不算常量折叠这种最常见的优化)，所以看代码时，知道什么时候注册可用的优化规则是第一步(调用VolcanoPlanner.addRule实现)，这一步比较简单。<br>接下来就是如何筛选规则了，当把语法树转成RelNode Tree后是没有必要把前面注册的90条优化规则都用上的，所以需要有个筛选的过程，因为每种规则是有应用范围的，按RelNode Tree的不同节点类型就可以筛选出实际需要用到的优化规则了。这一步说起来很简单，但在calcite的代码实现里是相当复杂的，也是非常关键的一步，是从调用VolcanoPlanner.setRoot方法开始间接触发的，如果只是静态的看代码不跑起来跟踪调试多半摸不清它的核心流程的。筛选出来的优化规则会封装成VolcanoRuleMatch，然后扔到RuleQueue里，而这个RuleQueue正是接下来执行动态规划算法要用到的核心类。筛选规则这一步的代码实现很晦涩。<br>第三步才到VolcanoPlanner.findBestExp，本质上就是一个动态规划算法的实现，但是最值得关注的还是怎么用第二步筛选出来的规则对RelNode Tree进行变换，变换后的形式还是一棵RelNode Tree，最常见的是把LogicalXXX开头的RelNode子类换成了EnumerableXXX或BindableXXX，总而言之，看看具体优化规则的实现就对了，都是繁琐的体力活。<br>一个优化器，理解了上面所说的三步基本上就抓住重点了。<br>—— 来自【zhh-4096 】的微博</p>
</blockquote>
<p>下面详细讲述一下这两种 planner 在 Calcite 内部的具体实现。</p>
<h2 id="HepPlanner"><a href="#HepPlanner" class="headerlink" title="HepPlanner"></a>HepPlanner</h2><p>使用 HepPlanner 实现的完整代码见 <a href="https://github.com/wangzzu/program-example/blob/master/calcite-example/src/main/java/com/matt/test/calcite/sql/SqlHepTest.java" target="_blank" rel="external">SqlHepTest</a>。</p>
<h3 id="HepPlanner-中的基本概念"><a href="#HepPlanner-中的基本概念" class="headerlink" title="HepPlanner 中的基本概念"></a>HepPlanner 中的基本概念</h3><p>这里先看下 HepPlanner 的一些基本概念，对于后面的理解很有帮助。</p>
<h4 id="HepRelVertex"><a href="#HepRelVertex" class="headerlink" title="HepRelVertex"></a>HepRelVertex</h4><p>HepRelVertex 是对 RelNode 进行了简单封装。HepPlanner 中的所有节点都是 HepRelVertex，每个 HepRelVertex 都指向了一个真正的 RelNode 节点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.calcite.plan.hep.HepRelVertex</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * HepRelVertex wraps a real &#123;<span class="doctag">@link</span> RelNode&#125; as a vertex in a DAG representing</span></span><br><span class="line"><span class="comment"> * the entire query expression.</span></span><br><span class="line"><span class="comment"> * note：HepRelVertex 将一个 RelNode 封装为一个 DAG 中的 vertex（DAG 代表整个 query expression）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HepRelVertex</span> <span class="keyword">extends</span> <span class="title">AbstractRelNode</span> </span>&#123;</span><br><span class="line">  <span class="comment">//~ Instance fields --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Wrapped rel currently chosen for implementation of expression.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> RelNode currentRel;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="HepInstruction"><a href="#HepInstruction" class="headerlink" title="HepInstruction"></a>HepInstruction</h4><p>HepInstruction 是 HepPlanner 对一些内容的封装，具体的子类实现比较多，其中 RuleInstance 是 HepPlanner 中对 Rule 的一个封装，注册的 Rule 最后都会转换为这种形式。</p>
<blockquote>
<p>HepInstruction represents one instruction in a HepProgram. </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepInstruction</span></span><br><span class="line"><span class="comment">/** Instruction that executes a given rule. */</span></span><br><span class="line"><span class="comment">//note: 执行指定 rule 的 Instruction</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RuleInstance</span> <span class="keyword">extends</span> <span class="title">HepInstruction</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Description to look for, or null if rule specified explicitly.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  String ruleDescription;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Explicitly specified rule, or rule looked up by planner from</span></span><br><span class="line"><span class="comment">   * description.</span></span><br><span class="line"><span class="comment">   * note：设置其 Rule</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelOptRule rule;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">initialize</span><span class="params">(<span class="keyword">boolean</span> clearCache)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!clearCache) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ruleDescription != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// Look up anew each run.</span></span><br><span class="line">      rule = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">execute</span><span class="params">(HepPlanner planner)</span> </span>&#123;</span><br><span class="line">    planner.executeInstruction(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="HepPlanner-处理流程"><a href="#HepPlanner-处理流程" class="headerlink" title="HepPlanner 处理流程"></a>HepPlanner 处理流程</h3><p>下面这个示例是上篇文章（<a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a>）的示例，通过这段代码来看下 HepPlanner 的内部实现机制。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HepProgramBuilder builder = <span class="keyword">new</span> HepProgramBuilder();</span><br><span class="line">builder.addRuleInstance(FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN); <span class="comment">//note: 添加 rule</span></span><br><span class="line">HepPlanner hepPlanner = <span class="keyword">new</span> HepPlanner(builder.build());</span><br><span class="line">hepPlanner.setRoot(relNode);</span><br><span class="line">relNode = hepPlanner.findBestExp();</span><br></pre></td></tr></table></figure>
<p>上面的代码总共分为三步：</p>
<ol>
<li>初始化 HepProgram 对象；</li>
<li>初始化 HepPlanner 对象，并通过 <code>setRoot()</code> 方法将 RelNode 树转换成 HepPlanner 内部使用的 Graph；</li>
<li>通过 <code>findBestExp()</code> 找到最优的 plan，规则的匹配都是在这里进行。</li>
</ol>
<h4 id="1-初始化-HepProgram"><a href="#1-初始化-HepProgram" class="headerlink" title="1. 初始化 HepProgram"></a>1. 初始化 HepProgram</h4><p>这几步代码实现没有太多需要介绍的地方，先初始化 HepProgramBuilder 也是为了后面初始化 HepProgram 做准备，HepProgramBuilder 主要也就是提供了一些配置设置和添加规则的方法等，常用的方法如下：</p>
<ol>
<li><code>addRuleInstance()</code>：注册相应的规则；</li>
<li><code>addRuleCollection()</code>：这里是注册一个规则集合，先把规则放在一个集合里，再注册整个集合，如果规则多的话，一般是这种方式；</li>
<li><code>addMatchLimit()</code>：设置 MatchLimit，这个 rule match 次数的最大限制；</li>
</ol>
<p>HepProgram 这个类对于后面 HepPlanner 的优化很重要，它定义 Rule 匹配的顺序，默认按【深度优先】顺序，它可以提供以下几种（见 HepMatchOrder 类）：</p>
<ol>
<li><strong>ARBITRARY</strong>：按任意顺序匹配（因为它是有效的，而且大部分的 Rule 并不关心匹配顺序）；</li>
<li><strong>BOTTOM_UP</strong>：自下而上，先从子节点开始匹配；</li>
<li><strong>TOP_DOWN</strong>：自上而下，先从父节点开始匹配；</li>
<li><strong>DEPTH_FIRST</strong>：深度优先匹配，某些情况下比 ARBITRARY 高效（为了避免新的 vertex 产生后又从 root 节点开始匹配）。</li>
</ol>
<p>这个匹配顺序到底是什么呢？对于规则集合 rules，HepPlanner 的算法是：从一个节点开始，跟 rules 的所有 Rule 进行匹配，匹配上就进行转换操作，这个节点操作完，再进行下一个节点，这里的匹配顺序就是指的<strong>节点遍历顺序</strong>（这种方式的优劣，我们下面再说）。</p>
<h4 id="2-HepPlanner-setRoot（RelNode-–-gt-Graph）"><a href="#2-HepPlanner-setRoot（RelNode-–-gt-Graph）" class="headerlink" title="2. HepPlanner.setRoot（RelNode –&gt; Graph）"></a>2. HepPlanner.setRoot（RelNode –&gt; Graph）</h4><p>先看下 <code>setRoot()</code> 方法的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRoot</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 将 RelNode 转换为 DAG 表示</span></span><br><span class="line">  root = addRelToGraph(rel);</span><br><span class="line">  <span class="comment">//note: 仅仅是在 trace 日志中输出 Graph 信息</span></span><br><span class="line">  dumpGraph();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>HepPlanner 会先将所有 relNode tree 转化为 HepRelVertex，这时就构建了一个 Graph：将所有的 elNode 节点使用 Vertex 表示，Gragh 会记录每个 HepRelVertex 的 input 信息，这样就是构成了一张 graph。</p>
<p>在真正的实现时，递归逐渐将每个 relNode 转换为 HepRelVertex，并在 <code>graph</code> 中记录相关的信息，实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">//note: 根据 RelNode 构建一个 Graph</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> HepRelVertex <span class="title">addRelToGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Check if a transformation already produced a reference</span></span><br><span class="line">  <span class="comment">// to an existing vertex.</span></span><br><span class="line">  <span class="comment">//note: 检查这个 rel 是否在 graph 中转换了</span></span><br><span class="line">  <span class="keyword">if</span> (graph.vertexSet().contains(rel)) &#123;</span><br><span class="line">    <span class="keyword">return</span> (HepRelVertex) rel;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Recursively add children, replacing this rel's inputs</span></span><br><span class="line">  <span class="comment">// with corresponding child vertices.</span></span><br><span class="line">  <span class="comment">//note: 递归地增加子节点，使用子节点相关的 vertices 代替 rel 的 input</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; inputs = rel.getInputs();</span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; newInputs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="keyword">for</span> (RelNode input1 : inputs) &#123;</span><br><span class="line">    HepRelVertex childVertex = addRelToGraph(input1); <span class="comment">//note: 递归进行转换</span></span><br><span class="line">    newInputs.add(childVertex); <span class="comment">//note: 每个 HepRelVertex 只记录其 Input</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!Util.equalShallow(inputs, newInputs)) &#123; <span class="comment">//note: 不相等的情况下</span></span><br><span class="line">    RelNode oldRel = rel;</span><br><span class="line">    rel = rel.copy(rel.getTraitSet(), newInputs);</span><br><span class="line">    onCopy(oldRel, rel);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Compute digest first time we add to DAG,</span></span><br><span class="line">  <span class="comment">// otherwise can't get equivVertex for common sub-expression</span></span><br><span class="line">  <span class="comment">//note: 计算 relNode 的 digest</span></span><br><span class="line">  <span class="comment">//note: Digest 的意思是：</span></span><br><span class="line">  <span class="comment">//note: A short description of this relational expression's type, inputs, and</span></span><br><span class="line">  <span class="comment">//note: other properties. The string uniquely identifies the node; another node</span></span><br><span class="line">  <span class="comment">//note: is equivalent if and only if it has the same value.</span></span><br><span class="line">  rel.recomputeDigest();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// try to find equivalent rel only if DAG is allowed</span></span><br><span class="line">  <span class="comment">//note: 如果允许 DAG 的话，检查是否有一个等价的 HepRelVertex，有的话直接返回</span></span><br><span class="line">  <span class="keyword">if</span> (!noDag) &#123;</span><br><span class="line">    <span class="comment">// Now, check if an equivalent vertex already exists in graph.</span></span><br><span class="line">    String digest = rel.getDigest();</span><br><span class="line">    HepRelVertex equivVertex = mapDigestToVertex.get(digest);</span><br><span class="line">    <span class="keyword">if</span> (equivVertex != <span class="keyword">null</span>) &#123; <span class="comment">//note: 已经存在</span></span><br><span class="line">      <span class="comment">// Use existing vertex.</span></span><br><span class="line">      <span class="keyword">return</span> equivVertex;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// No equivalence:  create a new vertex to represent this rel.</span></span><br><span class="line">  <span class="comment">//note: 创建一个 vertex 代替 rel</span></span><br><span class="line">  HepRelVertex newVertex = <span class="keyword">new</span> HepRelVertex(rel);</span><br><span class="line">  graph.addVertex(newVertex); <span class="comment">//note: 记录 Vertex</span></span><br><span class="line">  updateVertex(newVertex, rel);<span class="comment">//note: 更新相关的缓存，比如 mapDigestToVertex map</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (RelNode input : rel.getInputs()) &#123; <span class="comment">//note: 设置 Edge</span></span><br><span class="line">    graph.addEdge(newVertex, (HepRelVertex) input);<span class="comment">//note: 记录与整个 Vertex 先关的 input</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  nTransformations++;</span><br><span class="line">  <span class="keyword">return</span> newVertex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里 HepPlanner 需要的 gragh 已经构建完成，通过 DEBUG 方式也能看到此时 HepPlanner root 变量的内容：</p>
<p><img src="/images/calcite/10-calcite.png" alt="Root 转换之后的内容"></p>
<h4 id="3-HepPlanner-findBestExp-规则优化"><a href="#3-HepPlanner-findBestExp-规则优化" class="headerlink" title="3. HepPlanner findBestExp 规则优化"></a>3. HepPlanner findBestExp 规则优化</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">// implement RelOptPlanner</span></span><br><span class="line"><span class="comment">//note: 优化器的核心，匹配规则进行优化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">findBestExp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> root != <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 运行 HepProgram 算法(按 HepProgram 中的 instructions 进行相应的优化)</span></span><br><span class="line">  executeProgram(mainProgram);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get rid of everything except what's in the final plan.</span></span><br><span class="line">  <span class="comment">//note: 垃圾收集</span></span><br><span class="line">  collectGarbage();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> buildFinalPlan(root); <span class="comment">//note: 返回最后的结果，还是以 RelNode 表示</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的实现是在 <code>executeProgram()</code> 方法中，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">executeProgram</span><span class="params">(HepProgram program)</span> </span>&#123;</span><br><span class="line">  HepProgram savedProgram = currentProgram; <span class="comment">//note: 保留当前的 Program</span></span><br><span class="line">  currentProgram = program;</span><br><span class="line">  currentProgram.initialize(program == mainProgram);<span class="comment">//note: 如果是在同一个 Program 的话，保留上次 cache</span></span><br><span class="line">  <span class="keyword">for</span> (HepInstruction instruction : currentProgram.instructions) &#123;</span><br><span class="line">    instruction.execute(<span class="keyword">this</span>); <span class="comment">//note: 按 Rule 进行优化(会调用 executeInstruction 方法)</span></span><br><span class="line">    <span class="keyword">int</span> delta = nTransformations - nTransformationsLastGC;</span><br><span class="line">    <span class="keyword">if</span> (delta &gt; graphSizeLastGC) &#123;</span><br><span class="line">      <span class="comment">// The number of transformations performed since the last</span></span><br><span class="line">      <span class="comment">// garbage collection is greater than the number of vertices in</span></span><br><span class="line">      <span class="comment">// the graph at that time.  That means there should be a</span></span><br><span class="line">      <span class="comment">// reasonable amount of garbage to collect now.  We do it this</span></span><br><span class="line">      <span class="comment">// way to amortize garbage collection cost over multiple</span></span><br><span class="line">      <span class="comment">// instructions, while keeping the highwater memory usage</span></span><br><span class="line">      <span class="comment">// proportional to the graph size.</span></span><br><span class="line">      <span class="comment">//note: 进行转换的次数已经大于 DAG Graph 中的顶点数，这就意味着已经产生大量垃圾需要进行清理</span></span><br><span class="line">      collectGarbage();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  currentProgram = savedProgram;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里会遍历 HepProgram 中 instructions（记录注册的所有 HepInstruction），然后根据 instruction 的类型执行相应的 <code>executeInstruction()</code> 方法，如果instruction 是 <code>HepInstruction.MatchLimit</code> 类型，会执行 <code>executeInstruction(HepInstruction.MatchLimit instruction)</code> 方法，这个方法就是初始化 matchLimit 变量。对于 <code>HepInstruction.RuleInstance</code> 类型的 instruction 会执行下面的方法（前面的示例注册规则使用的是 <code>addRuleInstance()</code> 方法，所以返回的 rules 只有一个规则，如果注册规则的时候使用的是 <code>addRuleCollection()</code> 方法注册一个规则集合的话，这里会返回的 rules 就是那个规则集合）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">//note: 执行相应的 RuleInstance</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">executeInstruction</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    HepInstruction.RuleInstance instruction)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (skippingGroup()) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (instruction.rule == <span class="keyword">null</span>) &#123;<span class="comment">//note: 如果 rule 为 null，那么就按照 description 查找具体的 rule</span></span><br><span class="line">    <span class="keyword">assert</span> instruction.ruleDescription != <span class="keyword">null</span>;</span><br><span class="line">    instruction.rule =</span><br><span class="line">        getRuleByDescription(instruction.ruleDescription);</span><br><span class="line">    LOGGER.trace(<span class="string">"Looking up rule with description &#123;&#125;, found &#123;&#125;"</span>,</span><br><span class="line">        instruction.ruleDescription, instruction.rule);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 执行相应的 rule</span></span><br><span class="line">  <span class="keyword">if</span> (instruction.rule != <span class="keyword">null</span>) &#123;</span><br><span class="line">    applyRules(</span><br><span class="line">        Collections.singleton(instruction.rule),</span><br><span class="line">        <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来看 <code>applyRules()</code> 的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">//note: 执行 rule（forceConversions 默认 true）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">applyRules</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Collection&lt;RelOptRule&gt; rules,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> forceConversions)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (currentProgram.group != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">assert</span> currentProgram.group.collecting;</span><br><span class="line">    currentProgram.group.ruleSet.addAll(rules);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  LOGGER.trace(<span class="string">"Applying rule set &#123;&#125;"</span>, rules);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 当遍历规则是 ARBITRARY 或 DEPTH_FIRST 时，设置为 false，此时不会从 root 节点开始，否则每次 restart 都从 root 节点开始</span></span><br><span class="line">  <span class="keyword">boolean</span> fullRestartAfterTransformation =</span><br><span class="line">      currentProgram.matchOrder != HepMatchOrder.ARBITRARY</span><br><span class="line">      &amp;&amp; currentProgram.matchOrder != HepMatchOrder.DEPTH_FIRST;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nMatches = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> fixedPoint;</span><br><span class="line">  <span class="comment">//note: 两种情况会跳出循环，一种是达到 matchLimit 限制，一种是遍历一遍不会再有新的 transform 产生</span></span><br><span class="line">  <span class="keyword">do</span> &#123;</span><br><span class="line">    <span class="comment">//note: 按照遍历规则获取迭代器</span></span><br><span class="line">    Iterator&lt;HepRelVertex&gt; iter = getGraphIterator(root);</span><br><span class="line">    fixedPoint = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">      HepRelVertex vertex = iter.next();<span class="comment">//note: 遍历每个 HepRelVertex</span></span><br><span class="line">      <span class="keyword">for</span> (RelOptRule rule : rules) &#123;<span class="comment">//note: 遍历每个 rules</span></span><br><span class="line">        <span class="comment">//note: 进行规制匹配，也是真正进行相关操作的地方</span></span><br><span class="line">        HepRelVertex newVertex =</span><br><span class="line">            applyRule(rule, vertex, forceConversions);</span><br><span class="line">        <span class="keyword">if</span> (newVertex == <span class="keyword">null</span> || newVertex == vertex) &#123;</span><br><span class="line">          <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ++nMatches;</span><br><span class="line">        <span class="comment">//note: 超过 MatchLimit 的限制</span></span><br><span class="line">        <span class="keyword">if</span> (nMatches &gt;= currentProgram.matchLimit) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (fullRestartAfterTransformation) &#123;</span><br><span class="line">          <span class="comment">//note: 发生 transformation 后，从 root 节点再次开始</span></span><br><span class="line">          iter = getGraphIterator(root);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// To the extent possible, pick up where we left</span></span><br><span class="line">          <span class="comment">// off; have to create a new iterator because old</span></span><br><span class="line">          <span class="comment">// one was invalidated by transformation.</span></span><br><span class="line">          <span class="comment">//note: 尽可能从上次进行后的节点开始</span></span><br><span class="line">          iter = getGraphIterator(newVertex);</span><br><span class="line">          <span class="keyword">if</span> (currentProgram.matchOrder == HepMatchOrder.DEPTH_FIRST) &#123;</span><br><span class="line">            <span class="comment">//note: 这样做的原因就是为了防止有些 HepRelVertex 遗漏了 rule 的匹配（每次从 root 开始是最简单的算法），因为可能出现下推</span></span><br><span class="line">            nMatches =</span><br><span class="line">                depthFirstApply(iter, rules, forceConversions, nMatches);</span><br><span class="line">            <span class="keyword">if</span> (nMatches &gt;= currentProgram.matchLimit) &#123;</span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// Remember to go around again since we're</span></span><br><span class="line">          <span class="comment">// skipping some stuff.</span></span><br><span class="line">          <span class="comment">//note: 再来一遍，因为前面有跳过一些节点</span></span><br><span class="line">          fixedPoint = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">while</span> (!fixedPoint);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里会调用 <code>getGraphIterator()</code> 方法获取 HepRelVertex 的迭代器，迭代的策略（遍历的策略）跟前面说的顺序有关，默认使用的是【深度优先】，这段代码比较简单，就是遍历规则+遍历节点进行匹配转换，直到满足条件再退出，从这里也能看到 HepPlanner 的实现效率不是很高，它也无法保证能找出最优的结果。</p>
<p>总结一下，HepPlanner 在优化过程中，是先遍历规则，然后再对每个节点进行匹配转换，直到满足条件（超过限制次数或者规则遍历完一遍不会再有新的变化），其方法调用流程如下：</p>
<p><img src="/images/calcite/11-hep.png" alt="HepPlanner 处理流程"> </p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><h4 id="1-为什么要把-RelNode-转换-HepRelVertex-进行优化？带来的收益在哪里？"><a href="#1-为什么要把-RelNode-转换-HepRelVertex-进行优化？带来的收益在哪里？" class="headerlink" title="1. 为什么要把 RelNode 转换 HepRelVertex 进行优化？带来的收益在哪里？"></a>1. 为什么要把 RelNode 转换 HepRelVertex 进行优化？带来的收益在哪里？</h4><p>关于这个，能想到的就是：RelNode 是底层提供的抽象、偏底层一些，在优化器这一层，需要记录更多的信息，所以又做了一层封装。</p>
<h2 id="VolcanoPlanner"><a href="#VolcanoPlanner" class="headerlink" title="VolcanoPlanner"></a>VolcanoPlanner</h2><p>介绍完 HepPlanner 之后，接下来再来看下基于成本优化（CBO）模型在 Calcite 中是如何实现、如何落地的，关于 Volcano 理论内容建议先看下相关理论知识，否则直接看实现的话可能会有一些头大。从 Volcano 模型的理论落地到实践是有很大区别的，这里先看一张 VolcanoPlanner 整体实现图，如下所示（图片来自 <a href="https://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-phoenix-using-apache-calcite?qid=b7a1ca0f-e7bf-49ad-bc51-0615ec8a4971&amp;v=&amp;b=&amp;from_search=4" target="_blank" rel="external">Cost-based Query Optimization in Apache Phoenix using Apache Calcite</a>）：</p>
<p><img src="/images/calcite/12-VolcanoPlanner.png" alt="Calcite VolcanoPlanner Process"></p>
<p>上面基本展现了 VolcanoPlanner 内部实现的流程，也简单介绍了 VolcanoPlanner 在实现中的一些关键点（有些概念暂时不了解也不要紧，后面会介绍）：</p>
<ol>
<li>Add Rule matches to Queue：向 Rule Match Queue 中添加相应的 Rule Match；</li>
<li>Apply Rule match transformations to plan gragh：应用 Rule Match 对 plan graph 做 transformation 优化（Rule specifies an Operator sub-graph to match and logic to generate equivalent better sub-graph）；</li>
<li>Iterate for fixed iterations or until cost doesn’t change：进行相应的迭代，直到 cost 不再变化或者 Rule Match Queue 中 rule match 已经全部应用完成；</li>
<li>Match importance based on cost of RelNode and height：Rule Match 的 importance 依赖于 RelNode 的 cost 和深度。</li>
</ol>
<p>使用 VolcanoPlanner 实现的完整代码见 <a href="https://github.com/wangzzu/program-example/blob/master/calcite-example/src/main/java/com/matt/test/calcite/sql/SqlVolcanoTest.java" target="_blank" rel="external">SqlVolcanoTest</a>。</p>
<p>下面来看下 VolcanoPlanner 实现具体的细节。</p>
<h3 id="VolcanoPlanner-中的基本概念"><a href="#VolcanoPlanner-中的基本概念" class="headerlink" title="VolcanoPlanner 中的基本概念"></a>VolcanoPlanner 中的基本概念</h3><p>VolcanoPlanner 在实现中引入了一些基本概念，先明白这些概念对于理解 VolcanoPlanner 的实现非常有帮助。</p>
<h4 id="RelSet"><a href="#RelSet" class="headerlink" title="RelSet"></a>RelSet</h4><p>关于 RelSet，源码中介绍如下：</p>
<blockquote>
<p>RelSet is an equivalence-set of expressions that is, a set of expressions which have <strong>identical semantics</strong>.<br>We are generally interested in using the expression which has <strong>the lowest cost</strong>.<br>All of the expressions in an RelSet have the <strong>same calling convention</strong>.</p>
</blockquote>
<p>它有以下特点：</p>
<ol>
<li>描述一组等价 Relation Expression，所有的 RelNode 会记录在 <code>rels</code> 中；</li>
<li>have the same calling convention；</li>
<li>具有相同物理属性的 Relational Expression 会记录在其成员变量 <code>List&lt;RelSubset&gt; subsets</code> 中.</li>
</ol>
<p>RelSet 中比较重要成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RelSet</span> </span>&#123;</span><br><span class="line">   <span class="comment">// 记录属于这个 RelSet 的所有 RelNode</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; rels = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Relational expressions that have a subset in this set as a child. This</span></span><br><span class="line"><span class="comment">   * is a multi-set. If multiple relational expressions in this set have the</span></span><br><span class="line"><span class="comment">   * same parent, there will be multiple entries.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; parents = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">//note: 具体相同物理属性的子集合（本质上 RelSubset 并不记录 RelNode，也是通过 RelSet 按物理属性过滤得到其 RelNode 子集合，见下面的 RelSubset 部分）</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelSubset&gt; subsets = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * List of &#123;<span class="doctag">@link</span> AbstractConverter&#125; objects which have not yet been</span></span><br><span class="line"><span class="comment">   * satisfied.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;AbstractConverter&gt; abstractConverters = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Set to the superseding set when this is found to be equivalent to another</span></span><br><span class="line"><span class="comment">   * set.</span></span><br><span class="line"><span class="comment">   * note：当发现与另一个 RelSet 有相同的语义时，设置为替代集合</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelSet equivalentSet;</span><br><span class="line">  RelNode rel;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Variables that are set by relational expressions in this set and available for use by parent and child expressions.</span></span><br><span class="line"><span class="comment">   * note：在这个集合中 relational expression 设置的变量，父类和子类 expression 可用的变量</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> Set&lt;CorrelationId&gt; variablesPropagated;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Variables that are used by relational expressions in this set.</span></span><br><span class="line"><span class="comment">   * note：在这个集合中被 relational expression 使用的变量</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> Set&lt;CorrelationId&gt; variablesUsed;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Reentrancy flag.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">boolean</span> inMetadataQuery;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="RelSubset"><a href="#RelSubset" class="headerlink" title="RelSubset"></a>RelSubset</h4><p>关于 RelSubset，源码中介绍如下：</p>
<blockquote>
<p>Subset of an equivalence class where all relational expressions have the same physical properties.</p>
</blockquote>
<p>它的特点如下：</p>
<ol>
<li>描述一组物理属性相同的等价 Relation Expression，即它们具有相同的 Physical Properties；</li>
<li>每个 RelSubset 都会记录其所属的 RelSet；</li>
<li>RelSubset 继承自 AbstractRelNode，它也是一种 RelNode，物理属性记录在其成员变量 traitSet 中。</li>
</ol>
<p>RelSubset 一些比较重要的成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RelSubset</span> <span class="keyword">extends</span> <span class="title">AbstractRelNode</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * cost of best known plan (it may have improved since)</span></span><br><span class="line"><span class="comment">   * note: 已知最佳 plan 的 cost</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelOptCost bestCost;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The set this subset belongs to.</span></span><br><span class="line"><span class="comment">   * RelSubset 所属的 RelSet，在 RelSubset 中并不记录具体的 RelNode，直接记录在 RelSet 的 rels 中</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> RelSet set;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * best known plan</span></span><br><span class="line"><span class="comment">   * note: 已知的最佳 plan</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelNode best;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Flag indicating whether this RelSubset's importance was artificially</span></span><br><span class="line"><span class="comment">   * boosted.</span></span><br><span class="line"><span class="comment">   * note: 标志这个 RelSubset 的 importance 是否是人为地提高了</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">boolean</span> boosted;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//~ Constructors -----------------------------------------------------------</span></span><br><span class="line">  RelSubset(</span><br><span class="line">      RelOptCluster cluster,</span><br><span class="line">      RelSet set,</span><br><span class="line">      RelTraitSet traits) &#123;</span><br><span class="line">    <span class="keyword">super</span>(cluster, traits); <span class="comment">// 继承自 AbstractRelNode，会记录其相应的 traits 信息</span></span><br><span class="line">    <span class="keyword">this</span>.set = set;</span><br><span class="line">    <span class="keyword">this</span>.boosted = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">assert</span> traits.allSimple();</span><br><span class="line">    computeBestCost(cluster.getPlanner()); <span class="comment">//note: 计算 best</span></span><br><span class="line">    recomputeDigest(); <span class="comment">//note: 计算 digest</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个 RelSubset 都将会记录其最佳 plan（<code>best</code>）和最佳 plan 的 cost（<code>bestCost</code>）信息。</p>
<h4 id="RuleMatch"><a href="#RuleMatch" class="headerlink" title="RuleMatch"></a>RuleMatch</h4><p>RuleMatch 是这里对 Rule 和 RelSubset 关系的一个抽象，它会记录这两者的信息。</p>
<blockquote>
<p>A match of a rule to a particular set of target relational expressions, frozen in time.</p>
</blockquote>
<h4 id="importance"><a href="#importance" class="headerlink" title="importance"></a>importance</h4><p>importance 决定了在进行 Rule 优化时 Rule 应用的顺序，它是一个相对概念，在 VolcanoPlanner 中有两个 importance，分别是 RelSubset 和 RuleMatch 的 importance，这里先提前介绍一下。</p>
<h5 id="RelSubset-的-importance"><a href="#RelSubset-的-importance" class="headerlink" title="RelSubset 的 importance"></a>RelSubset 的 importance</h5><p>RelSubset importance 计算方法见其 api 定义（<strong>图中的 sum 改成 Math.max{}</strong>这个地方有误）：</p>
<p><img src="/images/calcite/13-compute.png" alt="computeImportance"> </p>
<p>举个例子：假设一个 RelSubset（记为 $s_0$） 的 cost 是3，对应的 importance 是0.5，这个 RelNode 有两个输入（inputs），对应的 RelSubset 记为 $s_1$、$s_2$（假设 $s_1$、$s_2$ 不再有输入 RelNode），其 cost 分别为 2和5，那么 $s_1$ 的 importance 为</p>
<p>Importance of $s_1$ = $\frac{2}{3+2+5}$ $\cdot$ 0.5 = 0.1</p>
<p>Importance of $s_2$ = $\frac{5}{3+2+5}$ $\cdot$ 0.5 = 0.25</p>
<p>其中，2代表的是 $s_1$ 的 cost，$3+2+5$ 代表的是 $s_0$ 的 cost（本节点的 cost 加上其所有 input 的 cost）。下面看下其具体的代码实现（调用 RuleQueue 中的 <code>recompute()</code> 计算其 importance）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.RuleQueue</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Recomputes the importance of the given RelSubset.</span></span><br><span class="line"><span class="comment"> * note：重新计算指定的 RelSubset 的 importance</span></span><br><span class="line"><span class="comment"> * note：如果为 true，即使 subset 没有注册，也会强制 importance 更新</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subset RelSubset whose importance is to be recomputed</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> force  if true, forces an importance update even if the subset has</span></span><br><span class="line"><span class="comment"> *               not been registered</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">recompute</span><span class="params">(RelSubset subset, <span class="keyword">boolean</span> force)</span> </span>&#123;</span><br><span class="line">  Double previousImportance = subsetImportances.get(subset);</span><br><span class="line">  <span class="keyword">if</span> (previousImportance == <span class="keyword">null</span>) &#123; <span class="comment">//note: subset 还没有注册的情况下</span></span><br><span class="line">    <span class="keyword">if</span> (!force) &#123; <span class="comment">//note: 如果不是强制，可以直接先返回</span></span><br><span class="line">      <span class="comment">// Subset has not been registered yet. Don't worry about it.</span></span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    previousImportance = Double.NEGATIVE_INFINITY;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 计算器 importance 值</span></span><br><span class="line">  <span class="keyword">double</span> importance = computeImportance(subset);</span><br><span class="line">  <span class="keyword">if</span> (previousImportance == importance) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 缓存中更新其 importance</span></span><br><span class="line">  updateImportance(subset, importance);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算一个节点的 importance</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">computeImportance</span><span class="params">(RelSubset subset)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">double</span> importance;</span><br><span class="line">  <span class="keyword">if</span> (subset == planner.root) &#123;</span><br><span class="line">    <span class="comment">// The root always has importance = 1</span></span><br><span class="line">    <span class="comment">//note: root RelSubset 的 importance 为1</span></span><br><span class="line">    importance = <span class="number">1.0</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> RelMetadataQuery mq = subset.getCluster().getMetadataQuery();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The importance of a subset is the max of its importance to its</span></span><br><span class="line">    <span class="comment">// parents</span></span><br><span class="line">    <span class="comment">//note: 计算其相对于 parent 的最大 importance，多个 parent 的情况下，选择一个最大值</span></span><br><span class="line">    importance = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (RelSubset parent : subset.getParentSubsets(planner)) &#123;</span><br><span class="line">      <span class="comment">//note: 计算这个 RelSubset 相对于 parent 的 importance</span></span><br><span class="line">      <span class="keyword">final</span> <span class="keyword">double</span> childImportance =</span><br><span class="line">          computeImportanceOfChild(mq, subset, parent);</span><br><span class="line">      <span class="comment">//note: 选择最大的 importance</span></span><br><span class="line">      importance = Math.max(importance, childImportance);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  LOGGER.trace(<span class="string">"Importance of [&#123;&#125;] is &#123;&#125;"</span>, subset, importance);</span><br><span class="line">  <span class="keyword">return</span> importance;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note：根据 cost 计算 child 相对于 parent 的 importance（这是个相对值）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">double</span> <span class="title">computeImportanceOfChild</span><span class="params">(RelMetadataQuery mq, RelSubset child,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelSubset parent)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 获取 parent 的 importance</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> parentImportance = getImportance(parent);</span><br><span class="line">  <span class="comment">//note: 获取对应的 cost 信息</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> childCost = toDouble(planner.getCost(child, mq));</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> parentCost = toDouble(planner.getCost(parent, mq));</span><br><span class="line">  <span class="keyword">double</span> alpha = childCost / parentCost;</span><br><span class="line">  <span class="keyword">if</span> (alpha &gt;= <span class="number">1.0</span>) &#123;</span><br><span class="line">    <span class="comment">// child is always less important than parent</span></span><br><span class="line">    alpha = <span class="number">0.99</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 根据 cost 比列计算其 importance</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> importance = parentImportance * alpha;</span><br><span class="line">  LOGGER.trace(<span class="string">"Importance of [&#123;&#125;] to its parent [&#123;&#125;] is &#123;&#125; (parent importance=&#123;&#125;, child cost=&#123;&#125;,"</span></span><br><span class="line">      + <span class="string">" parent cost=&#123;&#125;)"</span>, child, parent, importance, parentImportance, childCost, parentCost);</span><br><span class="line">  <span class="keyword">return</span> importance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>computeImportanceOfChild()</code> 中计算 RelSubset 相对于 parent RelSubset 的 importance 时，一个比较重要的地方就是如何计算 cost，关于 cost 的计算见：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">//note: Computes the cost of a RelNode.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelOptCost <span class="title">getCost</span><span class="params">(RelNode rel, RelMetadataQuery mq)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> rel != <span class="keyword">null</span> : <span class="string">"pre-condition: rel != null"</span>;</span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> RelSubset) &#123; <span class="comment">//note: 如果是 RelSubset，证明是已经计算 cost 的 subset</span></span><br><span class="line">    <span class="keyword">return</span> ((RelSubset) rel).bestCost;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (rel.getTraitSet().getTrait(ConventionTraitDef.INSTANCE)</span><br><span class="line">      == Convention.NONE) &#123;</span><br><span class="line">    <span class="keyword">return</span> costFactory.makeInfiniteCost(); <span class="comment">//note: 这种情况下也会返回 infinite Cost</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 计算其 cost</span></span><br><span class="line">  RelOptCost cost = mq.getNonCumulativeCost(rel);</span><br><span class="line">  <span class="keyword">if</span> (!zeroCost.isLt(cost)) &#123; <span class="comment">//note: cost 比0还小的情况</span></span><br><span class="line">    <span class="comment">// cost must be positive, so nudge it</span></span><br><span class="line">    cost = costFactory.makeTinyCost();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: RelNode 的 cost 会把其 input 全部加上</span></span><br><span class="line">  <span class="keyword">for</span> (RelNode input : rel.getInputs()) &#123;</span><br><span class="line">    cost = cost.plus(getCost(input, mq));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> cost;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面就是 RelSubset importance 计算的代码实现，从实现中可以发现这个特点：</p>
<ol>
<li>越靠近 root 的 RelSubset，其 importance 越大，这个带来的好处就是在优化时，会尽量先优化靠近 root 的 RelNode，这样带来的收益也会最大。</li>
</ol>
<h5 id="RuleMatch-的-importance"><a href="#RuleMatch-的-importance" class="headerlink" title="RuleMatch 的 importance"></a>RuleMatch 的 importance</h5><p>RuleMatch 的 importance 定义为以下两个中比较大的一个（如果对应的 RelSubset 有 importance 的情况下）：</p>
<ol>
<li>这个 RuleMatch 对应 RelSubset（这个 rule match 的 RelSubset）的 importance；</li>
<li>输出的 RelSubset（taget RelSubset）的 importance（如果这个 RelSubset 在 VolcanoPlanner 的缓存中存在的话）。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoRuleMatch</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Computes the importance of this rule match.</span></span><br><span class="line"><span class="comment"> * note：计算 rule match 的 importance</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> importance of this rule match</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">computeImportance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> rels[<span class="number">0</span>] != <span class="keyword">null</span>; <span class="comment">//note: rels[0] 这个 Rule Match 对应的 RelSubset</span></span><br><span class="line">  RelSubset subset = volcanoPlanner.getSubset(rels[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">double</span> importance = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (subset != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">//note: 获取 RelSubset 的 importance</span></span><br><span class="line">    importance = volcanoPlanner.ruleQueue.getImportance(subset);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: Returns a guess as to which subset the result of this rule will belong to.</span></span><br><span class="line">  <span class="keyword">final</span> RelSubset targetSubset = guessSubset();</span><br><span class="line">  <span class="keyword">if</span> ((targetSubset != <span class="keyword">null</span>) &amp;&amp; (targetSubset != subset)) &#123;</span><br><span class="line">    <span class="comment">// If this rule will generate a member of an equivalence class</span></span><br><span class="line">    <span class="comment">// which is more important, use that importance.</span></span><br><span class="line">    <span class="comment">//note: 获取 targetSubset 的 importance</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">double</span> targetImportance =</span><br><span class="line">        volcanoPlanner.ruleQueue.getImportance(targetSubset);</span><br><span class="line">    <span class="keyword">if</span> (targetImportance &gt; importance) &#123;</span><br><span class="line">      importance = targetImportance;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If the equivalence class is cheaper than the target, bump up</span></span><br><span class="line">      <span class="comment">// the importance of the rule. A converter is an easy way to</span></span><br><span class="line">      <span class="comment">// make the plan cheaper, so we'd hate to miss this opportunity.</span></span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="comment">// REVIEW: jhyde, 2007/12/21: This rule seems to make sense, but</span></span><br><span class="line">      <span class="comment">// is disabled until it has been proven.</span></span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="comment">// CHECKSTYLE: IGNORE 3</span></span><br><span class="line">      <span class="keyword">if</span> ((subset != <span class="keyword">null</span>)</span><br><span class="line">          &amp;&amp; subset.bestCost.isLt(targetSubset.bestCost)</span><br><span class="line">          &amp;&amp; <span class="keyword">false</span>) &#123; <span class="comment">//note: 肯定不会进入</span></span><br><span class="line">        importance *=</span><br><span class="line">            targetSubset.bestCost.divideBy(subset.bestCost);</span><br><span class="line">        importance = Math.min(importance, <span class="number">0.99</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> importance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>RuleMatch 的 importance 主要是决定了在选择 RuleMatch 时，应该先处理哪一个？它本质上还是直接用的 RelSubset 的 importance。</p>
<h3 id="VolcanoPlanner-处理流程"><a href="#VolcanoPlanner-处理流程" class="headerlink" title="VolcanoPlanner 处理流程"></a>VolcanoPlanner 处理流程</h3><p>还是以前面的示例，只不过这里把优化器换成 VolcanoPlanner 来实现，通过这个示例来详细看下 VolcanoPlanner 内部的实现逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1. 初始化 VolcanoPlanner 对象，并添加相应的 Rule</span></span><br><span class="line">VolcanoPlanner planner = <span class="keyword">new</span> VolcanoPlanner();</span><br><span class="line">planner.addRelTraitDef(ConventionTraitDef.INSTANCE);</span><br><span class="line">planner.addRelTraitDef(RelDistributionTraitDef.INSTANCE);</span><br><span class="line"><span class="comment">// 添加相应的 rule</span></span><br><span class="line">planner.addRule(FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN);</span><br><span class="line">planner.addRule(ReduceExpressionsRule.PROJECT_INSTANCE);</span><br><span class="line">planner.addRule(PruneEmptyRules.PROJECT_INSTANCE);</span><br><span class="line"><span class="comment">// 添加相应的 ConverterRule</span></span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_MERGE_JOIN_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_SORT_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_VALUES_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_PROJECT_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_FILTER_RULE);</span><br><span class="line"><span class="comment">//2. Changes a relational expression to an equivalent one with a different set of traits.</span></span><br><span class="line">RelTraitSet desiredTraits =</span><br><span class="line">    relNode.getCluster().traitSet().replace(EnumerableConvention.INSTANCE);</span><br><span class="line">relNode = planner.changeTraits(relNode, desiredTraits);</span><br><span class="line"><span class="comment">//3. 通过 VolcanoPlanner 的 setRoot 方法注册相应的 RelNode，并进行相应的初始化操作</span></span><br><span class="line">planner.setRoot(relNode);</span><br><span class="line"><span class="comment">//4. 通过动态规划算法找到 cost 最小的 plan</span></span><br><span class="line">relNode = planner.findBestExp();</span><br></pre></td></tr></table></figure>
<p>优化后的结果为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EnumerableSort(sort0=[<span class="variable">$0</span>], dir0=[ASC])</span><br><span class="line">  EnumerableProject(USER_ID=[<span class="variable">$0</span>], USER_NAME=[<span class="variable">$1</span>], USER_COMPANY=[<span class="variable">$5</span>], USER_AGE=[<span class="variable">$2</span>])</span><br><span class="line">    EnumerableMergeJoin(condition=[=(<span class="variable">$0</span>, <span class="variable">$3</span>)], joinType=[inner])</span><br><span class="line">      EnumerableFilter(condition=[&gt;(<span class="variable">$2</span>, 30)])</span><br><span class="line">        EnumerableTableScan(table=[[USERS]])</span><br><span class="line">      EnumerableFilter(condition=[&gt;(<span class="variable">$0</span>, 10)])</span><br><span class="line">        EnumerableTableScan(table=[[JOBS]])</span><br></pre></td></tr></table></figure>
<p>在应用 VolcanoPlanner 时，整体分为以下四步：</p>
<ol>
<li>初始化 VolcanoPlanner，并添加相应的 Rule（包括 ConverterRule）；</li>
<li>对 RelNode 做等价转换，这里只是改变其物理属性（<code>Convention</code>）；</li>
<li>通过 VolcanoPlanner 的 <code>setRoot()</code> 方法注册相应的 RelNode，并进行相应的初始化操作；</li>
<li>通过动态规划算法找到 cost 最小的 plan；</li>
</ol>
<p>下面来分享一下上面的详细流程。</p>
<h4 id="1-VolcanoPlanner-初始化"><a href="#1-VolcanoPlanner-初始化" class="headerlink" title="1. VolcanoPlanner 初始化"></a>1. VolcanoPlanner 初始化</h4><p>在这里总共有三步，分别是 VolcanoPlanner 初始化，<code>addRelTraitDef()</code> 添加 RelTraitDef，<code>addRule()</code> 添加 rule，先看下 VolcanoPlanner 的初始化：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a uninitialized &lt;code&gt;VolcanoPlanner&lt;/code&gt;. To fully initialize it, the caller must register the desired set of relations, rules, and calling conventions.</span></span><br><span class="line"><span class="comment"> * note: 创建一个没有初始化的 VolcanoPlanner，如果要进行初始化，调用者必须注册 set of relations、rules、calling conventions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">VolcanoPlanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a &#123;<span class="doctag">@code</span> VolcanoPlanner&#125; with a given cost factory.</span></span><br><span class="line"><span class="comment"> * note: 创建 VolcanoPlanner 实例，并制定 costFactory（默认为 VolcanoCost.FACTORY）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">VolcanoPlanner</span><span class="params">(RelOptCostFactory costFactory, //</span></span></span><br><span class="line"><span class="function"><span class="params">    Context externalContext)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">super</span>(costFactory == <span class="keyword">null</span> ? VolcanoCost.FACTORY : costFactory, <span class="comment">//</span></span><br><span class="line">      externalContext);</span><br><span class="line">  <span class="keyword">this</span>.zeroCost = <span class="keyword">this</span>.costFactory.makeZeroCost();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里其实并没有做什么，只是做了一些简单的初始化，如果要想设置相应 RelTraitDef 的话，需要调用 <code>addRelTraitDef()</code> 进行添加，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">//note: 添加 RelTraitDef</span></span><br><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addRelTraitDef</span><span class="params">(RelTraitDef relTraitDef)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> !traitDefs.contains(relTraitDef) &amp;&amp; traitDefs.add(relTraitDef);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果要给 VolcanoPlanner 添加 Rule 的话，需要调用 <code>addRule()</code> 进行添加，<strong>在这个方法里重点做的一步是将具体的 RelNode 与 RelOptRuleOperand 之间的关系记录下来，记录到 <code>classOperands</code> 中</strong>，相当于在优化时，哪个 RelNode 可以应用哪些 Rule 都是记录在这个缓存里的。其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">//note: 添加 rule</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addRule</span><span class="params">(RelOptRule rule)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (locked) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (ruleSet.contains(rule)) &#123;</span><br><span class="line">    <span class="comment">// Rule already exists.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">boolean</span> added = ruleSet.add(rule);</span><br><span class="line">  <span class="keyword">assert</span> added;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> String ruleName = rule.toString();</span><br><span class="line">  <span class="comment">//note: 这里的 ruleNames 允许重复的 key 值，但是这里还是要求 rule description 保持唯一的，与 rule 一一对应</span></span><br><span class="line">  <span class="keyword">if</span> (ruleNames.put(ruleName, rule.getClass())) &#123;</span><br><span class="line">    Set&lt;Class&gt; x = ruleNames.get(ruleName);</span><br><span class="line">    <span class="keyword">if</span> (x.size() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Rule description '"</span> + ruleName</span><br><span class="line">          + <span class="string">"' is not unique; classes: "</span> + x);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 注册一个 rule 的 description（保存在 mapDescToRule 中）</span></span><br><span class="line">  mapRuleDescription(rule);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Each of this rule's operands is an 'entry point' for a rule call. Register each operand against all concrete sub-classes that could match it.</span></span><br><span class="line">  <span class="comment">//note: 记录每个 sub-classes 与 operand 的关系（如果能 match 的话，就记录一次）。一个 RelOptRuleOperand 只会有一个 class 与之对应，这里找的是 subclass</span></span><br><span class="line">  <span class="keyword">for</span> (RelOptRuleOperand operand : rule.getOperands()) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Class&lt;? extends RelNode&gt; subClass</span><br><span class="line">        : subClasses(operand.getMatchedClass())) &#123;</span><br><span class="line">      classOperands.put(subClass, operand);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If this is a converter rule, check that it operates on one of the</span></span><br><span class="line">  <span class="comment">// kinds of trait we are interested in, and if so, register the rule</span></span><br><span class="line">  <span class="comment">// with the trait.</span></span><br><span class="line">  <span class="comment">//note: 对于 ConverterRule 的操作，如果其 ruleTraitDef 类型包含在我们初始化的 traitDefs 中，</span></span><br><span class="line">  <span class="comment">//note: 就注册这个 converterRule 到 ruleTraitDef 中</span></span><br><span class="line">  <span class="comment">//note: 如果不包含 ruleTraitDef，这个 ConverterRule 在本次优化的过程中是用不到的</span></span><br><span class="line">  <span class="keyword">if</span> (rule <span class="keyword">instanceof</span> ConverterRule) &#123;</span><br><span class="line">    ConverterRule converterRule = (ConverterRule) rule;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> RelTrait ruleTrait = converterRule.getInTrait();</span><br><span class="line">    <span class="keyword">final</span> RelTraitDef ruleTraitDef = ruleTrait.getTraitDef();</span><br><span class="line">    <span class="keyword">if</span> (traitDefs.contains(ruleTraitDef)) &#123; <span class="comment">//note: 这里注册好像也没有用到</span></span><br><span class="line">      ruleTraitDef.registerConverterRule(<span class="keyword">this</span>, converterRule);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-RelNode-changeTraits"><a href="#2-RelNode-changeTraits" class="headerlink" title="2. RelNode changeTraits"></a>2. RelNode changeTraits</h4><p>这里分为两步：</p>
<ol>
<li>通过 RelTraitSet 的 <code>replace()</code> 方法，将 RelTraitSet 中对应的 RelTraitDef 做对应的更新，其他的 RelTrait 不变；</li>
<li>这一步简单来说就是：Changes a relational expression to an equivalent one with a different set of traits，对相应的 RelNode 做 converter 操作，这里实际上也会做很多的内容，这部分会放在第三步讲解，主要是 <code>registerImpl()</code> 方法的实现。</li>
</ol>
<h4 id="3-VolcanoPlanner-setRoot"><a href="#3-VolcanoPlanner-setRoot" class="headerlink" title="3. VolcanoPlanner setRoot"></a>3. VolcanoPlanner setRoot</h4><p>VolcanoPlanner 会调用 <code>setRoot()</code> 方法注册相应的 Root RelNode，并进行一系列 Volcano 必须的初始化操作，很多的操作都是在这里实现的，这里来详细看下其实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRoot</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// We're registered all the rules, and therefore RelNode classes,</span></span><br><span class="line">  <span class="comment">// we're interested in, and have not yet started calling metadata providers.</span></span><br><span class="line">  <span class="comment">// So now is a good time to tell the metadata layer what to expect.</span></span><br><span class="line">  registerMetadataRels();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 注册相应的 RelNode，会做一系列的初始化操作, RelNode 会有对应的 RelSubset</span></span><br><span class="line">  <span class="keyword">this</span>.root = registerImpl(rel, <span class="keyword">null</span>);</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>.originalRoot == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.originalRoot = rel;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Making a node the root changes its importance.</span></span><br><span class="line">  <span class="comment">//note: 重新计算 root subset 的 importance</span></span><br><span class="line">  <span class="keyword">this</span>.ruleQueue.recompute(<span class="keyword">this</span>.root);</span><br><span class="line">  <span class="comment">//Ensures that the subset that is the root relational expression contains converters to all other subsets in its equivalence set.</span></span><br><span class="line">  ensureRootConverters();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于 <code>setRoot()</code> 方法来说，核心的处理流程是在 <code>registerImpl()</code> 方法中，在这个方法会进行相应的初始化操作（包括 RelNode 到 RelSubset 的转换、计算 RelSubset 的 importance 等），其他的方法在上面有相应的备注，这里我们看下 <code>registerImpl()</code> 具体做了哪些事情：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Registers a new expression &lt;code&gt;exp&lt;/code&gt; and queues up rule matches.</span></span><br><span class="line"><span class="comment"> * If &lt;code&gt;set&lt;/code&gt; is not null, makes the expression part of that</span></span><br><span class="line"><span class="comment"> * equivalence set. If an identical expression is already registered, we</span></span><br><span class="line"><span class="comment"> * don't need to register this one and nor should we queue up rule matches.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：注册一个新的 expression；对 rule match 进行排队；</span></span><br><span class="line"><span class="comment"> * note：如果 set 不为 null，那么就使 expression 成为等价集合（RelSet）的一部分</span></span><br><span class="line"><span class="comment"> * note：rel：必须是 RelSubset 或者未注册的 RelNode</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rel relational expression to register. Must be either a</span></span><br><span class="line"><span class="comment"> *         &#123;<span class="doctag">@link</span> RelSubset&#125;, or an unregistered &#123;<span class="doctag">@link</span> RelNode&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> set set that rel belongs to, or &lt;code&gt;null&lt;/code&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the equivalence-set</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RelSubset <span class="title">registerImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    RelNode rel,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelSet set)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> RelSubset) &#123; <span class="comment">//note: 如果是 RelSubset 类型，已经注册过了</span></span><br><span class="line">    <span class="keyword">return</span> registerSubset(set, (RelSubset) rel); <span class="comment">//note: 做相应的 merge</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> !isRegistered(rel) : <span class="string">"already been registered: "</span> + rel;</span><br><span class="line">  <span class="keyword">if</span> (rel.getCluster().getPlanner() != <span class="keyword">this</span>) &#123; <span class="comment">//note: cluster 中 planner 与这里不同</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Relational expression "</span> + rel</span><br><span class="line">        + <span class="string">" belongs to a different planner than is currently being used."</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Now is a good time to ensure that the relational expression</span></span><br><span class="line">  <span class="comment">// implements the interface required by its calling convention.</span></span><br><span class="line">  <span class="comment">//note: 确保 relational expression 可以实施其 calling convention 所需的接口</span></span><br><span class="line">  <span class="comment">//note: 获取 RelNode 的 RelTraitSet</span></span><br><span class="line">  <span class="keyword">final</span> RelTraitSet traits = rel.getTraitSet();</span><br><span class="line">  <span class="comment">//note: 获取其 ConventionTraitDef</span></span><br><span class="line">  <span class="keyword">final</span> Convention convention = traits.getTrait(ConventionTraitDef.INSTANCE);</span><br><span class="line">  <span class="keyword">assert</span> convention != <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">if</span> (!convention.getInterface().isInstance(rel)</span><br><span class="line">      &amp;&amp; !(rel <span class="keyword">instanceof</span> Converter)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Relational expression "</span> + rel</span><br><span class="line">        + <span class="string">" has calling-convention "</span> + convention</span><br><span class="line">        + <span class="string">" but does not implement the required interface '"</span></span><br><span class="line">        + convention.getInterface() + <span class="string">"' of that convention"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (traits.size() != traitDefs.size()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Relational expression "</span> + rel</span><br><span class="line">        + <span class="string">" does not have the correct number of traits: "</span> + traits.size()</span><br><span class="line">        + <span class="string">" != "</span> + traitDefs.size());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Ensure that its sub-expressions are registered.</span></span><br><span class="line">  <span class="comment">//note: 其实现在 AbstractRelNode 对应的方法中，实际上调用的还是 ensureRegistered 方法进行注册</span></span><br><span class="line">  <span class="comment">//note: 将 RelNode 的所有 inputs 注册到 planner 中</span></span><br><span class="line">  <span class="comment">//note: 这里会递归调用 registerImpl 注册 relNode 与 RelSet，直到其 inputs 全部注册</span></span><br><span class="line">  <span class="comment">//note: 返回的是一个 RelSubset 类型</span></span><br><span class="line">  rel = rel.onRegister(<span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Record its provenance. (Rule call may be null.)</span></span><br><span class="line">  <span class="comment">//note: 记录 RelNode 的来源</span></span><br><span class="line">  <span class="keyword">if</span> (ruleCallStack.isEmpty()) &#123; <span class="comment">//note: 不知道来源时</span></span><br><span class="line">    provenanceMap.put(rel, Provenance.EMPTY);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 来自 rule 触发的情况</span></span><br><span class="line">    <span class="keyword">final</span> VolcanoRuleCall ruleCall = ruleCallStack.peek();</span><br><span class="line">    provenanceMap.put(</span><br><span class="line">        rel,</span><br><span class="line">        <span class="keyword">new</span> RuleProvenance(</span><br><span class="line">            ruleCall.rule,</span><br><span class="line">            ImmutableList.copyOf(ruleCall.rels),</span><br><span class="line">            ruleCall.id));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If it is equivalent to an existing expression, return the set that</span></span><br><span class="line">  <span class="comment">// the equivalent expression belongs to.</span></span><br><span class="line">  <span class="comment">//note: 根据 RelNode 的 digest（摘要，全局唯一）判断其是否已经有对应的 RelSubset，有的话直接放回</span></span><br><span class="line">  String key = rel.getDigest();</span><br><span class="line">  RelNode equivExp = mapDigestToRel.get(key);</span><br><span class="line">  <span class="keyword">if</span> (equivExp == <span class="keyword">null</span>) &#123; <span class="comment">//note: 还没注册的情况</span></span><br><span class="line">    <span class="comment">// do nothing</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (equivExp == rel) &#123;<span class="comment">//note: 已经有其缓存信息</span></span><br><span class="line">    <span class="keyword">return</span> getSubset(rel);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">assert</span> RelOptUtil.equal(</span><br><span class="line">        <span class="string">"left"</span>, equivExp.getRowType(),</span><br><span class="line">        <span class="string">"right"</span>, rel.getRowType(),</span><br><span class="line">        Litmus.THROW);</span><br><span class="line">    RelSet equivSet = getSet(equivExp); <span class="comment">//note: 有 RelSubset 但对应的 RelNode 不同时，这里对其 RelSet 做下 merge</span></span><br><span class="line">    <span class="keyword">if</span> (equivSet != <span class="keyword">null</span>) &#123;</span><br><span class="line">      LOGGER.trace(</span><br><span class="line">          <span class="string">"Register: rel#&#123;&#125; is equivalent to &#123;&#125;"</span>, rel.getId(), equivExp.getDescription());</span><br><span class="line">      <span class="keyword">return</span> registerSubset(set, getSubset(equivExp));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note： Converters are in the same set as their children.</span></span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> Converter) &#123;</span><br><span class="line">    <span class="keyword">final</span> RelNode input = ((Converter) rel).getInput();</span><br><span class="line">    <span class="keyword">final</span> RelSet childSet = getSet(input);</span><br><span class="line">    <span class="keyword">if</span> ((set != <span class="keyword">null</span>)</span><br><span class="line">        &amp;&amp; (set != childSet)</span><br><span class="line">        &amp;&amp; (set.equivalentSet == <span class="keyword">null</span>)) &#123;</span><br><span class="line">      LOGGER.trace(</span><br><span class="line">          <span class="string">"Register #&#123;&#125; &#123;&#125; (and merge sets, because it is a conversion)"</span>,</span><br><span class="line">          rel.getId(), rel.getDigest());</span><br><span class="line">      merge(set, childSet);</span><br><span class="line">      registerCount++;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// During the mergers, the child set may have changed, and since</span></span><br><span class="line">      <span class="comment">// we're not registered yet, we won't have been informed. So</span></span><br><span class="line">      <span class="comment">// check whether we are now equivalent to an existing</span></span><br><span class="line">      <span class="comment">// expression.</span></span><br><span class="line">      <span class="keyword">if</span> (fixUpInputs(rel)) &#123;</span><br><span class="line">        rel.recomputeDigest();</span><br><span class="line">        key = rel.getDigest();</span><br><span class="line">        RelNode equivRel = mapDigestToRel.get(key);</span><br><span class="line">        <span class="keyword">if</span> ((equivRel != rel) &amp;&amp; (equivRel != <span class="keyword">null</span>)) &#123;</span><br><span class="line">          <span class="keyword">assert</span> RelOptUtil.equal(</span><br><span class="line">              <span class="string">"rel rowtype"</span>,</span><br><span class="line">              rel.getRowType(),</span><br><span class="line">              <span class="string">"equivRel rowtype"</span>,</span><br><span class="line">              equivRel.getRowType(),</span><br><span class="line">              Litmus.THROW);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// make sure this bad rel didn't get into the</span></span><br><span class="line">          <span class="comment">// set in any way (fixupInputs will do this but it</span></span><br><span class="line">          <span class="comment">// doesn't know if it should so it does it anyway)</span></span><br><span class="line">          set.obliterateRelNode(rel);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// There is already an equivalent expression. Use that</span></span><br><span class="line">          <span class="comment">// one, and forget about this one.</span></span><br><span class="line">          <span class="keyword">return</span> getSubset(equivRel);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      set = childSet;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Place the expression in the appropriate equivalence set.</span></span><br><span class="line">  <span class="comment">//note: 把 expression 放到合适的 等价集 中</span></span><br><span class="line">  <span class="comment">//note: 如果 RelSet 不存在，这里会初始化一个 RelSet</span></span><br><span class="line">  <span class="keyword">if</span> (set == <span class="keyword">null</span>) &#123;</span><br><span class="line">    set = <span class="keyword">new</span> RelSet(</span><br><span class="line">        nextSetId++,</span><br><span class="line">        Util.minus(</span><br><span class="line">            RelOptUtil.getVariablesSet(rel),</span><br><span class="line">            rel.getVariablesSet()),</span><br><span class="line">        RelOptUtil.getVariablesUsed(rel));</span><br><span class="line">    <span class="keyword">this</span>.allSets.add(set);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Chain to find 'live' equivalent set, just in case several sets are</span></span><br><span class="line">  <span class="comment">// merging at the same time.</span></span><br><span class="line">  <span class="comment">//note: 递归查询，一直找到最开始的 语义相等的集合，防止不同集合同时被 merge</span></span><br><span class="line">  <span class="keyword">while</span> (set.equivalentSet != <span class="keyword">null</span>) &#123;</span><br><span class="line">    set = set.equivalentSet;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Allow each rel to register its own rules.</span></span><br><span class="line">  registerClass(rel);</span><br><span class="line"></span><br><span class="line">  registerCount++;</span><br><span class="line">  <span class="comment">//note: 初始时是 0</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> subsetBeforeCount = set.subsets.size();</span><br><span class="line">  <span class="comment">//note: 向等价集中添加相应的 RelNode，并更新其 best 信息</span></span><br><span class="line">  RelSubset subset = addRelToSet(rel, set);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 缓存相关信息，返回的 key 之前对应的 value</span></span><br><span class="line">  <span class="keyword">final</span> RelNode xx = mapDigestToRel.put(key, rel);</span><br><span class="line">  <span class="keyword">assert</span> xx == <span class="keyword">null</span> || xx == rel : rel.getDigest();</span><br><span class="line"></span><br><span class="line">  LOGGER.trace(<span class="string">"Register &#123;&#125; in &#123;&#125;"</span>, rel.getDescription(), subset.getDescription());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This relational expression may have been registered while we</span></span><br><span class="line">  <span class="comment">// recursively registered its children. If this is the case, we're done.</span></span><br><span class="line">  <span class="keyword">if</span> (xx != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> subset;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create back-links from its children, which makes children more</span></span><br><span class="line">  <span class="comment">// important.</span></span><br><span class="line">  <span class="comment">//note: 如果是 root，初始化其 importance 为 1.0</span></span><br><span class="line">  <span class="keyword">if</span> (rel == <span class="keyword">this</span>.root) &#123;</span><br><span class="line">    ruleQueue.subsetImportances.put(</span><br><span class="line">        subset,</span><br><span class="line">        <span class="number">1.0</span>); <span class="comment">// todo: remove</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 将 Rel 的 input 对应的 RelSubset 的 parents 设置为当前的 Rel</span></span><br><span class="line">  <span class="comment">//note: 也就是说，一个 RelNode 的 input 为其对应 RelSubset 的 children 节点</span></span><br><span class="line">  <span class="keyword">for</span> (RelNode input : rel.getInputs()) &#123;</span><br><span class="line">    RelSubset childSubset = (RelSubset) input;</span><br><span class="line">    childSubset.set.parents.add(rel);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Child subset is more important now a new parent uses it.</span></span><br><span class="line">    <span class="comment">//note: 重新计算 RelSubset 的 importance</span></span><br><span class="line">    ruleQueue.recompute(childSubset);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (rel == <span class="keyword">this</span>.root) &#123;<span class="comment">// <span class="doctag">TODO:</span> 2019-03-11 这里为什么要删除呢？</span></span><br><span class="line">    ruleQueue.subsetImportances.remove(subset);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Remember abstract converters until they're satisfied</span></span><br><span class="line">  <span class="comment">//note: 如果是 AbstractConverter 示例，添加到 abstractConverters 集合中</span></span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> AbstractConverter) &#123;</span><br><span class="line">    set.abstractConverters.add((AbstractConverter) rel);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If this set has any unsatisfied converters, try to satisfy them.</span></span><br><span class="line">  <span class="comment">//note: check set.abstractConverters</span></span><br><span class="line">  checkForSatisfiedConverters(set, rel);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make sure this rel's subset importance is updated</span></span><br><span class="line">  <span class="comment">//note: 强制更新（重新计算） subset 的 importance</span></span><br><span class="line">  ruleQueue.recompute(subset, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 触发所有匹配的 rule，这里是添加到对应的 RuleQueue 中</span></span><br><span class="line">  <span class="comment">// Queue up all rules triggered by this relexp's creation.</span></span><br><span class="line">  fireRules(rel, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// It's a new subset.</span></span><br><span class="line">  <span class="comment">//note: 如果是一个 new subset，再做一次触发</span></span><br><span class="line">  <span class="keyword">if</span> (set.subsets.size() &gt; subsetBeforeCount) &#123;</span><br><span class="line">    fireRules(subset, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> subset;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>registerImpl()</code> 处理流程比较复杂，其方法实现，可以简单总结为以下几步：</p>
<ol>
<li>在经过最上面的一些验证之后，会通过 <code>rel.onRegister(this)</code> 这步操作，递归地调用 VolcanoPlanner 的 <code>ensureRegistered()</code> 方法对其 <code>inputs</code> RelNode 进行注册，最后还是调用 <code>registerImpl()</code> 方法先注册叶子节点，然后再父节点，最后到根节点；</li>
<li>根据 RelNode 的 digest 信息（一般这个对于 RelNode 来说是全局唯一的），判断其是否已经存在 <code>mapDigestToRel</code> 缓存中，如果存在的话，那么判断会 RelNode 是否相同，如果相同的话，证明之前已经注册过，直接通过 <code>getSubset()</code> 返回其对应的 RelSubset 信息，否则就对其 RelSubset 做下 merge；</li>
<li>如果 RelNode 对应的 RelSet 为 null，这里会新建一个 RelSet，并通过 <code>addRelToSet()</code> 将 RelNode 添加到 RelSet 中，并且更新 VolcanoPlanner 的 <code>mapRel2Subset</code> 缓存记录（RelNode 与 RelSubset 的对应关系），在 <code>addRelToSet()</code> 的最后还会更新 RelSubset 的 best plan 和 best cost（每当往一个 RelSubset 添加相应的 RelNode 时，都会判断这个 RelNode 是否代表了 best plan，如果是的话，就更新）；</li>
<li>将这个 RelNode 的 inputs 设置为其对应 RelSubset 的 children 节点（实际的操作时，是在 RelSet 的 <code>parents</code> 中记录其父节点）；</li>
<li>强制重新计算当前 RelNode 对应 RelSubset 的 importance；</li>
<li>如果这个 RelSubset 是新建的，会再触发一次 <code>fireRules()</code> 方法（会先对 RelNode 触发一次），遍历找到所有可以 match 的 Rule，对每个 Rule 都会创建一个 VolcanoRuleMatch 对象（会记录 RelNode、RelOptRuleOperand 等信息，RelOptRuleOperand 中又会记录 Rule 的信息），并将这个 VolcanoRuleMatch 添加到对应的 RuleQueue 中（就是前面图中的那个 RuleQueue）。</li>
</ol>
<p>这里，来看下 <code>fireRules()</code> 方法的实现，它的目的是把配置的 RuleMatch 添加到 RuleQueue 中，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Fires all rules matched by a relational expression.</span></span><br><span class="line"><span class="comment"> * note： 触发满足这个 relational expression 的所有 rules</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rel      Relational expression which has just been created (or maybe</span></span><br><span class="line"><span class="comment"> *                 from the queue)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deferred If true, each time a rule matches, just add an entry to</span></span><br><span class="line"><span class="comment"> *                 the queue.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fireRules</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    RelNode rel,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> deferred)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (RelOptRuleOperand operand : classOperands.get(rel.getClass())) &#123;</span><br><span class="line">    <span class="keyword">if</span> (operand.matches(rel)) &#123; <span class="comment">//note: rule 匹配的情况</span></span><br><span class="line">      <span class="keyword">final</span> VolcanoRuleCall ruleCall;</span><br><span class="line">      <span class="keyword">if</span> (deferred) &#123; <span class="comment">//note: 这里默认都是 true，会把 RuleMatch 添加到 queue 中</span></span><br><span class="line">        ruleCall = <span class="keyword">new</span> DeferringRuleCall(<span class="keyword">this</span>, operand);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ruleCall = <span class="keyword">new</span> VolcanoRuleCall(<span class="keyword">this</span>, operand);</span><br><span class="line">      &#125;</span><br><span class="line">      ruleCall.match(rel);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A rule call which defers its actions. Whereas &#123;<span class="doctag">@link</span> RelOptRuleCall&#125;</span></span><br><span class="line"><span class="comment"> * invokes the rule when it finds a match, a &lt;code&gt;DeferringRuleCall&lt;/code&gt;</span></span><br><span class="line"><span class="comment"> * creates a &#123;<span class="doctag">@link</span> VolcanoRuleMatch&#125; which can be invoked later.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DeferringRuleCall</span> <span class="keyword">extends</span> <span class="title">VolcanoRuleCall</span> </span>&#123;</span><br><span class="line">  DeferringRuleCall(</span><br><span class="line">      VolcanoPlanner planner,</span><br><span class="line">      RelOptRuleOperand operand) &#123;</span><br><span class="line">    <span class="keyword">super</span>(planner, operand);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Rather than invoking the rule (as the base method does), creates a</span></span><br><span class="line"><span class="comment">   * &#123;<span class="doctag">@link</span> VolcanoRuleMatch&#125; which can be invoked later.</span></span><br><span class="line"><span class="comment">   * note：不是直接触发 rule，而是创建一个后续可以被触发的 VolcanoRuleMatch</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onMatch</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> VolcanoRuleMatch match =</span><br><span class="line">        <span class="keyword">new</span> VolcanoRuleMatch(</span><br><span class="line">            volcanoPlanner,</span><br><span class="line">            getOperand0(), <span class="comment">//note: 其实就是 operand</span></span><br><span class="line">            rels,</span><br><span class="line">            nodeInputs);</span><br><span class="line">    volcanoPlanner.ruleQueue.addMatch(match);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的方法中，对于匹配的 Rule，将会创建一个 VolcanoRuleMatch 对象，之后再把这个 VolcanoRuleMatch 对象添加到对应的 RuleQueue 中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.RuleQueue</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Adds a rule match. The rule-matches are automatically added to all</span></span><br><span class="line"><span class="comment"> * existing &#123;<span class="doctag">@link</span> PhaseMatchList per-phase rule-match lists&#125; which allow</span></span><br><span class="line"><span class="comment"> * the rule referenced by the match.</span></span><br><span class="line"><span class="comment"> * note：添加一个 rule match（添加到所有现存的 match phase 中）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addMatch</span><span class="params">(VolcanoRuleMatch match)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> String matchName = match.toString();</span><br><span class="line">  <span class="keyword">for</span> (PhaseMatchList matchList : matchListMap.values()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!matchList.names.add(matchName)) &#123;</span><br><span class="line">      <span class="comment">// Identical match has already been added.</span></span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    String ruleClassName = match.getRule().getClass().getSimpleName();</span><br><span class="line"></span><br><span class="line">    Set&lt;String&gt; phaseRuleSet = phaseRuleMapping.get(matchList.phase);</span><br><span class="line">    <span class="comment">//note: 如果 phaseRuleSet 不为 ALL_RULES，并且 phaseRuleSet 不包含这个 ruleClassName 时，就跳过(其他三个阶段都属于这个情况)</span></span><br><span class="line">    <span class="comment">//note: 在添加 rule match 时，phaseRuleSet 可以控制哪些 match 可以添加、哪些不能添加</span></span><br><span class="line">    <span class="comment">//note: 这里的话，默认只有处在 OPTIMIZE 阶段的 PhaseMatchList 可以添加相应的 rule match</span></span><br><span class="line">    <span class="keyword">if</span> (phaseRuleSet != ALL_RULES) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!phaseRuleSet.contains(ruleClassName)) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOGGER.trace(<span class="string">"&#123;&#125; Rule-match queued: &#123;&#125;"</span>, matchList.phase.toString(), matchName);</span><br><span class="line"></span><br><span class="line">    matchList.list.add(match);</span><br><span class="line"></span><br><span class="line">    matchList.matchMap.put(</span><br><span class="line">        planner.getSubset(match.rels[<span class="number">0</span>]), match);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里 VolcanoPlanner 需要初始化的内容都初始化完成了，下面就到了具体的优化部分。</p>
<h4 id="4-VolcanoPlanner-findBestExp"><a href="#4-VolcanoPlanner-findBestExp" class="headerlink" title="4. VolcanoPlanner findBestExp"></a>4. VolcanoPlanner findBestExp</h4><p>VolcanoPlanner 的 <code>findBestExp()</code> 是具体进行优化的地方，先介绍一下这里的优化策略（每进行一次迭代，<code>cumulativeTicks</code> 加1，它记录了总的迭代次数）：</p>
<ol>
<li>第一次找到可执行计划的迭代次数记为 <code>firstFiniteTick</code>，其对应的 Cost 暂时记为 BestCost；</li>
<li>制定下一次优化要达到的目标为 <code>BestCost*0.9</code>，再根据 <code>firstFiniteTick</code> 及当前的迭代次数计算 <code>giveUpTick</code>，这个值代表的意思是：如果迭代次数超过这个值还没有达到优化目标，那么将会放弃迭代，认为当前的 plan 就是 best plan；</li>
<li>如果 RuleQueue 中 RuleMatch 为空，那么也会退出迭代，认为当前的 plan 就是 best plan；</li>
<li>在每次迭代时都会从 RuleQueue 中选择一个 RuleMatch，策略是选择一个最高 importance 的 RuleMatch，可以保证在每次规则优化时都是选择当前优化效果最好的 Rule 去优化；</li>
<li>最后根据 best plan，构建其对应的 RelNode。</li>
</ol>
<p>上面就是 <code>findBestExp()</code> 主要设计理念，这里来看其具体的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Finds the most efficient expression to implement the query given via</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> org.apache.calcite.plan.RelOptPlanner#setRoot(org.apache.calcite.rel.RelNode)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：找到最有效率的 relational expression，这个算法包含一系列阶段，每个阶段被触发的 rules 可能不同</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The algorithm executes repeatedly in a series of phases. In each phase</span></span><br><span class="line"><span class="comment"> * the exact rules that may be fired varies. The mapping of phases to rule</span></span><br><span class="line"><span class="comment"> * sets is maintained in the &#123;<span class="doctag">@link</span> #ruleQueue&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：在每个阶段，planner 都会初始化这个 RelSubset 的 importance，planner 会遍历 rule queue 中 rules 直到：</span></span><br><span class="line"><span class="comment"> * note：1. rule queue 变为空；</span></span><br><span class="line"><span class="comment"> * note：2. 对于 ambitious planner，最近 cost 不再提高时（具体来说，第一次找到一个可执行计划时，需要达到需要迭代总数的10%或更大）；</span></span><br><span class="line"><span class="comment"> * note：3. 对于 non-ambitious planner，当找到一个可执行的计划就行；</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;In each phase, the planner sets the initial importance of the existing</span></span><br><span class="line"><span class="comment"> * RelSubSets (&#123;<span class="doctag">@link</span> #setInitialImportance()&#125;). The planner then iterates</span></span><br><span class="line"><span class="comment"> * over the rule matches presented by the rule queue until:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;The rule queue becomes empty.&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;For ambitious planners: No improvements to the plan have been made</span></span><br><span class="line"><span class="comment"> * recently (specifically within a number of iterations that is 10% of the</span></span><br><span class="line"><span class="comment"> * number of iterations necessary to first reach an implementable plan or 25</span></span><br><span class="line"><span class="comment"> * iterations whichever is larger).&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;For non-ambitious planners: When an implementable plan is found.&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：此外，如果每10次迭代之后，没有一个可实现的计划，包含 logical RelNode 的 RelSubSets 将会通过 injectImportanceBoost 给一个 importance；</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Furthermore, after every 10 iterations without an implementable plan,</span></span><br><span class="line"><span class="comment"> * RelSubSets that contain only logical RelNodes are given an importance</span></span><br><span class="line"><span class="comment"> * boost via &#123;<span class="doctag">@link</span> #injectImportanceBoost()&#125;. Once an implementable plan is</span></span><br><span class="line"><span class="comment"> * found, the artificially raised importance values are cleared (see</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> #clearImportanceBoost()&#125;).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the most efficient RelNode tree found for implementing the given</span></span><br><span class="line"><span class="comment"> * query</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">findBestExp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 确保 root relational expression 的 subset（RelSubset）在它的等价集（RelSet）中包含所有 RelSubset 的 converter</span></span><br><span class="line">  <span class="comment">//note: 来保证 planner 从其他的 subsets 找到的实现方案可以转换为 root，否则可能因为 convention 不同，无法实施</span></span><br><span class="line">  ensureRootConverters();</span><br><span class="line">  <span class="comment">//note: materialized views 相关，这里可以先忽略~</span></span><br><span class="line">  registerMaterializations();</span><br><span class="line">  <span class="keyword">int</span> cumulativeTicks = <span class="number">0</span>; <span class="comment">//note: 四个阶段通用的变量</span></span><br><span class="line">  <span class="comment">//note: 不同的阶段，总共四个阶段，实际上只有 OPTIMIZE 这个阶段有效，因为其他阶段不会有 RuleMatch</span></span><br><span class="line">  <span class="keyword">for</span> (VolcanoPlannerPhase phase : VolcanoPlannerPhase.values()) &#123;</span><br><span class="line">    <span class="comment">//note: 在不同的阶段，初始化 RelSubSets 相应的 importance</span></span><br><span class="line">    <span class="comment">//note: root 节点往下子节点的 importance 都会被初始化</span></span><br><span class="line">    setInitialImportance();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 默认是 VolcanoCost</span></span><br><span class="line">    RelOptCost targetCost = costFactory.makeHugeCost();</span><br><span class="line">    <span class="keyword">int</span> tick = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> firstFiniteTick = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> splitCount = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> giveUpTick = Integer.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      ++tick;</span><br><span class="line">      ++cumulativeTicks;</span><br><span class="line">      <span class="comment">//note: 第一次运行是 false，两个不是一个对象，一个是 costFactory.makeHugeCost， 一个是 costFactory.makeInfiniteCost</span></span><br><span class="line">      <span class="comment">//note: 如果低于目标 cost，这里再重新设置一个新目标、新的 giveUpTick</span></span><br><span class="line">      <span class="keyword">if</span> (root.bestCost.isLe(targetCost)) &#123;</span><br><span class="line">        <span class="comment">//note: 本阶段第一次运行，目的是为了调用 clearImportanceBoost 方法，清除相应的 importance 信息</span></span><br><span class="line">        <span class="keyword">if</span> (firstFiniteTick &lt; <span class="number">0</span>) &#123;</span><br><span class="line">          firstFiniteTick = cumulativeTicks;</span><br><span class="line"></span><br><span class="line">          <span class="comment">//note: 对于那些手动提高 importance 的 RelSubset 进行重新计算</span></span><br><span class="line">          clearImportanceBoost();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ambitious) &#123;</span><br><span class="line">          <span class="comment">// Choose a slightly more ambitious target cost, and</span></span><br><span class="line">          <span class="comment">// try again. If it took us 1000 iterations to find our</span></span><br><span class="line">          <span class="comment">// first finite plan, give ourselves another 100</span></span><br><span class="line">          <span class="comment">// iterations to reduce the cost by 10%.</span></span><br><span class="line">          <span class="comment">//note: 设置 target 为当前 best cost 的 0.9，调整相应的目标，再进行优化</span></span><br><span class="line">          targetCost = root.bestCost.multiplyBy(<span class="number">0.9</span>);</span><br><span class="line">          ++splitCount;</span><br><span class="line">          <span class="keyword">if</span> (impatient) &#123;</span><br><span class="line">            <span class="keyword">if</span> (firstFiniteTick &lt; <span class="number">10</span>) &#123;</span><br><span class="line">              <span class="comment">// It's possible pre-processing can create</span></span><br><span class="line">              <span class="comment">// an implementable plan -- give us some time</span></span><br><span class="line">              <span class="comment">// to actually optimize it.</span></span><br><span class="line">              <span class="comment">//note: 有可能在 pre-processing 阶段就实现一个 implementable plan，所以先设置一个值，后面再去优化</span></span><br><span class="line">              giveUpTick = cumulativeTicks + <span class="number">25</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              giveUpTick =</span><br><span class="line">                  cumulativeTicks</span><br><span class="line">                      + Math.max(firstFiniteTick / <span class="number">10</span>, <span class="number">25</span>);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">//note: 最近没有任何进步（超过 giveUpTick 限制，还没达到目标值），直接采用当前的 best plan</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cumulativeTicks &gt; giveUpTick) &#123;</span><br><span class="line">        <span class="comment">// We haven't made progress recently. Take the current best.</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (root.bestCost.isInfinite() &amp;&amp; ((tick % <span class="number">10</span>) == <span class="number">0</span>)) &#123;</span><br><span class="line">        injectImportanceBoost();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      LOGGER.debug(<span class="string">"PLANNER = &#123;&#125;; TICK = &#123;&#125;/&#123;&#125;; PHASE = &#123;&#125;; COST = &#123;&#125;"</span>,</span><br><span class="line">          <span class="keyword">this</span>, cumulativeTicks, tick, phase.toString(), root.bestCost);</span><br><span class="line"></span><br><span class="line">      VolcanoRuleMatch match = ruleQueue.popMatch(phase);</span><br><span class="line">      <span class="comment">//note: 如果没有规则，会直接退出当前的阶段</span></span><br><span class="line">      <span class="keyword">if</span> (match == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">assert</span> match.getRule().matches(match);</span><br><span class="line">      <span class="comment">//note: 做相应的规则匹配</span></span><br><span class="line">      match.onMatch();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// The root may have been merged with another</span></span><br><span class="line">      <span class="comment">// subset. Find the new root subset.</span></span><br><span class="line">      root = canonize(root);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 当期阶段完成，移除 ruleQueue 中记录的 rule-match list</span></span><br><span class="line">    ruleQueue.phaseCompleted(phase);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (LOGGER.isTraceEnabled()) &#123;</span><br><span class="line">    StringWriter sw = <span class="keyword">new</span> StringWriter();</span><br><span class="line">    <span class="keyword">final</span> PrintWriter pw = <span class="keyword">new</span> PrintWriter(sw);</span><br><span class="line">    dump(pw);</span><br><span class="line">    pw.flush();</span><br><span class="line">    LOGGER.trace(sw.toString());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 根据 plan 构建其 RelNode 树</span></span><br><span class="line">  RelNode cheapest = root.buildCheapestPlan(<span class="keyword">this</span>);</span><br><span class="line">  <span class="keyword">if</span> (LOGGER.isDebugEnabled()) &#123;</span><br><span class="line">    LOGGER.debug(</span><br><span class="line">        <span class="string">"Cheapest plan:\n&#123;&#125;"</span>, RelOptUtil.toString(cheapest, SqlExplainLevel.ALL_ATTRIBUTES));</span><br><span class="line"></span><br><span class="line">    LOGGER.debug(<span class="string">"Provenance:\n&#123;&#125;"</span>, provenance(cheapest));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> cheapest;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整体的流程正如前面所述，这里来看下 RuleQueue 中 <code>popMatch()</code> 方法的实现，它的目的是选择 the highest importance 的 RuleMatch，这个方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.RuleQueue</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Removes the rule match with the highest importance, and returns it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：返回最高 importance 的 rule，并从 Rule Match 中移除（处理过后的就移除）</span></span><br><span class="line"><span class="comment"> * note：如果集合为空，就返回 null</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Returns &#123;<span class="doctag">@code</span> null&#125; if there are no more matches.&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Note that the VolcanoPlanner may still decide to reject rule matches</span></span><br><span class="line"><span class="comment"> * which have become invalid, say if one of their operands belongs to an</span></span><br><span class="line"><span class="comment"> * obsolete set or has importance=0.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> java.lang.AssertionError if this method is called with a phase</span></span><br><span class="line"><span class="comment"> *                              previously marked as completed via</span></span><br><span class="line"><span class="comment"> *                              &#123;<span class="doctag">@link</span> #phaseCompleted(VolcanoPlannerPhase)&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">VolcanoRuleMatch <span class="title">popMatch</span><span class="params">(VolcanoPlannerPhase phase)</span> </span>&#123;</span><br><span class="line">  dump();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 选择当前阶段对应的 PhaseMatchList</span></span><br><span class="line">  PhaseMatchList phaseMatchList = matchListMap.get(phase);</span><br><span class="line">  <span class="keyword">if</span> (phaseMatchList == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Used match list for phase "</span> + phase</span><br><span class="line">        + <span class="string">" after phase complete"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> List&lt;VolcanoRuleMatch&gt; matchList = phaseMatchList.list;</span><br><span class="line">  VolcanoRuleMatch match;</span><br><span class="line">  <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="comment">//note: 按照前面的逻辑只有在 OPTIMIZE 阶段，PhaseMatchList 才不为空，其他阶段都是空</span></span><br><span class="line">    <span class="comment">// 参考 addMatch 方法</span></span><br><span class="line">    <span class="keyword">if</span> (matchList.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (LOGGER.isTraceEnabled()) &#123;</span><br><span class="line">      matchList.sort(MATCH_COMPARATOR);</span><br><span class="line">      match = matchList.remove(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">      StringBuilder b = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">      b.append(<span class="string">"Sorted rule queue:"</span>);</span><br><span class="line">      <span class="keyword">for</span> (VolcanoRuleMatch match2 : matchList) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">double</span> importance = match2.computeImportance();</span><br><span class="line">        b.append(<span class="string">"\n"</span>);</span><br><span class="line">        b.append(match2);</span><br><span class="line">        b.append(<span class="string">" importance "</span>);</span><br><span class="line">        b.append(importance);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      LOGGER.trace(b.toString());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 直接遍历找到 importance 最大的 match（上面先做排序，是为了输出日志）</span></span><br><span class="line">      <span class="comment">// If we're not tracing, it's not worth the effort of sorting the</span></span><br><span class="line">      <span class="comment">// list to find the minimum.</span></span><br><span class="line">      match = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">int</span> bestPos = -<span class="number">1</span>;</span><br><span class="line">      <span class="keyword">int</span> i = -<span class="number">1</span>;</span><br><span class="line">      <span class="keyword">for</span> (VolcanoRuleMatch match2 : matchList) &#123;</span><br><span class="line">        ++i;</span><br><span class="line">        <span class="keyword">if</span> (match == <span class="keyword">null</span></span><br><span class="line">            || MATCH_COMPARATOR.compare(match2, match) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">          bestPos = i;</span><br><span class="line">          match = match2;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      match = matchList.remove(bestPos);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (skipMatch(match)) &#123;</span><br><span class="line">      LOGGER.debug(<span class="string">"Skip match: &#123;&#125;"</span>, match);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A rule match's digest is composed of the operand RelNodes' digests,</span></span><br><span class="line">  <span class="comment">// which may have changed if sets have merged since the rule match was</span></span><br><span class="line">  <span class="comment">// enqueued.</span></span><br><span class="line">  <span class="comment">//note: 重新计算一下这个 RuleMatch 的 digest</span></span><br><span class="line">  match.recomputeDigest();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 从 phaseMatchList 移除这个 RuleMatch</span></span><br><span class="line">  phaseMatchList.matchMap.remove(</span><br><span class="line">      planner.getSubset(match.rels[<span class="number">0</span>]), match);</span><br><span class="line"></span><br><span class="line">  LOGGER.debug(<span class="string">"Pop match: &#123;&#125;"</span>, match);</span><br><span class="line">  <span class="keyword">return</span> match;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里，我们就把 VolcanoPlanner 的优化讲述完了，当然并没有面面俱到所有的细节，VolcanoPlanner 的整体处理图如下：</p>
<p><img src="/images/calcite/14-volcano.png" alt="VolcanoPlanner 整体处理流程"> </p>
<h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><h4 id="1-初始化-RuleQueue-时，添加的-one-useless-rule-name-有什么用？"><a href="#1-初始化-RuleQueue-时，添加的-one-useless-rule-name-有什么用？" class="headerlink" title="1. 初始化 RuleQueue 时，添加的 one useless rule name 有什么用？"></a>1. 初始化 RuleQueue 时，添加的 one useless rule name 有什么用？</h4><p>在初始化 RuleQueue 时，会给 VolcanoPlanner 的四个阶段 <code>PRE_PROCESS_MDR, PRE_PROCESS, OPTIMIZE, CLEANUP</code> 都初始化一个 PhaseMatchList 对象（记录这个阶段对应的 RuleMatch），这时候会给其中的三个阶段添加一个 useless rule，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> VolcanoPlannerPhaseRuleMappingInitializer</span><br><span class="line">    getPhaseRuleMappingInitializer() &#123;</span><br><span class="line">  <span class="keyword">return</span> phaseRuleMap -&gt; &#123;</span><br><span class="line">    <span class="comment">// Disable all phases except OPTIMIZE by adding one useless rule name.</span></span><br><span class="line">    <span class="comment">//note: 通过添加一个无用的 rule name 来 disable 优化器的其他三个阶段</span></span><br><span class="line">    phaseRuleMap.get(VolcanoPlannerPhase.PRE_PROCESS_MDR).add(<span class="string">"xxx"</span>);</span><br><span class="line">    phaseRuleMap.get(VolcanoPlannerPhase.PRE_PROCESS).add(<span class="string">"xxx"</span>);</span><br><span class="line">    phaseRuleMap.get(VolcanoPlannerPhase.CLEANUP).add(<span class="string">"xxx"</span>);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>开始时还困惑这个什么用？后来看到下面的代码基本就明白了</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (VolcanoPlannerPhase phase : VolcanoPlannerPhase.values()) &#123;</span><br><span class="line">  <span class="comment">// empty phases get converted to "all rules"</span></span><br><span class="line">  <span class="comment">//note: 如果阶段对应的 rule set 为空，那么就给这个阶段对应的 rule set 添加一个 【ALL_RULES】</span></span><br><span class="line">  <span class="comment">//也就是只有 OPTIMIZE 这个阶段对应的会添加 ALL_RULES</span></span><br><span class="line">  <span class="keyword">if</span> (phaseRuleMapping.get(phase).isEmpty()) &#123;</span><br><span class="line">    phaseRuleMapping.put(phase, ALL_RULES);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后面在调用 RuleQueue 的 <code>addMatch()</code> 方法会做相应的判断，如果 phaseRuleSet 不为 ALL_RULES，并且 phaseRuleSet 不包含这个 ruleClassName 时，那么就跳过这个 RuleMatch，也就是说实际上只有 <strong>OPTIMIZE</strong> 这个阶段是发挥作用的，其他阶段没有添加任何 RuleMatch。</p>
<h4 id="2-四个-phase-实际上只用了-1个阶段，为什么要设置4个阶段？"><a href="#2-四个-phase-实际上只用了-1个阶段，为什么要设置4个阶段？" class="headerlink" title="2. 四个 phase 实际上只用了 1个阶段，为什么要设置4个阶段？"></a>2. 四个 phase 实际上只用了 1个阶段，为什么要设置4个阶段？</h4><p>VolcanoPlanner 的四个阶段 <code>PRE_PROCESS_MDR, PRE_PROCESS, OPTIMIZE, CLEANUP</code>，实际只有 <code>OPTIMIZE</code> 进行真正的优化操作，其他阶段并没有，这里自己是有一些困惑的：</p>
<ol>
<li>为什么要分为4个阶段，在添加 RuleMatch 时，是向四个阶段同时添加，这个设计有什么好处？为什么要优化四次？</li>
<li>设计了4个阶段，为什么默认只用了1个？</li>
</ol>
<p>这两个问题，暂时也没有头绪，有想法的，欢迎交流。</p>
<p>这部分的内容比较多，到这里 Calcite 主要处理流程的文章也终于梳理完了，因为是初次接触，文章理解有误的地方，欢迎各位指教~</p>
<p>附上上一篇文章：<a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a>。</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/wangxingxing2006/article/details/78907278" target="_blank" rel="external">HepPlanner源码分析——Calcite</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/48735419" target="_blank" rel="external">SQL 查询优化原理与 Volcano Optimizer 介绍</a>；</li>
<li><a href="https://blog.csdn.net/u013007900/article/details/78993101" target="_blank" rel="external">高级数据库十六：查询优化器（二）</a>；</li>
<li><a href="http://rann.cc/2018/08/23/sql-optimized-principles.html" target="_blank" rel="external">【SQL】SQL优化器原理——查询优化器综述</a>；</li>
<li><a href="http://hbasefly.com/2017/03/01/sparksql-catalyst/" target="_blank" rel="external">SparkSQL – 从0到1认识Catalyst</a>；</li>
<li><a href="http://hbasefly.com/2017/05/04/bigdata%EF%BC%8Dcbo/" target="_blank" rel="external">BigData－‘基于代价优化’究竟是怎么一回事？</a>；</li>
<li><a href="https://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-phoenix-using-apache-calcite?qid=b7a1ca0f-e7bf-49ad-bc51-0615ec8a4971&amp;v=&amp;b=&amp;from_search=4" target="_blank" rel="external">Cost-based Query Optimization in Apache Phoenix using Apache Calcite</a>；</li>
<li><a href="https://cs.uwaterloo.ca/~david/cs848/volcano.pdf" target="_blank" rel="external">The Volcano Optimizer Generator: Extensibility and Efficient Search</a>：Volcano 模型的经典论文；</li>
<li><a href="https://pdfs.semanticscholar.org/c1a3/9da04a072f695e9a7f36bf397fba5c19b93c.pdf?_ga=2.162106044.1003201390.1552806109-329306565.1552806109" target="_blank" rel="external">The Cascades Framework for Query Optimization</a>：Cascades 模型的经典论文。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;紧接上篇文章&lt;a href=&quot;http://matt33.com/2019/03/07/apache-calcite-process-flow/&quot;&gt;Apache Calcite 处理流程详解（一）&lt;/a&gt;，这里是 Calcite 系列文章的第二篇，后面还会有文章讲述 Cal
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="calcite" scheme="http://matt33.com/tags/calcite/"/>
    
  </entry>
  
  <entry>
    <title>Apache Calcite 处理流程详解（一）</title>
    <link href="http://matt33.com/2019/03/07/apache-calcite-process-flow/"/>
    <id>http://matt33.com/2019/03/07/apache-calcite-process-flow/</id>
    <published>2019-03-07T12:40:38.000Z</published>
    <updated>2020-06-23T14:13:17.222Z</updated>
    
    <content type="html"><![CDATA[<p>关于 Apache Calcite 的简单介绍可以参考 <a href="https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite" target="_blank" rel="external">Apache Calcite：Hadoop 中新型大数据查询引擎</a> 这篇文章，Calcite 一开始设计的目标就是 <strong>one size fits all</strong>，它希望能为不同计算存储引擎提供统一的 SQL 查询引擎，当然 Calcite 并不仅仅是一个简单的 SQL 查询引擎，在论文 <a href="https://arxiv.org/pdf/1802.10233.pdf" target="_blank" rel="external">Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</a> 的摘要（摘要见下面）部分，关于 Calcite 的核心点有简单的介绍，Calcite 的架构有三个特点：flexible, embeddable, and extensible，就是灵活性、组件可插拔、可扩展，它的 SQL Parser 层、Optimizer 层等都可以单独使用，这也是 Calcite 受总多开源框架欢迎的原因之一。</p>
<blockquote>
<p>Apache Calcite is a foundational software framework that provides <strong>query processing, optimization, and query language</strong> support to many popular open-source data processing systems such as Apache Hive, Apache Storm, Apache Flink, Druid, and MapD. Calcite’s architecture consists of </p>
<ol>
<li>a modular and extensible query optimizer with hundreds of built-in optimization rules, </li>
<li>a query processor capable of processing a variety of query languages, </li>
<li>an adapter architecture designed for extensibility, </li>
<li>and support for heterogeneous data models and stores (relational, semi-structured, streaming, and geospatial).<br><strong>This flexible, embeddable, and extensible architecture</strong> is what makes Calcite an attractive choice for adoption in bigdata frameworks. It is an active project that continues to introduce support for the new types of data sources, query languages, and approaches to query processing and optimization.</li>
</ol>
</blockquote>
<h1 id="Calcite-概念"><a href="#Calcite-概念" class="headerlink" title="Calcite 概念"></a>Calcite 概念</h1><p>在介绍 Calcite 架构之前，先来看下与 Calcite 相关的基础性内容。</p>
<h2 id="关系代数的基本知识"><a href="#关系代数的基本知识" class="headerlink" title="关系代数的基本知识"></a>关系代数的基本知识</h2><p>关系代数是关系型数据库操作的理论基础，关系代数支持并、差、笛卡尔积、投影和选择等基本运算。关系代数也是 Calcite 的核心，任何一个查询都可以表示成由关系运算符组成的树。在 Calcite 中，它会先将 SQL 转换成关系表达式（relational expression），然后通过规则匹配（rules match）进行相应的优化，优化会有一个成本（cost）模型为参考。</p>
<p>这里先看下关系代数相关内容，这对于理解 Calcite 很有帮助，特别是 Calcite Optimizer 这块的内容，关系代数的基础可以参考这篇文章 <a href="https://blog.csdn.net/QuinnNorris/article/details/70739094" target="_blank" rel="external">SQL 形式化语言——关系代数</a>，简单总结如下：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>英文</th>
<th>符号</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>选择</td>
<td>select</td>
<td>σ</td>
<td>类似于 SQL 中的 where</td>
</tr>
<tr>
<td>投影</td>
<td>project</td>
<td>Π</td>
<td>类似于 SQL 中的 select</td>
</tr>
<tr>
<td>并</td>
<td>union</td>
<td>∪</td>
<td>类似于 SQL 中的 union</td>
</tr>
<tr>
<td>集合差</td>
<td>set-difference</td>
<td>-</td>
<td>SQL中没有对应的操作符</td>
</tr>
<tr>
<td>笛卡儿积</td>
<td>Cartesian-product</td>
<td>×</td>
<td>类似于 SQL 中不带 on 条件的 inner join</td>
</tr>
<tr>
<td>重命名</td>
<td>rename</td>
<td>ρ</td>
<td>类似于 SQL 中的 as</td>
</tr>
<tr>
<td>集合交</td>
<td>intersection</td>
<td>∩</td>
<td>SQL中没有对应的操作符</td>
</tr>
<tr>
<td>自然连接</td>
<td>natural join</td>
<td>⋈</td>
<td>类似于 SQL 中的 inner join</td>
</tr>
<tr>
<td>赋值</td>
<td>assignment</td>
<td>←</td>
</tr>
</tbody>
</table>
<h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>查询优化主要是围绕着 <strong>等价交换</strong> 的原则做相应的转换，这部分可以参考【《数据库系统概念（中文第六版）》第13章——查询优化】，关于查询优化理论知识，这里就不再详述，列出一些个人不错不错的博客，大家可以参考一下：</p>
<ol>
<li><a href="https://www.jianshu.com/p/edf503a2a1e7" target="_blank" rel="external">数据库查询优化入门: 代数与物理优化基础</a>；</li>
<li><a href="https://blog.csdn.net/u013007900/article/details/78978271" target="_blank" rel="external">高级数据库十五：查询优化器（一）</a>；</li>
<li><a href="https://blog.csdn.net/u013007900/article/details/78993101" target="_blank" rel="external">高级数据库十六：查询优化器（二）</a>；</li>
<li><a href="http://www.ptbird.cn/optimization-of-relational-algebraic-expression.html" target="_blank" rel="external">「 数据库原理 」查询优化（关系代数表达式优化）</a>；</li>
<li><a href="http://book.51cto.com/art/201306/400084.htm" target="_blank" rel="external">4.1.3 关系数据库系统的查询优化（1）</a>；</li>
<li><a href="http://book.51cto.com/art/201306/400085.htm" target="_blank" rel="external">4.1.3 关系数据库系统的查询优化（10）</a>；</li>
</ol>
<h2 id="Calcite-中的一些概念"><a href="#Calcite-中的一些概念" class="headerlink" title="Calcite 中的一些概念"></a>Calcite 中的一些概念</h2><p>Calcite 抛出的概念非常多，笔者最开始在看代码时就被这些概念绕得云里雾里，这时候先从代码的细节里跳出来，先把这些概念理清楚、归归类后再去看代码，思路就清晰很多，因此，在介绍 Calcite 整体实现前，先把这些概念梳理一下，需要对这些概念有个基本的理解，相关的概念如下图所示：</p>
<p><img src="/images/calcite/0-calcite.png" alt="calcite 基本概念"></p>
<p>整理如下表所示：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>RelOptRule</td>
<td>transforms an expression into another。对 expression 做等价转换</td>
<td>根据传递给它的 RelOptRuleOperand 来对目标 RelNode 树进行规则匹配，匹配成功后，会再次调用 <code>matches()</code> 方法（默认返回真）进行进一步检查。如果 <code>mathes()</code> 结果为真，则调用 <code>onMatch()</code> 进行转换。</td>
</tr>
<tr>
<td>ConverterRule</td>
<td>Abstract base class for a rule which converts from one calling convention to another without changing semantics.</td>
<td>它是 RelOptRule 的子类，专门用来做数据源之间的转换（Calling convention），<strong>ConverterRule 一般会调用对应的 Converter 来完成工作</strong>，比如说：JdbcToSparkConverterRule 调用 JdbcToSparkConverter 来完成对 JDBC Table 到 Spark RDD 的转换。</td>
</tr>
<tr>
<td>RelNode</td>
<td>relational expression，RelNode 会标识其 input RelNode 信息，这样就构成了一棵 RelNode 树</td>
<td>代表了<strong>对数据的一个处理操作</strong>，常见的操作有 Sort、Join、Project、Filter、Scan 等。它蕴含的是对整个 Relation 的操作，而不是对具体数据的处理逻辑。</td>
</tr>
<tr>
<td>Converter</td>
<td>A relational expression implements the interface <code>Converter</code> to indicate that it converts a physical attribute, or RelTrait of a relational expression from one value to another.</td>
<td><strong>用来把一种 RelTrait 转换为另一种 RelTrait 的 RelNode</strong>。如 JdbcToSparkConverter 可以把 JDBC 里的 table 转换为 Spark RDD。如果需要在一个 RelNode 中处理来源于异构系统的逻辑表，Calcite 要求先用 Converter 把异构系统的逻辑表转换为同一种 Convention。</td>
</tr>
<tr>
<td>RexNode</td>
<td>Row-level expression</td>
<td>行表达式（标量表达式），蕴含的是对一行数据的处理逻辑。每个行表达式都有数据的类型。这是因为在 Valdiation 的过程中，编译器会推导出表达式的结果类型。常见的行表达式包括字面量 RexLiteral， 变量 RexVariable， 函数或操作符调用 RexCall 等。 RexNode 通过 RexBuilder 进行构建。</td>
</tr>
<tr>
<td>RelTrait</td>
<td>RelTrait represents the manifestation of a relational expression trait within a trait definition.</td>
<td>用来定义逻辑表的物理相关属性（physical property），三种主要的 trait 类型是：Convention、RelCollation、RelDistribution；</td>
</tr>
<tr>
<td>Convention</td>
<td>Calling convention used to repressent a single data source, inputs must be in the same convention</td>
<td>继承自 RelTrait，类型很少，代表一个单一的数据源，一个  relational expression 必须在同一个 convention 中；</td>
</tr>
<tr>
<td>RelTraitDef</td>
<td></td>
<td>主要有三种：ConventionTraitDef：用来代表数据源 RelCollationTraitDef：用来定义参与排序的字段；RelDistributionTraitDef：用来定义数据在物理存储上的分布方式（比如：single、hash、range、random 等）；</td>
</tr>
<tr>
<td>RelOptCluster</td>
<td>An environment for related relational expressions during the optimization of a query.</td>
<td>palnner 运行时的环境，保存上下文信息；</td>
</tr>
<tr>
<td>RelOptPlanner</td>
<td>A RelOptPlanner is a query optimizer: it transforms a relational expression into a semantically equivalent relational expression, according to a given set of rules and a cost model.</td>
<td>也就是<strong>优化器</strong>，Calcite 支持RBO（Rule-Based Optimizer） 和 CBO（Cost-Based Optimizer）。Calcite 的 RBO （HepPlanner）称为启发式优化器（heuristic implementation ），它简单地按 AST 树结构匹配所有已知规则，直到没有规则能够匹配为止；Calcite 的 CBO 称为火山式优化器（VolcanoPlanner）成本优化器也会匹配并应用规则，当整棵树的成本降低趋于稳定后，优化完成，成本优化器依赖于比较准确的成本估算。RelOptCost 和 Statistic 与成本估算相关；</td>
</tr>
<tr>
<td>RelOptCost</td>
<td>defines an interface for optimizer cost in terms of number of rows processed, CPU cost, and I/O cost.</td>
<td>优化器成本模型会依赖；</td>
</tr>
</tbody>
</table>
<h1 id="Calcite-架构"><a href="#Calcite-架构" class="headerlink" title="Calcite 架构"></a>Calcite 架构</h1><p>关于 Calcite 的架构，可以参考下图（图片来自前面那篇论文），它与传统数据库管理系统有一些相似之处，相比而言，它将数据存储、数据处理算法和元数据存储这些部分忽略掉了，这样设计带来的好处是：对于涉及多种数据源和多种计算引擎的应用而言，Calcite 因为可以兼容多种存储和计算引擎，使得 Calcite 可以提供统一查询服务，Calcite 将会是这些应用的最佳选择。</p>
<p><img src="/images/calcite/1-calcite.png" alt="Calcite Architecture，图片来自论文"></p>
<p>在 Calcite 架构中，最核心地方就是 Optimizer，也就是优化器，一个 Optimization Engine 包含三个组成部分：</p>
<ol>
<li>rules：也就是匹配规则，Calcite 内置上百种 Rules 来优化 relational expression，当然也支持自定义 rules；</li>
<li>metadata providers：主要是向优化器提供信息，这些信息会有助于指导优化器向着目标（减少整体 cost）进行优化，信息可以包括行数、table 哪一列是唯一列等，也包括计算 RelNode 树中执行 subexpression cost 的函数；</li>
<li>planner engines：它的主要目标是进行触发 rules 来达到指定目标，比如像 cost-based optimizer（CBO）的目标是减少cost（Cost 包括处理的数据行数、CPU cost、IO cost 等）。</li>
</ol>
<h1 id="Calcite-处理流程"><a href="#Calcite-处理流程" class="headerlink" title="Calcite 处理流程"></a>Calcite 处理流程</h1><p>Sql 的执行过程一般可以分为下图中的四个阶段，Calcite 同样也是这样：</p>
<p><img src="/images/calcite/dataflow.png" alt="Sql 执行过程"></p>
<p>但这里为了讲述方便，把 SQL 的执行分为下面五个阶段（跟上面比比又独立出了一个阶段）：</p>
<ol>
<li>解析 SQL， 把 SQL 转换成为 AST （抽象语法树），在 Calcite 中用 SqlNode 来表示；</li>
<li>语法检查，根据数据库的元数据信息进行语法验证，验证之后还是用 SqlNode 表示 AST 语法树；</li>
<li>语义分析，根据 SqlNode 及元信息构建 RelNode 树，也就是最初版本的逻辑计划（Logical Plan）；</li>
<li>逻辑计划优化，优化器的核心，根据前面生成的逻辑计划按照相应的规则（Rule）进行优化；</li>
<li>物理执行，生成物理计划，物理执行计划执行。</li>
</ol>
<p>这里我们只关注前四步的内容，会配合源码实现以及一个示例来讲解。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>示例 SQL 如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> u.id <span class="keyword">as</span> user_id, u.name <span class="keyword">as</span> user_name, j.company <span class="keyword">as</span> user_company, u.age <span class="keyword">as</span> user_age </span><br><span class="line"><span class="keyword">from</span> <span class="keyword">users</span> u <span class="keyword">join</span> jobs j <span class="keyword">on</span> u.name=j.name</span><br><span class="line"><span class="keyword">where</span> u.age &gt; <span class="number">30</span> <span class="keyword">and</span> j.id&gt;<span class="number">10</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> user_id</span><br></pre></td></tr></table></figure>
<p>这里有两张表，其表各个字段及类型定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">SchemaPlus rootSchema = Frameworks.createRootSchema(<span class="keyword">true</span>);</span><br><span class="line">rootSchema.add(<span class="string">"USERS"</span>, <span class="keyword">new</span> AbstractTable() &#123; <span class="comment">//note: add a table</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RelDataType <span class="title">getRowType</span><span class="params">(<span class="keyword">final</span> RelDataTypeFactory typeFactory)</span> </span>&#123;</span><br><span class="line">        RelDataTypeFactory.Builder builder = typeFactory.builder();</span><br><span class="line"></span><br><span class="line">        builder.add(<span class="string">"ID"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.INTEGER));</span><br><span class="line">        builder.add(<span class="string">"NAME"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.CHAR));</span><br><span class="line">        builder.add(<span class="string">"AGE"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.INTEGER));</span><br><span class="line">        <span class="keyword">return</span> builder.build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">rootSchema.add(<span class="string">"JOBS"</span>, <span class="keyword">new</span> AbstractTable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RelDataType <span class="title">getRowType</span><span class="params">(<span class="keyword">final</span> RelDataTypeFactory typeFactory)</span> </span>&#123;</span><br><span class="line">        RelDataTypeFactory.Builder builder = typeFactory.builder();</span><br><span class="line"></span><br><span class="line">        builder.add(<span class="string">"ID"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.INTEGER));</span><br><span class="line">        builder.add(<span class="string">"NAME"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.CHAR));</span><br><span class="line">        builder.add(<span class="string">"COMPANY"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.CHAR));</span><br><span class="line">        <span class="keyword">return</span> builder.build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h2 id="Step1-SQL-解析阶段（SQL–-gt-SqlNode）"><a href="#Step1-SQL-解析阶段（SQL–-gt-SqlNode）" class="headerlink" title="Step1: SQL 解析阶段（SQL–&gt;SqlNode）"></a>Step1: SQL 解析阶段（SQL–&gt;SqlNode）</h2><p>使用 Calcite 进行 Sql 解析的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SqlParser parser = SqlParser.create(sql, SqlParser.Config.DEFAULT);</span><br><span class="line">SqlNode sqlNode = parser.parseStmt();</span><br></pre></td></tr></table></figure>
<p>Calcite 使用 JavaCC 做 SQL 解析，JavaCC 根据 Calcite 中定义的 <a href="https://github.com/apache/calcite/blob/master/core/src/main/codegen/templates/Parser.jj" target="_blank" rel="external">Parser.jj</a> 文件，生成一系列的 java 代码，生成的 Java 代码会把 SQL 转换成 AST 的数据结构（这里是 SqlNode 类型）。</p>
<blockquote>
<p>与 Javacc 相似的工具还有 ANTLR，JavaCC 中的 jj 文件也跟 ANTLR 中的 G4文件类似，Apache Spark 中使用这个工具做类似的事情。</p>
</blockquote>
<h3 id="Javacc"><a href="#Javacc" class="headerlink" title="Javacc"></a>Javacc</h3><p>关于 Javacc 内容可以参考下面这几篇文章，这里就不再详细展开，可以通过下面文章的例子把 JavaCC 的语法了解一下，这样我们也可以自己设计一个 DSL（Doomain Specific Language）。</p>
<ul>
<li><a href="https://www.cnblogs.com/Gavin_Liu/archive/2009/03/07/1405029.html" target="_blank" rel="external">JavaCC 研究与应用( 8000字 心得 源程序)</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/xml/x-javacc/part1/index.html" target="_blank" rel="external">JavaCC、解析树和 XQuery 语法，第 1 部分</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/xml/x-javacc/part2/index.html" target="_blank" rel="external">JavaCC、解析树和 XQuery 语法，第 2 部分</a>；</li>
<li><a href="https://www.yangguo.info/2014/12/13/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-Javacc%E4%BD%BF%E7%94%A8/" target="_blank" rel="external">编译原理之Javacc使用</a>；</li>
<li><a href="http://www.engr.mun.ca/~theo/JavaCC-Tutorial/javacc-tutorial.pdf" target="_blank" rel="external">javacc tutorial</a>；</li>
</ul>
<p>回到 Calcite，Javacc 这里要实现一个 SQL Parser，它的功能有以下两个，这里都是需要在 jj 文件中定义的。</p>
<ol>
<li>设计词法和语义，定义 SQL 中具体的元素；</li>
<li>实现词法分析器（Lexer）和语法分析器（Parser），完成对 SQL 的解析，完成相应的转换。</li>
</ol>
<h3 id="SQL-Parser-流程"><a href="#SQL-Parser-流程" class="headerlink" title="SQL Parser 流程"></a>SQL Parser 流程</h3><p>当 SqlParser 调用 <code>parseStmt()</code> 方法后，其相应的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.calcite.sql.parser.SqlParser</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseStmt</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> parseQuery();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseQuery</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> parser.parseSqlStmtEof(); <span class="comment">//note: 解析sql语句</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">    <span class="keyword">if</span> (ex <span class="keyword">instanceof</span> CalciteContextException) &#123;</span><br><span class="line">      <span class="keyword">final</span> String originalSql = parser.getOriginalSql();</span><br><span class="line">      <span class="keyword">if</span> (originalSql != <span class="keyword">null</span>) &#123;</span><br><span class="line">        ((CalciteContextException) ex).setOriginalStatement(originalSql);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> parser.normalizeException(ex);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 SqlParser 中 parser 指的是 <code>SqlParserImpl</code> 类（<code>SqlParser.Config.DEFAULT</code> 指定的），它就是由 JJ 文件生成的解析类，其处理流程如下，具体解析逻辑还是要看 JJ 文件中的定义。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.sql.parser.impl.SqlParserImpl</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseSqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> SqlStmtEof();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parses an SQL statement followed by the end-of-file symbol.</span></span><br><span class="line"><span class="comment"> * note:解析SQL语句(后面有文件结束符号)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">public</span> SqlNode <span class="title">SqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">  SqlNode stmt;</span><br><span class="line">  stmt = SqlStmt();</span><br><span class="line">  jj_consume_token(<span class="number">0</span>);</span><br><span class="line">      &#123;<span class="keyword">if</span> (<span class="keyword">true</span>) <span class="keyword">return</span> stmt;&#125;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Missing return statement in function"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">//note: 解析 SQL statement</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">public</span> SqlNode <span class="title">SqlStmt</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">  SqlNode stmt;</span><br><span class="line">  <span class="keyword">if</span> (jj_2_34(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlSetOption(Span.of(), <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_35(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlAlter();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_36(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_37(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlExplain();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_38(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlDescribe();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_39(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlInsert();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_40(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlDelete();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_41(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlUpdate();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_42(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlMerge();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_43(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlProcedureCall();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    jj_consume_token(-<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ParseException();</span><br><span class="line">  &#125;</span><br><span class="line">      &#123;<span class="keyword">if</span> (<span class="keyword">true</span>) <span class="keyword">return</span> stmt;&#125;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Missing return statement in function"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>示例中 SQL 经过前面的解析之后，会生成一个 SqlNode，这个 SqlNode 是一个 SqlOrder 类型，DEBUG 后的 SqlOrder 对象如下图所示。</p>
<p><img src="/images/calcite/2-calciter.jpg" alt="SqlNode 结果"> </p>
<h2 id="Step2-SqlNode-验证（SqlNode–-gt-SqlNode）"><a href="#Step2-SqlNode-验证（SqlNode–-gt-SqlNode）" class="headerlink" title="Step2: SqlNode 验证（SqlNode–&gt;SqlNode）"></a>Step2: SqlNode 验证（SqlNode–&gt;SqlNode）</h2><p>经过上面的第一步，会生成一个 SqlNode 对象，它是一个<strong>未经验证</strong>的抽象语法树，下面就进入了一个<strong>语法检查</strong>阶段，语法检查前需要知道元数据信息，这个检查会包括表名、字段名、函数名、数据类型的检查。进行语法检查的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 二、sql validate（会先通过Catalog读取获取相应的metadata和namespace）</span></span><br><span class="line"><span class="comment">//note: get metadata and namespace</span></span><br><span class="line">SqlTypeFactoryImpl factory = <span class="keyword">new</span> SqlTypeFactoryImpl(RelDataTypeSystem.DEFAULT);</span><br><span class="line">CalciteCatalogReader calciteCatalogReader = <span class="keyword">new</span> CalciteCatalogReader(</span><br><span class="line">    CalciteSchema.from(rootScheme),</span><br><span class="line">    CalciteSchema.from(rootScheme).path(<span class="keyword">null</span>),</span><br><span class="line">    factory,</span><br><span class="line">    <span class="keyword">new</span> CalciteConnectionConfigImpl(<span class="keyword">new</span> Properties()));</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 校验（包括对表名，字段名，函数名，字段类型的校验。）</span></span><br><span class="line">SqlValidator validator = SqlValidatorUtil.newValidator(SqlStdOperatorTable.instance(), calciteCatalogReader, factory,</span><br><span class="line">    conformance(frameworkConfig));</span><br><span class="line">SqlNode validateSqlNode = validator.validate(sqlNode);</span><br></pre></td></tr></table></figure>
<p>我们知道 Calcite 本身是不管理和存储元数据的，在检查之前，需要先把元信息注册到 Calcite 中，一般的操作方法是实现 SchemaFactory，由它去创建相应的 Schema，在 Schema 中可以注册相应的元数据信息（如：通过 <code>getTableMap()</code> 方法注册表信息），如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.schema.impl.AbstractSchema</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a map of tables in this schema by name.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The implementations of &#123;<span class="doctag">@link</span> #getTableNames()&#125;</span></span><br><span class="line"><span class="comment"> * and &#123;<span class="doctag">@link</span> #getTable(String)&#125; depend on this map.</span></span><br><span class="line"><span class="comment"> * The default implementation of this method returns the empty map.</span></span><br><span class="line"><span class="comment"> * Override this method to change their behavior.&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Map of tables in this schema by name</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Map&lt;String, Table&gt; <span class="title">getTableMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> ImmutableMap.of();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//org.apache.calcite.adapter.csvorg.apache.calcite.adapter.csv.CsvSchemasvSchema</span></span><br><span class="line"><span class="comment">//note: 创建表</span></span><br><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">protected</span> Map&lt;String, Table&gt; <span class="title">getTableMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (tableMap == <span class="keyword">null</span>) &#123;</span><br><span class="line">    tableMap = createTableMap();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> tableMap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CsvSchemasvSchema 中的 <code>getTableMap()</code> 方法通过 <code>createTableMap()</code> 来注册相应的表信息。</p>
<p>结合前面的例子再来分析，在前面定义了 CalciteCatalogReader 实例，该实例就是用来读取 Schema 中的元数据信息的。真正检查的逻辑是在 <code>SqlValidatorImpl</code> 类中实现的，这个 check 的逻辑比较复杂，在看代码时通过两种手段来看：</p>
<ol>
<li>DEBUG 的方式，可以看到其方法调用的过程；</li>
<li>测试程序中故意构造一些 Case，观察其异常栈。</li>
</ol>
<p>比如，在示例中 SQL 中，如果把一个字段名写错，写成 ids，其报错信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">org.apache.calcite.runtime.CalciteContextException: From line 1, column 156 to line 1, column 158: Column <span class="string">'IDS'</span> not found <span class="keyword">in</span> table <span class="string">'J'</span></span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">    at org.apache.calcite.runtime.Resources<span class="variable">$ExInstWithCause</span>.ex(Resources.java:463)</span><br><span class="line">    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:787)</span><br><span class="line">    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:772)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4788)</span><br><span class="line">    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visit(SqlValidatorImpl.java:5683)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visit(SqlValidatorImpl.java:5665)</span><br><span class="line">    at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:334)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:134)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:101)</span><br><span class="line">    at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:865)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visitScoped(SqlValidatorImpl.java:5701)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:50)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:33)</span><br><span class="line">    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:138)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:134)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:101)</span><br><span class="line">    at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:865)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visitScoped(SqlValidatorImpl.java:5701)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:50)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:33)</span><br><span class="line">    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:138)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5272)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereClause(SqlValidatorImpl.java:3977)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3305)</span><br><span class="line">    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)</span><br><span class="line">    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:977)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:953)</span><br><span class="line">    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:928)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:632)</span><br><span class="line">    at com.matt.test.calcite.test.SqlTest3.sqlToRelNode(SqlTest3.java:200)</span><br><span class="line">    at com.matt.test.calcite.test.SqlTest3.main(SqlTest3.java:117)</span><br><span class="line">Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column <span class="string">'IDS'</span> not found <span class="keyword">in</span> table <span class="string">'J'</span></span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">    at org.apache.calcite.runtime.Resources<span class="variable">$ExInstWithCause</span>.ex(Resources.java:463)</span><br><span class="line">    at org.apache.calcite.runtime.Resources<span class="variable">$ExInst</span>.ex(Resources.java:572)</span><br><span class="line">    ... 33 more</span><br><span class="line">java.lang.NullPointerException</span><br><span class="line">    at org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:806)</span><br><span class="line">    at org.apache.calcite.plan.hep.HepPlanner.setRoot(HepPlanner.java:152)</span><br><span class="line">    at com.matt.test.calcite.test.SqlTest3.main(SqlTest3.java:124)</span><br></pre></td></tr></table></figure>
<h3 id="SqlValidatorImpl-检查过程"><a href="#SqlValidatorImpl-检查过程" class="headerlink" title="SqlValidatorImpl 检查过程"></a>SqlValidatorImpl 检查过程</h3><p>语法检查验证是通过 SqlValidatorImpl 的 <code>validate()</code> 方法进行操作的，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">org.apache.calcite.sql.validate.SqlValidatorImpl</span><br><span class="line"><span class="comment">//note: 做相应的语法树校验</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">validate</span><span class="params">(SqlNode topNode)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: root 对应的 Scope</span></span><br><span class="line">  SqlValidatorScope scope = <span class="keyword">new</span> EmptyScope(<span class="keyword">this</span>);</span><br><span class="line">  scope = <span class="keyword">new</span> CatalogScope(scope, ImmutableList.of(<span class="string">"CATALOG"</span>));</span><br><span class="line">  <span class="comment">//note: 1.rewrite expression</span></span><br><span class="line">  <span class="comment">//note: 2.做相应的语法检查</span></span><br><span class="line">  <span class="keyword">final</span> SqlNode topNode2 = validateScopedExpression(topNode, scope); <span class="comment">//note: 验证</span></span><br><span class="line">  <span class="keyword">final</span> RelDataType type = getValidatedNodeType(topNode2);</span><br><span class="line">  Util.discard(type);</span><br><span class="line">  <span class="keyword">return</span> topNode2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的实现是在 <code>validateScopedExpression()</code> 方法中，其实现如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> SqlNode <span class="title">validateScopedExpression</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlNode topNode,</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlValidatorScope scope)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 1. rewrite expression，将其标准化，便于后面的逻辑计划优化</span></span><br><span class="line">  SqlNode outermostNode = performUnconditionalRewrites(topNode, <span class="keyword">false</span>);</span><br><span class="line">  cursorSet.add(outermostNode);</span><br><span class="line">  top = outermostNode;</span><br><span class="line">  TRACER.trace(<span class="string">"After unconditional rewrite: &#123;&#125;"</span>, outermostNode);</span><br><span class="line">  <span class="comment">//note: 2. Registers a query in a parent scope.</span></span><br><span class="line">  <span class="comment">//note: register scopes and namespaces implied a relational expression</span></span><br><span class="line">  <span class="keyword">if</span> (outermostNode.isA(SqlKind.TOP_LEVEL)) &#123;</span><br><span class="line">    registerQuery(scope, <span class="keyword">null</span>, outermostNode, outermostNode, <span class="keyword">null</span>, <span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 3. catalog 验证，调用 SqlNode 的 validate 方法，</span></span><br><span class="line">  outermostNode.validate(<span class="keyword">this</span>, scope);</span><br><span class="line">  <span class="keyword">if</span> (!outermostNode.isA(SqlKind.TOP_LEVEL)) &#123;</span><br><span class="line">    <span class="comment">// force type derivation so that we can provide it to the</span></span><br><span class="line">    <span class="comment">// caller later without needing the scope</span></span><br><span class="line">    deriveType(scope, outermostNode);</span><br><span class="line">  &#125;</span><br><span class="line">  TRACER.trace(<span class="string">"After validation: &#123;&#125;"</span>, outermostNode);</span><br><span class="line">  <span class="keyword">return</span> outermostNode;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它的处理逻辑主要分为三步：</p>
<ol>
<li>rewrite expression，将其标准化，便于后面的逻辑计划优化；</li>
<li>注册这个 relational expression 的 scopes 和 namespaces（这两个对象代表了其元信息）；</li>
<li>进行相应的验证，这里会依赖第二步注册的 scopes 和 namespaces 信息。</li>
</ol>
<h4 id="Rewrite"><a href="#Rewrite" class="headerlink" title="Rewrite"></a>Rewrite</h4><p>关于 Rewrite 这一步，一直困惑比较，因为根据 <code>After unconditional rewrite:</code> 这条日志的结果看，其实前后 SqlNode 并没有太大变化，看 <code>performUnconditionalRewrites()</code> 这部分代码时，看得不是很明白，不过还是注意到了 SqlOrderBy 的注释（注释如下），它的意思是 SqlOrderBy 通过 <code>performUnconditionalRewrites()</code> 方法已经被 SqlSelect 对象中的 <code>ORDER_OPERAND</code> 取代了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parse tree node that represents an &#123;<span class="doctag">@code</span> ORDER BY&#125; on a query other than a</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@code</span> SELECT&#125; (e.g. &#123;<span class="doctag">@code</span> VALUES&#125; or &#123;<span class="doctag">@code</span> UNION&#125;).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;It is a purely syntactic operator, and is eliminated by</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> org.apache.calcite.sql.validate.SqlValidatorImpl#performUnconditionalRewrites&#125;</span></span><br><span class="line"><span class="comment"> * and replaced with the ORDER_OPERAND of SqlSelect.&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlOrderBy</span> <span class="keyword">extends</span> <span class="title">SqlCall</span> </span>&#123;</span><br></pre></td></tr></table></figure>
<p>注意到 SqlOrderBy 的原因是因为在 <code>performUnconditionalRewrites()</code> 方法前面都是递归对每个对象进行处理，在后面进行真正的 ransform 时，主要在围绕着 ORDER_BY 这个类型做处理，而且从代码中可以看出，将其类型从 SqlOrderBy 转换成了 SqlSelect，BUDEG 前面的示例，发现 outermostNode 与 topNode 的类型确实发生了变化，如下图所示。</p>
<p><img src="/images/calcite/3-calcite.png" alt="Rewrite 前后的对比"></p>
<p>这个方法有个好的地方就是，在不改变原有 SQL Parser 的逻辑的情况下，可以在这个方法里做一些改动，当然如果 SQL Parser 的结果如果直接可用当然是最好的，就不需要再进行一次 Rewrite 了。</p>
<h4 id="registerQuery"><a href="#registerQuery" class="headerlink" title="registerQuery"></a>registerQuery</h4><p>这里的功能主要就是将[元数据]转换成 SqlValidator 内部的 对象 进行表示，也就是 SqlValidatorScope 和 SqlValidatorNamespace 两种类型的对象：</p>
<ol>
<li>SqlValidatorNamespace：a description of a data source used in a query，它代表了 SQL 查询的数据源，它是一个逻辑上数据源，可以是一张表，也可以是一个子查询；</li>
<li>SqlValidatorScope：describes the tables and columns accessible at a particular point in the query，代表了在某一个程序运行点，当前可见的字段名和表名。</li>
</ol>
<p>这个理解起来并不是那么容易，在 SelectScope 类中有一个示例讲述，这个示例对这两个概念的理解很有帮助。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;h3&gt;Scopes&lt;/h3&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;In the query&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;blockquote&gt;</span></span><br><span class="line"><span class="comment"> * &lt;pre&gt;</span></span><br><span class="line"><span class="comment"> * SELECT expr1</span></span><br><span class="line"><span class="comment"> * FROM t1,</span></span><br><span class="line"><span class="comment"> *     t2,</span></span><br><span class="line"><span class="comment"> *     (SELECT expr2 FROM t3) AS q3</span></span><br><span class="line"><span class="comment"> * WHERE c1 IN (SELECT expr3 FROM t4)</span></span><br><span class="line"><span class="comment"> * ORDER BY expr4&lt;/pre&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/blockquote&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The scopes available at various points of the query are as follows:&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr1 can see t1, t2, q3&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr2 can see t3&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr3 can see t4, t1, t2&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr4 can see t1, t2, q3, plus (depending upon the dialect) any aliases</span></span><br><span class="line"><span class="comment"> * defined in the SELECT clause&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ul&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;h3&gt;Namespaces&lt;/h3&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;In the above query, there are 4 namespaces:&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;t1&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;t2&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;(SELECT expr2 FROM t3) AS q3&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;(SELECT expr3 FROM t4)&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<h4 id="validate-验证"><a href="#validate-验证" class="headerlink" title="validate 验证"></a>validate 验证</h4><p>接着回到最复杂的一步，就是 outermostNode 实例调用 <code>validate(this, scope)</code> 方法进行验证的部分，对于我们这个示例，这里最后调用的是 SqlSelect 的 <code>validate()</code> 方法，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">validate</span><span class="params">(SqlValidator validator, SqlValidatorScope scope)</span> </span>&#123;</span><br><span class="line">  validator.validateQuery(<span class="keyword">this</span>, scope, validator.getUnknownType());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它调用的是 SqlValidatorImpl 的 <code>validateQuery()</code> 方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">validateQuery</span><span class="params">(SqlNode node, SqlValidatorScope scope,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlValidatorNamespace ns = getNamespace(node, scope);</span><br><span class="line">  <span class="keyword">if</span> (node.getKind() == SqlKind.TABLESAMPLE) &#123;</span><br><span class="line">    List&lt;SqlNode&gt; operands = ((SqlCall) node).getOperandList();</span><br><span class="line">    SqlSampleSpec sampleSpec = SqlLiteral.sampleValue(operands.get(<span class="number">1</span>));</span><br><span class="line">    <span class="keyword">if</span> (sampleSpec <span class="keyword">instanceof</span> SqlSampleSpec.SqlTableSampleSpec) &#123;</span><br><span class="line">      validateFeature(RESOURCE.sQLFeature_T613(), node.getParserPosition());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sampleSpec</span><br><span class="line">        <span class="keyword">instanceof</span> SqlSampleSpec.SqlSubstitutionSampleSpec) &#123;</span><br><span class="line">      validateFeature(RESOURCE.sQLFeatureExt_T613_Substitution(),</span><br><span class="line">          node.getParserPosition());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  validateNamespace(ns, targetRowType);<span class="comment">//note: 检查</span></span><br><span class="line">  <span class="keyword">switch</span> (node.getKind()) &#123;</span><br><span class="line">  <span class="keyword">case</span> EXTEND:</span><br><span class="line">    <span class="comment">// Until we have a dedicated namespace for EXTEND</span></span><br><span class="line">    deriveType(scope, node);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (node == top) &#123;</span><br><span class="line">    validateModality(node);</span><br><span class="line">  &#125;</span><br><span class="line">  validateAccess(</span><br><span class="line">      node,</span><br><span class="line">      ns.getTable(),</span><br><span class="line">      SqlAccessEnum.SELECT);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Validates a namespace.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace Namespace</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> targetRowType Desired row type, must not be null, may be the data</span></span><br><span class="line"><span class="comment"> *                      type 'unknown'.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">validateNamespace</span><span class="params">(<span class="keyword">final</span> SqlValidatorNamespace namespace,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  namespace.validate(targetRowType);<span class="comment">//note: 验证</span></span><br><span class="line">  <span class="keyword">if</span> (namespace.getNode() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    setValidatedNodeType(namespace.getNode(), namespace.getType());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这部分的调用逻辑非常复杂，主要的语法验证是 SqlValidatorScope 部分（它里面有相应的表名、字段名等信息），而 namespace 表示需要进行验证的数据源，最开始的这个 SqlNode 有一个 root namespace，上面的 <code>validateNamespace()</code> 方法会首先调用其 namespace 的 <code>validate()</code> 方法进行验证，以前面的示例为例，这里是 SelectNamespace，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.sql.validate.AbstractNamespace</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">validate</span><span class="params">(RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">switch</span> (status) &#123;</span><br><span class="line">  <span class="keyword">case</span> UNVALIDATED: <span class="comment">//note: 还没开始 check</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      status = SqlValidatorImpl.Status.IN_PROGRESS; <span class="comment">//note: 更新当前 namespace 的状态</span></span><br><span class="line">      Preconditions.checkArgument(rowType == <span class="keyword">null</span>,</span><br><span class="line">          <span class="string">"Namespace.rowType must be null before validate has been called"</span>);</span><br><span class="line">      RelDataType type = validateImpl(targetRowType); <span class="comment">//note: 检查验证</span></span><br><span class="line">      Preconditions.checkArgument(type != <span class="keyword">null</span>,</span><br><span class="line">          <span class="string">"validateImpl() returned null"</span>);</span><br><span class="line">      setType(type);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      status = SqlValidatorImpl.Status.VALID;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IN_PROGRESS: <span class="comment">//note: 已经开始 check 了，死循环了</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Cycle detected during type-checking"</span>);</span><br><span class="line">  <span class="keyword">case</span> VALID:<span class="comment">//note: 检查结束</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">throw</span> Util.unexpected(status);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//org.apache.calcite.sql.validate.SelectNamespace</span></span><br><span class="line"><span class="comment">//note: 检查，还是调用 SqlValidatorImpl 的方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelDataType <span class="title">validateImpl</span><span class="params">(RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  validator.validateSelect(select, targetRowType);</span><br><span class="line">  <span class="keyword">return</span> rowType;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后验证方法的实现是 SqlValidatorImpl 的 <code>validateSelect()</code> 方法（对本示例而言），其调用过程如下图所示：</p>
<p><img src="/images/calcite/4-sqlvalidator.png" alt="验证部分的处理流程"> </p>
<h2 id="Step3-语义分析（SqlNode–-gt-RelNode-RexNode）"><a href="#Step3-语义分析（SqlNode–-gt-RelNode-RexNode）" class="headerlink" title="Step3: 语义分析（SqlNode–&gt;RelNode/RexNode）"></a>Step3: 语义分析（SqlNode–&gt;RelNode/RexNode）</h2><p>经过第二步之后，这里的 SqlNode 就是经过语法校验的 SqlNode 树，接下来这一步就是将 SqlNode 转换成 RelNode/RexNode，也就是生成相应的逻辑计划（Logical Plan），示例的代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create the rexBuilder</span></span><br><span class="line"><span class="keyword">final</span> RexBuilder rexBuilder =  <span class="keyword">new</span> RexBuilder(factory);</span><br><span class="line"><span class="comment">// init the planner</span></span><br><span class="line"><span class="comment">// 这里也可以注册 VolcanoPlanner，这一步 planner 并没有使用</span></span><br><span class="line">HepProgramBuilder builder = <span class="keyword">new</span> HepProgramBuilder();</span><br><span class="line">RelOptPlanner planner = <span class="keyword">new</span> HepPlanner(builder.build());</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: init cluster: An environment for related relational expressions during the optimization of a query.</span></span><br><span class="line"><span class="keyword">final</span> RelOptCluster cluster = RelOptCluster.create(planner, rexBuilder);</span><br><span class="line"><span class="comment">//note: init SqlToRelConverter</span></span><br><span class="line"><span class="keyword">final</span> SqlToRelConverter.Config config = SqlToRelConverter.configBuilder()</span><br><span class="line">    .withConfig(frameworkConfig.getSqlToRelConverterConfig())</span><br><span class="line">    .withTrimUnusedFields(<span class="keyword">false</span>)</span><br><span class="line">    .withConvertTableAccess(<span class="keyword">false</span>)</span><br><span class="line">    .build(); <span class="comment">//note: config</span></span><br><span class="line"><span class="comment">// 创建 SqlToRelConverter 实例，cluster、calciteCatalogReader、validator 都传进去了，SqlToRelConverter 会缓存这些对象</span></span><br><span class="line"><span class="keyword">final</span> SqlToRelConverter sqlToRelConverter = <span class="keyword">new</span> SqlToRelConverter(<span class="keyword">new</span> DogView(), validator, calciteCatalogReader, cluster, StandardConvertletTable.INSTANCE, config);</span><br><span class="line"><span class="comment">// convert to RelNode</span></span><br><span class="line">RelRoot root = sqlToRelConverter.convertQuery(validateSqlNode, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">root = root.withRel(sqlToRelConverter.flattenTypes(root.rel, <span class="keyword">true</span>));</span><br><span class="line"><span class="keyword">final</span> RelBuilder relBuilder = config.getRelBuilderFactory().create(cluster, <span class="keyword">null</span>);</span><br><span class="line">root = root.withRel(RelDecorrelator.decorrelateQuery(root.rel, relBuilder));</span><br><span class="line"></span><br><span class="line">RelNode relNode = root.rel;</span><br><span class="line"></span><br><span class="line"><span class="comment">//DogView 的实现</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DogView</span> <span class="keyword">implements</span> <span class="title">RelOptTable</span>.<span class="title">ViewExpander</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DogView</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RelRoot <span class="title">expandView</span><span class="params">(RelDataType rowType, String queryString, List&lt;String&gt; schemaPath,</span></span></span><br><span class="line"><span class="function"><span class="params">                              List&lt;String&gt; viewPath)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便分析，这里也把上面的过程分为以下几步：</p>
<ol>
<li>初始化 RexBuilder；</li>
<li>初始化 RelOptPlanner;</li>
<li>初始化 RelOptCluster；</li>
<li>初始化 SqlToRelConverter；</li>
<li>进行转换；</li>
</ol>
<p>第1、2、4步在上述代码已经有相应的注释，这里不再介绍，下面从第三步开始讲述。</p>
<h3 id="初始化-RelOptCluster"><a href="#初始化-RelOptCluster" class="headerlink" title="初始化 RelOptCluster"></a>初始化 RelOptCluster</h3><p>RelOptCluster 初始化的代码如下，这里基本都走默认的参数配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">org.apache.calcite.plan.RelOptCluster</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Creates a cluster. */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> RelOptCluster <span class="title">create</span><span class="params">(RelOptPlanner planner,</span></span></span><br><span class="line"><span class="function"><span class="params">    RexBuilder rexBuilder)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> RelOptCluster(planner, rexBuilder.getTypeFactory(),</span><br><span class="line">      rexBuilder, <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>), <span class="keyword">new</span> HashMap&lt;&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a cluster.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For use only from &#123;<span class="doctag">@link</span> #create&#125; and &#123;<span class="doctag">@link</span> RelOptQuery&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">RelOptCluster(RelOptPlanner planner, RelDataTypeFactory typeFactory,</span><br><span class="line">    RexBuilder rexBuilder, AtomicInteger nextCorrel,</span><br><span class="line">    Map&lt;String, RelNode&gt; mapCorrelToRel) &#123;</span><br><span class="line">  <span class="keyword">this</span>.nextCorrel = nextCorrel;</span><br><span class="line">  <span class="keyword">this</span>.mapCorrelToRel = mapCorrelToRel;</span><br><span class="line">  <span class="keyword">this</span>.planner = Objects.requireNonNull(planner);</span><br><span class="line">  <span class="keyword">this</span>.typeFactory = Objects.requireNonNull(typeFactory);</span><br><span class="line">  <span class="keyword">this</span>.rexBuilder = rexBuilder;</span><br><span class="line">  <span class="keyword">this</span>.originalExpression = rexBuilder.makeLiteral(<span class="string">"?"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set up a default rel metadata provider,</span></span><br><span class="line">  <span class="comment">// giving the planner first crack at everything</span></span><br><span class="line">  <span class="comment">//note: 默认的 metadata provider</span></span><br><span class="line">  setMetadataProvider(DefaultRelMetadataProvider.INSTANCE);</span><br><span class="line">  <span class="comment">//note: trait（对于 HepPlaner 和 VolcanoPlanner 不一样)</span></span><br><span class="line">  <span class="keyword">this</span>.emptyTraitSet = planner.emptyTraitSet();</span><br><span class="line">  <span class="keyword">assert</span> emptyTraitSet.size() == planner.getRelTraitDefs().size();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="SqlToRelConverter-转换"><a href="#SqlToRelConverter-转换" class="headerlink" title="SqlToRelConverter 转换"></a>SqlToRelConverter 转换</h3><p>SqlToRelConverter 中的 <code>convertQuery()</code> 将 SqlNode 转换为 RelRoot，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Converts an unvalidated query's parse tree into a relational expression.</span></span><br><span class="line"><span class="comment"> * note：把一个 parser tree 转换为 relational expression</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> query           Query to convert</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> needsValidation Whether to validate the query before converting;</span></span><br><span class="line"><span class="comment"> *                        &lt;code&gt;false&lt;/code&gt; if the query has already been</span></span><br><span class="line"><span class="comment"> *                        validated.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> top             Whether the query is top-level, say if its result</span></span><br><span class="line"><span class="comment"> *                        will become a JDBC result set; &lt;code&gt;false&lt;/code&gt; if</span></span><br><span class="line"><span class="comment"> *                        the query will be part of a view.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelRoot <span class="title">convertQuery</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlNode query,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">boolean</span> needsValidation,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">boolean</span> top)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (needsValidation) &#123; <span class="comment">//note: 是否需要做相应的校验（如果校验过了，这里就不需要了）</span></span><br><span class="line">    query = validator.validate(query);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 设置 MetadataProvider</span></span><br><span class="line">  RelMetadataQuery.THREAD_PROVIDERS.set(</span><br><span class="line">      JaninoRelMetadataProvider.of(cluster.getMetadataProvider()));</span><br><span class="line">  <span class="comment">//note: 得到 RelNode(relational expression)</span></span><br><span class="line">  RelNode result = convertQueryRecursive(query, top, <span class="keyword">null</span>).rel;</span><br><span class="line">  <span class="keyword">if</span> (top) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isStream(query)) &#123;<span class="comment">//note: 如果 stream 的话</span></span><br><span class="line">      result = <span class="keyword">new</span> LogicalDelta(cluster, result.getTraitSet(), result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  RelCollation collation = RelCollations.EMPTY;</span><br><span class="line">  <span class="keyword">if</span> (!query.isA(SqlKind.DML)) &#123; <span class="comment">//note: 如果是 DML 语句</span></span><br><span class="line">    <span class="keyword">if</span> (isOrdered(query)) &#123; <span class="comment">//note: 如果需要做排序的话</span></span><br><span class="line">      collation = requiredCollation(result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 对转换前后的 RelDataType 做验证</span></span><br><span class="line">  checkConvertedType(query, result);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (SQL2REL_LOGGER.isDebugEnabled()) &#123;</span><br><span class="line">    SQL2REL_LOGGER.debug(</span><br><span class="line">        RelOptUtil.dumpPlan(<span class="string">"Plan after converting SqlNode to RelNode"</span>,</span><br><span class="line">            result, SqlExplainFormat.TEXT,</span><br><span class="line">            SqlExplainLevel.EXPPLAN_ATTRIBUTES));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> RelDataType validatedRowType = validator.getValidatedNodeType(query);</span><br><span class="line">  <span class="keyword">return</span> RelRoot.of(result, validatedRowType, query.getKind())</span><br><span class="line">      .withCollation(collation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>真正的实现是在 <code>convertQueryRecursive()</code> 方法中完成的，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Recursively converts a query to a relational expression.</span></span><br><span class="line"><span class="comment"> * note：递归地讲一个 query 转换为 relational expression</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> query         Query</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> top           Whether this query is the top-level query of the</span></span><br><span class="line"><span class="comment"> *                      statement</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> targetRowType Target row type, or null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Relational expression</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> RelRoot <span class="title">convertQueryRecursive</span><span class="params">(SqlNode query, <span class="keyword">boolean</span> top,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlKind kind = query.getKind();</span><br><span class="line">  <span class="keyword">switch</span> (kind) &#123;</span><br><span class="line">  <span class="keyword">case</span> SELECT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertSelect((SqlSelect) query, top), kind);</span><br><span class="line">  <span class="keyword">case</span> INSERT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertInsert((SqlInsert) query), kind);</span><br><span class="line">  <span class="keyword">case</span> DELETE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertDelete((SqlDelete) query), kind);</span><br><span class="line">  <span class="keyword">case</span> UPDATE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertUpdate((SqlUpdate) query), kind);</span><br><span class="line">  <span class="keyword">case</span> MERGE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertMerge((SqlMerge) query), kind);</span><br><span class="line">  <span class="keyword">case</span> UNION:</span><br><span class="line">  <span class="keyword">case</span> INTERSECT:</span><br><span class="line">  <span class="keyword">case</span> EXCEPT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertSetOp((SqlCall) query), kind);</span><br><span class="line">  <span class="keyword">case</span> WITH:</span><br><span class="line">    <span class="keyword">return</span> convertWith((SqlWith) query, top);</span><br><span class="line">  <span class="keyword">case</span> VALUES:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertValues((SqlCall) query, targetRowType), kind);</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"not a query: "</span> + query);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>依然以前面的示例为例，因为是 SqlSelect 类型，这里会调用下面的方法做相应的转换：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Converts a SELECT statement's parse tree into a relational expression.</span></span><br><span class="line"><span class="comment"> * note：将一个 Select parse tree 转换成一个关系表达式</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">convertSelect</span><span class="params">(SqlSelect select, <span class="keyword">boolean</span> top)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlValidatorScope selectScope = validator.getWhereScope(select);</span><br><span class="line">  <span class="keyword">final</span> Blackboard bb = createBlackboard(selectScope, <span class="keyword">null</span>, top);</span><br><span class="line">  convertSelectImpl(bb, select);<span class="comment">//note: 做相应的转换</span></span><br><span class="line">  <span class="keyword">return</span> bb.root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>convertSelectImpl()</code> 方法中会依次对 SqlSelect 的各个部分做相应转换，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implementation of &#123;<span class="doctag">@link</span> #convertSelect(SqlSelect, boolean)&#125;;</span></span><br><span class="line"><span class="comment"> * derived class may override.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">convertSelectImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> Blackboard bb,</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlSelect select)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: convertFrom</span></span><br><span class="line">  convertFrom(</span><br><span class="line">      bb,</span><br><span class="line">      select.getFrom());</span><br><span class="line">  <span class="comment">//note: convertWhere</span></span><br><span class="line">  convertWhere(</span><br><span class="line">      bb,</span><br><span class="line">      select.getWhere());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> List&lt;SqlNode&gt; orderExprList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="keyword">final</span> List&lt;RelFieldCollation&gt; collationList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">//note: 有 order by 操作时</span></span><br><span class="line">  gatherOrderExprs(</span><br><span class="line">      bb,</span><br><span class="line">      select,</span><br><span class="line">      select.getOrderList(),</span><br><span class="line">      orderExprList,</span><br><span class="line">      collationList);</span><br><span class="line">  <span class="keyword">final</span> RelCollation collation =</span><br><span class="line">      cluster.traitSet().canonize(RelCollations.of(collationList));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (validator.isAggregate(select)) &#123;</span><br><span class="line">    <span class="comment">//note: 当有聚合操作时，也就是含有 group by、having 或者 Select 和 order by 中含有聚合函数</span></span><br><span class="line">    convertAgg(</span><br><span class="line">        bb,</span><br><span class="line">        select,</span><br><span class="line">        orderExprList);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 对 select list 部分的处理</span></span><br><span class="line">    convertSelectList(</span><br><span class="line">        bb,</span><br><span class="line">        select,</span><br><span class="line">        orderExprList);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (select.isDistinct()) &#123; <span class="comment">//note: select 后面含有 DISTINCT 关键字时（去重）</span></span><br><span class="line">    distinctify(bb, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: Converts a query's ORDER BY clause, if any.</span></span><br><span class="line">  convertOrder(</span><br><span class="line">      select, bb, collation, orderExprList, select.getOffset(),</span><br><span class="line">      select.getFetch());</span><br><span class="line">  bb.setRoot(bb.root, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里以示例中的 From 部分为例介绍 SqlNode 到 RelNode 的逻辑，按照示例 DEUBG 后的结果如下图所示，因为 form 部分是一个 join 操作，会进入 join 相关的处理中。</p>
<p><img src="/images/calcite/5-calcite.jpg" alt="convertFrom 之 Join 的情况"> </p>
<p>这部分方法调用过程是：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">convertQuery --&gt;</span><br><span class="line">convertQueryRecursive --&gt;</span><br><span class="line">convertSelect --&gt;</span><br><span class="line">convertSelectImpl --&gt;</span><br><span class="line">convertFrom &amp; convertWhere &amp; convertSelectList</span><br></pre></td></tr></table></figure>
<p>到这里 SqlNode 到 RelNode 过程就完成了，生成的逻辑计划如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LogicalSort(sort0=[<span class="variable">$0</span>], dir0=[ASC])</span><br><span class="line">  LogicalProject(USER_ID=[<span class="variable">$0</span>], USER_NAME=[<span class="variable">$1</span>], USER_COMPANY=[<span class="variable">$5</span>], USER_AGE=[<span class="variable">$2</span>])</span><br><span class="line">    LogicalFilter(condition=[AND(&gt;(<span class="variable">$2</span>, 30), &gt;(<span class="variable">$3</span>, 10))])</span><br><span class="line">      LogicalJoin(condition=[=(<span class="variable">$1</span>, <span class="variable">$4</span>)], joinType=[inner])</span><br><span class="line">        LogicalTableScan(table=[[USERS]])</span><br><span class="line">        LogicalTableScan(table=[[JOBS]])</span><br></pre></td></tr></table></figure>
<p>到这里前三步就算全部完成了。</p>
<h2 id="Step4-优化阶段（RelNode–-gt-RelNode）"><a href="#Step4-优化阶段（RelNode–-gt-RelNode）" class="headerlink" title="Step4: 优化阶段（RelNode–&gt;RelNode）"></a>Step4: 优化阶段（RelNode–&gt;RelNode）</h2><p>终于来来到了第四阶段，也就是 Calcite 的核心所在，优化器进行优化的地方，前面 sql 中有一个明显可以优化的地方就是过滤条件的下压（push down），在进行 join 操作前，先进行 filter 操作，这样的话就不需要在 join 时进行全量 join，减少参与 join 的数据量。</p>
<p>关于filter 操作下压，在 Calcite 中已经有相应的 Rule 实现，就是 <code>FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN</code>，这里使用 HepPlanner 作为示例的 planer，并注册 FilterIntoJoinRule 规则进行相应的优化，其代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HepProgramBuilder builder = <span class="keyword">new</span> HepProgramBuilder();</span><br><span class="line">builder.addRuleInstance(FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN); <span class="comment">//note: 添加 rule</span></span><br><span class="line">HepPlanner hepPlanner = <span class="keyword">new</span> HepPlanner(builder.build());</span><br><span class="line">hepPlanner.setRoot(relNode);</span><br><span class="line">relNode = hepPlanner.findBestExp();</span><br></pre></td></tr></table></figure>
<p>在 Calcite 中，提供了两种 planner：HepPlanner 和 VolcanoPlanner，关于这块内容可以参考【Drill/Calcite查询优化系列】这几篇文章（讲述得非常详细，赞），这里先简单介绍一下 HepPlanner 和 VolcanoPlanner，后面会关于这两个 planner 的代码实现做深入的讲述。</p>
<h3 id="HepPlanner"><a href="#HepPlanner" class="headerlink" title="HepPlanner"></a>HepPlanner</h3><p>特点（来自 <a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite" target="_blank" rel="external">Apache Calcite介绍</a>）：</p>
<ol>
<li>HepPlanner is a heuristic optimizer similar to Spark’s optimizer，与 spark 的优化器相似，HepPlanner 是一个 heuristic 优化器；</li>
<li>Applies all matching rules until none can be applied：将会匹配所有的 rules 直到一个 rule 被满足；</li>
<li>Heuristic optimization is faster than cost- based optimization：它比 CBO 更快；</li>
<li>Risk of infinite recursion if rules make opposing changes to the plan：如果没有每次都不匹配规则，可能会有无限递归风险；</li>
</ol>
<h3 id="VolcanoPlanner"><a href="#VolcanoPlanner" class="headerlink" title="VolcanoPlanner"></a>VolcanoPlanner</h3><p>特点（来自 <a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite" target="_blank" rel="external">Apache Calcite介绍</a>）：</p>
<ol>
<li>VolcanoPlanner is a cost-based optimizer：VolcanoPlanner是一个CBO优化器；</li>
<li>Applies matching rules iteratively, selecting the plan with the cheapest cost on each iteration：迭代地应用 rules，直到找到cost最小的plan；</li>
<li>Costs are provided by relational expressions；</li>
<li>Not all possible plans can be computed：不会计算所有可能的计划；</li>
<li>Stops optimization when the cost does not significantly improve through a determinable number of iterations：根据已知的情况，如果下面的迭代不能带来提升时，这些计划将会停止优化；</li>
</ol>
<h3 id="示例运行结果"><a href="#示例运行结果" class="headerlink" title="示例运行结果"></a>示例运行结果</h3><p>经过 HepPlanner 优化后的逻辑计划为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LogicalSort(sort0=[<span class="variable">$0</span>], dir0=[ASC])</span><br><span class="line">  LogicalProject(USER_ID=[<span class="variable">$0</span>], USER_NAME=[<span class="variable">$1</span>], USER_COMPANY=[<span class="variable">$5</span>], USER_AGE=[<span class="variable">$2</span>])</span><br><span class="line">    LogicalJoin(condition=[=(<span class="variable">$1</span>, <span class="variable">$4</span>)], joinType=[inner])</span><br><span class="line">      LogicalFilter(condition=[&gt;(<span class="variable">$2</span>, 30)])</span><br><span class="line">        EnumerableTableScan(table=[[USERS]])</span><br><span class="line">      LogicalFilter(condition=[&gt;(<span class="variable">$0</span>, 10)])</span><br><span class="line">        EnumerableTableScan(table=[[JOBS]])</span><br></pre></td></tr></table></figure>
<p>可以看到优化的结果是符合我们预期的，HepPlanner 和 VolcanoPlanner 详细流程比较复杂，后面会有单独的文章进行讲述。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Calcite 本身的架构比较好理解，但是具体到代码层面就不是那么好理解了，它抛出了很多的概念，如果不把这些概念搞明白，代码基本看得也是云里雾里，特别是之前没有接触过这块内容的同学（我最开始看 Calcite 代码时是真的头大），入门的门槛确实高一些，但是当这些流程梳理清楚之后，其实再回头看，也没有多少东西，在生产中用的时候主要也是针对具体的业务场景扩展相应的 SQL 语法、进行具体的规则优化。</p>
<p>Calcite 架构设计得比较好，其中各个组件都可以单独使用，Rule（规则）扩展性很强，用户可以根据业务场景自定义相应的优化规则，它支持标准的 SQL，支持不同的存储和计算引擎，目前在业界应用也比较广泛，这也证明其牛叉之处。</p>
<blockquote>
<p>本文只是个人理解的总结，由于本人也是刚接触这块，理解有偏差的地方，欢迎指正~</p>
</blockquote>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite" target="_blank" rel="external">Apache Calcite：Hadoop 中新型大数据查询引擎</a>；</li>
<li><a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite" target="_blank" rel="external">Apache Calcite介绍</a>；</li>
<li><a href="https://arxiv.org/pdf/1802.10233.pdf" target="_blank" rel="external">Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</a>；</li>
<li><a href="http://calcite.apache.org/docs/tutorial.html" target="_blank" rel="external">Calcite Tutorial</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于 Apache Calcite 的简单介绍可以参考 &lt;a href=&quot;https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite&quot; target=&quot;_blank&quot; rel=&quot;e
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="calcite" scheme="http://matt33.com/tags/calcite/"/>
    
  </entry>
  
  <entry>
    <title>BookKeeper 原理浅谈</title>
    <link href="http://matt33.com/2019/01/28/bk-store-realize/"/>
    <id>http://matt33.com/2019/01/28/bk-store-realize/</id>
    <published>2019-01-28T15:23:35.000Z</published>
    <updated>2020-06-23T14:13:17.222Z</updated>
    
    <content type="html"><![CDATA[<p>接着之前的一篇文章 <a href="http://matt33.com/2018/10/19/bk-cluster-install-and-use/">BookKeeper 集群搭建及使用</a>，本文是 BookKeeper 系列的第二篇，短期来看应该也是最后一篇，本篇文章主要聚焦于 BookKeeper 内核的实现机制上，会从 BookKeeper 的基本概念、架构、读写一致性实现、读写分离实现、容错机制等方面来讲述，因为我并没有看过 BookKeeper 的源码，所以这里的讲述主要还是从原理、方案实现上来介绍，具体如何从解决方案落地到具体的代码实现，有兴趣的可以去看下 BookKeeper 的源码实现。</p>
<h2 id="BookKeeper-基础"><a href="#BookKeeper-基础" class="headerlink" title="BookKeeper 基础"></a>BookKeeper 基础</h2><p>正如 Apache BookKeeper 官网介绍的一样：A scalable, fault-tolerant, and low-latency storage service optimized for real-time workloads。BookKeeper 的定位是一个可用于实时场景下的高扩展性、强容错、低延迟的存储服务。<a href="https://www.slidestalk.com/s/pulsar_cloud_native_messaging_and_streaming" target="_blank" rel="external">Pulsar-Cloud Native Messaging &amp; Streaming - 示说网</a> 中也做了一个简单总结：</p>
<ol>
<li>低延迟多副本复制：Quorum Parallel Replication；</li>
<li>持久化：所有操作保证在刷盘后才 ack；</li>
<li>强一致性：可重复读的一致性（Repeatable Read Consistency）;</li>
<li>读写高可用；</li>
<li>读写分离。</li>
</ol>
<h3 id="BookKeeper-基本概念"><a href="#BookKeeper-基本概念" class="headerlink" title="BookKeeper 基本概念"></a>BookKeeper 基本概念</h3><p><a href="http://matt33.com/2018/10/19/bk-cluster-install-and-use/#BookKeeper-%E7%AE%80%E4%BB%8B">BookKeeper 简介</a> 部分已经对 BookKeeper 的基本概念做了一些讲解，这里再重新回顾一下，只有明白这些概念之后才能对更好地理解后面的内容，如下图所示，一个 Log/Stream/Topic 可以由下面的部分组成（图片来自 <a href="https://www.slidestalk.com/s/pulsar_cloud_native_messaging_and_streaming" target="_blank" rel="external">Pulsar-Cloud Native Messaging &amp; Streaming</a>）。</p>
<p><img src="/images/bookkeeper/bk5.png" alt="BookKeeper 中的基本概念"></p>
<p>其中：</p>
<ol>
<li>Ledger：它是 BK 的一个基本存储单元（本质上还是一种抽象），BK Client 的读写操作也都是以 Ledger 为粒度的；</li>
<li>Fragment：BK 的最小分布单元（实际上也是物理上的最小存储单元），也是 Ledger 的组成单位，默认情况下一个 Ledger 会对应的一个 Fragment（一个 Ledger 也可能由多个 Fragment 组成）；</li>
<li>Entry：每条日志都是一个 Entry，它代表一个 record，每条 record 都会有一个对应的 entry id；</li>
</ol>
<p>关于 Fragment，它是 Ledger 的物理组成单元，也是最小的物理存储单元，在以下两种情况下会创建新的 Fragment：</p>
<ol>
<li>当创建新的 Ledger 时；</li>
<li>当前 Fragment 使用的 Bookies 发生写入错误或超时，系统会在剩下的 Bookie 中新建 Fragment，但这时并不会新建 Ledger，因为 Ledger 的创建和关闭是由 Client 控制的，这里只是新建了 Fragment（需要注意的是：<strong>这两个 Fragment 对应的 Ensemble Bookie 已经不一样了</strong>，但它们都属于一个 Ledger，这里并不一定是一个 Ensemble Change 操作）。</li>
</ol>
<h3 id="BookKeeper-架构设计"><a href="#BookKeeper-架构设计" class="headerlink" title="BookKeeper 架构设计"></a>BookKeeper 架构设计</h3><p>Apache BookKeeper 的架构如下图所示，它主要由三个组件构成：客户端 (client)、数据存储节点 (Bookie) 和元数据存储 Service Discovery（ZooKeeper），Bookies 在启动的时候向 ZooKeeper 注册节点，Client 通过 ZooKeeper 发现可用的 Bookie。</p>
<p><img src="/images/bookkeeper/bk-articture.png" alt="Apache BookKeeper 架构"></p>
<p>这里，我们可以看到 BookKeeper 架构属于典型的 slave-slave 架构，zk 存储其集群的 meta 信息（zk 虽是单点，但 zk 目前的高可用还是很有保障的），这种模式的好处显而易见，server 端变得非常简单，所有节点都是一样的角色和处理逻辑，能够这样设计的主要原因是其副本没有 leader 和 follower 之分，这是它与一些常见 mq（如：kafka、RocketMQ）系统的典型区别，每种设计都有其 trade-off，BeekKeeper 从设计之初就是为了高可靠而设计。</p>
<h2 id="BookKeeper-存储层实现"><a href="#BookKeeper-存储层实现" class="headerlink" title="BookKeeper 存储层实现"></a>BookKeeper 存储层实现</h2><p>Apache BookKeeper 是一个高可靠的分布式存储系统，存储层的实现是其核心，对一个存储系统来说，关键的几点实现，无非是：一致性如何保证、IO 如何优化、高可用如何实现等，这小节就让我们揭开其神秘面纱。</p>
<h3 id="新建-Ledger"><a href="#新建-Ledger" class="headerlink" title="新建 Ledger"></a>新建 Ledger</h3><p>Ledger 是 BookKeeper 的基本存储抽象单元，这里先看下一个 Ledger 是如何创建的，这里会介绍一些关于 Ledger 存储层的一些重要概念（图片来自 <a href="https://www.slidestalk.com/s/pulsar_cloud_native_messaging_and_streaming" target="_blank" rel="external">Pulsar-Cloud Native Messaging &amp; Streaming</a>）。</p>
<p><img src="/images/bookkeeper/bk2.png" alt="BookKeeper Ledger 的创建"></p>
<p>Ledger 是一组追加有序的记录，它是由 Client 创建的，然后由其进行追加写操作。每个 Ledger 在创建时会被赋予全局唯一的 ID，其他的 Client 可以根据 Ledger ID，对其进行读取操作。创建 Ledger 及 Entry 写入的相关过程如下：</p>
<ol>
<li>Client 在创建 Ledger 的时候，从 Bookie Pool 里面按照指定的数据放置策略挑选出一定数量的 Bookie，构成一个 Ensemble；</li>
<li>每条 Entry 会被并行地发送给 Ensemble 里面的部分 Bookies（每条 Entry 发送多少个 Bookie 是由 Write Quorum size 设置、具体发送哪些 Bookie 是由 Round Robin 算法来计算），并且所有 Entry 的发送以流水线的方式进行，也就是意味着发送第 N + 1 条记录的写请求不需要等待发送第 N 条记录的写请求返回；</li>
<li>对于每条 Entry 的写操作而言，当它收到 Ensemble 里面大多数 Bookie 的确认后（这个由 Ack Quorum size 来设置），Client 认为这条记录已经持久化到这个 Ensemble 中，并且有大多数副本。</li>
</ol>
<p><img src="/images/bookkeeper/bk3.png" alt="BookKeeper Ledger 多副本复制"></p>
<p>这里引入了三个重要的概念，它们也是 BookKeeper 一致性的基础：</p>
<ol>
<li>Ensemble size(E)：Set of Bookies across which a ledger is striped，一个 Ledger 所涉及的 Bookie 集合；</li>
<li>Write Quorum Size（Qw）：Number of replicas，副本数；</li>
<li>Ack Quorum Size（Qa）：Number of responses needed before client’s write is satisfied。</li>
</ol>
<p>从上面 Ensemble、Qw、Qa 的概念可以得到以下这些推论：</p>
<ol>
<li>Ensemble：可以控制一个 Ledger 的读写带宽；</li>
<li>Write Quorum：控制一条记录的复本数；</li>
<li>Ack Quorum：写每条记录需要等待的 Ack 数 ，控制时延； </li>
<li>增加 Ensemble，可以增加读写带宽（增加了可写的机器数）；</li>
<li>减少 Ack Quorum，可以减长尾时延。</li>
</ol>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><p>对于分布式存储系统，为了高可用，多副本是其通用的解决方案，但也带来了一致性的问题，这里就看下 Apache BookKeeper 是如何解决其带来的一致性问题的。</p>
<h4 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h4><p>在介绍其读写一致性之前，先看下 BK 的一致性模型（图片来自 <a href="https://mp.weixin.qq.com/s/0dkgA8swNPkpcY5H6CU62w" target="_blank" rel="external">Twitter高性能分布式日志系统架构解析</a>）。</p>
<p><img src="/images/bookkeeper/bk-consistency1.png" alt="BookKeeper 一致性模型-开始时的状态"></p>
<p>对于 Write 操作而言，writer 不断添加记录，每条记录会被 writer 赋予一个严格递增的 id，所有的追加操作都是异步的，也就是说：第二条记录不用等待第一条记录返回结果。所有写成功的操作都会按照 id 递增顺序返回 ack 给 writer。（图片来自 <a href="https://mp.weixin.qq.com/s/0dkgA8swNPkpcY5H6CU62w" target="_blank" rel="external">Twitter高性能分布式日志系统架构解析</a>）。</p>
<p><img src="/images/bookkeeper/bk-consistency2.png" alt="BookKeeper 一致性模型-追加数据时的中间状态"></p>
<p>伴随着写成功的 ack，writer 不断地更新一个指针叫做 <strong>Last-Add-Confirm（LAC）</strong>，所有 Entry id 小于等于 LAC 的记录保证持久化并复制到大多数副本上，而 LAC 与 <strong>LAP（Last-Add-Pushed）</strong>之间的记录就是已经发送到 Bookie 上但还未被 ack 的数据。</p>
<h4 id="读的一致性"><a href="#读的一致性" class="headerlink" title="读的一致性"></a>读的一致性</h4><p>所有的 Reader 都可以安全读取 Entry ID 小于或者等于 LAC 的记录，从而保证 reader 不会读取未确认的数据，从而保证了 reader 之间的一致性（图片来自 <a href="https://mp.weixin.qq.com/s/0dkgA8swNPkpcY5H6CU62w" target="_blank" rel="external">Twitter高性能分布式日志系统架构解析</a>）。</p>
<p><img src="/images/bookkeeper/bk-consistency3.png" alt="BookKeeper 一致性模型-读的一致性"></p>
<h4 id="写的一致性"><a href="#写的一致性" class="headerlink" title="写的一致性"></a>写的一致性</h4><p>从上面的介绍中，也可以看出，对于 BK 的多个副本，其并没有 leader 和 follower 之分，因此，BK 并不会进行相应的选主（leader election）操作，并且限制<strong>每个 Ledger 只能被一个 Writer 写，BK 通过 Fencing 机制来防止出现多个 Writer 的状态</strong>，从而保证写的一致性。</p>
<h3 id="读写分离"><a href="#读写分离" class="headerlink" title="读写分离"></a>读写分离</h3><p>下面来看下 BK 存储层一个很重要的设计，那就是读写分离机制。在论文 <a href="https://www.deepdyve.com/lp/association-for-computing-machinery/durability-with-bookkeeper-85Q3t4l8SF" target="_blank" rel="external">Durability with BookKeeper</a> 中，关于读写分离机制的介绍如下所示（图片来自 <a href="https://www.deepdyve.com/lp/association-for-computing-machinery/durability-with-bookkeeper-85Q3t4l8SF" target="_blank" rel="external">Durability with BookKeeper</a>）：</p>
<p><img src="/images/bookkeeper/bk7.png" alt="BookKeeper 读写分离"></p>
<p>A bookie uses two devices, ideally in separate physical disks: </p>
<ol>
<li><strong>The journal device</strong> is a write-ahead log and stores synchronously and sequentially all updates the bookie executes. </li>
<li><strong>The ledger device</strong> contains an indexed copy of a ledger fragment, which a bookie uses to respond to read requests. </li>
</ol>
<p>上面是论文中关于 BK 读写分离机制实现的介绍，我当时在看完上面的记录之后，脑海中有以下疑问：</p>
<ol>
<li>一个写请求是怎么处理？什么时候数据被认为是 ack 了；</li>
<li>数据肯定先写到 Journal Device 中的，那么数据是如何到 Ledger Device 中的？</li>
<li>Ledger Device 中的顺序写跟随机读是什么意思？难道跟 RocketMQ 的存储结构一样？</li>
<li>Ledger Device 底层是怎么切分实际的物理文件的？</li>
<li>数据在什么时候才能可见？</li>
<li>在从 Ledger Device 读数据时，它是通过什么机制提高查询速度的？</li>
</ol>
<p>带着这些疑问，接下来来分析其实现（图片来自 <a href="https://www.slidestalk.com/s/pulsar_cloud_native_messaging_and_streaming" target="_blank" rel="external">Pulsar-Cloud Native Messaging &amp; Streaming</a>）：</p>
<p><img src="/images/bookkeeper/bk1.png" alt="BookKeeper 读写分离"></p>
<p>Journal Device 分析：</p>
<ul>
<li>处理写入请求时，如果 Journal 是在专用的磁盘上，由于是顺序写入刷盘，性能会很高；</li>
</ul>
<p>Ledger Device 的实现：</p>
<ul>
<li>Bookie 最初的设计方案是每个 Ledger 对应一个物理文件，但这样会极大消耗写性能，所以 Bookie 当前的设计方案是所有 Ledger 都写一个单独的文件中，这个文件又叫 entry log；</li>
<li>写入时，不但会写入到 Journal 中还会写入到缓存（memtable）中，定期会做刷盘（<strong>刷盘前会做排序</strong>，通过 <strong>聚合+排序</strong> 优化读取性能）；</li>
<li>优化查找：Ledger Device 中会维护一个索引结构，存储在 RocksDB 中，它会<strong>将 (LedgerId，EntryId) 映射到(EntryLogId，文件中的偏移量)</strong>。</li>
</ul>
<h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><p>了解完 BK 的一致性模型和读写分离机制之后，这里来看下 BK 的读写流程。</p>
<h4 id="Entry-写入流程"><a href="#Entry-写入流程" class="headerlink" title="Entry 写入流程"></a>Entry 写入流程</h4><p>这里以一个例子来说明，假设 E 是3，Qw 和 Qa 是2，那么 Entry 写入如下图（图片来自 <a href="https://www.deepdyve.com/lp/association-for-computing-machinery/durability-with-bookkeeper-85Q3t4l8SF" target="_blank" rel="external">Durability with BookKeeper</a>）：</p>
<p><img src="/images/bookkeeper/bk8.png" alt="BookKeeper Entry 写入流程"></p>
<ol>
<li>Writer 会先分配对应的 id，然后按照 round-robin 算法从3个 Bookie 中选取2个 Bookie；</li>
<li>Writer 会向两个 Bookie 发送写入请求，因为 Qa 设置为2，只有收到两个 ack 响应后，才会认为这条 Entry 写入成功；</li>
</ol>
<p><strong>如果写入过程中有一台 Bookie 挂了怎么办？</strong></p>
<ol>
<li>那么只能向另外2台 Bookie 写入数据；</li>
<li>这时候这个 Ledger 会新建一个 Fragment，假设挂的是A，之前 Ensemble 是 A、B、C，现在的是 B、C；</li>
<li>这个变化会更新到 zk 中这个 Ledger 的 meta 中。</li>
</ol>
<p><strong>如果写入过程中有两个 Bookie 挂了怎么办？</strong></p>
<ol>
<li>Ensemble 里面的存活的 Bookies 不能满足 Qw 的要求；</li>
<li>Client 会进行一个 <strong>Ensemble Change</strong> 操作；</li>
<li>Ensemble Change 将从 Bookie Pool 中根据数据放置策略挑选出额外的 Bookie 用来取代那些不存活的 Bookie 。</li>
</ol>
<h4 id="Entry-读取流程"><a href="#Entry-读取流程" class="headerlink" title="Entry 读取流程"></a>Entry 读取流程</h4><p>这里依然以一个例子做说明，例子是紧接着上面的示例，如下图所示（图片来自 <a href="https://www.deepdyve.com/lp/association-for-computing-machinery/durability-with-bookkeeper-85Q3t4l8SF" target="_blank" rel="external">Durability with BookKeeper</a>）：</p>
<p><img src="/images/bookkeeper/bk9.png" alt="BookKeeper Entry 读取流程"></p>
<p>如何想要读取 id 为1的那条 Entry 应该怎么做？</p>
<ul>
<li>在读取会选择最优的 Bookie，有了 Entry 的 id 和 Ledger 的 Ensemble 就可以根据 round-robin 计算出其所在 Bookie 信息，会选择向其中一个 Bookie 发送读请求。</li>
</ul>
<p>这种机制会导致，读取数据时可能需要从多个 Bookie 获取数据，需要并发访问多个 Bookie，性能会变差，极端情况会有这个问题。</p>
<ul>
<li>BK 有一个优化策略：读取时一般是选择读一段数据，如果 entries 在同一台机器上，会从同一个 Bookie 把这批 Entry 全部读取。</li>
</ul>
<p>BK 怎么处理长尾效应的问题（长尾效应指的是某台机器上某段或者某条数据读取得比较慢，进而影响了整体的效率）？</p>
<ul>
<li>Client 可以向任意一个副本读取相应的 Entry，但为了保证低延时，这里使用了一个叫 <strong>Speculative Read</strong> 的机制。读请求首先发送给第一个副本后，如果在指定的时间内没有收到 reponse，则发送读请求给第二个副本，然后同时等待第一个和第二个副本。谁第一个返回，即读取成功。通过有效的 Speculative read，可以很大程度减少长尾效应。</li>
</ul>
<h2 id="BookKeeper-容错机制"><a href="#BookKeeper-容错机制" class="headerlink" title="BookKeeper 容错机制"></a>BookKeeper 容错机制</h2><p>这里来简单来看下 BookKeeper 容错机制的实现。</p>
<h3 id="Fencing-机制"><a href="#Fencing-机制" class="headerlink" title="Fencing 机制"></a>Fencing 机制</h3><p>Fencing 机制在前面已经简单介绍过了，它目的主要是为了保证写的一致性，<strong>严格保证一个 Ledger 只能被一个 Writer 来写</strong>。</p>
<p>Fencing 怎么触发呢？</p>
<ul>
<li>如果一个 Writer 打开一个 Ledger，发现这个 Ledger 存在，并且没有 close，这种情况下，就会触发 Fencing 策略，并且触发 Ledger Recovery。</li>
</ul>
<h3 id="Log-Recovery-机制"><a href="#Log-Recovery-机制" class="headerlink" title="Log Recovery 机制"></a>Log Recovery 机制</h3><p>一个 Ledger 正常关闭后，会在其 Metadata 中存储 the last entry 的信息，所以正常关闭一个 Ledger 是非常重要的（<strong>Ledger 一旦关闭，其就是不可变的</strong>，读取的时候可以从任意一个 Bookie 上读取，而不需要再取 care 这个 Ledger 的 LAC 信息），否则可能会出现这样一种情况：</p>
<p>由于 Writer 挂了（Ledger 未正常关闭），导致部分数据写入成功，实际上这个条消息并不满足 Qw（可能满足了 Qa），会导致不同 Reader 读取的结果不一致！如下图所示：</p>
<p><img src="/images/bookkeeper/bk10.png" alt="不同 Reader 读取不一致的情况"></p>
<p>解决方案就是： <strong>Log Recovery</strong>，正常关闭这个 Ledger，并将 The Last Entry 及状态更新到 metadata 中。</p>
<p>Log Recovery 怎么实现呢？通常有两种方案：</p>
<ol>
<li>遍历这个 Ledger 所有 Entry 进行恢复；</li>
<li>利用 LAC 机制可以加速 recovery：恢复前，先获取每个 Ledger 的 LAC 信息，然后从 LAC 开始恢复；</li>
</ol>
<p>很明显，第二种方案是比较合理的恢复速度更快。</p>
<h3 id="Bookie-容错"><a href="#Bookie-容错" class="headerlink" title="Bookie 容错"></a>Bookie 容错</h3><p>当一个 Bookie 故障时：</p>
<ul>
<li>所有在这个 Bookie 上的 Ledgers 都处于 under-replica 状态，恢复就是复制 Fragment （Ledger 的组成单位）的过程，以确保每个 Ledger 维护的副本数打到 Qw。</li>
</ul>
<p>Bk 提供自动和手动两种方式：两种方式的复制协议是一样的；自动恢复是 BK 内部自动触发，手动过程需要手动干预，这里重点介绍自动过程：</p>
<ol>
<li>自动恢复是在 Bookie 上运行 <strong>AutoRecoveryMain 线程</strong>来实现，它会首先通过 zk 选举一个 Auditor；</li>
<li>Auditor 的作用是<strong>检查不可用的 Bookie</strong>，然后做下面的操作：读取 zk 上完整的 Ledgers 信息，找到失败的 Ledgers（副本不满足条件的）；然后在 zk 的 <code>/underreplicated</code> znode 节点创建重新复制任务；</li>
<li>AutoRecoveryMain 还有 Replicator Worker 线程会复制相应的 Fragment 到自己的 Ledger 上，如果复制后满足 Fully Replicated，那么就从 zk 的节点中删除这个任务；</li>
</ol>
<p><img src="/images/bookkeeper/bk11.png" alt="Bookie 容错机制"></p>
<p>每个 Bookie 在发现任务时会尝试锁定，如果无法锁定就会执行后面的任务。如果获得锁，那么：</p>
<ol>
<li>扫描 Ledgers，查找不属于当前 Bookie 的 Fragment；</li>
<li>对于每个匹配的 Fragment，它将另一个 Bookie 的数据复制到它自己的 Bookie，用新的集合更新 Zookeeper 并将 Fragment 标识为 Fully Replicated。</li>
</ol>
<p>如果 Ledgers 仍然存在副本数不足的 Fragment，则释放锁。如果所有 Fragment 都已经Fully Replicated，则从 <code>/underreplicated</code> 删除重复复制任务。</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>到这里，关于 BK 内核实现的主要部分已经介绍完毕，这篇文章的主要内容来自之前在团队的一次分享，一直想整理成博客的，但一直拖到了现在（因为并没有去看代码实现，主要是跟 bk 的论文及相关资料来整理的，有问题的地方欢迎指正）。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://www.deepdyve.com/lp/association-for-computing-machinery/durability-with-bookkeeper-85Q3t4l8SF" target="_blank" rel="external">Durability with BookKeeper</a>;</li>
<li><a href="https://www.slidestalk.com/s/pulsar_cloud_native_messaging_and_streaming" target="_blank" rel="external">Pulsar-Cloud Native Messaging &amp; Streaming</a>；</li>
<li><a href="https://bookkeeper.apache.org/docs/4.8.0/overview/overview/" target="_blank" rel="external">Apache BookKeeper Documentation</a>;</li>
<li><a href="https://streaml.io/blog/intro-to-bookkeeper" target="_blank" rel="external">Introduction to Apache BookKeeper</a>;</li>
<li><a href="https://streaml.io/blog/why-apache-bookkeeper" target="_blank" rel="external">Why Apache BookKeeper? Part 1: consistency, durability, availability</a>;</li>
<li><a href="https://streaml.io/blog/why-bookkeeper-part-2" target="_blank" rel="external">Why Apache Bookkeeper? Part 2</a>；</li>
<li><a href="https://jack-vanlightly.com/blog/2018/10/2/understanding-how-apache-pulsar-works?from=timeline" target="_blank" rel="external">Understanding How Apache Pulsar Works</a>；</li>
<li><a href="https://jack-vanlightly.com/blog/2018/9/10/how-to-lose-messages-on-a-rabbitmq-cluster" target="_blank" rel="external">How to Lose Messages on a RabbitMQ Cluster</a>；</li>
<li><a href="https://www.slidestalk.com/s/pulsar_cloud_native_messaging_and_streaming" target="_blank" rel="external">Pulsar-Cloud Native Messaging &amp; Streaming - 示说网</a>；</li>
<li><a href="https://mp.weixin.qq.com/s/0dkgA8swNPkpcY5H6CU62w" target="_blank" rel="external">Twitter高性能分布式日志系统架构解析</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接着之前的一篇文章 &lt;a href=&quot;http://matt33.com/2018/10/19/bk-cluster-install-and-use/&quot;&gt;BookKeeper 集群搭建及使用&lt;/a&gt;，本文是 BookKeeper 系列的第二篇，短期来看应该也是最后一篇，本篇
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="bk" scheme="http://matt33.com/tags/bk/"/>
    
  </entry>
  
  <entry>
    <title>如何高效学习</title>
    <link href="http://matt33.com/2018/11/21/effective-learning/"/>
    <id>http://matt33.com/2018/11/21/effective-learning/</id>
    <published>2018-11-21T02:12:51.000Z</published>
    <updated>2020-06-23T14:13:17.222Z</updated>
    
    <content type="html"><![CDATA[<p>在这个知识爆炸、科技日新月异的时代，技术的变化远比我们想象的要快很多，这就对工程师的要求就提高了很多，特别是对于那些在技术上有所追求的工程师而言。对于一些互联网大厂，学习能力也成了面试中重点考察的内容。如何快速学习、掌握一门新的技术，如何提高自己的学习效率，对于有一定工作经验的人来说，可能每个人都有一个自己的学习方法论，但是我们也需要去学习借鉴别人（特别是那些有一定技术影响力的技术大咖）的经验，来不断更新和完善自己的方法轮。今天这篇《高效学习》，就是与大家一起探讨技术学习的方法论，本文的内容主要来自耗子叔的《左耳听风 —— 高效学习篇》，中间会穿插个人的一些经验，算是对这个系列的一个总结。如果想看原文内容，欢迎订阅耗子叔的这个专栏，这个专栏质量还是非常高的，耗子叔推荐了很多优秀的学习资源（通过文章末尾处的二维码链接购买）。</p>
<h2 id="端正学习态度"><a href="#端正学习态度" class="headerlink" title="端正学习态度"></a>端正学习态度</h2><p>对于大多数人来说，我们并不是那种天赋异禀的天才，所以那些速成的学习方法并不适合我们，因为，<strong>对于非天才的我们来说，学习是不可能速成的</strong>，学习本来就是一件【逆人性】的事，就像锻炼身体一样，<strong>需要人持续付出，会让人感到痛苦，并随时想找理由放弃，实际上，痛苦是成长的必经阶段</strong>。</p>
<p>大部分人都认为自己热爱学习，但是有多少能真正付出实践、并一直坚持下去，能做到实践和坚持的人，一般运气都不会太差。如果我们去研究一下古今中外的成功人士，就会发现，他们基本上都是非常自律的，也都是非常热爱学习的，他们可以沉得下心来不断学习，在学习中不断地思考、探索和实践。懒，是人类的天性，如果不能克服自己 DNA 中的弱点，不能端正自己的态度，不能自律，不能坚持，不能举一反三，不能不断追问等，那么，无论多好的方法，你都不可能学好。所以，<strong>有正确的态度非常重要</strong>。</p>
<blockquote>
<p>当然只做到上面说的，并不一定能保证能够实现所谓的成功，但是完全可以让你在某个领域做到足够优秀。</p>
</blockquote>
<h3 id="主动学习和被动学习"><a href="#主动学习和被动学习" class="headerlink" title="主动学习和被动学习"></a>主动学习和被动学习</h3><p>下面这张图，大部分人应该都见过，这张图又称为学习金字塔：</p>
<p><img src="/images/share/learn.png" alt="学习效率"></p>
<p>人的学习，可以分为【被动学习】和【主动学习】两个层次：</p>
<ul>
<li><strong>被动学习</strong>：如听讲、阅读、视听、演示，学习内容的平均留存率为 5%、10%、20% 和 30%；</li>
<li><strong>主动学习</strong>：如通过讨论、实践、教授给他人，会将原来被动学习的内容留存率从 5% 提升到 50%、75%、90%。</li>
</ul>
<p>关于这个，我是深有体会的，如果我们只是看书或听一下别人的分享，不去实践，可能不到半个月，能记住 10% 的内容就不错了，我认为最好的学习方法是 <strong>实践，总结，教授给别人（要让别人听明白，教授的过程要有深度的讨论，而不是 PPT 走一遍）</strong> 。</p>
<p>过去一年多，很幸运的是，遇到了几个热爱学习的小伙伴，我们经常周末一起组织分享，每次分享只涉及很少的一块内容，分享过程中我们以讨论为主，这对分享者的能力锻炼有很好的效果（通过讨论听众也能收获很多），首先他需要自己能够理解这个问题，其次他需要把自己的理解给别人讲清楚，还需要回答其他人提出的问题（这些问题可能是分享者压根没注意的问题）。我也一直想在团队内部推广这种学习方法（这种方法人数太多的话就不太适合了），但是在团队内部去推，效果没有想象中得那么好，而且在团队内部反而很难坚持下去（大家的时间都比较有限，如果占据了别人的工作时间，别人可能需要加班才能完成自己的工作，所以大家兴趣并没有那么高昂）。相反，如果能找几个愿意一起学习的小伙伴一同学习、成长，这样反而效果好很多，如果你能找到这样的一群小伙伴，我是非常推荐这种学习方式，把自己学习的内容分享给其他人（大家一起学习、讨论这种学习效果，考虑问题的深度要比自己独自学习高出很多）。</p>
<h3 id="浅度学习和深度学习"><a href="#浅度学习和深度学习" class="headerlink" title="浅度学习和深度学习"></a>浅度学习和深度学习</h3><p><strong>学习并不是努力读更多的书，盲目追求阅读的速度和数量，这会让人产生低层次的勤奋和成长的感觉，这只是在使蛮力。要思辩，要践行，要总结和归纳，否则，你只是在机械地重复某件事，而不会有质的成长。</strong></p>
<p>在知识的领域其实也有阶层之分（类似于富人和穷人在财富方面的阶层之分，阶层的跨越非常难，但不是没有可能），那么长期在底层知识阶层的人，需要等着高层的人来喂养，他们长期陷入各种谣言和不准确的信息环境中，于是就导致错误和幼稚的认知，并习惯于哪些不费劲儿的轻度学习方式，<strong>从而一点点地丧失了深度学习的独立思考能力，从而再也没有能力打破知识阶层的限制，被困在认知底层翻不了身</strong>（就像我们经常说的，美国那些在穷人区生活的人们，他们在没有受到很好教育的前提下想突破自己的阶层，真的很难）。</p>
<p>对于知识的学习，我们应该如何进行深度学习呢？下面几点是关键：</p>
<ol>
<li><strong>高质量的信息源和第一手的知识</strong>；</li>
<li><strong>把知识连成地图，将自己的理解反述出来</strong>；</li>
<li><strong>不断地反思和思辩，与不同年龄段的人讨论</strong>：讨论、交流很多情况下，比自己看书、看代码收获要多很多；</li>
<li><strong>举一反三，并践行之，把知识转换成技能</strong>。</li>
</ol>
<p>学习有三个步骤：</p>
<ol>
<li><strong>知识采集</strong>：信息源是非常重要的，<strong>获取信息源头、破解表面信息的内在本质、多方数据印证</strong>，是这个步骤的关键；</li>
<li><strong>知识缝合</strong>：所谓缝合就是把信息组织起来，成为结构体的知识，这里，<strong>连接记忆，逻辑推理，知识梳理</strong> 是很重要的三部分；</li>
<li><strong>技能转换</strong>：通过 <strong>举一反三、实践和练习</strong>，以及<strong>教授传导</strong>，把知识转换成自己的技能，这种技能可以让你进入更高的阶层；</li>
</ol>
<h3 id="学习的目的"><a href="#学习的目的" class="headerlink" title="学习的目的"></a>学习的目的</h3><p>学习目的是什么呢？</p>
<ol>
<li><strong>学习是为了找到方法</strong>：学习不仅仅是为了找到答案，而更是为了<strong>找到方法</strong>，掌握了通往答案的路径和方法之后，便拥有了无师自通的能力；</li>
<li><strong>学习是为了找到原理</strong>：学习不仅仅是为了知道，而更是为了<strong>思考和理解</strong>（真正的学习，从来都不是轻松的，而是那种你知道得越多，你的问题就会越多，你的问题越多，你就会思考得越多，你思考得越多，你就会觉得自己直到越少，于是你就会想要了解更多，这是一种螺旋式上升上下求索的状态），一旦掌握了这些本质的东西，你就会发现，整个复杂多变的世界在变得越来越简单；</li>
<li><strong>学习是为了了解自己</strong>：学习不仅仅是为了开拓眼界，而更是为了找到自己的未知，为了了解自己，开拓眼界的目的就是为了发现自己的不足和上升空间，从而才能让自己成长；</li>
<li><strong>学习是为了改变自己</strong>：学习不仅仅是为了成长，而更是为了改变自己（改变自己的思考方式和思维方式，改变自己与生俱来的那些垃圾和低效的算法）。</li>
</ol>
<h2 id="源头、原理和知识地图"><a href="#源头、原理和知识地图" class="headerlink" title="源头、原理和知识地图"></a>源头、原理和知识地图</h2><h3 id="挑选知识和信息源"><a href="#挑选知识和信息源" class="headerlink" title="挑选知识和信息源"></a>挑选知识和信息源</h3><p>对于计算机知识来说，<strong>学习英文</strong>是是否能够成长的关键，如果我们能用 Google 英文关键词就可以找到自己想要的知识，那么我们只是算得上能跟得上这个时代，但如果能在社区里跟社区里的大牛交流得到答案，这样才算是领先于这个时代。</p>
<p>信息源应该有以下几个特质：</p>
<ol>
<li><strong>第一手的资料</strong>，不是被别人理解过、消化过的二手资料，尤其对于知识性的东西来说，更是这样；</li>
<li>应该是有佐证、有数据、有引用的，或是有权威人士或大公司生产系统背书的资料，应该是被时间和实践检验过的，或是小心求证过的，不是拍脑袋野路子或是道听途说的资料；</li>
<li>应该是加入了一些自己的经验和思考，可以引发人深思的，是所谓信息的密集很大的文章。</li>
</ol>
<p>耗子叔比较推荐 Medium 上的文章，这个上面的文章质量比较高。</p>
<h3 id="注重基础和原理"><a href="#注重基础和原理" class="headerlink" title="注重基础和原理"></a>注重基础和原理</h3><p><strong>基础知识和原理性的东西是无比重要的</strong>，无论是 JVM 还是 Node，或者是 Python 解释器里干了什么，它都无法逾越底层操作系统 API 对 『物理世界』的限制。</p>
<p>比如，当学习一门新的语言时，除了看每个语言都有的 if-else、for/while-loop、function 等东西外，还需要重点看的就是：</p>
<ul>
<li>出错处理是怎么玩的？</li>
<li>内存管理是怎么玩的？</li>
<li>数据封装和扩展是怎么玩的？</li>
<li>多态和泛型是怎么搞的？</li>
<li>运行时识别和反射是怎么玩的？</li>
<li>并发编程是怎么玩的？</li>
<li>…</li>
</ul>
<p>所以，最关键的是，<strong>这些基础知识和原理性的东西和技术，都是经历过长时间的考验的，这些基础技术也有很多人类历史上的智慧结晶，会给你很多启示和帮助</strong>（基础知识虽然很枯燥不实用、工作上用不到，学习这些知识是为了学得更快，基础打牢，学什么都快，而学得快就会学得多，学得多，就会思考得多，思考得多，就会学得更快…）。</p>
<h3 id="使用知识图"><a href="#使用知识图" class="headerlink" title="使用知识图"></a>使用知识图</h3><p>耗子叔在这里介绍一个<strong>知识图</strong>的学习方式，通过这种方式可以让我们从一个技术最重要的主干的地方开始出发遍历所有的技术细节，以 C++ 为例，分为三部分：</p>
<ol>
<li>C++ 是用来解决 C 语言问题的，那么 C 语言有什么问题呢？指针、宏、错误处理、数据拷贝…C++是用什么技术来解决这些问题的？</li>
<li>C++ 的面向对象特性：封装、继承、多态。封装，让我想起了构造函数、析构函数等。析构函数让我想起了初始化列表，想到了默认构造函数，想到了拷贝构造函数，想到了 new…多态，让我想到了虚函数，想到了 RTTI，RTTI 让我想起了 <code>dynamic_cast</code> 和 <code>typeid</code> 等；</li>
<li>C++ 的泛型编程，我想到了 <code>templete</code>，想到了操作符重载，想到了函数对象，想到了 STL，想到数据容器，想到了 iterator，想到了通用算法等等。</li>
</ol>
<p>有了这样一颗知识树之后，当出现一些不知道的知识点时，可以往这棵知识树上挂，而这样一来，也使得我们的学习更为系统和全面。</p>
<h2 id="深度、归纳和坚持实践"><a href="#深度、归纳和坚持实践" class="headerlink" title="深度、归纳和坚持实践"></a>深度、归纳和坚持实践</h2><h3 id="系统地学习"><a href="#系统地学习" class="headerlink" title="系统地学习"></a>系统地学习</h3><p>在系统性地学习一项技术时，耗子叔总结了一个<strong>学习模板</strong>，模板内容如下：</p>
<ol>
<li><strong>这个技术出现的背景、初衷和要达到什么样的目标或是要解决什么样的问题</strong>，这是这个技术的成因和目标（设计理念），也是这个技术的灵魂；</li>
<li><strong>这个技术的优势和劣势分别是什么，或者说，这个技术的 tradeoff 是什么</strong>，任何技术都有其好坏，在解决一个问题的时候，也会带来新的问题，一般来说，任何设计都有 tradeoff，所以，需要知道这个技术的优势和劣势，以及带来的挑战；</li>
<li><strong>这个技术的适用场景</strong>，要注意没有一个技术是普适的，每个技术都其特别适合的场景，所谓的场景一般分为两个：一个是业务场景，一个是技术场景；</li>
<li><strong>技术的组成部分和关键点</strong>，这是技术的核心思想，也是这个技术的灵魂所在，学习技术的核心部分是快速掌握的关键；</li>
<li><strong>技术的底层原理和关键实现</strong>，任何一个技术都有其底层的关键基础技术，学习这些关键的底层技术，可以让我们未来很快地掌握其他技术；</li>
<li><strong>已有的实现和它之间的对比</strong>，一般来说，任何一个技术都会有不同的实现，不同的实现都会有不同的侧重，学习不同的实现，可以让你得到不同的想法和思路，对于开阔思维、深入细节是非常重要的。</li>
</ol>
<h3 id="举一反三"><a href="#举一反三" class="headerlink" title="举一反三"></a>举一反三</h3><p>重点是如何才能让自己拥有举一反三的能力，在这方面，耗子叔对自己训练如下：</p>
<ol>
<li>对于一个场景，制造出各种不同的问题或难题；</li>
<li>对于一个问题，努力寻找尽可能多的解，并比较这些解的优劣；</li>
<li>对于一个解，努力寻找各种不同的测试案例，以图让其健壮。</li>
</ol>
<p>举一反三的能力，可以分解为：</p>
<ol>
<li><strong>联想能力</strong>：这种能力的锻炼需要你平时就在不停地思考同一个事物的不同的用法，或是联想与之有关的别的事物。对于软件开发和技术学习也一样；</li>
<li><strong>抽象能力</strong>：抽象能力是举一反三的基本技能。平时你解决问题的时候，如果你能对这个问题进行抽象，你就可以获得更多的表现形式。抽象能力需要找到解决问题的通用模型，比如数学就是对现实世界的一种抽象。只要我们能把现实世界的各种问题建立成数据模型（如，建立各种维度的向量），我们就可以用数学来求解，这也是机器学习的本质；</li>
<li><strong>自省能力</strong>：所谓自省能力就是自己找自己的难看。当你得到一个解的时候，要站在自己的对立面来找这个解的漏洞。有点像左右手互博。这种自己和自己辩论的能力又叫思辨能力。将自己分裂成正反方，左右方，甚至多方，站在不同的立场上来和自己辩论，从而做到不漏过一个 case，从而获得完整全面的问题分析能力。</li>
</ol>
<p>如果要获得这三种能力，除了你要很喜欢思考和找其它人来辩论或讨论以外，还要看你自己是否真的善于思考，是否有好奇心，是否喜欢打破沙锅问到底，是否喜欢关注细节，做事是否认真，是否严谨……</p>
<h3 id="总结和归纳"><a href="#总结和归纳" class="headerlink" title="总结和归纳"></a>总结和归纳</h3><p>我们把学到的东西用自己的语言和理解重新组织并表达出来，本质上是对信息进行消化和再加工的过程，这个过程可能会有信息损失，但也可能会有新信息加入，本质上是信息重构的过程。<strong>我们积累的知识越多，在知识间进行联系和区辨的能力就越强，对知识进行总结和归纳也就越轻松</strong>。而想要提高总结归纳的能力，首先要多阅读，多积累素材，扩大自己的知识面，多和别人讨论，多思辨，从而见多识广。</p>
<p>不过，我们需要注意的是，如果只学了部分知识或者还没有学透，就开始对知识进行总结归纳，那么总结归纳出来的知识结构也只能是混乱和幼稚的。因此，学习的开始阶段，可以不急于总结归纳，不急于下判断，做结论，而应该<strong>保留部分知识的不确定性，保持对知识的开放状态</strong>。当对整个知识的理解更深入，自己站的位置更高以后，总结和归纳才会更有条理。总结归纳更多是在复习中对知识的回顾和重组，而不是一边学习一边就总结归纳。</p>
<p>最后再总结一下做总结归纳的方法：<strong>把你看到和学习到的信息，归整好，排列好，关联好，总之把信息碎片给结构化掉，然后在结构化的信息中，找到规律，找到相通之处，找到共同之处，进行简化、归纳和总结，最终形成一种套路，一种模式，一种通用方法</strong>。</p>
<h3 id="实践出真知"><a href="#实践出真知" class="headerlink" title="实践出真知"></a>实践出真知</h3><p><strong>实践是很累很痛苦的事，但只有痛苦才会让人反思，而反思则是学习和改变自己的动力。Grow up through the pain，是非常有道理的。</strong></p>
<h3 id="坚持不懈"><a href="#坚持不懈" class="headerlink" title="坚持不懈"></a>坚持不懈</h3><p>坚持本来也是一件反人性的事情，关于坚持的问题，大家应该都见过很多相似的文章，总之，坚持是一件看似简单、但是完成率非常低的事情。如果想要让自己能够坚持下去，最好能够让自己处于一个<strong>正反馈的循环</strong>中，比如，学习一个技术之后，与大家去分享自己的经验，或者整理出一篇博客让其他学习，都是一种很好的学习方法。</p>
<h2 id="如何学习和阅读代码"><a href="#如何学习和阅读代码" class="headerlink" title="如何学习和阅读代码"></a>如何学习和阅读代码</h2><h3 id="读书还是读代码？"><a href="#读书还是读代码？" class="headerlink" title="读书还是读代码？"></a>读书还是读代码？</h3><p>关于书/文档和代码的关系：</p>
<ul>
<li>代码：What、How &amp; Details；</li>
<li>书/文档：What、How &amp; Why；</li>
</ul>
<p>代码是具体的实现，但是并不能告诉你为什么？<strong>书和文档是人对人说的话，代码是人对机器说的话</strong>：</p>
<ol>
<li><strong>如果想知道为什么要这么搞，应该去看书、看文档</strong>：特别当我们想了解一种思想、一种方法、一种原理、一种经验时，书和文档是最佳的方式、更有效率一些；</li>
<li><strong>如果想知道是怎么实现的，实现的细节，应该去看代码</strong>：对于具体的实现，比如：某协程的实现、某模块的性能、某个算法的实现，这时候最好的方式就是去读代码；</li>
</ol>
<p>至于从代码中收获大还是从书中收获大，不同的场景、不同的目的下，会有不同的答案，我个人对这部分的想法是：</p>
<ol>
<li>工作的前几年，更多的时候应该关注代码、关注细节的实现、多写代码（当然不是说完全不看书，书是必须要看的，特别是当有了相关实战经验之后再去看书看，效果会更好），这个阶段，Google、Stack Overflow、Github 将会是最好的学习渠道，如果在过程中，还能获得一些技术影响力，那将再好不过了；</li>
<li>有一定经验之后，这时候需要更多的【理性认识】，在这个阶段，我们的想法不再是实现某个功能，可能是想做出更牛逼的东西来，这时候应该多读那些大牛的书、与大牛交流、关注国际顶级会议的论文，应该让自己往技术 leader 这个方向发展。</li>
</ol>
<h3 id="如何阅读源代码"><a href="#如何阅读源代码" class="headerlink" title="如何阅读源代码"></a>如何阅读源代码</h3><p>关于如何阅读源代码，耗子叔分享了一些干货，我这里简单总结一下</p>
<p>首先是阅读代码之前，最好先有以下了解：</p>
<ol>
<li>基础知识：相关的语言和基础技术的知识；</li>
<li>软件功能：需要知道这个软件是做什么的、有哪些特性、哪些配置项，最好能够读一遍用户手册，然后让软件跑起来，自己先用一下感受一下；</li>
<li>相关文档：读一下相关的内部文档；</li>
<li>代码的组织结构：先简单看下源码的组织结构。</li>
</ol>
<p>接下来，就是详细地看代码的实现，这里耗子叔分享了一个源代码阅读的经验：</p>
<ol>
<li><strong>接口抽象定义</strong>：任何代码都会有很多接口或抽象定义，其描述了代码需要处理的数据结构或者业务实体，以及它们之间的关系，理清楚这些关系是非常重要的；</li>
<li><strong>模块粘合层</strong>：我们的代码有很多都是用来粘合代码的，比如中间件（middleware）、Promises 模式、回调（Callback）、代理委托、依赖注入等。这些代码模块间的粘合技术是非常重要的，因为它们会把本来平铺直述的代码给分裂开来，让你不容易看明白它们的关系；</li>
<li><strong>业务流程</strong>：这是代码运行的过程。<strong>一开始，我们不要进入细节，但需要在高层搞清楚整个业务的流程是什么样的</strong>，在这个流程中，数据是怎么被传递和处理的。一般来说，我们需要<strong>画程序流程图或者时序处理图</strong>；</li>
<li><strong>具体实现</strong>：了解上述的三个方面的内容，相信你对整个代码的框架和逻辑已经有了总体认识。这个时候，你就可以深入细节，开始阅读具体实现的代码了。对于代码的具体实现，一般来说，你需要知道下面一些事实，这样有助于你在阅读代码时找到重点。<ul>
<li><strong>代码逻辑</strong>：代码有两种逻辑，一种是<strong>业务逻辑</strong>，这种逻辑是真正的业务处理逻辑；另一种是<strong>控制逻辑</strong>，这种逻辑只是用控制程序流转的，不是业务逻辑。比如：flag 之类的控制变量，多线程处理的代码，异步控制的代码，远程通讯的代码，对象序列化反序列化的代码等。这两种逻辑你要分开，很多代码之所以混乱就是把这两种逻辑混在一起了；</li>
<li><strong>出错处理</strong>：根据 2：8 原则，20% 的代码是正常的逻辑，80% 的代码是在处理各种错误，所以，你在读代码的时候，完全可以把处理错误的代码全部删除掉，这样就会留下比较干净和简单的正常逻辑的代码。排除干扰因素，可以更高效地读代码；</li>
<li><strong>数据处理</strong>：只要你认真观察，就会发现，我们好多代码就是在那里倒腾数据。比如 DAO、DTO，比如 JSON、XML，这些代码冗长无聊，不是主要逻辑，可以不理；</li>
<li><strong>重要的算法</strong>：一般来说，我们的代码里会有很多重要的算法，我说的并不一定是什么排序或是搜索算法，可能会是一些其它的核心算法，比如一些索引表的算法，全局唯一 ID 的算法，信息推荐的算法、统计算法、通读算法（如 Gossip）等。这些比较核心的算法可能会非常难读，但它们往往是最有技术含量的部分；</li>
<li><strong>底层交互</strong>：有一些代码是和底层系统的交互，一般来说是和操作系统或是 JVM 的交互。因此，读这些代码通常需要一定的底层技术知识，不然，很难读懂；</li>
</ul>
</li>
<li><strong>运行时调试</strong>：很多时候，代码只有运行起来了，才能知道具体发生了什么事，所以，我们让代码运行进来，然后用日志也好，debug 设置断点跟踪也好。实际看一下代码的运行过程，是了解代码的一种很好的方式。</li>
</ol>
<p>总结一下，阅读代码的方法如下。</p>
<ul>
<li>一般采用自顶向下，从总体到细节的【剥洋葱皮】的读法；</li>
<li>画图是必要的，程序流程图，调用时序图，模块组织图；</li>
<li>代码逻辑归一下类，排除杂音，主要逻辑才会更清楚；</li>
<li>debug 跟踪一下代码是了解代码在执行中发生了什么的最好方式。</li>
</ul>
<h2 id="面对枯燥和量大的知识"><a href="#面对枯燥和量大的知识" class="headerlink" title="面对枯燥和量大的知识"></a>面对枯燥和量大的知识</h2><p>知识很多，在学习的时候要<strong>抓住本质，关注本质和原理</strong>，这些才是不容易改变的，是经得住时间考验的。<strong>带着问题去学习</strong>也是一种非常好的学习方式，耗子叔根据自己经验在专栏中分享以下几个 tips：</p>
<ol>
<li><strong>认真阅读文档</strong>：使用前之前看文档，跟遇到问题之后再看一遍使用文档，收获可能会完全不一样；</li>
<li><strong>用不同的方式学习同一个东西</strong>：比如，看书、听课、写博客、讲课等；</li>
<li><strong>不要被打断</strong>：被打断简直是学习天敌，保持自己注意力的集中；</li>
<li><strong>总结压缩信息</strong>：面对太多的信息时，用一个自己的【压缩算法】抓住问题的关键点；</li>
<li><strong>把未知关联到已知</strong>：把新学的知识关联到已知的事物上来；</li>
<li><strong>用教的方式学习</strong>：这种方式对自己的能力会是一个极大的提升；</li>
<li><strong>学以致用</strong>：把学到的东西用起来，在实践中深化自己的学习效果；</li>
<li><strong>不要记忆</strong>：聪明的人不会记忆知识的，他们会找方法，那些可以推导出知识或答案的方法；</li>
<li><strong>多犯错误</strong>：犯错会让你学到更多，通过错误总结教训。</li>
</ol>
<blockquote>
<p>这里有一个 TED 的演讲，<a href="https://weibo.com/tv/v/Gj1tol62s?fid=1034:3e9bbb1d315b62c5deb90e13efd09981" target="_blank" rel="external">TED演讲：只需20个小时，你就能学会任何事情！</a>，保证自己全身心投入、不受外界打扰的情况下，只要20个小时，我们就能达到这里 <a href="http://matt33.com/2018/08/01/system-learn-summary/#3-%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6">如何学习开源项目-第三步</a>，当然这20个小时要求是一个非常专注的20个小时，我还没有尝试过这种学习方法，近期准备尝试一次这种学习方法，到时候会写一篇文章来总结一下自己的经验。</p>
</blockquote>
<p>最后，以矮大紧的一句话作为结束：【时代变来变去，确实有一些新的东西，但是在这样一个时代里，有一样东西没有变，就是有这样一群人，然后我们都读了一点书，受过不错的教育，然后对自己的心灵能长出什么东西，虽然不知道具体会长什么东西，但是拒绝全部种玉米、拒绝全部长土豆，希望心里有一亩田，有一天能长出一朵不知道是什么的花。（—来自《晓说》）】（这段话好像跟文章的主题没有什么关系，但不知为何突然想到了这段话，这里就列了出来）。</p>
<p><img src="/images/share/zuoer-tingfeng.jpeg" alt="《极客时间-左耳听风》专栏，这里订阅有优惠"></p>
<hr>
<p>参考：</p>
<ul>
<li>极客时间-左耳听风-《高效学习》系列整理；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在这个知识爆炸、科技日新月异的时代，技术的变化远比我们想象的要快很多，这就对工程师的要求就提高了很多，特别是对于那些在技术上有所追求的工程师而言。对于一些互联网大厂，学习能力也成了面试中重点考察的内容。如何快速学习、掌握一门新的技术，如何提高自己的学习效率，对于有一定工作经
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="思考" scheme="http://matt33.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Exactly-Once 之事务性实现</title>
    <link href="http://matt33.com/2018/11/04/kafka-transaction/"/>
    <id>http://matt33.com/2018/11/04/kafka-transaction/</id>
    <published>2018-11-04T12:36:34.000Z</published>
    <updated>2020-06-23T14:13:17.221Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempotent Producer 不提供跨多个 Partition 和跨会话场景下的保证，因此，我们是需要一种更强的事务保证，能够原子处理多个 Partition 的写入操作，数据要么全部写入成功，要么全部失败，不期望出现中间状态。这就是 Kafka Transactions 希望解决的问题，简单来说就是能够实现 <code>atomic writes across partitions</code>，本文以 Apache Kafka 2.0.0 代码实现为例，深入分析一下 Kafka 是如何实现这一机制的。</p>
<p>Apache Kafka 在 Exactly-Once Semantics（EOS）上三种粒度的保证如下（来自 <a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="external">Exactly-once Semantics in Apache Kafka</a>）：</p>
<ol>
<li>Idempotent Producer：Exactly-once，in-order，delivery per partition；</li>
<li>Transactions：Atomic writes across partitions；</li>
<li>Exactly-Once stream processing across read-process-write tasks；</li>
</ol>
<p>第二种情况就是本文讲述的主要内容，在讲述整个事务处理流程时，也顺便分析第三种情况。</p>
<h2 id="Kafka-Transactions"><a href="#Kafka-Transactions" class="headerlink" title="Kafka Transactions"></a>Kafka Transactions</h2><p>Kafka 事务性最开始的出发点是为了在 Kafka Streams 中实现 Exactly-Once 语义的数据处理，这个问题提出之后，在真正的方案讨论阶段，社区又挖掘了更多的应用场景，也为了尽可能覆盖更多的应用场景，在真正的实现中，在很多地方做了相应的 tradeoffs，后面会写篇文章对比一下 RocketMQ 事务性的实现，就能明白 Kafka 事务性实现及应用场景的复杂性了。</p>
<p>Kafka 的事务处理，主要是允许应用可以把消费和生产的 batch 处理（涉及多个 Partition）在一个原子单元内完成，操作要么全部完成、要么全部失败。为了实现这种机制，我们需要应用能提供一个唯一 id，即使故障恢复后也不会改变，这个 id 就是 TransactionnalId（也叫 txn.id，后面会详细讲述），txn.id 可以跟内部的 PID 1:1 分配，它们不同的是 txn.id 是用户提供的，而 PID 是 Producer 内部自动生成的（并且故障恢复后这个 PID 会变化），有了 txn.id 这个机制，就可以实现多 partition、跨会话的 EOS 语义。</p>
<p>当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：</p>
<ol>
<li>跨会话的幂等性写入：即使中间故障，恢复后依然可以保持幂等性；</li>
<li>跨会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；</li>
<li>跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。</li>
</ol>
<p>上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer 端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：</p>
<ol>
<li>对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；</li>
<li>一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；</li>
<li>Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；</li>
<li>Consumer 可能没有订阅这个事务涉及的全部 Partition。</li>
</ol>
<p>简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：</p>
<ol>
<li>Atomic writes across multiple partitions.</li>
<li>All messages in a transaction are made visible together, or none are.</li>
<li>Consumers must be configured to skip uncommitted messages.</li>
</ol>
<h2 id="事务性示例"><a href="#事务性示例" class="headerlink" title="事务性示例"></a>事务性示例</h2><p>Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 <code>transactional.id</code>，通过 <code>initTransactions()</code> 初始化事务状态信息，再通过 <code>beginTransaction()</code> 标识一个事务的开始，然后通过 <code>commitTransaction()</code> 或 <code>abortTransaction()</code> 对事务进行 commit 或 abort，示例如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"client.id"</span>, <span class="string">"ProducerTranscationnalExample"</span>);</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"test-transactional"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line">producer.initTransactions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    String msg = <span class="string">"matt test"</span>;</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"0"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"1"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"2"</span>, msg.toString()));</span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException e1) &#123;</span><br><span class="line">    e1.printStackTrace();</span><br><span class="line">    producer.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e2) &#123;</span><br><span class="line">    e2.printStackTrace();</span><br><span class="line">    producer.abortTransaction();</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>
<p>事务性的 API 也同样保持了 Kafka 一直以来的简洁性，使用起来是非常方便的。</p>
<h2 id="事务性要解决的问题"><a href="#事务性要解决的问题" class="headerlink" title="事务性要解决的问题"></a>事务性要解决的问题</h2><p>回想一下，前面一篇文章中关于幂等性要解决的问题（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98">幂等性要解决的问题</a>），事务性其实更多的是解决幂等性中没有解决的问题，比如：</p>
<ol>
<li>在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），这相当于出现了中间的状态，这并不是我们期望的结果；</li>
<li>Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；</li>
</ol>
<p>再来分析一下，Kafka 提供的事务性是如何解决上面两个问题的：</p>
<ol>
<li>如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；</li>
<li>其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactly-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactly-Once（这只是设计的思想，具体的实现后续会有文章详细解揭秘）。</li>
</ol>
<h2 id="事务性实现的关键"><a href="#事务性实现的关键" class="headerlink" title="事务性实现的关键"></a>事务性实现的关键</h2><p>对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。对于一个事务操作而言，其会涉及到多个 Topic-Partition 数据的写入，如果是一个 long transaction 操作，可能会涉及到非常多的数据，如何才能保证这个事务操作的原子性（要么全部完成，要么全部失败）呢？</p>
<ol>
<li>关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，如果大家一致认为可以 commit，那么就执行 commit，否则执行 abort：<ul>
<li>我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；</li>
<li>Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；</li>
<li>这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 <strong>TransactionCoordinator</strong> 角色；</li>
</ul>
</li>
<li>有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？<ul>
<li>TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 <strong>事务状态信息如何恢复？</strong> 也就是事务的状态信息需要<strong>很强的容错性、一致性</strong>；</li>
<li>关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），那就是 <code>min.isr + ack</code> 机制；</li>
<li>分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 <code>__consumer_offset</code> 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，而对应事务数据（transaction log）就是 <code>__transaction_state</code> 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator 在做故障恢复也是从这个 topic 中恢复数据；</li>
</ul>
</li>
<li>有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：<ul>
<li>Kafka 在 Producer 端引入了一个 <strong>TransactionalId</strong> 来解决这个问题，这个 txn.id 是由应用来配置的；</li>
<li>TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方（同一个 TransactionCoordinator）进行处理；</li>
</ul>
</li>
<li>再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：<ul>
<li>这个就是<strong>事务状态转移</strong>，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态（前提是这个状态转移是有效的）。</li>
</ul>
</li>
</ol>
<blockquote>
<p>上面的分析都是个人见解，有问题欢迎指正~</p>
</blockquote>
<p>下面这节就讲述一下事务性实现的一些关键的实现机制（对这些细节不太感兴趣或者之前没有深入接触过 Kafka，可以直接跳过，直接去看下一节的事务流程处理，先去了解一下一个事务操作的主要流程步骤）。</p>
<h3 id="TransactionCoordinator"><a href="#TransactionCoordinator" class="headerlink" title="TransactionCoordinator"></a>TransactionCoordinator</h3><p>TransactionCoordinator 与 GroupCoordinator 有一些相似之处，它主要是处理来自 Transactional Producer 的一些与事务相关的请求，涉及的请求如下表所示（关于这些请求处理的详细过程会在下篇文章详细讲述，这里先有个大概的认识即可）：</p>
<table>
<thead>
<tr>
<th>请求类型</th>
<th>用途说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>ApiKeys.FIND_COORDINATOR</td>
<td>Transaction Producer 会发送这个 FindCoordinatorRequest 请求，来查询当前事务（txn.id）对应的 TransactionCoordinator，这个与 GroupCoordinator 查询类似，是根据 txn.id 的 hash 值取模找到对应 Partition 的 leader，这个 leader 就是该事务对应的 TransactionCoordinator</td>
</tr>
<tr>
<td>ApiKeys.INIT_PRODUCER_ID</td>
<td>Producer 初始化时，会发送一个 InitProducerIdRequest 请求，来获取其分配的 PID 信息，对于幂等性的 Producer，会随机选择一台 broker 发送请求，而对于 Transaction Producer 会选择向其对应的 TransactionCoordinator 发送该请求（目的是为了根据 txn.id 对应的事务状态做一些判断）</td>
</tr>
<tr>
<td>ApiKeys.ADD_PARTITIONS_TO_TXN</td>
<td>将这个事务涉及到的 topic-partition 列表添加到事务的 meta 信息中（通过 AddPartitionsToTxnRequest 请求），事务 meta 信息需要知道当前的事务操作涉及到了哪些 Topic-Partition 的写入</td>
</tr>
<tr>
<td>ApiKeys.ADD_OFFSETS_TO_TXN</td>
<td>Transaction Producer 的这个 AddOffsetsToTxnRequest 请求是由 <code>sendOffsetsToTransaction()</code> 接口触发的，它主要是用在 consume-process-produce 的场景中，这时候 consumer 也是整个事务的一部分，只有这个事务 commit 时，offset 才会被真正 commit（主要还是用于 Failover）</td>
</tr>
<tr>
<td>ApiKeys.END_TXN</td>
<td>当提交事务时， Transaction Producer 会向 TransactionCoordinator 发送一个 EndTxnRequest 请求，来 commit 或者 abort 事务</td>
</tr>
</tbody>
</table>
<p>TransactionCoordinator 对象中还有两个关键的对象，分别是:</p>
<ol>
<li>TransactionStateManager：这个对象，从名字应该就能大概明白其作用是关于事务的状态管理，它会维护分配到这个 TransactionCoordinator 的所有事务的 meta 信息；</li>
<li>TransactionMarkerChannelManager：这个主要是用于向其他的 Broker 发送 Transaction Marker 数据，关于 Transaction Marker，第一次接触的人，可能会有一些困惑，什么是 Transaction Marker，Transaction Marker 是用来解决什么问题的呢？这里先留一个疑问，后面会来解密。</li>
</ol>
<p>总结一下，TransactionCoordinator 主要的功能有三个，分别是：</p>
<ol>
<li>处理事务相关的请求；</li>
<li>维护事务的状态信息；</li>
<li>向其他 Broker 发送 Transaction Marker 数据。</li>
</ol>
<h3 id="Transaction-Log（-transaction-state）"><a href="#Transaction-Log（-transaction-state）" class="headerlink" title="Transaction Log（__transaction_state）"></a>Transaction Log（__transaction_state）</h3><p>在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 topic <code>__transaction_state</code>，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 <code>__transaction_state</code> 的 partition 数取模得到，<code>__transaction_state</code> Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 <code>__transaction_state</code> Partition 2 的 leader 来处理。</p>
<p>对于 <code>__transaction_state</code> 这个 topic 默认是由 Server 端的 <code>transaction.state.log.replication.factor</code> 参数来配置，默认是3，如果当前 leader 故障，需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？</p>
<p>正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 <code>__transaction_state</code> 中读取之前事务的日志信息，来恢复其状态信息，前提是要求事务日志写入做相应的不丢配置。这也是 <code>__transaction_state</code> 一个重要作用之一，用于 TransactionCoordinator 的恢复，<code>__transaction_state</code>  与 <code>__consumer_offsets</code> 一样是 compact 类型的 topic，其 scheme 如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Key =&gt; Version TransactionalId</span><br><span class="line">    Version =&gt; 0 (int16)</span><br><span class="line">    TransactionalId =&gt; String</span><br><span class="line"></span><br><span class="line">Value =&gt; Version ProducerId ProducerEpoch TxnTimeoutDuration TxnStatus [TxnPartitions] TxnEntryLastUpdateTime TxnStartTime</span><br><span class="line">    Version =&gt; 0 (int16)</span><br><span class="line">    ProducerId =&gt; int64</span><br><span class="line">    ProducerEpoch =&gt; int16</span><br><span class="line">    TxnTimeoutDuration =&gt; int32</span><br><span class="line">    TxnStatus =&gt; int8</span><br><span class="line">    TxnPartitions =&gt; [Topic [Partition]]</span><br><span class="line">        Topic =&gt; String</span><br><span class="line">        Partition =&gt; int32</span><br><span class="line">    TxnLastUpdateTime =&gt; int64</span><br><span class="line">    TxnStartTime =&gt; int64</span><br></pre></td></tr></table></figure>
<h3 id="Transaction-Marker"><a href="#Transaction-Marker" class="headerlink" title="Transaction Marker"></a>Transaction Marker</h3><p>终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？</p>
<p>Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）</p>
<ol>
<li>Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；</li>
<li><p>在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？</p>
<ul>
<li>这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；</li>
</ul>
<p>再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？</p>
<ol>
<li>首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，是可以共享这个 TCP 连接的；</li>
<li>有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 信息做相应的处理。</li>
</ol>
</li>
</ol>
<p>Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ControlMessageKey =&gt; Version ControlMessageType</span><br><span class="line">    Version =&gt; int16</span><br><span class="line">    ControlMessageType =&gt; int16</span><br><span class="line"></span><br><span class="line">TransactionControlMessageValue =&gt; Version CoordinatorEpoch</span><br><span class="line">    Version =&gt; int16</span><br><span class="line">    CoordinatorEpoch =&gt; int32</span><br></pre></td></tr></table></figure>
<p>这里再讲一个额外的内容，对于事务写入的数据，为了给消息添加一个标识（标识这条消息是不是来自事务写入的），<strong>数据格式（消息协议）发生了变化</strong>，这个改动主要是在 Attribute 字段，对于 MessageSet，Attribute 是16位，新的格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">| Unused (6-15) | Control (5) | Transactional (4) | Timestamp Type (3) | Compression Type (0-2) |</span><br></pre></td></tr></table></figure>
<p>对于 Message，也就是单条数据存储时（其中 Marker 数据都是单条存储的），在 Kafka 中，只有 MessageSet 才可以做压缩，所以 Message 就没必要设置压缩字段，其格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">| Unused (1-7) | Control Flag(0) |</span><br></pre></td></tr></table></figure>
<h3 id="Server-端事务状态管理"><a href="#Server-端事务状态管理" class="headerlink" title="Server 端事务状态管理"></a>Server 端事务状态管理</h3><p>TransactionCoordinator 会维护相应的事务的状态信息（也就是 TxnStatus），对于一个事务，总共有以下几种状态：</p>
<table>
<thead>
<tr>
<th>状态</th>
<th>状态码</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Empty</td>
<td>0</td>
<td>Transaction has not existed yet</td>
</tr>
<tr>
<td>Ongoing</td>
<td>1</td>
<td>Transaction has started and ongoing</td>
</tr>
<tr>
<td>PrepareCommit</td>
<td>2</td>
<td>Group is preparing to commit</td>
</tr>
<tr>
<td>PrepareAbort</td>
<td>3</td>
<td>Group is preparing to abort</td>
</tr>
<tr>
<td>CompleteCommit</td>
<td>4</td>
<td>Group has completed commit</td>
</tr>
<tr>
<td>CompleteAbort</td>
<td>5</td>
<td>Group has completed abort</td>
</tr>
<tr>
<td>Dead</td>
<td>6</td>
<td>TransactionalId has expired and is about to be removed from the transaction cache</td>
</tr>
<tr>
<td>PrepareEpochFence</td>
<td>7</td>
<td>We are in the middle of bumping the epoch and fencing out older producers</td>
</tr>
</tbody>
</table>
<p>其相应有效的状态转移图如下：</p>
<p><img src="/images/kafka/server-txn.png" alt="Server 端 Transaction 的状态转移图"></p>
<p>正常情况下，对于一个事务而言，其状态状态流程应该是 Empty –&gt; Ongoing –&gt; PrepareCommit –&gt; CompleteCommit –&gt; Empty 或者是 Empty –&gt; Ongoing –&gt; PrepareAbort –&gt; CompleteAbort –&gt; Empty。</p>
<h3 id="Client-端事务状态管理"><a href="#Client-端事务状态管理" class="headerlink" title="Client 端事务状态管理"></a>Client 端事务状态管理</h3><p>Client 的事务状态信息主要记录本地事务的状态，当然跟其他的系统类似，本地的状态信息与 Server 端的状态信息并不完全一致（状态的设置，就像 GroupCoodinator 会维护一个 Group 的状态，每个 Consumer 也会维护本地的 Consumer 对象的状态一样）。Client 端的事务状态信息主要用于 Client 端的事务状态处理，其主要有以下几种：</p>
<ol>
<li>UNINITIALIZED：Transactional Producer 初始化时的状态，此时还没有事务处理；</li>
<li>INITIALIZING：Transactional Producer 调用 <code>initTransactions()</code> 方法初始化事务相关的内容，比如发送 InitProducerIdRequest 请求；</li>
<li>READY：对于新建的事务，Transactional Producer 收到来自 TransactionCoordinator 的 InitProducerIdResponse 后，其状态会置为 READY（对于已有的事务而言，是当前事务完成后 Client 的状态会转移为 READY）；</li>
<li>IN_TRANSACTION：Transactional Producer 调用 <code>beginTransaction()</code> 方法，开始一个事务，标志着一个事务开始初始化；</li>
<li>COMMITTING_TRANSACTION：Transactional Producer 调用 <code>commitTransaction()</code> 方法时，会先更新本地的状态信息；</li>
<li>ABORTING_TRANSACTION：Transactional Producer 调用 <code>abortTransaction()</code> 方法时，会先更新本地的状态信息；</li>
<li>ABORTABLE_ERROR：在一个事务操作中，如果有数据发送失败，本地状态会转移到这个状态，之后再自动 abort 事务；</li>
<li>FATAL_ERROR：转移到这个状态之后，再进行状态转移时，会抛出异常；</li>
</ol>
<p>Client 端状态如下图：</p>
<p><img src="/images/kafka/client-txn.png" alt="Client 端 Transaction 的状态转移图"></p>
<h2 id="事务性的整体流程"><a href="#事务性的整体流程" class="headerlink" title="事务性的整体流程"></a>事务性的整体流程</h2><p>有了前面对 Kafka 事务性关键实现的讲述之后，这里详细讲述一个事务操作的处理流程，当然这里只是重点讲述事务性相关的内容，官方版的流程图可参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-DataFlow" target="_blank" rel="external">Kafka Exactly-Once Data Flow</a>，这里我做了一些改动，其流程图如下：</p>
<p><img src="/images/kafka/txn-data-flow.png" alt="consume-process-produce 事务的处理流程"></p>
<p>这个流程是以 consume-process-produce 场景为例（主要是 kafka streams 的场景），图中红虚框及 4.3a 部分是关于 consumer 的操作，去掉这部分的话，就是只考虑写入情况的场景。这种只考虑写入场景的事务操作目前在业内应用也是非常广泛的，比如 Flink + Kafka 端到端的 Exactly-Once 实现就是这种场景，下面来详细讲述一下整个流程。</p>
<h3 id="1-Finding-a-TransactionCoordinator"><a href="#1-Finding-a-TransactionCoordinator" class="headerlink" title="1. Finding a TransactionCoordinator"></a>1. Finding a TransactionCoordinator</h3><p>对于事务性的处理，第一步首先需要做的就是找到这个事务 txn.id 对应的 TransactionCoordinator，Transaction Producer 会向 Broker （随机选择一台 broker，一般选择本地连接最少的这台 broker）发送 FindCoordinatorRequest 请求，获取其 TransactionCoordinator。</p>
<p>怎么找到对应的 TransactionCoordinator 呢？这个前面已经讲过了，主要是通过下面的方法获取 <code>__transaction_state</code> 的 Partition，该 Partition 对应的 leader 就是这个 txn.id 对应的 TransactionCoordinator。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionFor</span></span>(transactionalId: <span class="type">String</span>): <span class="type">Int</span> = <span class="type">Utils</span>.abs(transactionalId.hashCode) % transactionTopicPartitionCount</span><br></pre></td></tr></table></figure>
<h3 id="2-Getting-a-PID"><a href="#2-Getting-a-PID" class="headerlink" title="2. Getting a PID"></a>2. Getting a PID</h3><p>PID 这里就不再介绍了，不了解的可以看前面那篇文章（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#PID">Producer ID</a>）。</p>
<p>Transaction Producer 在 <code>initializeTransactions()</code> 方法中会向 TransactionCoordinator 发送 InitPidRequest 请求获取其分配的 PID，有了 PID，事务写入时可以保证幂等性，PID 如何分配可以参考 <a href="http://matt33.com/2018/10/24/kafka-idempotent/#Producer-PID-%E7%94%B3%E8%AF%B7">PID 分配</a>，但是 TransactionCoordinator 在给事务 Producer 分配 PID 会做一些判断，主要的内容是：</p>
<ol>
<li>如果这个 txn.id 之前没有相应的事务状态（new txn.id），那么会初始化其事务 meta 信息 TransactionMetadata（会给其分配一个 PID，初始的 epoch 为-1），如果有事务状态，获取之前的状态；</li>
<li>校验其 TransactionMetadata 的状态信息（参考下面代码中 <code>prepareInitProduceIdTransit()</code> 方法）：<ol>
<li>如果前面还有状态转移正在进行，直接返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果此时的状态为 PrepareAbort 或 PrepareCommit，返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果之前的状态为 CompleteAbort、CompleteCommit 或 Empty，那么先将状态转移为 Empty，然后更新一下 epoch 值；</li>
<li>如果之前的状态为 Ongoing，状态会转移成 PrepareEpochFence，然后再 abort 当前的事务，并向 client 返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果状态为 Dead 或 PrepareEpochFence，直接抛出相应的 FATAL 异常；</li>
</ol>
</li>
<li>将 txn.id 与相应的 TransactionMetadata 持久化到事务日志中，对于 new txn.id，这个持久化的数据主要时 txn.id 与 pid 关系信息，如图中的 3a 所示。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: producer 启用事务性的情况下，检测此时事务的状态信息</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareInitProduceIdTransit</span></span>(transactionalId: <span class="type">String</span>,</span><br><span class="line">                                        transactionTimeoutMs: <span class="type">Int</span>,</span><br><span class="line">                                        coordinatorEpoch: <span class="type">Int</span>,</span><br><span class="line">                                        txnMetadata: <span class="type">TransactionMetadata</span>): <span class="type">ApiResult</span>[(<span class="type">Int</span>, <span class="type">TxnTransitMetadata</span>)] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (txnMetadata.pendingTransitionInProgress) &#123;</span><br><span class="line">    <span class="comment">// return a retriable exception to let the client backoff and retry</span></span><br><span class="line">    <span class="type">Left</span>(<span class="type">Errors</span>.<span class="type">CONCURRENT_TRANSACTIONS</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// caller should have synchronized on txnMetadata already</span></span><br><span class="line">    txnMetadata.state <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">PrepareAbort</span> | <span class="type">PrepareCommit</span> =&gt;</span><br><span class="line">        <span class="comment">// reply to client and let it backoff and retry</span></span><br><span class="line">        <span class="type">Left</span>(<span class="type">Errors</span>.<span class="type">CONCURRENT_TRANSACTIONS</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">CompleteAbort</span> | <span class="type">CompleteCommit</span> | <span class="type">Empty</span> =&gt; <span class="comment">//note: 此时需要将状态转移到 Empty（此时状态并没有转移，只是在 PendingState 记录了将要转移的状态）</span></span><br><span class="line">        <span class="keyword">val</span> transitMetadata = <span class="keyword">if</span> (txnMetadata.isProducerEpochExhausted) &#123;</span><br><span class="line">          <span class="keyword">val</span> newProducerId = producerIdManager.generateProducerId()</span><br><span class="line">          txnMetadata.prepareProducerIdRotation(newProducerId, transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 增加 producer 的 epoch 值</span></span><br><span class="line">          txnMetadata.prepareIncrementProducerEpoch(transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">Right</span>(coordinatorEpoch, transitMetadata)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Ongoing</span> =&gt; <span class="comment">//note: abort 当前的事务，并返回一个 CONCURRENT_TRANSACTIONS 异常，强制 client 去重试</span></span><br><span class="line">        <span class="comment">// indicate to abort the current ongoing txn first. Note that this epoch is never returned to the</span></span><br><span class="line">        <span class="comment">// user. We will abort the ongoing transaction and return CONCURRENT_TRANSACTIONS to the client.</span></span><br><span class="line">        <span class="comment">// This forces the client to retry, which will ensure that the epoch is bumped a second time. In</span></span><br><span class="line">        <span class="comment">// particular, if fencing the current producer exhausts the available epochs for the current producerId,</span></span><br><span class="line">        <span class="comment">// then when the client retries, we will generate a new producerId.</span></span><br><span class="line">        <span class="type">Right</span>(coordinatorEpoch, txnMetadata.prepareFenceProducerEpoch())</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Dead</span> | <span class="type">PrepareEpochFence</span> =&gt; <span class="comment">//note: 返回错误</span></span><br><span class="line">        <span class="keyword">val</span> errorMsg = <span class="string">s"Found transactionalId <span class="subst">$transactionalId</span> with state <span class="subst">$&#123;txnMetadata.state&#125;</span>. "</span> +</span><br><span class="line">          <span class="string">s"This is illegal as we should never have transitioned to this state."</span></span><br><span class="line">        fatal(errorMsg)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(errorMsg)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-Starting-a-Transaction"><a href="#3-Starting-a-Transaction" class="headerlink" title="3. Starting a Transaction"></a>3. Starting a Transaction</h3><p>前面两步都是 Transaction Producer 调用 <code>initTransactions()</code> 部分，到这里，Producer 可以调用 <code>beginTransaction()</code> 开始一个事务操作，其实现方法如下面所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 应该在一个事务操作之前进行调用</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    transactionManager.beginTransaction();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TransactionManager</span></span><br><span class="line"><span class="comment">//note: 在一个事务开始之前进行调用，这里实际上只是转换了状态（只在 producer 本地记录了状态的开始）</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.IN_TRANSACTION);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里只是将本地事务状态转移成 IN_TRANSACTION，并没有与 Server 端进行交互，所以在流程图中没有体现出来（TransactionManager 初始化时，其状态为 UNINITIALIZED，Producer 调用 <code>initializeTransactions()</code> 方法，其状态转移成 INITIALIZING）。</p>
<h3 id="4-Consume-Porcess-Produce-Loop"><a href="#4-Consume-Porcess-Produce-Loop" class="headerlink" title="4. Consume-Porcess-Produce Loop"></a>4. Consume-Porcess-Produce Loop</h3><p>在这个阶段，Transaction Producer 会做相应的处理，主要包括：从 consumer 拉取数据、对数据做相应的处理、通过 Producer 写入到下游系统中（对于只有写入场景，忽略前面那一步即可），下面有一个示例（start 和 end 中间的部分），是一个典型的 consume-process-produce 场景：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords records = consumer.poll(Long.MAX_VALUE);</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    <span class="comment">//start</span></span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord record : records)&#123;</span><br><span class="line">        producer.send(producerRecord(“outputTopic1”, record));</span><br><span class="line">        producer.send(producerRecord(“outputTopic2”, record));</span><br><span class="line">    &#125;</span><br><span class="line">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</span><br><span class="line">    <span class="comment">//end</span></span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面来结合前面的流程图来讲述一下这部分的实现。</p>
<h4 id="4-1-AddPartitionsToTxnRequest"><a href="#4-1-AddPartitionsToTxnRequest" class="headerlink" title="4.1. AddPartitionsToTxnRequest"></a>4.1. AddPartitionsToTxnRequest</h4><p>Producer 在调用 <code>send()</code> 方法时，Producer 会将这个对应的 Topic—Partition 添加到 TransactionManager 的记录中，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如何开启了幂等性或事务性，需要做一些处理</span></span><br><span class="line"><span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">    transactionManager.maybeAddPartitionToTransaction(tp);</span><br></pre></td></tr></table></figure>
<p>如果这个 Topic-Partition 之前不存在，那么就添加到 newPartitionsInTransaction 集合中，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 tp 添加到 newPartitionsInTransaction 中，记录当前进行事务操作的 tp</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">maybeAddPartitionToTransaction</span><span class="params">(TopicPartition topicPartition)</span> </span>&#123;</span><br><span class="line">    failIfNotReadyForSend();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 如果 partition 已经添加到 partitionsInTransaction、pendingPartitionsInTransaction、newPartitionsInTransaction中</span></span><br><span class="line">    <span class="keyword">if</span> (isPartitionAdded(topicPartition) || isPartitionPendingAdd(topicPartition))</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Begin adding new partition &#123;&#125; to transaction"</span>, topicPartition);</span><br><span class="line">    newPartitionsInTransaction.add(topicPartition);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Producer 端的 Sender 线程会将这个信息通过 AddPartitionsToTxnRequest 请求发送给 TransactionCoordinator，也就是图中的 4.1 过程，TransactionCoordinator 会将这个 Topic-Partition 列表更新到 txn.id 对应的 TransactionMetadata 中，并且会持久化到事务日志中，也就是图中的 4.1 a 部分，这里持久化的数据主要是 txn.id 与其涉及到的 Topic-Partition 信息。</p>
<h4 id="4-2-ProduceRequest"><a href="#4-2-ProduceRequest" class="headerlink" title="4.2. ProduceRequest"></a>4.2. ProduceRequest</h4><p>这一步与正常 Producer 写入基本上一样，就是相应的 Leader 在持久化数据时会在头信息中标识这条数据是不是来自事务 Producer 的写入（主要是数据协议有变动，Server 处理并不需要做额外的处理）。</p>
<h4 id="4-3-AddOffsetsToTxnRequest"><a href="#4-3-AddOffsetsToTxnRequest" class="headerlink" title="4.3. AddOffsetsToTxnRequest"></a>4.3. AddOffsetsToTxnRequest</h4><p>Producer 在调用 <code>sendOffsetsToTransaction()</code> 方法时，第一步会首先向 TransactionCoordinator 发送相应的 AddOffsetsToTxnRequest 请求，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProcducer</span></span><br><span class="line"><span class="comment">//note: 当你需要 batch 的消费-处理-写入消息，这个方法需要被使用</span></span><br><span class="line"><span class="comment">//note: 发送指定的 offset 给 group coordinator，用来标记这些 offset 是作为当前事务的一部分，只有这次事务成功时</span></span><br><span class="line"><span class="comment">//note: 这些 offset 才会被认为 commit 了</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     String consumerGroupId)</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 发送 AddOffsetsToTxRequest</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                                        String consumerGroupId)</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.IN_TRANSACTION)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Cannot send offsets to transaction either because the producer is not in an "</span> +</span><br><span class="line">                <span class="string">"active transaction"</span>);</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Begin adding offsets &#123;&#125; for consumer group &#123;&#125; to transaction"</span>, offsets, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnRequest.Builder builder = <span class="keyword">new</span> AddOffsetsToTxnRequest.Builder(transactionalId,</span><br><span class="line">            producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnHandler handler = <span class="keyword">new</span> AddOffsetsToTxnHandler(builder, offsets);</span><br><span class="line">    enqueueRequest(handler);</span><br><span class="line">    <span class="keyword">return</span> handler.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TransactionCoordinator 在收到这个请求时，处理方法与 4.1 中的一样，把这个 group.id 对应的 <code>__consumer_offsets</code> 的 Partition （与写入涉及的 Topic-Partition 一样）保存到事务对应的 meta 中，之后会持久化相应的事务日志，如图中 4.3a 所示。</p>
<h4 id="4-4-TxnOffsetsCommitRequest"><a href="#4-4-TxnOffsetsCommitRequest" class="headerlink" title="4.4. TxnOffsetsCommitRequest"></a>4.4. TxnOffsetsCommitRequest</h4><p>Producer 在收到 TransactionCoordinator 关于 AddOffsetsToTxnRequest 请求的结果后，后再次发送 TxnOffsetsCommitRequest 请求给对应的 GroupCoordinator，AddOffsetsToTxnHandler 的 <code>handleResponse()</code> 的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleResponse</span><span class="params">(AbstractResponse response)</span> </span>&#123;</span><br><span class="line">    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;</span><br><span class="line">    Errors error = addOffsetsToTxnResponse.error();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (error == Errors.NONE) &#123;</span><br><span class="line">        log.debug(<span class="string">"Successfully added partition for consumer group &#123;&#125; to transaction"</span>, builder.consumerGroupId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// note the result is not completed until the TxnOffsetCommit returns</span></span><br><span class="line">        <span class="comment">//note: AddOffsetsToTnxRequest 之后，还会再发送 TxnOffsetCommitRequest</span></span><br><span class="line">        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));</span><br><span class="line">        transactionStarted = <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123;</span><br><span class="line">        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.CONCURRENT_TRANSACTIONS) &#123;</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.INVALID_PRODUCER_EPOCH) &#123;</span><br><span class="line">        fatalError(error.exception());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">        fatalError(error.exception());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">        abortableError(<span class="keyword">new</span> GroupAuthorizationException(builder.consumerGroupId()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fatalError(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error in AddOffsetsToTxnResponse: "</span> + error.message()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>GroupCoordinator 在收到相应的请求后，会将 offset 信息持久化到 consumer offsets log 中（包含对应的 PID 信息），但是<strong>不会更新到缓存</strong>中，除非这个事务 commit 了，这样的话就可以保证这个 offset 信息对 consumer 是不可见的（没有更新到缓存中的数据是不可见的，通过接口是获取的，这是 GroupCoordinator 本身来保证的）。</p>
<h3 id="5-Committing-or-Aborting-a-Transaction"><a href="#5-Committing-or-Aborting-a-Transaction" class="headerlink" title="5.Committing or Aborting a Transaction"></a>5.Committing or Aborting a Transaction</h3><p>在一个事务操作处理完成之后，Producer 需要调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p>
<h4 id="5-1-EndTxnRequest"><a href="#5-1-EndTxnRequest" class="headerlink" title="5.1. EndTxnRequest"></a>5.1. EndTxnRequest</h4><p>无论是 Commit 还是 Abort，对于 Producer 而言，都是向 TransactionCoordinator 发送 EndTxnRequest 请求，这个请求的内容里会标识是 commit 操作还是 abort 操作，Producer 的 <code>commitTransaction()</code> 方法实现如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: commit 正在进行的事务操作，这个方法在真正发送 commit 之后将会 flush 所有未发送的数据</span></span><br><span class="line"><span class="comment">//note: 如果在发送中遇到任何一个不能修复的错误，这个方法抛出异常，事务也不会被提交，所有 send 必须成功，这个事务才能 commit 成功</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginCommit();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 开始 commit，转移本地本地保存的状态以及发送相应的请求</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">beginCommit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.COMMITTING_TRANSACTION);</span><br><span class="line">    <span class="keyword">return</span> beginCompletingTransaction(TransactionResult.COMMIT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Producer 的 <code>abortTransaction()</code> 方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 取消正在进行事务，任何没有 flush 的数据都会被丢弃</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginAbort();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">beginAbort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.ABORTABLE_ERROR)</span><br><span class="line">        maybeFailWithError();</span><br><span class="line">    transitionTo(State.ABORTING_TRANSACTION);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We're aborting the transaction, so there should be no need to add new partitions</span></span><br><span class="line">    newPartitionsInTransaction.clear();</span><br><span class="line">    <span class="keyword">return</span> beginCompletingTransaction(TransactionResult.ABORT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它们最终都是调用了 TransactionManager 的 <code>beginCompletingTransaction()</code> 方法，这个方法会向其 待发送请求列表 中添加 EndTxnRequest 请求，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 发送 EndTxnRequest 请求，添加到 pending 队列中</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> TransactionalRequestResult <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult transactionResult)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!newPartitionsInTransaction.isEmpty())</span><br><span class="line">        enqueueRequest(addPartitionsToTransactionHandler());</span><br><span class="line">    EndTxnRequest.Builder builder = <span class="keyword">new</span> EndTxnRequest.Builder(transactionalId, producerIdAndEpoch.producerId,</span><br><span class="line">            producerIdAndEpoch.epoch, transactionResult);</span><br><span class="line">    EndTxnHandler handler = <span class="keyword">new</span> EndTxnHandler(builder);</span><br><span class="line">    enqueueRequest(handler);</span><br><span class="line">    <span class="keyword">return</span> handler.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TransactionCoordinator 在收到 EndTxnRequest 请求后，会做以下处理：</p>
<ol>
<li>更新事务的 meta 信息，状态转移成 PREPARE_COMMIT 或 PREPARE_ABORT，并将事务状态信息持久化到事务日志中；</li>
<li>根据事务 meta 信息，向其涉及到的所有 Topic-Partition 的 leader 发送 Transaction Marker 信息（也就是 WriteTxnMarkerRquest 请求，见下面的 5.2 分析）；</li>
<li>最后将事务状态更新为 COMMIT 或者 ABORT，并将事务的 meta 持久化到事务日志中，也就是 5.3 步骤。</li>
</ol>
<h4 id="5-2-WriteTxnMarkerRquest"><a href="#5-2-WriteTxnMarkerRquest" class="headerlink" title="5.2. WriteTxnMarkerRquest"></a>5.2. WriteTxnMarkerRquest</h4><p>WriteTxnMarkerRquest 是 TransactionCoordinator 收到 Producer 的 EndTxnRequest 请求后向其他 Broker 发送的请求，主要是告诉它们事务已经完成。不论是普通的 Topic-Partition 还是 <code>__consumer_offsets</code>，在收到这个请求后，都会把事务结果（Transaction Marker 的格数据式见前面）持久化到对应的日志文件中，这样下游 Consumer 在消费这个数据时，就知道这个事务是 commit 还是 abort。</p>
<h4 id="5-3-Writing-the-Final-Commit-or-Abort-Message"><a href="#5-3-Writing-the-Final-Commit-or-Abort-Message" class="headerlink" title="5.3. Writing the Final Commit or Abort Message"></a>5.3. Writing the Final Commit or Abort Message</h4><p>当这个事务涉及到所有 Topic-Partition 都已经把这个 marker 信息持久化到日志文件之后，TransactionCoordinator 会将这个事务的状态置为 COMMIT 或 ABORT，并持久化到事务日志文件中，到这里，这个事务操作就算真正完成了，TransactionCoordinator 缓存的很多关于这个事务的数据可以被清除了。</p>
<h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>在上面讲述完 Kafka 事务性处理之后，我们来思考一下以下这些问题，上面的流程可能会出现下面这些问题或者很多人可能会有下面的疑问：</p>
<ol>
<li>txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？</li>
<li>TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？</li>
<li>对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？</li>
<li>对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？</li>
<li>如果 txn.id 长期不使用，server 端怎么处理？</li>
<li>PID Snapshot 是做什么的？是用来解决什么问题？</li>
</ol>
<p>下面，来详细分析一下上面提到的这些问题。</p>
<h3 id="如果多个-Producer-使用同一个-txn-id-会出现什么情况？"><a href="#如果多个-Producer-使用同一个-txn-id-会出现什么情况？" class="headerlink" title="如果多个 Producer 使用同一个 txn.id 会出现什么情况？"></a>如果多个 Producer 使用同一个 txn.id 会出现什么情况？</h3><p>对于这个情况，我们这里直接做了一个相应的实验，两个 Producer 示例都使用了同一个 txn.id（为 test-transactional-matt），Producer 1 先启动，然后过一会再启动 Producer 2，这时候会发现一个现象，那就是 Producer 1 进程会抛出异常退出进程，其异常信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are <span class="keyword">in</span> an error state</span><br><span class="line">	at org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:784)</span><br><span class="line">	at org.apache.kafka.clients.producer.internals.TransactionManager.beginTransaction(TransactionManager.java:215)</span><br><span class="line">	at org.apache.kafka.clients.producer.KafkaProducer.beginTransaction(KafkaProducer.java:606)</span><br><span class="line">	at com.matt.test.kafka.producer.ProducerTransactionExample.main(ProducerTransactionExample.java:68)</span><br><span class="line">Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></span><br></pre></td></tr></table></figure>
<p>这里抛出了 ProducerFencedException 异常，如果打开相应的 Debug 日志，在 Producer 1 的日志文件会看到下面的日志信息</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[2018-11-03 12:48:52,495] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Transition from state COMMITTING_TRANSACTION to error state FATAL_ERROR (org.apache.kafka.clients.producer.internals.TransactionManager)</span><br><span class="line">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></span><br><span class="line"><span class="string">[2018-11-03 12:48:52,498] ERROR [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)</span></span><br><span class="line"><span class="string">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer'</span>s transaction has been expired by the broker.</span><br><span class="line">[2018-11-03 12:48:52,599] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[2018-11-03 12:48:52,599] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Beginning shutdown of Kafka producer I/O thread, sending remaining records. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[2018-11-03 12:48:52,601] DEBUG Removed sensor with name connections-closed: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,601] DEBUG Removed sensor with name connections-created: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name successful-authentication: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name failed-authentication: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name bytes-sent-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,603] DEBUG Removed sensor with name bytes-sent: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,603] DEBUG Removed sensor with name bytes-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name select-time: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name io-time: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name node--1.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node--1.bytes-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node--1.latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node-33.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-33.bytes-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-33.latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.bytes-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,607] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Shutdown of Kafka producer I/O thread has completed. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[2018-11-03 12:48:52,607] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[2018-11-03 12:48:52,808] ERROR Forcing producer close! (com.matt.test.kafka.producer.ProducerTransactionExample)</span><br><span class="line">[2018-11-03 12:48:52,808] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[2018-11-03 12:48:52,808] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br></pre></td></tr></table></figure>
<p>Producer 1 本地事务状态从 COMMITTING_TRANSACTION 变成了 FATAL_ERROR 状态，导致 Producer 进程直接退出了，出现这个异常的原因，就是抛出的 ProducerFencedException 异常，简单来说 Producer 1 被 Fencing 了（这是 Producer Fencing 的情况）。因此，这个问题的答案就很清除了，如果多个 Producer 共用一个 txn.id，那么最后启动的 Producer 会成功运行，会它之前启动的 Producer 都 Fencing 掉（至于为什么会 Fencing 下一小节会做分析）。</p>
<h3 id="Fencing"><a href="#Fencing" class="headerlink" title="Fencing"></a>Fencing</h3><p>关于 Fencing 这个机制，在分布式系统还是很常见的，我第一个见到这个机制是在 HDFS 中，可以参考我之前总结的一篇文章 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98">HDFS NN 脑裂问题</a>，Fencing 机制解决的主要也是这种类型的问题 —— 脑裂问题，简单来说就是，本来系统这个组件在某个时刻应该只有一个处于 active 状态的，但是在实际生产环境中，特别是切换期间，可能会同时出现两个组件处于 active 状态，这就是脑裂问题，在 Kafka 的事务场景下，用到 Fencing 机制有两个地方：</p>
<ol>
<li>TransactionCoordinator Fencing；</li>
<li>Producer Fencing；</li>
</ol>
<h4 id="TransactionCoordinator-Fencing"><a href="#TransactionCoordinator-Fencing" class="headerlink" title="TransactionCoordinator Fencing"></a>TransactionCoordinator Fencing</h4><p>TransactionCoordinator 在遇到上 long FGC 时，可能会导致 脑裂 问题，FGC 时会 stop-the-world，这时候可能会与 zk 连接超时导致临时节点消失进而触发 leader 选举，如果 <code>__transaction_state</code> 发生了 leader 选举，TransactionCoordinator 就会切换，如果此时旧的 TransactionCoordinator FGC 完成，在还没来得及同步到最细 meta 之前，会有一个短暂的时刻，对于一个 txn.id 而言就是这个时刻可能出现了两个 TransactionCoordinator。</p>
<p>相应的解决方案就是 TransactionCoordinator Fencing，这里 Fencing 策略不像离线场景 HDFS 这种直接 Kill 旧的 NN 进程或者强制切换状态这么暴力，而是通过 CoordinatorEpoch 来判断，每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 <code>__transaction_state</code> Partition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。</p>
<p>明白了 TransactionCoordinator 脑裂问题发生情况及解决方案之后，来分析下，Fencing 机制会在哪里发挥作用？仔细想想，是可以推断出来的，只可能是 TransactionCoordinator 向别人发请求时影响才会比较严重（特别是乱发 admin 命令）。有了 CoordinatorEpoch 之后，其他 Server 在收到请求时做相应的判断，如果发现 CoordinatorEpoch 值比缓存的最新的值小，那么 Fencing 就生效，拒绝这个请求，也就是 TransactionCoordinator 发送 WriteTxnMarkerRequest 时可能会触发这一机制。</p>
<h4 id="Producer-Fencing"><a href="#Producer-Fencing" class="headerlink" title="Producer Fencing"></a>Producer Fencing</h4><p>Producer Fencing 与前面的类似，如果对于相同 PID 和 txn.id 的 Producer，Server 端会记录最新的 Epoch 值，拒绝来自 zombie Producer （Epoch 值小的 Producer）的请求。前面第一个问题的情况，Producer 2 在启动时，会向 TransactionCoordinator 发送 InitPIDRequest 请求，此时 TransactionCoordinator 已经有了这个 txn.id 对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样 Producer 2 就被认为是最新的 Producer，而 Producer 1 就会被认为是 zombie Producer，因此，TransactionCoordinator 在处理 Producer 1 的事务请求时，会返回相应的异常信息。</p>
<h3 id="Consumer-端如何消费事务数据"><a href="#Consumer-端如何消费事务数据" class="headerlink" title="Consumer 端如何消费事务数据"></a>Consumer 端如何消费事务数据</h3><p>在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 <code>isolation.level</code> 配置，这个是配置对于事务性数据的消费策略，有以下两种可选配置：</p>
<ol>
<li><code>read_committed</code>: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.</li>
<li><code>read_uncommitted</code>: consume all available messages in offset ordering. This is the <strong>default value</strong>.</li>
</ol>
<p>简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），这部分的难点在于 read_committed 机制的实现。</p>
<h4 id="Last-Stable-Offset（LSO）"><a href="#Last-Stable-Offset（LSO）" class="headerlink" title="Last Stable Offset（LSO）"></a>Last Stable Offset（LSO）</h4><p>在事务机制的实现中，Kafka 又设置了一个新的 offset 概念，那就是 Last Stable Offset，简称 LSO（其他的 Offset 概念可参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Kafka Offset 那些事</a>），先看下 LSO 的定义：</p>
<blockquote>
<p>The LSO is defined as the latest offset such that the status of all transactional messages at lower offsets have been determined (i.e. committed or aborted).</p>
</blockquote>
<p>对于一个 Partition 而言，offset 小于 LSO 的数据，全都是已经确定的数据，这个主要是对于事务操作而言，在这个 offset 之前的事务操作都是已经完成的事务（已经 commit 或 abort），如果这个 Partition 没有涉及到事务数据，那么 LSO 就是其 HW（水位）。</p>
<h4 id="Server-处理-read-committed-类型的-Fetch-请求"><a href="#Server-处理-read-committed-类型的-Fetch-请求" class="headerlink" title="Server 处理 read_committed 类型的 Fetch 请求"></a>Server 处理 read_committed 类型的 Fetch 请求</h4><p>如果 Consumer 的消费策略设置的是 read_committed，其在向 Server 发送 Fetch 请求时，Server 端<strong>只会返回 LSO 之前的数据</strong>，在 LSO 之后的数据不会返回。</p>
<p>这种机制有没有什么问题呢？我现在能想到的就是如果有一个 long transaction，比如其 first offset 是 1000，另外有几个已经完成的小事务操作，比如：txn1（offset：1100~1200）、txn2（offset：1400~1500），假设此时的 LSO 是 1000，也就是说这个 long transaction 还没有完成，那么已经完成的 txn1、txn2 也会对 consumer 不可见（假设都是 commit 操作），此时<strong>受 long transaction 的影响可能会导致数据有延迟</strong>。</p>
<p>那么我们再来想一下，如果不设计 LSO，又会有什么问题呢？可能分两种情况：</p>
<ol>
<li>允许读未完成的事务：那么 Consumer 可以直接读取到 Partition 的 HW 位置，对于未完成的事务，因为设置的是 read_committed 机制，所以不能对用户可见，需要在 Consumer 端做缓存，这个缓存应该设置多大？（不限制肯定会出现 OOM 的情况，当然也可以现在 client 端持久化到硬盘，这样的设计太过于复杂，还需要考虑 client 端 IO、磁盘故障等风险），明显这种设计方案是不可行的；</li>
<li>如果不允许读未完成的事务：相当于还是在 Server 端处理，与前面的区别是，这里需要先把示例中的 txn1、txn2 的数据发送给 Consumer，这样的设计会带来什么问题呢？<ol>
<li>假设这个 long transaction commit 了，其 end offset 是 2000，这时候有两种方案：第一种是把 1000-2000 的数据全部读出来（可能是磁盘读），把这个 long transaction 的数据过滤出来返回给 Consumer；第二种是随机读，只读这个 long transaction 的数据，无论哪种都有多触发一次磁盘读的风险，可能影响影响 Server 端的性能；</li>
<li>Server 端需要维护每个 consumer group 有哪些事务读了、哪些事务没读的 meta 信息，因为 consumer 是随机可能挂掉，需要接上次消费的，这样实现就复杂很多了；</li>
<li>还有一个问题是，消费的顺序性无法保证，两次消费其读取到的数据顺序可能是不同的（两次消费启动时间不一样）；</li>
</ol>
</li>
</ol>
<p>从这些分析来看，个人认为 LSO 机制还是一种相当来说 实现起来比较简单、而且不影响原来 server 端性能、还能保证顺序性的一种设计方案，它不一定是最好的，但也不会差太多。在实际的生产场景中，尽量避免 long transaction 这种操作，而且 long transaction可能也会容易触发事务超时。</p>
<h4 id="Consumer-如何过滤-abort-的事务数据"><a href="#Consumer-如何过滤-abort-的事务数据" class="headerlink" title="Consumer 如何过滤 abort 的事务数据"></a>Consumer 如何过滤 abort 的事务数据</h4><p>Consumer 在拉取到相应的数据之后，后面该怎么处理呢？它拉取到的这批数据并不能保证都是完整的事务数据，很有可能是拉取到一个事务的部分数据（marker 数据还没有拉取到），这时候应该怎么办？难道 Consumer 先把这部分数据缓存下来，等后面的 marker 数据到来时再确认数据应该不应该丢弃？（还是又 OOM 的风险）有没有更好的实现方案？</p>
<p>Kafka 的设计总是不会让我们失望，这部分做的优化也是非常高明，Broker 会追踪每个 Partition 涉及到的 abort transactions，Partition 的每个 log segment 都会有一个单独只写的文件（append-only file）来存储 abort transaction 信息，因为 abort transaction 并不是很多，所以这个开销是可以可以接受的，之所以要持久化到磁盘，主要是为了故障后快速恢复，要不然 Broker 需要把这个 Partition 的所有数据都读一遍，才能直到哪些事务是 abort 的，这样的话，开销太大（如果这个 Partition 没有事务操作，就不会生成这个文件）。这个持久化的文件是以 <code>.txnindex</code> 做后缀，前面依然是这个 log segment 的 offset 信息，存储的数据格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TransactionEntry =&gt;</span><br><span class="line">    Version =&gt; int16</span><br><span class="line">    PID =&gt; int64</span><br><span class="line">    FirstOffset =&gt; int64</span><br><span class="line">    LastOffset =&gt; int64</span><br><span class="line">    LastStableOffset =&gt; int64</span><br></pre></td></tr></table></figure>
<p>有了这个设计，Consumer 在拉取数据时，Broker 会把这批数据涉及到的所有 abort transaction 信息都返回给 Consumer，Server 端会根据拉取的 offset 范围与 abort transaction 的 offset 做对比，返回涉及到的 abort transaction 集合，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAbortedTxns</span></span>(fetchOffset: <span class="type">Long</span>, upperBoundOffset: <span class="type">Long</span>): <span class="type">TxnIndexSearchResult</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> abortedTransactions = <span class="type">ListBuffer</span>.empty[<span class="type">AbortedTxn</span>]</span><br><span class="line">  <span class="keyword">for</span> ((abortedTxn, _) &lt;- iterator()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastOffset &gt;= fetchOffset &amp;&amp; abortedTxn.firstOffset &lt; upperBoundOffset)</span><br><span class="line">      abortedTransactions += abortedTxn <span class="comment">//note: 这个 abort 的事务有在在这个范围内，就返回</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastStableOffset &gt;= upperBoundOffset)</span><br><span class="line">      <span class="keyword">return</span> <span class="type">TxnIndexSearchResult</span>(abortedTransactions.toList, isComplete = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">TxnIndexSearchResult</span>(abortedTransactions.toList, isComplete = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Consumer 在拿到这些数据之后，会进行相应的过滤，大概的判断逻辑如下（Server 端返回的 abort transaction 列表就保存在 <code>abortedTransactions</code>  集合中，<code>abortedProducerIds</code>  最开始时是为空的）：</p>
<ol>
<li>如果这个数据是 control msg（也即是 marker 数据），是 ABORT 的话，那么与这个事务相关的 PID 信息从 <code>abortedProducerIds</code> 集合删掉，是 COMMIT 的话，就忽略（每个这个 PID 对应的 marker 数据收到之后，就从 <code>abortedProducerIds</code> 中清除这个 PID 信息）；</li>
<li>如果这个数据是正常的数据，把它的 PID 和 offset 信息与 <code>abortedTransactions</code> 队列（有序队列，头部 transaction 的 first offset 最小）第一个 transaction 做比较，如果 PID 相同，并且 offset 大于等于这个 transaction 的 first offset，就将这个 PID 信息添加到 <code>abortedProducerIds</code> 集合中，同时从 <code>abortedTransactions</code> 队列中删除这个 transaction，最后再丢掉这个 batch（它是 abort transaction 的数据）；</li>
<li>检查这个 batch 的 PID 是否在 <code>abortedProducerIds</code> 集合中，在的话，就丢弃，不在的话就返回上层应用。</li>
</ol>
<p>这部分的实现确实有些绕（有兴趣的可以慢慢咀嚼一下），它严重依赖了 Kafka 提供的下面两种保证：</p>
<ol>
<li>Consumer 拉取到的数据，在处理时，其 offset 是严格有序的；</li>
<li>同一个 txn.id（PID 相同）在某一个时刻最多只能有一个事务正在进行；</li>
</ol>
<p>这部分代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Record <span class="title">nextFetchedRecord</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (records == <span class="keyword">null</span> || !records.hasNext()) &#123; <span class="comment">//note: records 为空（数据全部丢掉了），records 没有数据（是 control msg）</span></span><br><span class="line">            maybeCloseRecordStream();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!batches.hasNext()) &#123;</span><br><span class="line">                <span class="comment">// Message format v2 preserves the last offset in a batch even if the last record is removed</span></span><br><span class="line">                <span class="comment">// through compaction. By using the next offset computed from the last offset in the batch,</span></span><br><span class="line">                <span class="comment">// we ensure that the offset of the next fetch will point to the next batch, which avoids</span></span><br><span class="line">                <span class="comment">// unnecessary re-fetching of the same batch (in the worst case, the consumer could get stuck</span></span><br><span class="line">                <span class="comment">// fetching the same batch repeatedly).</span></span><br><span class="line">                <span class="keyword">if</span> (currentBatch != <span class="keyword">null</span>)</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                drain();</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            currentBatch = batches.next();</span><br><span class="line">            maybeEnsureValid(currentBatch);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isolationLevel == IsolationLevel.READ_COMMITTED &amp;&amp; currentBatch.hasProducerId()) &#123;</span><br><span class="line">                <span class="comment">//note: 需要做相应的判断</span></span><br><span class="line">                <span class="comment">// remove from the aborted transaction queue all aborted transactions which have begun</span></span><br><span class="line">                <span class="comment">// before the current batch's last offset and add the associated producerIds to the</span></span><br><span class="line">                <span class="comment">// aborted producer set</span></span><br><span class="line">                <span class="comment">//note: 如果这个 batch 的 offset 已经大于等于 abortedTransactions 中第一事务的 first offset</span></span><br><span class="line">                <span class="comment">//note: 那就证明下个 abort transaction 的数据已经开始到来，将 PID 添加到 abortedProducerIds 中</span></span><br><span class="line">                consumeAbortedTransactionsUpTo(currentBatch.lastOffset());</span><br><span class="line"></span><br><span class="line">                <span class="keyword">long</span> producerId = currentBatch.producerId();</span><br><span class="line">                <span class="keyword">if</span> (containsAbortMarker(currentBatch)) &#123;</span><br><span class="line">                    abortedProducerIds.remove(producerId); <span class="comment">//note: 这个 PID（当前事务）涉及到的数据已经处理完</span></span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isBatchAborted(currentBatch)) &#123; <span class="comment">//note: 丢弃这个数据</span></span><br><span class="line">                    log.debug(<span class="string">"Skipping aborted record batch from partition &#123;&#125; with producerId &#123;&#125; and "</span> +</span><br><span class="line">                                  <span class="string">"offsets &#123;&#125; to &#123;&#125;"</span>,</span><br><span class="line">                              partition, producerId, currentBatch.baseOffset(), currentBatch.lastOffset());</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            records = currentBatch.streamingIterator(decompressionBufferSupplier);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            Record record = records.next();</span><br><span class="line">            <span class="comment">// skip any records out of range</span></span><br><span class="line">            <span class="keyword">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</span><br><span class="line">                <span class="comment">// we only do validation when the message should not be skipped.</span></span><br><span class="line">                maybeEnsureValid(record);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// control records are not returned to the user</span></span><br><span class="line">                <span class="keyword">if</span> (!currentBatch.isControlBatch()) &#123; <span class="comment">//note: 过滤掉 marker 数据</span></span><br><span class="line">                    <span class="keyword">return</span> record;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// Increment the next fetch offset when we skip a control batch.</span></span><br><span class="line">                    nextFetchOffset = record.offset() + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Consumer-消费数据时，其顺序如何保证"><a href="#Consumer-消费数据时，其顺序如何保证" class="headerlink" title="Consumer 消费数据时，其顺序如何保证"></a>Consumer 消费数据时，其顺序如何保证</h3><p>有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。</p>
<h3 id="如果-txn-id-长期不使用，server-端怎么处理？"><a href="#如果-txn-id-长期不使用，server-端怎么处理？" class="headerlink" title="如果 txn.id 长期不使用，server 端怎么处理？"></a>如果 txn.id 长期不使用，server 端怎么处理？</h3><p>Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 <code>transaction.timeout.ms</code>，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间（参数是 <code>transaction.timeout.ms</code>，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。</p>
<p>上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，Server 端提供了一个 <code>transaction.id.expiration.ms</code> 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，那么 TransactionCoordinator 将会使这个 txn.id 过期。</p>
<h3 id="PID-Snapshot-是做什么的？用来解决什么问题？"><a href="#PID-Snapshot-是做什么的？用来解决什么问题？" class="headerlink" title="PID Snapshot 是做什么的？用来解决什么问题？"></a>PID Snapshot 是做什么的？用来解决什么问题？</h3><p>对于每个 Topic-Partition，Broker 都会在内存中维护其 PID 与 sequence number（最后成功写入的 msg 的 sequence number）的对应关系（这个在上面幂等性文章应讲述过，主要是为了不丢补充的实现）。</p>
<p>Broker 重启时，如果想恢复上面的状态信息，那么它读取所有的 log 文件。相比于之下，定期对这个 state 信息做 checkpoint（Snapshot），明显收益是非常大的，此时如果 Broker 重启，只需要读取最近一个 Snapshot 文件，之后的数据再从 log 文件中恢复即可。</p>
<p>这个 PID Snapshot 样式如 00000000000235947656.snapshot，以 <code>.snapshot</code> 作为后缀，其数据格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX-35 app.matt_test_transaction_json_3-2]$ /usr/<span class="built_in">local</span>/java18/bin/java -Djava.ext.dirs=/XXX/kafka/libs kafka.tools.DumpLogSegments --files 00000000000235947656.snapshot</span><br><span class="line">Dumping 00000000000235947656.snapshot</span><br><span class="line">producerId: 2000 producerEpoch: 1 coordinatorEpoch: 4 currentTxnFirstOffset: None firstSequence: 95769510 lastSequence: 95769511 lastOffset: 235947654 offsetDelta: 1 timestamp: 1541325156503</span><br><span class="line">producerId: 3000 producerEpoch: 5 coordinatorEpoch: 6 currentTxnFirstOffset: None firstSequence: 91669662 lastSequence: 91669666 lastOffset: 235947651 offsetDelta: 4 timestamp: 1541325156454</span><br></pre></td></tr></table></figure>
<p>在实际的使用中，这个 snapshot 文件一般只会保存最近的两个文件。</p>
<h3 id="中间流程故障如何恢复"><a href="#中间流程故障如何恢复" class="headerlink" title="中间流程故障如何恢复"></a>中间流程故障如何恢复</h3><p>对于上面所讲述的一个事务操作流程，实际生产环境中，任何一个地方都有可能出现的失败：</p>
<ol>
<li>Producer 在发送 <code>beginTransaction()</code> 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li>
<li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li>
<li>Producer 发送 <code>commitTransaction()</code> 时出现 timeout 或者错误：Producer 应该重试这个请求；</li>
<li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li>
</ol>
<p>陆陆续续写了几天，终于把这篇文章总结完了。</p>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="external">Idempotent Producer</a>；</li>
<li><a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="external">Exactly-once Semantics in Apache Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka" target="_blank" rel="external">Transactional Messaging in Kafka</a>；</li>
<li><a href="https://www.confluent.io/blog/transactions-apache-kafka/" target="_blank" rel="external">Transactions in Apache Kafka</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempo
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 事务性之幂等性实现</title>
    <link href="http://matt33.com/2018/10/24/kafka-idempotent/"/>
    <id>http://matt33.com/2018/10/24/kafka-idempotent/</id>
    <published>2018-10-24T06:11:25.000Z</published>
    <updated>2020-06-23T14:13:17.221Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 Kafka 事务性系列的文章我们只重点关注前两种层面上的事务性，与 Kafka Streams 相关的内容暂时不做讨论。社区从开始讨论事务性，前后持续近半年时间，相关的设计文档有六十几页（参考 <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>）。事务性这部分的实现也是非常复杂的，之前 Producer 端的代码实现其实是非常简单的，增加事务性的逻辑之后，这部分代码复杂度提高了很多，本篇及后面几篇关于事务性的文章会以 2.0.0 版的代码实现为例，对这部分做了一下分析，计划分为五篇文章：</p>
<ol>
<li>第一篇：Kafka 幂等性实现；</li>
<li>第二篇：Kafka 事务性实现；</li>
<li>第三篇：Kafka 事务性相关处理请求在 Server 端如何处理及其实现细节；</li>
<li>第四篇：关于 Kafka 事务性实现的一些思考，也会简单介绍一下 RocketMQ 事务性的实现，做一下对比；</li>
<li>第五篇：Flink + Kafka 如何实现 Exactly Once；</li>
</ol>
<p>这篇是 Kafka 事务性系列的第一篇文章，主要讲述幂等性实现的整体流程，幂等性的实现相对于事务性的实现简单很多，也是事务性实现的基础。</p>
<h2 id="Producer-幂等性"><a href="#Producer-幂等性" class="headerlink" title="Producer 幂等性"></a>Producer 幂等性</h2><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p>
<ul>
<li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li>
<li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li>
</ul>
<p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p>
<h2 id="幂等性示例"><a href="#幂等性示例" class="headerlink" title="幂等性示例"></a>幂等性示例</h2><p>Producer 使用幂等性的示例非常简单，与正常情况下 Producer 使用相比变化不大，只需要把 Producer 的配置 enable.idempotence 设置为 true 即可，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>); <span class="comment">// 当 enable.idempotence 为 true，这里默认为 all</span></span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line"></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"test"</span>);</span><br></pre></td></tr></table></figure>
<p>Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好。</p>
<h2 id="幂等性要解决的问题"><a href="#幂等性要解决的问题" class="headerlink" title="幂等性要解决的问题"></a>幂等性要解决的问题</h2><p>在看 Producer 是如何实现幂等性之前，首先先考虑一个问题：<strong>幂等性是来解决什么问题的？</strong> 在 0.11.0 之前，Kafka 通过 Producer 端和 Server 端的相关配置可以做到<strong>数据不丢</strong>，也就是 at least once，但是在一些情况下，可能会导致数据重复，比如：网络请求延迟等导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。</p>
<p>对于大多数应用而言，数据保证不丢是可以满足其需求的，但是对于一些其他的应用场景（比如支付数据等），它们是要求精确计数的，这时候如果上游数据有重复，下游应用只能在消费数据时进行相应的去重操作，应用在去重时，最常用的手段就是根据唯一 id 键做 check 去重。</p>
<p>在这种场景下，因为上游生产导致的数据重复问题，会导致所有有精确计数需求的下游应用都需要做这种复杂的、重复的去重处理。试想一下：如果在发送时，系统就能保证 exactly once，这对下游将是多么大的解脱。这就是幂等性要解决的问题，主要是解决数据重复的问题，正如前面所述，数据重复问题，通用的解决方案就是加唯一 id，然后根据 id 判断数据是否重复，Producer 的幂等性也是这样实现的，这一小节就让我们看下 Kafka 的 Producer 如何保证数据的 exactly once 的。</p>
<h2 id="幂等性的实现原理"><a href="#幂等性的实现原理" class="headerlink" title="幂等性的实现原理"></a>幂等性的实现原理</h2><p>在讲述幂等性处理流程之前，先看下 Producer 是如何来保证幂等性的，正如前面所述，幂等性要解决的问题是：Producer 设置 at least once 时，由于异常触发重试机制导致数据重复，幂等性的目的就是为了解决这个数据重复的问题，简单来说就是：</p>
<p><strong>at least once + 幂等 = exactly once</strong></p>
<p>通过在 al least once 的基础上加上 幂等性来做到 exactly once，当然这个层面的 exactly once 是有限制的，比如它会要求单会话内有效或者跨会话使用事务性有效等。这里我们先分析最简单的情况，那就是在单会话内如何做到幂等性，进而保证 exactly once。</p>
<p>要做到幂等性，要解决下面的问题：</p>
<ol>
<li>系统需要有能力鉴别一条数据到底是不是重复的数据？常用的手段是通过 <strong>唯一键/唯一 id</strong> 来判断，这时候系统一般是需要缓存已经处理的唯一键记录，这样才能更有效率地判断一条数据是不是重复；</li>
<li>唯一键应该选择什么粒度？对于分布式存储系统来说，肯定不能用全局唯一键（全局是针对集群级别），核心的解决思路依然是 <strong>分而治之</strong>，数据密集型系统为了实现分布式都是有分区概念的，而分区之间是有相应的隔离，对于 Kafka 而言，这里的解决方案就是在分区的维度上去做，重复数据的判断让 partition 的 leader 去判断处理，前提是 Produce 请求需要把唯一键值告诉 leader；</li>
<li>分区粒度实现唯一键会不会有其他问题？这里需要考虑的问题是当一个 Partition 有来自多个 client 写入的情况，这些 client 之间是很难做到使用同一个唯一键（一个是它们之间很难做到唯一键的实时感知，另一个是这样实现是否有必要）。而如果系统在实现时做到了  <strong>client + partition</strong> 粒度，这样实现的好处是每个 client 都是完全独立的（它们之间不需要有任何的联系，这是非常大的优点），只是在 Server 端对不同的 client 做好相应的区分即可，当然同一个 client 在处理多个 Topic-Partition 时是完全可以使用同一个 PID 的。</li>
</ol>
<p>有了上面的分析（都是个人见解，如果有误，欢迎指教），就不难理解 Producer 幂等性的实现原理，Kafka Producer 在实现时有以下两个重要机制：</p>
<ol>
<li>PID（Producer ID），用来标识每个 producer client；</li>
<li>sequence numbers，client 发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复。</li>
</ol>
<p>下面详细讲述这两个实现机制。</p>
<h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>每个 Producer 在初始化时都会被分配一个唯一的 PID，这个 PID 对应用是透明的，完全没有暴露给用户。对于一个给定的 PID，sequence number 将会从0开始自增，每个 Topic-Partition 都会有一个独立的 sequence number。Producer 在发送数据时，将会给每条 msg 标识一个 sequence number，Server 也就是通过这个来验证数据是否重复。这里的 PID 是全局唯一的，Producer 故障后重新启动后会被分配一个新的 PID，这也是幂等性无法做到跨会话的一个原因。</p>
<h4 id="Producer-PID-申请"><a href="#Producer-PID-申请" class="headerlink" title="Producer PID 申请"></a>Producer PID 申请</h4><p>这里看下 PID 在 Server 端是如何分配的？Client 通过向 Server 发送一个 InitProducerIdRequest 请求获取 PID（幂等性时，是选择一台连接数最少的 Broker 发送这个请求），这里看下 Server 端是如何处理这个请求的？KafkaApis 中 <code>handleInitProducerIdRequest()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleInitProducerIdRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> initProducerIdRequest = request.body[<span class="type">InitProducerIdRequest</span>]</span><br><span class="line">  <span class="keyword">val</span> transactionalId = initProducerIdRequest.transactionalId</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId != <span class="literal">null</span>) &#123; <span class="comment">//note: 设置 txn.id 时，验证对 txn.id 的权限</span></span><br><span class="line">    <span class="keyword">if</span> (!authorize(request.session, <span class="type">Write</span>, <span class="type">Resource</span>(<span class="type">TransactionalId</span>, transactionalId, <span class="type">LITERAL</span>))) &#123;</span><br><span class="line">      sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">TRANSACTIONAL_ID_AUTHORIZATION_FAILED</span>.exception)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!authorize(request.session, <span class="type">IdempotentWrite</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123; <span class="comment">//note: 没有设置 txn.id 时，验证对集群是否有幂等性权限</span></span><br><span class="line">    sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.exception)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(result: <span class="type">InitProducerIdResult</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createResponse</span></span>(requestThrottleMs: <span class="type">Int</span>): <span class="type">AbstractResponse</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> responseBody = <span class="keyword">new</span> <span class="type">InitProducerIdResponse</span>(requestThrottleMs, result.error, result.producerId, result.producerEpoch)</span><br><span class="line">      trace(<span class="string">s"Completed <span class="subst">$transactionalId</span>'s InitProducerIdRequest with result <span class="subst">$result</span> from client <span class="subst">$&#123;request.header.clientId&#125;</span>."</span>)</span><br><span class="line">      responseBody</span><br><span class="line">    &#125;</span><br><span class="line">    sendResponseMaybeThrottle(request, createResponse)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 生成相应的了 pid，返回给 producer</span></span><br><span class="line">  txnCoordinator.handleInitProducerId(transactionalId, initProducerIdRequest.transactionTimeoutMs, sendResponseCallback)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里实际上是调用了 TransactionCoordinator （Broker 在启动 server 服务时都会初始化这个实例）的 <code>handleInitProducerId()</code> 方法做了相应的处理，其实现如下（这里只关注幂等性的处理）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleInitProducerId</span></span>(transactionalId: <span class="type">String</span>,</span><br><span class="line">                         transactionTimeoutMs: <span class="type">Int</span>,</span><br><span class="line">                         responseCallback: <span class="type">InitProducerIdCallback</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId == <span class="literal">null</span>) &#123; <span class="comment">//note: 只设置幂等性时，直接分配 pid 并返回</span></span><br><span class="line">    <span class="comment">// if the transactional id is null, then always blindly accept the request</span></span><br><span class="line">    <span class="comment">// and return a new producerId from the producerId manager</span></span><br><span class="line">    <span class="keyword">val</span> producerId = producerIdManager.generateProducerId()</span><br><span class="line">    responseCallback(<span class="type">InitProducerIdResult</span>(producerId, producerEpoch = <span class="number">0</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Server 在给一个 client 初始化 PID 时，实际上是通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID。</p>
<h4 id="Server-PID-管理"><a href="#Server-PID-管理" class="headerlink" title="Server PID 管理"></a>Server PID 管理</h4><p>如前面所述，在幂等性的情况下，直接通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID，其中 ProducerIdManager 是在 TransactionCoordinator 对象初始化时初始化的，这个对象主要是用来管理 PID 信息：</p>
<ul>
<li>在本地的 PID 端用完了或者处于新建状态时，申请 PID 段（默认情况下，每次申请 1000 个 PID）；</li>
<li>TransactionCoordinator 对象通过 <code>generateProducerId()</code> 方法获取下一个可以使用的 PID；</li>
</ul>
<p><strong>PID 端申请是向 ZooKeeper 申请</strong>，zk 中有一个 <code>/latest_producer_id_block</code> 节点，每个 Broker 向 zk 申请一个 PID 段后，都会把自己申请的 PID 段信息写入到这个节点，这样当其他 Broker 再申请 PID 段时，会首先读写这个节点的信息，然后根据 block_end 选择一个 PID 段，最后再把信息写会到 zk 的这个节点，这个节点信息格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,<span class="attr">"broker"</span>:<span class="number">35</span>,<span class="attr">"block_start"</span>:<span class="string">"4000"</span>,<span class="attr">"block_end"</span>:<span class="string">"4999"</span>&#125;</span><br></pre></td></tr></table></figure>
<p>ProducerIdManager 向 zk 申请 PID 段的方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getNewProducerIdBlock</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> zkWriteComplete = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">while</span> (!zkWriteComplete) &#123; <span class="comment">//note: 直到从 zk 拿取到分配的 PID 段</span></span><br><span class="line">    <span class="comment">// refresh current producerId block from zookeeper again</span></span><br><span class="line">    <span class="keyword">val</span> (dataOpt, zkVersion) = zkClient.getDataAndVersion(<span class="type">ProducerIdBlockZNode</span>.path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// generate the new producerId block</span></span><br><span class="line">    currentProducerIdBlock = dataOpt <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(data) =&gt;</span><br><span class="line">        <span class="comment">//note: 从 zk 获取当前最新的 pid 信息，如果后面更新失败，这里也会重新从 zk 获取</span></span><br><span class="line">        <span class="keyword">val</span> currProducerIdBlock = <span class="type">ProducerIdManager</span>.parseProducerIdBlockData(data)</span><br><span class="line">        debug(<span class="string">s"Read current producerId block <span class="subst">$currProducerIdBlock</span>, Zk path version <span class="subst">$zkVersion</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currProducerIdBlock.blockEndId &gt; <span class="type">Long</span>.<span class="type">MaxValue</span> - <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span>) &#123;<span class="comment">//note: 不足以分配1000个 PID</span></span><br><span class="line">          <span class="comment">// we have exhausted all producerIds (wow!), treat it as a fatal error</span></span><br><span class="line">          <span class="comment">//note: 当 PID 分配超过限制时，直接报错了（每秒分配1个，够用2百亿年了）</span></span><br><span class="line">          fatal(<span class="string">s"Exhausted all producerIds as the next block's end producerId is will has exceeded long type limit (current block end producerId is <span class="subst">$&#123;currProducerIdBlock.blockEndId&#125;</span>)"</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Have exhausted all producerIds."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">ProducerIdBlock</span>(brokerId, currProducerIdBlock.blockEndId + <span class="number">1</span>L, currProducerIdBlock.blockEndId + <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: 该节点还不存在，第一次初始化</span></span><br><span class="line">        debug(<span class="string">s"There is no producerId block yet (Zk path version <span class="subst">$zkVersion</span>), creating the first block"</span>)</span><br><span class="line">        <span class="type">ProducerIdBlock</span>(brokerId, <span class="number">0</span>L, <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span> - <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> newProducerIdBlockData = <span class="type">ProducerIdManager</span>.generateProducerIdBlockJson(currentProducerIdBlock)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try to write the new producerId block into zookeeper</span></span><br><span class="line">    <span class="comment">//note: 将新的 pid 信息写入到 zk，如果写入失败（写入之前会比对 zkVersion，如果这个有变动，证明这期间有别的 Broker 在操作，那么写入失败），重新申请</span></span><br><span class="line">    <span class="keyword">val</span> (succeeded, version) = zkClient.conditionalUpdatePath(<span class="type">ProducerIdBlockZNode</span>.path,</span><br><span class="line">      newProducerIdBlockData, zkVersion, <span class="type">Some</span>(checkProducerIdBlockZkData))</span><br><span class="line">    zkWriteComplete = succeeded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (zkWriteComplete)</span><br><span class="line">      info(<span class="string">s"Acquired new producerId block <span class="subst">$currentProducerIdBlock</span> by writing to Zk with path version <span class="subst">$version</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ProducerIdManager 申请 PID 段的流程如下：</p>
<ol>
<li>先从 zk 的 <code>/latest_producer_id_block</code> 节点读取最新已经分配的 PID 段信息；</li>
<li>如果该节点不存在，直接从 0 开始分配，选择 0~1000 的 PID 段（ProducerIdManager 的 PidBlockSize 默认为 1000，即是每次申请的 PID 段大小）；</li>
<li>如果该节点存在，读取其中数据，根据 block_end 选择 <block_end+1, block_end+1000> 这个 PID 段（如果 PID 段超过 Long 类型的最大值，这里会直接返回一个异常）；</block_end+1,></li>
<li>在选择了相应的 PID 段后，将这个 PID 段信息写回到 zk 的这个节点中，如果写入成功，那么 PID 段就证明申请成功，如果写入失败（写入时会判断当前节点的 zkVersion 是否与步骤1获取的 zkVersion 相同，如果相同，那么可以成功写入，否则写入就会失败，证明这个节点被修改过），证明此时可能其他的 Broker 已经更新了这个节点（当前的 PID 段可能已经被其他 Broker 申请），那么从步骤 1 重新开始，直到写入成功。</li>
</ol>
<p>明白了 ProducerIdManager 如何申请 PID 段之后，再看 <code>generateProducerId()</code> 这个方法就简单很多了，这个方法在每次调用时，都会更新 nextProducerId 值（下一次可以使用 PID 值），如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateProducerId</span></span>(): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">this</span> synchronized &#123;</span><br><span class="line">    <span class="comment">// grab a new block of producerIds if this block has been exhausted</span></span><br><span class="line">    <span class="keyword">if</span> (nextProducerId &gt; currentProducerIdBlock.blockEndId) &#123;</span><br><span class="line">      <span class="comment">//note: 如果分配的 pid 用完了，重新再向 zk 申请一批</span></span><br><span class="line">      getNewProducerIdBlock()</span><br><span class="line">      nextProducerId = currentProducerIdBlock.blockStartId + <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      nextProducerId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    nextProducerId - <span class="number">1</span> <span class="comment">//note: 返回当前分配的 pid</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里就是 Producer PID 如何申请（事务性情况下 PID 的申请会复杂一些，下篇文章再讲述）以及 Server 端如何管理 PID 的。</p>
<h3 id="sequence-numbers"><a href="#sequence-numbers" class="headerlink" title="sequence numbers"></a>sequence numbers</h3><p>再有了 PID 之后，在 PID + Topic-Partition 级别上添加一个 sequence numbers 信息，就可以实现 Producer 的幂等性了。ProducerBatch 也提供了一个 <code>setProducerState()</code> 方法，它可以给一个 batch 添加一些 meta 信息（pid、baseSequence、isTransactional），这些信息是会伴随着 ProduceRequest 发到 Server 端，Server 端也正是通过这些 meta 来做相应的判断，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ProducerBatch</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(ProducerIdAndEpoch producerIdAndEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemoryRecordsBuilder</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(<span class="keyword">long</span> producerId, <span class="keyword">short</span> producerEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (isClosed()) &#123;</span><br><span class="line">        <span class="comment">// Sequence numbers are assigned when the batch is closed while the accumulator is being drained.</span></span><br><span class="line">        <span class="comment">// If the resulting ProduceRequest to the partition leader failed for a retriable error, the batch will</span></span><br><span class="line">        <span class="comment">// be re queued. In this case, we should not attempt to set the state again, since changing the producerId and sequence</span></span><br><span class="line">        <span class="comment">// once a batch has been sent to the broker risks introducing duplicates.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Trying to set producer state of an already closed batch. This indicates a bug on the client."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.producerId = producerId;</span><br><span class="line">    <span class="keyword">this</span>.producerEpoch = producerEpoch;</span><br><span class="line">    <span class="keyword">this</span>.baseSequence = baseSequence;</span><br><span class="line">    <span class="keyword">this</span>.isTransactional = isTransactional;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="幂等性实现整体流程"><a href="#幂等性实现整体流程" class="headerlink" title="幂等性实现整体流程"></a>幂等性实现整体流程</h2><p>在前面讲述完 Kafka 幂等性的两个实现机制（PID+sequence numbers）之后，这里详细讲述一下，幂等性时其整体的处理流程，主要讲述幂等性相关的内容，其他的部分会简单介绍（可以参考前面【Kafka 源码分析系列文章】了解 Producer 端处理流程以及 Server 端关于 ProduceRequest 请求的处理流程），其流程如下图所示：</p>
<p><img src="/images/kafka/kafka-idemoptent.png" alt="Producer 幂等性时处理流程"></p>
<p>这个图只展示了幂等性情况下，Producer 的大概流程，很多部分在前面的文章中做过分析，本文不再讲述，这里重点关注与幂等性相关的内容（事务性实现更加复杂，后面的文章再讲述），首先 KafkaProducer 在初始化时会初始化一个 TransactionManager 实例，它的作用有以下几个部分：</p>
<ol>
<li>记录本地的事务状态（事务性时必须）；</li>
<li>记录一些状态信息以保证幂等性，比如：每个 topic-partition 对应的下一个 sequence numbers 和 last acked batch（最近一个已经确认的 batch）的最大的 sequence number 等；</li>
<li>记录 ProducerIdAndEpoch 信息（PID 信息）。</li>
</ol>
<h3 id="Client-幂等性时发送流程"><a href="#Client-幂等性时发送流程" class="headerlink" title="Client 幂等性时发送流程"></a>Client 幂等性时发送流程</h3><p>如前面图中所示，幂等性时，Producer 的发送流程如下：</p>
<ol>
<li>应用通过 KafkaProducer 的 <code>send()</code> 方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的；</li>
<li>Producer 后台发送线程 Sender，在 <code>run()</code> 方法中，会先根据 TransactionManager 的 <code>shouldResetProducerStateAfterResolvingSequences()</code> 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有 topic-partition 的 batch 重试多次失败最后因为超时而被移除，这时 sequence number 将无法做到连续，因为 sequence number 有部分已经分配出去，这时系统依赖自身的机制无法继续进行下去（因为幂等性是要保证不丢不重的），相当于程序遇到了一个 fatal 异常，PID 会进行重置，TransactionManager 相关的缓存信息被清空（Producer 不会重启），只是保存状态信息的 TransactionManager 做了 <code>clear+new</code> 操作，遇到这个问题时是无法保证 exactly once 的（有数据已经发送失败了，并且超过了重试次数）；</li>
<li>Sender 线程通过 <code>maybeWaitForProducerId()</code> 方法判断是否需要申请 PID，如果需要的话，这里会阻塞直到获取到相应的 PID 信息；</li>
<li>Sender 线程通过 <code>sendProducerData()</code> 方法发送数据，整体流程与之前的 Producer 流程相似，不同的地方是在 RecordAccumulator 的 <code>drain()</code> 方法中，在加了幂等性之后，<code>drain()</code> 方法多了如下几步判断：<ol>
<li>常规的判断：判断这个 topic-partition 是否可以继续发送（如果出现前面2中的情况是不允许发送的）、判断 PID 是否有效、如果这个 batch 是重试的 batch，那么需要判断这个 batch 之前是否还有 batch 没有发送完成，如果有，这里会先跳过这个 Topic-Partition 的发送，直到前面的 batch 发送完成，<strong>最坏情况下，这个 Topic-Partition 的 in-flight request 将会减少到1</strong>（这个涉及也是考虑到 server 端的一个设置，文章下面会详细分析）；</li>
<li>如果这个 ProducerBatch 还没有这个相应的 PID 和 sequence number 信息，会在这里进行相应的设置；</li>
</ol>
</li>
<li>最后 Sender 线程再调用 <code>sendProduceRequests()</code> 方法发送 ProduceRequest 请求，后面的就跟之前正常的流程保持一致了。</li>
</ol>
<p>这里看下几个关键方法的实现，首先是 Sender 线程获取 PID 信息的方法  <code>maybeWaitForProducerId()</code> ，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 等待直到 Producer 获取到相应的 PID 和 epoch 信息</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">maybeWaitForProducerId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!transactionManager.hasProducerId() &amp;&amp; !transactionManager.hasError()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Node node = awaitLeastLoadedNodeReady(requestTimeoutMs); <span class="comment">//note: 选取 node（本地连接数最少的 node）</span></span><br><span class="line">            <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">                ClientResponse response = sendAndAwaitInitProducerIdRequest(node); <span class="comment">//note: 发送 InitPidRequest</span></span><br><span class="line">                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();</span><br><span class="line">                Errors error = initProducerIdResponse.error();</span><br><span class="line">                <span class="keyword">if</span> (error == Errors.NONE) &#123; <span class="comment">//note: 更新 Producer 的 PID 和 epoch 信息</span></span><br><span class="line">                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">new</span> ProducerIdAndEpoch(</span><br><span class="line">                            initProducerIdResponse.producerId(), initProducerIdResponse.epoch());</span><br><span class="line">                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error.exception() <span class="keyword">instanceof</span> RetriableException) &#123;</span><br><span class="line">                    log.debug(<span class="string">"Retriable error from InitProducerId response"</span>, error.message());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    transactionManager.transitionToFatalError(error.exception());</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.debug(<span class="string">"Could not find an available broker to send InitProducerIdRequest to. "</span> +</span><br><span class="line">                        <span class="string">"We will back off and try again."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedVersionException e) &#123;</span><br><span class="line">            transactionManager.transitionToFatalError(e);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.debug(<span class="string">"Broker &#123;&#125; disconnected while awaiting InitProducerId response"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        log.trace(<span class="string">"Retry InitProducerIdRequest in &#123;&#125;ms."</span>, retryBackoffMs);</span><br><span class="line">        time.sleep(retryBackoffMs);</span><br><span class="line">        metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再看下 RecordAccumulator 的 <code>drain()</code> 方法，重点需要关注的是关于幂等性和事务性相关的处理，具体如下所示，这里面关于事务性相关的判断在上面的流程中已经讲述。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Drain all the data for the given nodes and collate them into a list of batches that will fit within the specified</span></span><br><span class="line"><span class="comment"> * size on a per-node basis. This method attempts to avoid choosing the same topic-node over and over.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nodes The list of node to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxSize The maximum number of bytes to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current unix time in milliseconds</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A list of &#123;<span class="doctag">@link</span> ProducerBatch&#125; for each node specified with total size less than the requested maxSize.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(Cluster cluster,</span><br><span class="line">                                               Set&lt;Node&gt; nodes,</span><br><span class="line">                                               <span class="keyword">int</span> maxSize,</span><br><span class="line">                                               <span class="keyword">long</span> now) &#123;</span><br><span class="line">    <span class="keyword">if</span> (nodes.isEmpty())</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line"></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodes) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</span><br><span class="line">        List&lt;ProducerBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">/* to make starvation less likely this loop doesn't start at 0 */</span></span><br><span class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            PartitionInfo part = parts.get(drainIndex);</span><br><span class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</span><br><span class="line">            <span class="comment">// Only proceed if the partition has no in-flight batches.</span></span><br><span class="line">            <span class="keyword">if</span> (!isMuted(tp, now)) &#123;</span><br><span class="line">                Deque&lt;ProducerBatch&gt; deque = getDeque(tp);</span><br><span class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (deque) &#123; <span class="comment">//note: 先判断有没有数据，然后后面真正处理时再加锁处理</span></span><br><span class="line">                        ProducerBatch first = deque.peekFirst();</span><br><span class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            <span class="keyword">boolean</span> backoff = first.attempts() &gt; <span class="number">0</span> &amp;&amp; first.waitedTimeMs(now) &lt; retryBackoffMs;</span><br><span class="line">                            <span class="comment">// Only drain the batch if it is not during backoff period.</span></span><br><span class="line">                            <span class="keyword">if</span> (!backoff) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (size + first.estimatedSizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) &#123;</span><br><span class="line">                                    <span class="comment">// there is a rare case that a single batch size is larger than the request size due</span></span><br><span class="line">                                    <span class="comment">// to compression; in this case we will still eventually send this batch in a single</span></span><br><span class="line">                                    <span class="comment">// request</span></span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">null</span>;</span><br><span class="line">                                    <span class="keyword">boolean</span> isTransactional = <span class="keyword">false</span>;</span><br><span class="line">                                    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123; <span class="comment">//note: 幂等性或事务性时， 做一些检查判断</span></span><br><span class="line">                                        <span class="keyword">if</span> (!transactionManager.isSendToPartitionAllowed(tp))</span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        producerIdAndEpoch = transactionManager.producerIdAndEpoch();</span><br><span class="line">                                        <span class="keyword">if</span> (!producerIdAndEpoch.isValid()) <span class="comment">//note: pid 是否有效</span></span><br><span class="line">                                            <span class="comment">// we cannot send the batch until we have refreshed the producer id</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        isTransactional = transactionManager.isTransactional();</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">if</span> (!first.hasSequence() &amp;&amp; transactionManager.hasUnresolvedSequence(first.topicPartition))</span><br><span class="line">                                            <span class="comment">//note: 当前这个 topic-partition 的数据出现过超时,不能发送,如果是新的 batch 数据直接跳过（没有 seq  number 信息）</span></span><br><span class="line">                                            <span class="comment">// Don't drain any new batches while the state of previous sequence numbers</span></span><br><span class="line">                                            <span class="comment">// is unknown. The previous batches would be unknown if they were aborted</span></span><br><span class="line">                                            <span class="comment">// on the client after being sent to the broker at least once.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line">                                        <span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">                                                &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">                                            <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">                                            <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">                                            <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">                                            <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">                                            <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">                                            <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">                                            <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">                                            <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">                                            <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">                                            <span class="comment">// in flight request count to 1.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line">                                    &#125;</span><br><span class="line"></span><br><span class="line">                                    ProducerBatch batch = deque.pollFirst();</span><br><span class="line">                                    <span class="keyword">if</span> (producerIdAndEpoch != <span class="keyword">null</span> &amp;&amp; !batch.hasSequence()) &#123;<span class="comment">//note: batch 的相关信息（seq id）是在这里设置的</span></span><br><span class="line">                                        <span class="comment">//note: 这个 batch 还没有 seq number 信息</span></span><br><span class="line">                                        <span class="comment">// If the batch already has an assigned sequence, then we should not change the producer id and</span></span><br><span class="line">                                        <span class="comment">// sequence number, since this may introduce duplicates. In particular,</span></span><br><span class="line">                                        <span class="comment">// the previous attempt may actually have been accepted, and if we change</span></span><br><span class="line">                                        <span class="comment">// the producer id and sequence here, this attempt will also be accepted,</span></span><br><span class="line">                                        <span class="comment">// causing a duplicate.</span></span><br><span class="line">                                        <span class="comment">//</span></span><br><span class="line">                                        <span class="comment">// Additionally, we update the next sequence number bound for the partition,</span></span><br><span class="line">                                        <span class="comment">// and also have the transaction manager track the batch so as to ensure</span></span><br><span class="line">                                        <span class="comment">// that sequence ordering is maintained even if we receive out of order</span></span><br><span class="line">                                        <span class="comment">// responses.</span></span><br><span class="line">                                        <span class="comment">//note: 给这个 batch 设置相应的 pid、seq id 等信息</span></span><br><span class="line">                                        batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);</span><br><span class="line">                                        transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount); <span class="comment">//note: 增加 partition 对应的下一个 seq id 值</span></span><br><span class="line">                                        log.debug(<span class="string">"Assigned producerId &#123;&#125; and producerEpoch &#123;&#125; to batch with base sequence "</span> +</span><br><span class="line">                                                        <span class="string">"&#123;&#125; being sent to partition &#123;&#125;"</span>, producerIdAndEpoch.producerId,</span><br><span class="line">                                                producerIdAndEpoch.epoch, batch.baseSequence(), tp);</span><br><span class="line"></span><br><span class="line">                                        transactionManager.addInFlightBatch(batch);</span><br><span class="line">                                    &#125;</span><br><span class="line">                                    batch.close();</span><br><span class="line">                                    size += batch.records().sizeInBytes();</span><br><span class="line">                                    ready.add(batch);</span><br><span class="line">                                    batch.drained(now);</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</span><br><span class="line">        &#125; <span class="keyword">while</span> (start != drainIndex);</span><br><span class="line">        batches.put(node.id(), ready);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="幂等性时-Server-端如何处理-ProduceRequest-请求"><a href="#幂等性时-Server-端如何处理-ProduceRequest-请求" class="headerlink" title="幂等性时 Server 端如何处理 ProduceRequest 请求"></a>幂等性时 Server 端如何处理 ProduceRequest 请求</h3><p>如前面途中所示，当 Broker 收到 ProduceRequest 请求之后，会通过 <code>handleProduceRequest()</code> 做相应的处理，其处理流程如下（这里只讲述关于幂等性相关的内容）：</p>
<ol>
<li>如果请求是事务请求，检查是否对 TXN.id 有 Write 权限，没有的话返回 TRANSACTIONAL_ID_AUTHORIZATION_FAILED；</li>
<li>如果请求设置了幂等性，检查是否对 ClusterResource 有 IdempotentWrite 权限，没有的话返回 CLUSTER_AUTHORIZATION_FAILED；</li>
<li>验证对 topic 是否有 Write 权限以及 Topic 是否存在，否则返回 TOPIC_AUTHORIZATION_FAILED 或 UNKNOWN_TOPIC_OR_PARTITION 异常；</li>
<li>检查是否有 PID 信息，没有的话走正常的写入流程；</li>
<li>LOG 对象会在 <code>analyzeAndValidateProducerState()</code> 方法先根据 batch 的 sequence number 信息检查这个 batch 是否重复（server 端会缓存 PID 对应这个 Topic-Partition 的最近5个 batch 信息），如果有重复，这里当做写入成功返回（不更新 LOG 对象中相应的状态信息，比如这个 replica 的 the end offset 等）；</li>
<li>有了 PID 信息，并且不是重复 batch 时，在更新 producer 信息时，会做以下校验：<ol>
<li>检查该 PID 是否已经缓存中存在（主要是在 ProducerStateManager 对象中检查）；</li>
<li>如果不存在，那么判断 sequence number 是否 从0 开始，是的话，在缓存中记录 PID 的 meta（PID，epoch， sequence number），并执行写入操作，否则返回 UnknownProducerIdException（PID 在 server 端已经过期或者这个 PID 写的数据都已经过期了，但是 Client 还在接着上次的 sequence number 发送数据）；</li>
<li>如果该 PID 存在，先检查 PID epoch 与 server 端记录的是否相同；</li>
<li>如果不同并且 sequence number 不从 0 开始，那么返回 OutOfOrderSequenceException 异常；</li>
<li>如果不同并且 sequence number 从 0 开始，那么正常写入；</li>
<li>如果相同，那么根据缓存中记录的最近一次 sequence number（currentLastSeq）检查是否为连续（会区分为 0、Int.MaxValue 等情况），不连续的情况下返回 OutOfOrderSequenceException 异常。</li>
</ol>
</li>
<li>下面与正常写入相同。</li>
</ol>
<p>幂等性时，Broker 在处理 ProduceRequest 请求时，多了一些校验操作，这里重点看一下其中一些重要实现，先看下 <code>analyzeAndValidateProducerState()</code> 方法的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">analyzeAndValidateProducerState</span></span>(records: <span class="type">MemoryRecords</span>, isFromClient: <span class="type">Boolean</span>): (mutable.<span class="type">Map</span>[<span class="type">Long</span>, <span class="type">ProducerAppendInfo</span>], <span class="type">List</span>[<span class="type">CompletedTxn</span>], <span class="type">Option</span>[<span class="type">BatchMetadata</span>]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> updatedProducers = mutable.<span class="type">Map</span>.empty[<span class="type">Long</span>, <span class="type">ProducerAppendInfo</span>]</span><br><span class="line">  <span class="keyword">val</span> completedTxns = <span class="type">ListBuffer</span>.empty[<span class="type">CompletedTxn</span>]</span><br><span class="line">  <span class="keyword">for</span> (batch &lt;- records.batches.asScala <span class="keyword">if</span> batch.hasProducerId) &#123; <span class="comment">//note: 有 pid 时,才会做相应的判断</span></span><br><span class="line">    <span class="keyword">val</span> maybeLastEntry = producerStateManager.lastEntry(batch.producerId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if this is a client produce request, there will be up to 5 batches which could have been duplicated.</span></span><br><span class="line">    <span class="comment">// If we find a duplicate, we return the metadata of the appended batch to the client.</span></span><br><span class="line">    <span class="keyword">if</span> (isFromClient) &#123;</span><br><span class="line">      maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach &#123; duplicate =&gt;</span><br><span class="line">        <span class="keyword">return</span> (updatedProducers, completedTxns.toList, <span class="type">Some</span>(duplicate)) <span class="comment">//note: 如果这个 batch 已经收到过，这里直接返回</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> maybeCompletedTxn = updateProducers(batch, updatedProducers, isFromClient = isFromClient) <span class="comment">//note: 这里</span></span><br><span class="line">    maybeCompletedTxn.foreach(completedTxns += _)</span><br><span class="line">  &#125;</span><br><span class="line">  (updatedProducers, completedTxns.toList, <span class="type">None</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果这个 batch 有 PID 信息，会首先检查这个 batch 是否为重复的 batch 数据，其实现如下，batchMetadata 会缓存最新 5个 batch 的数据（如果超过5个，添加时会进行删除，这个也是幂等性要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5 的原因，与这个值的设置有关），根据 batchMetadata 缓存的 batch 数据来判断这个 batch 是否为重复的数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findDuplicateBatch</span></span>(batch: <span class="type">RecordBatch</span>): <span class="type">Option</span>[<span class="type">BatchMetadata</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (batch.producerEpoch != producerEpoch)</span><br><span class="line">     <span class="type">None</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    batchWithSequenceRange(batch.baseSequence, batch.lastSequence)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Return the batch metadata of the cached batch having the exact sequence range, if any.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchWithSequenceRange</span></span>(firstSeq: <span class="type">Int</span>, lastSeq: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">BatchMetadata</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> duplicate = batchMetadata.filter &#123; metadata =&gt;</span><br><span class="line">    firstSeq == metadata.firstSeq &amp;&amp; lastSeq == metadata.lastSeq</span><br><span class="line">  &#125;</span><br><span class="line">  duplicate.headOption</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatchMetadata</span></span>(batch: <span class="type">BatchMetadata</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (batchMetadata.size == <span class="type">ProducerStateEntry</span>.<span class="type">NumBatchesToRetain</span>)</span><br><span class="line">    batchMetadata.dequeue() <span class="comment">//note: 只会保留最近 5 个 batch 的记录</span></span><br><span class="line">  batchMetadata.enqueue(batch) <span class="comment">//note: 添加到 batchMetadata 中记录，便于后续根据 seq id 判断是否重复</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果 batch 不是重复的数据，<code>analyzeAndValidateProducerState()</code> 会通过 <code>updateProducers()</code> 更新 producer 的相应记录，在更新的过程中，会做一步校验，校验方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 检查 seq number</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkSequence</span></span>(producerEpoch: <span class="type">Short</span>, appendFirstSeq: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (producerEpoch != updatedEntry.producerEpoch) &#123; <span class="comment">//note: epoch 不同时</span></span><br><span class="line">    <span class="keyword">if</span> (appendFirstSeq != <span class="number">0</span>) &#123; <span class="comment">//note: 此时要求 seq number 必须从0开始（如果不是的话，pid 可能是新建的或者 PID 在 Server 端已经过期）</span></span><br><span class="line">      <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 不是-1，证明时原来的 pid 过期了）</span></span><br><span class="line">      <span class="keyword">if</span> (updatedEntry.producerEpoch != <span class="type">RecordBatch</span>.<span class="type">NO_PRODUCER_EPOCH</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Invalid sequence number for new epoch: <span class="subst">$producerEpoch</span> "</span> +</span><br><span class="line">          <span class="string">s"(request epoch), <span class="subst">$appendFirstSeq</span> (seq. number)"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 为-1，证明 server 端 meta 新建的，PID 在 server 端已经过期，client 还在接着上次的 seq 发数据）</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownProducerIdException</span>(<span class="string">s"Found no record of producerId=<span class="subst">$producerId</span> on the broker. It is possible "</span> +</span><br><span class="line">          <span class="string">s"that the last message with t（）he producerId=<span class="subst">$producerId</span> has been removed due to hitting the retention limit."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> currentLastSeq = <span class="keyword">if</span> (!updatedEntry.isEmpty)</span><br><span class="line">      updatedEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (producerEpoch == currentEntry.producerEpoch)</span><br><span class="line">      currentEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="type">RecordBatch</span>.<span class="type">NO_SEQUENCE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (currentLastSeq == <span class="type">RecordBatch</span>.<span class="type">NO_SEQUENCE</span> &amp;&amp; appendFirstSeq != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">//note: 此时期望的 seq number 是从 0 开始,因为 currentLastSeq 是 -1,也就意味着这个 pid 还没有写入过数据</span></span><br><span class="line">      <span class="comment">// the epoch was bumped by a control record, so we expect the sequence number to be reset</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Out of order sequence number for producerId <span class="subst">$producerId</span>: found <span class="subst">$appendFirstSeq</span> "</span> +</span><br><span class="line">        <span class="string">s"(incoming seq. number), but expected 0"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inSequence(currentLastSeq, appendFirstSeq)) &#123;</span><br><span class="line">      <span class="comment">//note: 判断是否连续</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Out of order sequence number for producerId <span class="subst">$producerId</span>: <span class="subst">$appendFirstSeq</span> "</span> +</span><br><span class="line">        <span class="string">s"(incoming seq. number), <span class="subst">$currentLastSeq</span> (current end sequence number)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其校验逻辑如前面流程中所述。</p>
<h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>这里主要思考两个问题：</p>
<ol>
<li>Producer 在设置幂等性时，为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5，如果设置大于 5（不考虑 Producer 端参数校验的报错），会带来什么后果？</li>
<li>Producer 在设置幂等性时，如果我们设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，那么是否可以保证有序，如果可以，是怎么做到的？</li>
</ol>
<p>先说一下结论，问题 1 的这个设置要求其实上面分析的时候已经讲述过了，主要跟 server 端只会缓存最近 5 个 batch 的机制有关；问题 2，即使 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，幂等性时依然可以做到有序，下面来详细分析一下这两个问题。</p>
<h3 id="为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5"><a href="#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5" class="headerlink" title="为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档，忘记在哪了），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p>
<p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（相当于client 狂发错误请求）。</p>
<p>那有没有更好的方案呢？我认为是有的，那就是对于 OutOfOrderSequenceException 异常，再进行细分，区分这个 sequence number 是大于 nextSeq （期望的下次 sequence number  值）还是小于 nextSeq，如果是小于，那么肯定是重复的数据。</p>
<h3 id="当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序"><a href="#当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序" class="headerlink" title="当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序"></a>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序</h3><p>先来分析一下，在什么情况下 Producer 会出现乱序的问题？没有幂等性时，乱序的问题是在重试时出现的，举个例子：client 依然发送了 6 个请求 1、2、3、4、5、6（它们分别对应了一个 batch），这 6 个请求只有 2-6 成功 ack 了，1 失败了，这时候需要重试，重试时就会把 batch 1 的数据添加到待发送的数据列队中），那么下次再发送时，batch 1 的数据将会被发送，这时候数据就已经出现了乱序，因为 batch 1 的数据已经晚于了 batch 2-6。</p>
<p>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 1 时，是可以解决这个为题，因为同时只允许一个请求正在发送，只有当前的请求发送完成（成功 ack 后），才能继续下一条请求的发送，类似单线程处理这种模式，每次请求发送时都会等待上次的完成，效率非常差，但是可以解决乱序的问题（当然这里有序只是针对单 client 情况，多 client 并发写是无法做到的）。</p>
<p>系统能提供的方案，基本上就是有序性与性能之间二选一，无法做到兼容，实际上系统出现请求重试的几率是很小的（一般都是网络问题触发的），可能连 0.1% 的时间都不到，但是就是为了这 0.1% 时间都不到的情况，应用需要牺牲性能问题来解决，在大数据场景下，我们是希望有更友好的方式来解决这个问题。简单来说，就是当出现重试时，max-in-flight-request 可以动态减少到 1，在正常情况下还是按 5 （5是举例说明）来处理，这有点类似于分布式系统 CAP 理论中关于 P 的考虑，当出现问题时，可以容忍性能变差，但是其他的情况下，我们希望的是能拥有原来的性能，而不是一刀切。令人高兴的，在 Kafka 2.0.0 版本中，如果 Producer 开始了幂等性，Kafka 是可以做到这一点的，如果不开启幂等性，是无法做到的，因为它的实现是依赖了 sequence number。</p>
<p>当请求出现重试时，batch 会重新添加到队列中，这时候是根据 sequence number 添加到队列的合适位置（有些 batch 如果还没有 sequence number，那么就保持其相对位置不变），也就是队列中排在这个 batch 前面的 batch，其 sequence number 都比这个 batch 的 sequence number 小，其实现如下，这个方法保证了在重试时，其 batch 会被放到合适的位置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Re-enqueue the given record batch in the accumulator to retry</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reenqueue</span><span class="params">(ProducerBatch batch, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    batch.reenqueued(now); <span class="comment">//note: 重试,更新相应的 meta</span></span><br><span class="line">    Deque&lt;ProducerBatch&gt; deque = getOrCreateDeque(batch.topicPartition);</span><br><span class="line">    <span class="keyword">synchronized</span> (deque) &#123;</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>)</span><br><span class="line">            insertInSequenceOrder(deque, batch); <span class="comment">//note: 将 batch 添加到队列的合适位置（根据 seq num 信息）</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            deque.addFirst(batch);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外 Sender 在发送请求时，会首先通过 RecordAccumulator 的 <code>drain()</code> 方法获取其发送的数据，在遍历 Topic-Partition 对应的 queue 中的 batch 时，如果发现 batch 已经有了 sequence number 的话，则证明这个 batch 是重试的 batch，因为没有重试的 batch 其 sequence number 还没有设置，这时候会做一个判断，会等待其 in-flight-requests 中请求发送完成，才允许再次发送这个 Topic-Partition 的数据，其判断实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 获取 inFlightBatches 中第一个 batch 的 baseSequence, inFlightBatches 为 null 的话返回 RecordBatch.NO_SEQUENCE</span></span><br><span class="line"><span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line"><span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">        &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">    <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">    <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">    <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">    <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">    <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">    <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">    <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">    <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">    <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">    <span class="comment">// in flight request count to 1.</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>仅有 client 端这两个机制还不够，Server 端在处理 ProduceRequest 请求时，还会检查 batch 的 sequence number 值，它会要求这个值必须是连续的，如果不连续都会返回异常，Client 会进行相应的重试，举个栗子：假设 Client 发送的请求顺序是 1、2、3、4、5（分别对应了一个 batch），如果中间的请求 2 出现了异常，那么会导致 3、4、5 都返回异常进行重试（因为 sequence number 不连续），也就是说此时 2、3、4、5 都会进行重试操作添加到对应的 queue 中。</p>
<p>Producer 的 TransactionManager 实例的 inflightBatchesBySequence 成员变量会维护这个 Topic-Partition 与目前正在发送的 batch 的对应关系（通过 <code>addInFlightBatch()</code> 方法添加 batch 记录），只有这个 batch 成功 ack 后，才会通过 <code>removeInFlightBatch()</code> 方法将这个 batch 从 inflightBatchesBySequence 中移除。接着前面的例子，此时 inflightBatchesBySequence 中还有 2、3、4、5 这几个 batch（有顺序的，2 在前面），根据前面的 RecordAccumulator 的 <code>drain()</code> 方法可以知道只有这个 Topic-Partition 下次要发送的 batch 是 batch 2（跟 transactionManager 的这个 <code>firstInFlightSequence()</code> 方法获取 inFlightBatches 中第一个 batch 的 baseSequence 来判断） 时，才可以发送，否则会直接 break，跳过这个 Topic-Partition 的数据发送。这里相当于有一个等待，等待 batch 2 重新加入到 queue 中，才可以发送，不能跳过 batch 2，直接重试 batch 3、4、5，这是不允许的。</p>
<p>简单来说，其实现机制概括为：</p>
<ol>
<li>Server 端验证 batch 的 sequence number 值，不连续时，直接返回异常；</li>
<li>Client 端请求重试时，batch 在 reenqueue 时会根据 sequence number 值放到合适的位置（有序保证之一）；</li>
<li>Sender 线程发送时，在遍历 queue 中的 batch 时，会检查这个 batch 是否是重试的 batch，如果是的话，只有这个 batch 是最旧的那个需要重试的 batch，才允许发送，否则本次发送跳过这个 Topic-Partition 数据的发送等待下次发送。</li>
</ol>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="external">Idempotent Producer</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 K
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
</feed>
