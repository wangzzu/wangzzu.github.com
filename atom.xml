<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matt&#39;s Blog</title>
  <subtitle>王蒙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matt33.com/"/>
  <updated>2019-03-17T07:47:39.646Z</updated>
  <id>http://matt33.com/</id>
  
  <author>
    <name>Matt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Apache Calcite 优化器详解（二）</title>
    <link href="http://matt33.com/2019/03/17/apache-calcite-planner/"/>
    <id>http://matt33.com/2019/03/17/apache-calcite-planner/</id>
    <published>2019-03-17T12:41:35.000Z</published>
    <updated>2019-03-17T07:47:39.646Z</updated>
    
    <content type="html"><![CDATA[<p>紧接上篇文章<a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a>，这里是 Calcite 系列文章的第二篇，后面还会有文章讲述 Calcite 的实践（包括：如何开发用于 SQL 优化的 Rule）。本篇文章主要介绍 Apache Calcite 优化器部分的内容，会先简单介绍一下 RBO 和 CBO 模型，之后详细讲述 Calcite 关于这两个优化器的实现 —— HepPlanner 和 VolcanoPlanner，文章内容都是个人的一些理解，由于也是刚接触这块，理解有偏差的地方，欢迎指正。</p>
<h1 id="什么是查询优化器"><a href="#什么是查询优化器" class="headerlink" title="什么是查询优化器"></a>什么是查询优化器</h1><p>查询优化器是传统数据库的核心模块，也是大数据计算引擎的核心模块，开源大数据引擎如 Impala、Presto、Drill、HAWQ、 Spark、Hive 等都有自己的查询优化器。Calcite 就是从 Hive 的优化器演化而来的。</p>
<p>优化器的作用：将解析器生成的关系代数表达式转换成执行计划，供执行引擎执行，在这个过程中，会应用一些规则优化，以帮助生成更高效的执行计划。</p>
<blockquote>
<p>关于 Volcano 模型和 Cascades 模型的内容，建议看下相关的论文，这个是 Calcite 优化器的理论基础，代码只是把这个模型落地实现而已。 </p>
</blockquote>
<h2 id="基于规则优化（RBO）"><a href="#基于规则优化（RBO）" class="headerlink" title="基于规则优化（RBO）"></a>基于规则优化（RBO）</h2><p>基于规则的优化器（Rule-Based Optimizer，RBO）：根据优化规则对关系表达式进行转换，这里的转换是说一个关系表达式经过优化规则后会变成另外一个关系表达式，同时原有表达式会被裁剪掉，经过一系列转换后生成最终的执行计划。</p>
<p>RBO 中包含了一套有着严格顺序的优化规则，同样一条 SQL，无论读取的表中数据是怎么样的，最后生成的执行计划都是一样的。同时，在 RBO 中 SQL 写法的不同很有可能影响最终的执行计划，从而影响执行计划的性能。</p>
<h2 id="基于成本优化（CBO）"><a href="#基于成本优化（CBO）" class="headerlink" title="基于成本优化（CBO）"></a>基于成本优化（CBO）</h2><p>基于代价的优化器(Cost-Based Optimizer，CBO)：根据优化规则对关系表达式进行转换，这里的转换是说一个关系表达式经过优化规则后会生成另外一个关系表达式，同时原有表达式也会保留，经过一系列转换后会生成多个执行计划，然后 CBO 会根据统计信息和代价模型 (Cost Model) 计算每个执行计划的 Cost，从中挑选 Cost 最小的执行计划。</p>
<p>由上可知，CBO 中有两个依赖：统计信息和代价模型。统计信息的准确与否、代价模型的合理与否都会影响 CBO 选择最优计划。 从上述描述可知，CBO 是优于 RBO 的，原因是 RBO 是一种只认规则，对数据不敏感的呆板的优化器，而在实际过程中，数据往往是有变化的，通过 RBO 生成的执行计划很有可能不是最优的。事实上目前各大数据库和大数据计算引擎都倾向于使用 CBO，但是对于流式计算引擎来说，使用 CBO 还是有很大难度的，因为并不能提前预知数据量等信息，这会极大地影响优化效果，CBO 主要还是应用在离线的场景。</p>
<h1 id="优化规则"><a href="#优化规则" class="headerlink" title="优化规则"></a>优化规则</h1><p>无论是 RBO，还是 CBO 都包含了一系列优化规则，这些优化规则可以对关系表达式进行等价转换，常见的优化规则包含：</p>
<ol>
<li>谓词下推 Predicate Pushdown</li>
<li>常量折叠 Constant Folding</li>
<li>列裁剪 Column Pruning</li>
<li>其他</li>
</ol>
<p>在 Calcite 的代码里，有一个测试类（<code>org.apache.calcite.test.RelOptRulesTest</code>）汇集了对目前内置所有 Rules 的测试 case，这个测试类可以方便我们了解各个 Rule 的作用。在这里有下面一条 SQL，通过这条语句来说明一下上面介绍的这三种规则。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="number">10</span> + <span class="number">30</span>, users.name, users.age</span><br><span class="line"><span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">join</span> jobs <span class="keyword">on</span> users.id= user.id</span><br><span class="line"><span class="keyword">where</span> users.age &gt; <span class="number">30</span> <span class="keyword">and</span> jobs.id&gt;<span class="number">10</span></span><br></pre></td></tr></table></figure>
<h2 id="谓词下推（Predicate-Pushdown）"><a href="#谓词下推（Predicate-Pushdown）" class="headerlink" title="谓词下推（Predicate Pushdown）"></a>谓词下推（Predicate Pushdown）</h2><p>关于谓词下推，它主要还是从关系型数据库借鉴而来，关系型数据中将谓词下推到外部数据库用以减少数据传输；属于逻辑优化，优化器将谓词过滤下推到数据源，使物理执行跳过无关数据。最常见的例子就是 join 与 filter 操作一起出现时，提前执行 filter 操作以减少处理的数据量，将 filter 操作下推，以上面例子为例，示意图如下（对应 Calcite 中的 <code>FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN</code> Rule）：</p>
<p><img src="/images/calcite/6-filter-pushdown.png" alt="Filter操作下推前后的对比"></p>
<p>在进行 join 前进行相应的过滤操作，可以极大地减少参加 join 的数据量。</p>
<h2 id="常量折叠（Constant-Folding）"><a href="#常量折叠（Constant-Folding）" class="headerlink" title="常量折叠（Constant Folding）"></a>常量折叠（Constant Folding）</h2><p>常量折叠也是常见的优化策略，这个比较简单、也很好理解，可以看下 <a href="http://blog.caoxudong.info/blog/2013/10/23/compiler_optimizations_constant_folding" target="_blank" rel="external">编译器优化 – 常量折叠</a> 这篇文章，基本不用动脑筋就能理解，对于我们这里的示例，有一个常量表达式 <code>10 + 30</code>，如果不进行常量折叠，那么每行数据都需要进行计算，进行常量折叠后的结果如下图所示（ 对应 Calcite 中的 <code>ReduceExpressionsRule.PROJECT_INSTANCE</code> Rule）：</p>
<p><img src="/images/calcite/7-constant.png" alt="常量折叠前后的对比"></p>
<h2 id="列裁剪（Column-Pruning）"><a href="#列裁剪（Column-Pruning）" class="headerlink" title="列裁剪（Column Pruning）"></a>列裁剪（Column Pruning）</h2><p>列裁剪也是一个经典的优化规则，在本示例中对于jobs 表来说，并不需要扫描它的所有列值，而只需要列值 id，所以在扫描 jobs 之后需要将其他列进行裁剪，只留下列 id。这个优化带来的好处很明显，大幅度减少了网络 IO、内存数据量的消耗。裁剪前后的示意图如下（不过并没有找到 Calcite 对应的 Rule）：</p>
<p><img src="/images/calcite/8-pruning.png" alt="列裁剪前后的对比"></p>
<h1 id="Calcite-中的优化器实现"><a href="#Calcite-中的优化器实现" class="headerlink" title="Calcite 中的优化器实现"></a>Calcite 中的优化器实现</h1><p>有了前面的基础后，这里来看下 Calcite 中优化器的实现，RelOptPlanner 是 Calcite 中优化器的基类，其子类实现如下图所示：</p>
<p><img src="/images/calcite/9-RelOptPlanner.png" alt="RelOptPlanner"></p>
<p>Calcite 中关于优化器提供了两种实现：</p>
<ol>
<li>HepPlanner：就是前面 RBO 的实现，它是一个启发式的优化器，按照规则进行匹配，直到达到次数限制（match 次数限制）或者遍历一遍后不再出现 rule match 的情况才算完成；</li>
<li>VolcanoPlanner：就是前面 CBO 的实现，它会一直迭代 rules，直到找到 cost 最小的 paln。</li>
</ol>
<blockquote>
<p>前面提到过像calcite这类查询优化器最核心的两个问题之一是怎么把优化规则应用到关系代数相关的RelNode Tree上。所以在阅读calicite的代码时就得带着这个问题去看看它的实现过程，然后才能判断它的代码实现得是否优雅。<br>calcite的每种规则实现类(RelOptRule的子类)都会声明自己应用在哪种RelNode子类上，每个RelNode子类其实都可以看成是一种operator(中文常翻译成算子)。<br>VolcanoPlanner就是优化器，用的是动态规划算法，在创建VolcanoPlanner的实例后，通过calcite的标准jdbc接口执行sql时，默认会给这个VolcanoPlanner的实例注册将近90条优化规则(还不算常量折叠这种最常见的优化)，所以看代码时，知道什么时候注册可用的优化规则是第一步(调用VolcanoPlanner.addRule实现)，这一步比较简单。<br>接下来就是如何筛选规则了，当把语法树转成RelNode Tree后是没有必要把前面注册的90条优化规则都用上的，所以需要有个筛选的过程，因为每种规则是有应用范围的，按RelNode Tree的不同节点类型就可以筛选出实际需要用到的优化规则了。这一步说起来很简单，但在calcite的代码实现里是相当复杂的，也是非常关键的一步，是从调用VolcanoPlanner.setRoot方法开始间接触发的，如果只是静态的看代码不跑起来跟踪调试多半摸不清它的核心流程的。筛选出来的优化规则会封装成VolcanoRuleMatch，然后扔到RuleQueue里，而这个RuleQueue正是接下来执行动态规划算法要用到的核心类。筛选规则这一步的代码实现很晦涩。<br>第三步才到VolcanoPlanner.findBestExp，本质上就是一个动态规划算法的实现，但是最值得关注的还是怎么用第二步筛选出来的规则对RelNode Tree进行变换，变换后的形式还是一棵RelNode Tree，最常见的是把LogicalXXX开头的RelNode子类换成了EnumerableXXX或BindableXXX，总而言之，看看具体优化规则的实现就对了，都是繁琐的体力活。<br>一个优化器，理解了上面所说的三步基本上就抓住重点了。<br>—— 来自【zhh-4096 】的微博</p>
</blockquote>
<p>下面详细讲述一下这两种 planner 在 Calcite 内部的具体实现。</p>
<h2 id="HepPlanner"><a href="#HepPlanner" class="headerlink" title="HepPlanner"></a>HepPlanner</h2><h3 id="HepPlanner-中的基本概念"><a href="#HepPlanner-中的基本概念" class="headerlink" title="HepPlanner 中的基本概念"></a>HepPlanner 中的基本概念</h3><p>这里先看下 HepPlanner 的一些基本概念，对于后面的理解很有帮助。</p>
<h4 id="HepRelVertex"><a href="#HepRelVertex" class="headerlink" title="HepRelVertex"></a>HepRelVertex</h4><p>HepRelVertex 是对 RelNode 进行了简单封装。HepPlanner 中的所有节点都是 HepRelVertex，每个 HepRelVertex 都指向了一个真正的 RelNode 节点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.calcite.plan.hep.HepRelVertex</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * HepRelVertex wraps a real &#123;<span class="doctag">@link</span> RelNode&#125; as a vertex in a DAG representing</span></span><br><span class="line"><span class="comment"> * the entire query expression.</span></span><br><span class="line"><span class="comment"> * note：HepRelVertex 将一个 RelNode 封装为一个 DAG 中的 vertex（DAG 代表整个 query expression）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HepRelVertex</span> <span class="keyword">extends</span> <span class="title">AbstractRelNode</span> </span>&#123;</span><br><span class="line">  <span class="comment">//~ Instance fields --------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Wrapped rel currently chosen for implementation of expression.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> RelNode currentRel;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="HepInstruction"><a href="#HepInstruction" class="headerlink" title="HepInstruction"></a>HepInstruction</h4><p>HepInstruction 是 HepPlanner 对一些内容的封装，具体的子类实现比较多，其中 RuleInstance 是 HepPlanner 中对 Rule 的一个封装，注册的 Rule 最后都会转换为这种形式。</p>
<blockquote>
<p>HepInstruction represents one instruction in a HepProgram. </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepInstruction</span></span><br><span class="line"><span class="comment">/** Instruction that executes a given rule. */</span></span><br><span class="line"><span class="comment">//note: 执行指定 rule 的 Instruction</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RuleInstance</span> <span class="keyword">extends</span> <span class="title">HepInstruction</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Description to look for, or null if rule specified explicitly.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  String ruleDescription;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Explicitly specified rule, or rule looked up by planner from</span></span><br><span class="line"><span class="comment">   * description.</span></span><br><span class="line"><span class="comment">   * note：设置其 Rule</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelOptRule rule;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">initialize</span><span class="params">(<span class="keyword">boolean</span> clearCache)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!clearCache) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ruleDescription != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// Look up anew each run.</span></span><br><span class="line">      rule = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">execute</span><span class="params">(HepPlanner planner)</span> </span>&#123;</span><br><span class="line">    planner.executeInstruction(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="HepPlanner-处理流程"><a href="#HepPlanner-处理流程" class="headerlink" title="HepPlanner 处理流程"></a>HepPlanner 处理流程</h3><p>下面这个示例是上篇文章（<a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a>）的示例，通过这段代码来看下 HepPlanner 的内部实现机制。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HepProgramBuilder builder = <span class="keyword">new</span> HepProgramBuilder();</span><br><span class="line">builder.addRuleInstance(FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN); <span class="comment">//note: 添加 rule</span></span><br><span class="line">HepPlanner hepPlanner = <span class="keyword">new</span> HepPlanner(builder.build());</span><br><span class="line">hepPlanner.setRoot(relNode);</span><br><span class="line">relNode = hepPlanner.findBestExp();</span><br></pre></td></tr></table></figure>
<p>上面的代码总共分为三步：</p>
<ol>
<li>初始化 HepProgram 对象；</li>
<li>初始化 HepPlanner 对象，并通过 <code>setRoot()</code> 方法将 RelNode 树转换成 HepPlanner 内部使用的 Graph；</li>
<li>通过 <code>findBestExp()</code> 找到最优的 plan，规则的匹配都是在这里进行。</li>
</ol>
<h4 id="1-初始化-HepProgram"><a href="#1-初始化-HepProgram" class="headerlink" title="1. 初始化 HepProgram"></a>1. 初始化 HepProgram</h4><p>这几步代码实现没有太多需要介绍的地方，先初始化 HepProgramBuilder 也是为了后面初始化 HepProgram 做准备，HepProgramBuilder 主要也就是提供了一些配置设置和添加规则的方法等，常用的方法如下：</p>
<ol>
<li><code>addRuleInstance()</code>：注册相应的规则；</li>
<li><code>addRuleCollection()</code>：这里是注册一个规则集合，先把规则放在一个集合里，再注册整个集合，如果规则多的话，一般是这种方式；</li>
<li><code>addMatchLimit()</code>：设置 MatchLimit，这个 rule match 次数的最大限制；</li>
</ol>
<p>HepProgram 这个类对于后面 HepPlanner 的优化很重要，它定义 Rule 匹配的顺序，默认按【深度优先】顺序，它可以提供以下几种（见 HepMatchOrder 类）：</p>
<ol>
<li><strong>ARBITRARY</strong>：按任意顺序匹配（因为它是有效的，而且大部分的 Rule 并不关心匹配顺序）；</li>
<li><strong>BOTTOM_UP</strong>：自下而上，先从子节点开始匹配；</li>
<li><strong>TOP_DOWN</strong>：自上而下，先从父节点开始匹配；</li>
<li><strong>DEPTH_FIRST</strong>：深度优先匹配，某些情况下比 ARBITRARY 高效（为了避免新的 vertex 产生后又从 root 节点开始匹配）。</li>
</ol>
<p>这个匹配顺序到底是什么呢？对于规则集合 rules，HepPlanner 的算法是：从一个节点开始，跟 rules 的所有 Rule 进行匹配，匹配上就进行转换操作，这个节点操作完，再进行下一个节点，这里的匹配顺序就是指的<strong>节点遍历顺序</strong>（这种方式的优劣，我们下面再说）。</p>
<h4 id="2-HepPlanner-setRoot（RelNode-–-gt-Graph）"><a href="#2-HepPlanner-setRoot（RelNode-–-gt-Graph）" class="headerlink" title="2. HepPlanner.setRoot（RelNode –&gt; Graph）"></a>2. HepPlanner.setRoot（RelNode –&gt; Graph）</h4><p>先看下 <code>setRoot()</code> 方法的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRoot</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 将 RelNode 转换为 DAG 表示</span></span><br><span class="line">  root = addRelToGraph(rel);</span><br><span class="line">  <span class="comment">//note: 仅仅是在 trace 日志中输出 Graph 信息</span></span><br><span class="line">  dumpGraph();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>HepPlanner 会先将所有 relNode tree 转化为 HepRelVertex，这时就构建了一个 Graph：将所有的 elNode 节点使用 Vertex 表示，Gragh 会记录每个 HepRelVertex 的 input 信息，这样就是构成了一张 graph。</p>
<p>在真正的实现时，递归逐渐将每个 relNode 转换为 HepRelVertex，并在 <code>graph</code> 中记录相关的信息，实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">//note: 根据 RelNode 构建一个 Graph</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> HepRelVertex <span class="title">addRelToGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Check if a transformation already produced a reference</span></span><br><span class="line">  <span class="comment">// to an existing vertex.</span></span><br><span class="line">  <span class="comment">//note: 检查这个 rel 是否在 graph 中转换了</span></span><br><span class="line">  <span class="keyword">if</span> (graph.vertexSet().contains(rel)) &#123;</span><br><span class="line">    <span class="keyword">return</span> (HepRelVertex) rel;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Recursively add children, replacing this rel's inputs</span></span><br><span class="line">  <span class="comment">// with corresponding child vertices.</span></span><br><span class="line">  <span class="comment">//note: 递归地增加子节点，使用子节点相关的 vertices 代替 rel 的 input</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; inputs = rel.getInputs();</span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; newInputs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="keyword">for</span> (RelNode input1 : inputs) &#123;</span><br><span class="line">    HepRelVertex childVertex = addRelToGraph(input1); <span class="comment">//note: 递归进行转换</span></span><br><span class="line">    newInputs.add(childVertex); <span class="comment">//note: 每个 HepRelVertex 只记录其 Input</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!Util.equalShallow(inputs, newInputs)) &#123; <span class="comment">//note: 不相等的情况下</span></span><br><span class="line">    RelNode oldRel = rel;</span><br><span class="line">    rel = rel.copy(rel.getTraitSet(), newInputs);</span><br><span class="line">    onCopy(oldRel, rel);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Compute digest first time we add to DAG,</span></span><br><span class="line">  <span class="comment">// otherwise can't get equivVertex for common sub-expression</span></span><br><span class="line">  <span class="comment">//note: 计算 relNode 的 digest</span></span><br><span class="line">  <span class="comment">//note: Digest 的意思是：</span></span><br><span class="line">  <span class="comment">//note: A short description of this relational expression's type, inputs, and</span></span><br><span class="line">  <span class="comment">//note: other properties. The string uniquely identifies the node; another node</span></span><br><span class="line">  <span class="comment">//note: is equivalent if and only if it has the same value.</span></span><br><span class="line">  rel.recomputeDigest();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// try to find equivalent rel only if DAG is allowed</span></span><br><span class="line">  <span class="comment">//note: 如果允许 DAG 的话，检查是否有一个等价的 HepRelVertex，有的话直接返回</span></span><br><span class="line">  <span class="keyword">if</span> (!noDag) &#123;</span><br><span class="line">    <span class="comment">// Now, check if an equivalent vertex already exists in graph.</span></span><br><span class="line">    String digest = rel.getDigest();</span><br><span class="line">    HepRelVertex equivVertex = mapDigestToVertex.get(digest);</span><br><span class="line">    <span class="keyword">if</span> (equivVertex != <span class="keyword">null</span>) &#123; <span class="comment">//note: 已经存在</span></span><br><span class="line">      <span class="comment">// Use existing vertex.</span></span><br><span class="line">      <span class="keyword">return</span> equivVertex;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// No equivalence:  create a new vertex to represent this rel.</span></span><br><span class="line">  <span class="comment">//note: 创建一个 vertex 代替 rel</span></span><br><span class="line">  HepRelVertex newVertex = <span class="keyword">new</span> HepRelVertex(rel);</span><br><span class="line">  graph.addVertex(newVertex); <span class="comment">//note: 记录 Vertex</span></span><br><span class="line">  updateVertex(newVertex, rel);<span class="comment">//note: 更新相关的缓存，比如 mapDigestToVertex map</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (RelNode input : rel.getInputs()) &#123; <span class="comment">//note: 设置 Edge</span></span><br><span class="line">    graph.addEdge(newVertex, (HepRelVertex) input);<span class="comment">//note: 记录与整个 Vertex 先关的 input</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  nTransformations++;</span><br><span class="line">  <span class="keyword">return</span> newVertex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里 HepPlanner 需要的 gragh 已经构建完成，通过 DEBUG 方式也能看到此时 HepPlanner root 变量的内容：</p>
<p><img src="/images/calcite/10-calcite.png" alt="Root 转换之后的内容"></p>
<h4 id="3-HepPlanner-findBestExp-规则优化"><a href="#3-HepPlanner-findBestExp-规则优化" class="headerlink" title="3. HepPlanner findBestExp 规则优化"></a>3. HepPlanner findBestExp 规则优化</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">// implement RelOptPlanner</span></span><br><span class="line"><span class="comment">//note: 优化器的核心，匹配规则进行优化</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">findBestExp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> root != <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 运行 HepProgram 算法(按 HepProgram 中的 instructions 进行相应的优化)</span></span><br><span class="line">  executeProgram(mainProgram);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get rid of everything except what's in the final plan.</span></span><br><span class="line">  <span class="comment">//note: 垃圾收集</span></span><br><span class="line">  collectGarbage();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> buildFinalPlan(root); <span class="comment">//note: 返回最后的结果，还是以 RelNode 表示</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的实现是在 <code>executeProgram()</code> 方法中，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">executeProgram</span><span class="params">(HepProgram program)</span> </span>&#123;</span><br><span class="line">  HepProgram savedProgram = currentProgram; <span class="comment">//note: 保留当前的 Program</span></span><br><span class="line">  currentProgram = program;</span><br><span class="line">  currentProgram.initialize(program == mainProgram);<span class="comment">//note: 如果是在同一个 Program 的话，保留上次 cache</span></span><br><span class="line">  <span class="keyword">for</span> (HepInstruction instruction : currentProgram.instructions) &#123;</span><br><span class="line">    instruction.execute(<span class="keyword">this</span>); <span class="comment">//note: 按 Rule 进行优化(会调用 executeInstruction 方法)</span></span><br><span class="line">    <span class="keyword">int</span> delta = nTransformations - nTransformationsLastGC;</span><br><span class="line">    <span class="keyword">if</span> (delta &gt; graphSizeLastGC) &#123;</span><br><span class="line">      <span class="comment">// The number of transformations performed since the last</span></span><br><span class="line">      <span class="comment">// garbage collection is greater than the number of vertices in</span></span><br><span class="line">      <span class="comment">// the graph at that time.  That means there should be a</span></span><br><span class="line">      <span class="comment">// reasonable amount of garbage to collect now.  We do it this</span></span><br><span class="line">      <span class="comment">// way to amortize garbage collection cost over multiple</span></span><br><span class="line">      <span class="comment">// instructions, while keeping the highwater memory usage</span></span><br><span class="line">      <span class="comment">// proportional to the graph size.</span></span><br><span class="line">      <span class="comment">//note: 进行转换的次数已经大于 DAG Graph 中的顶点数，这就意味着已经产生大量垃圾需要进行清理</span></span><br><span class="line">      collectGarbage();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  currentProgram = savedProgram;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里会遍历 HepProgram 中 instructions（记录注册的所有 HepInstruction），然后根据 instruction 的类型执行相应的 <code>executeInstruction()</code> 方法，如果instruction 是 <code>HepInstruction.MatchLimit</code> 类型，会执行 <code>executeInstruction(HepInstruction.MatchLimit instruction)</code> 方法，这个方法就是初始化 matchLimit 变量。对于 <code>HepInstruction.RuleInstance</code> 类型的 instruction 会执行下面的方法（前面的示例注册规则使用的是 <code>addRuleInstance()</code> 方法，所以返回的 rules 只有一个规则，如果注册规则的时候使用的是 <code>addRuleCollection()</code> 方法注册一个规则集合的话，这里会返回的 rules 就是那个规则集合）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">//note: 执行相应的 RuleInstance</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">executeInstruction</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    HepInstruction.RuleInstance instruction)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (skippingGroup()) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (instruction.rule == <span class="keyword">null</span>) &#123;<span class="comment">//note: 如果 rule 为 null，那么就按照 description 查找具体的 rule</span></span><br><span class="line">    <span class="keyword">assert</span> instruction.ruleDescription != <span class="keyword">null</span>;</span><br><span class="line">    instruction.rule =</span><br><span class="line">        getRuleByDescription(instruction.ruleDescription);</span><br><span class="line">    LOGGER.trace(<span class="string">"Looking up rule with description &#123;&#125;, found &#123;&#125;"</span>,</span><br><span class="line">        instruction.ruleDescription, instruction.rule);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 执行相应的 rule</span></span><br><span class="line">  <span class="keyword">if</span> (instruction.rule != <span class="keyword">null</span>) &#123;</span><br><span class="line">    applyRules(</span><br><span class="line">        Collections.singleton(instruction.rule),</span><br><span class="line">        <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来看 <code>applyRules()</code> 的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.hep.HepPlanner</span></span><br><span class="line"><span class="comment">//note: 执行 rule（forceConversions 默认 true）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">applyRules</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    Collection&lt;RelOptRule&gt; rules,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> forceConversions)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (currentProgram.group != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">assert</span> currentProgram.group.collecting;</span><br><span class="line">    currentProgram.group.ruleSet.addAll(rules);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  LOGGER.trace(<span class="string">"Applying rule set &#123;&#125;"</span>, rules);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 当遍历规则是 ARBITRARY 或 DEPTH_FIRST 时，设置为 false，此时不会从 root 节点开始，否则每次 restart 都从 root 节点开始</span></span><br><span class="line">  <span class="keyword">boolean</span> fullRestartAfterTransformation =</span><br><span class="line">      currentProgram.matchOrder != HepMatchOrder.ARBITRARY</span><br><span class="line">      &amp;&amp; currentProgram.matchOrder != HepMatchOrder.DEPTH_FIRST;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> nMatches = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> fixedPoint;</span><br><span class="line">  <span class="comment">//note: 两种情况会跳出循环，一种是达到 matchLimit 限制，一种是遍历一遍不会再有新的 transform 产生</span></span><br><span class="line">  <span class="keyword">do</span> &#123;</span><br><span class="line">    <span class="comment">//note: 按照遍历规则获取迭代器</span></span><br><span class="line">    Iterator&lt;HepRelVertex&gt; iter = getGraphIterator(root);</span><br><span class="line">    fixedPoint = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">      HepRelVertex vertex = iter.next();<span class="comment">//note: 遍历每个 HepRelVertex</span></span><br><span class="line">      <span class="keyword">for</span> (RelOptRule rule : rules) &#123;<span class="comment">//note: 遍历每个 rules</span></span><br><span class="line">        <span class="comment">//note: 进行规制匹配，也是真正进行相关操作的地方</span></span><br><span class="line">        HepRelVertex newVertex =</span><br><span class="line">            applyRule(rule, vertex, forceConversions);</span><br><span class="line">        <span class="keyword">if</span> (newVertex == <span class="keyword">null</span> || newVertex == vertex) &#123;</span><br><span class="line">          <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ++nMatches;</span><br><span class="line">        <span class="comment">//note: 超过 MatchLimit 的限制</span></span><br><span class="line">        <span class="keyword">if</span> (nMatches &gt;= currentProgram.matchLimit) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (fullRestartAfterTransformation) &#123;</span><br><span class="line">          <span class="comment">//note: 发生 transformation 后，从 root 节点再次开始</span></span><br><span class="line">          iter = getGraphIterator(root);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// To the extent possible, pick up where we left</span></span><br><span class="line">          <span class="comment">// off; have to create a new iterator because old</span></span><br><span class="line">          <span class="comment">// one was invalidated by transformation.</span></span><br><span class="line">          <span class="comment">//note: 尽可能从上次进行后的节点开始</span></span><br><span class="line">          iter = getGraphIterator(newVertex);</span><br><span class="line">          <span class="keyword">if</span> (currentProgram.matchOrder == HepMatchOrder.DEPTH_FIRST) &#123;</span><br><span class="line">            <span class="comment">//note: 这样做的原因就是为了防止有些 HepRelVertex 遗漏了 rule 的匹配（每次从 root 开始是最简单的算法），因为可能出现下推</span></span><br><span class="line">            nMatches =</span><br><span class="line">                depthFirstApply(iter, rules, forceConversions, nMatches);</span><br><span class="line">            <span class="keyword">if</span> (nMatches &gt;= currentProgram.matchLimit) &#123;</span><br><span class="line">              <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// Remember to go around again since we're</span></span><br><span class="line">          <span class="comment">// skipping some stuff.</span></span><br><span class="line">          <span class="comment">//note: 再来一遍，因为前面有跳过一些节点</span></span><br><span class="line">          fixedPoint = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">while</span> (!fixedPoint);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里会调用 <code>getGraphIterator()</code> 方法获取 HepRelVertex 的迭代器，迭代的策略（遍历的策略）跟前面说的顺序有关，默认使用的是【深度优先】，这段代码比较简单，就是遍历规则+遍历节点进行匹配转换，直到满足条件再退出，从这里也能看到 HepPlanner 的实现效率不是很高，它也无法保证能找出最优的结果。</p>
<p>总结一下，HepPlanner 在优化过程中，是先遍历规则，然后再对每个节点进行匹配转换，直到满足条件（超过限制次数或者规则遍历完一遍不会再有新的变化），其方法调用流程如下：</p>
<p><img src="/images/calcite/11-hep.png" alt="HepPlanner 处理流程"> </p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><h4 id="1-为什么要把-RelNode-转换-HepRelVertex-进行优化？带来的收益在哪里？"><a href="#1-为什么要把-RelNode-转换-HepRelVertex-进行优化？带来的收益在哪里？" class="headerlink" title="1. 为什么要把 RelNode 转换 HepRelVertex 进行优化？带来的收益在哪里？"></a>1. 为什么要把 RelNode 转换 HepRelVertex 进行优化？带来的收益在哪里？</h4><p>关于这个，能想到的就是：RelNode 是底层提供的抽象、偏底层一些，在优化器这一层，需要记录更多的信息，所以又做了一层封装。</p>
<h2 id="VolcanoPlanner"><a href="#VolcanoPlanner" class="headerlink" title="VolcanoPlanner"></a>VolcanoPlanner</h2><p>介绍完 HepPlanner 之后，接下来再来看下基于成本优化（CBO）模型在 Calcite 中是如何实现、如何落地的，关于 Volcano 理论内容建议先看下相关理论知识，否则直接看实现的话可能会有一些头大。从 Volcano 模型的理论落地到实践是有很大区别的，这里先看一张 VolcanoPlanner 整体实现图，如下所示（图片来自 <a href="https://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-phoenix-using-apache-calcite?qid=b7a1ca0f-e7bf-49ad-bc51-0615ec8a4971&amp;v=&amp;b=&amp;from_search=4" target="_blank" rel="external">Cost-based Query Optimization in Apache Phoenix using Apache Calcite</a>）：</p>
<p><img src="/images/calcite/12-VolcanoPlanner.png" alt="Calcite VolcanoPlanner Process"></p>
<p>上面基本展现了 VolcanoPlanner 内部实现的流程，也简单介绍了 VolcanoPlanner 在实现中的一些关键点（有些概念暂时不了解也不要紧，后面会介绍）：</p>
<ol>
<li>Add Rule matches to Queue：向 Rule Match Queue 中添加相应的 Rule Match；</li>
<li>Apply Rule match transformations to plan gragh：应用 Rule Match 对 plan graph 做 transformation 优化（Rule specifies an Operator sub-graph to match and logic to generate equivalent better sub-graph）；</li>
<li>Iterate for fixed iterations or until cost doesn’t change：进行相应的迭代，直到 cost 不再变化或者 Rule Match Queue 中 rule match 已经全部应用完成；</li>
<li>Match importance based on cost of RelNode and height：Rule Match 的 importance 依赖于 RelNode 的 cost 和深度。</li>
</ol>
<p>下面来看下 VolcanoPlanner 实现具体的细节。</p>
<h3 id="VolcanoPlanner-中的基本概念"><a href="#VolcanoPlanner-中的基本概念" class="headerlink" title="VolcanoPlanner 中的基本概念"></a>VolcanoPlanner 中的基本概念</h3><p>VolcanoPlanner 在实现中引入了一些基本概念，先明白这些概念对于理解 VolcanoPlanner 的实现非常有帮助。</p>
<h4 id="RelSet"><a href="#RelSet" class="headerlink" title="RelSet"></a>RelSet</h4><p>关于 RelSet，源码中介绍如下：</p>
<blockquote>
<p>RelSet is an equivalence-set of expressions that is, a set of expressions which have <strong>identical semantics</strong>.<br>We are generally interested in using the expression which has <strong>the lowest cost</strong>.<br>All of the expressions in an RelSet have the <strong>same calling convention</strong>.</p>
</blockquote>
<p>它有以下特点：</p>
<ol>
<li>描述一组等价 Relation Expression，所有的 RelNode 会记录在 <code>rels</code> 中；</li>
<li>have the same calling convention；</li>
<li>具有相同物理属性的 Relational Expression 会记录在其成员变量 <code>List&lt;RelSubset&gt; subsets</code> 中.</li>
</ol>
<p>RelSet 中比较重要成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RelSet</span> </span>&#123;</span><br><span class="line">   <span class="comment">// 记录属于这个 RelSet 的所有 RelNode</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; rels = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Relational expressions that have a subset in this set as a child. This</span></span><br><span class="line"><span class="comment">   * is a multi-set. If multiple relational expressions in this set have the</span></span><br><span class="line"><span class="comment">   * same parent, there will be multiple entries.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelNode&gt; parents = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">//note: 具体相同物理属性的子集合（本质上 RelSubset 并不记录 RelNode，也是通过 RelSet 按物理属性过滤得到其 RelNode 子集合，见下面的 RelSubset 部分）</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;RelSubset&gt; subsets = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * List of &#123;<span class="doctag">@link</span> AbstractConverter&#125; objects which have not yet been</span></span><br><span class="line"><span class="comment">   * satisfied.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> List&lt;AbstractConverter&gt; abstractConverters = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Set to the superseding set when this is found to be equivalent to another</span></span><br><span class="line"><span class="comment">   * set.</span></span><br><span class="line"><span class="comment">   * note：当发现与另一个 RelSet 有相同的语义时，设置为替代集合</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelSet equivalentSet;</span><br><span class="line">  RelNode rel;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Variables that are set by relational expressions in this set and available for use by parent and child expressions.</span></span><br><span class="line"><span class="comment">   * note：在这个集合中 relational expression 设置的变量，父类和子类 expression 可用的变量</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> Set&lt;CorrelationId&gt; variablesPropagated;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Variables that are used by relational expressions in this set.</span></span><br><span class="line"><span class="comment">   * note：在这个集合中被 relational expression 使用的变量</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> Set&lt;CorrelationId&gt; variablesUsed;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Reentrancy flag.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">boolean</span> inMetadataQuery;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="RelSubset"><a href="#RelSubset" class="headerlink" title="RelSubset"></a>RelSubset</h4><p>关于 RelSubset，源码中介绍如下：</p>
<blockquote>
<p>Subset of an equivalence class where all relational expressions have the same physical properties.</p>
</blockquote>
<p>它的特点如下：</p>
<ol>
<li>描述一组物理属性相同的等价 Relation Expression，即它们具有相同的 Physical Properties；</li>
<li>每个 RelSubset 都会记录其所属的 RelSet；</li>
<li>RelSubset 继承自 AbstractRelNode，它也是一种 RelNode，物理属性记录在其成员变量 traitSet 中。</li>
</ol>
<p>RelSubset 一些比较重要的成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RelSubset</span> <span class="keyword">extends</span> <span class="title">AbstractRelNode</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * cost of best known plan (it may have improved since)</span></span><br><span class="line"><span class="comment">   * note: 已知最佳 plan 的 cost</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelOptCost bestCost;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The set this subset belongs to.</span></span><br><span class="line"><span class="comment">   * RelSubset 所属的 RelSet，在 RelSubset 中并不记录具体的 RelNode，直接记录在 RelSet 的 rels 中</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> RelSet set;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * best known plan</span></span><br><span class="line"><span class="comment">   * note: 已知的最佳 plan</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  RelNode best;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Flag indicating whether this RelSubset's importance was artificially</span></span><br><span class="line"><span class="comment">   * boosted.</span></span><br><span class="line"><span class="comment">   * note: 标志这个 RelSubset 的 importance 是否是人为地提高了</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">boolean</span> boosted;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//~ Constructors -----------------------------------------------------------</span></span><br><span class="line">  RelSubset(</span><br><span class="line">      RelOptCluster cluster,</span><br><span class="line">      RelSet set,</span><br><span class="line">      RelTraitSet traits) &#123;</span><br><span class="line">    <span class="keyword">super</span>(cluster, traits); <span class="comment">// 继承自 AbstractRelNode，会记录其相应的 traits 信息</span></span><br><span class="line">    <span class="keyword">this</span>.set = set;</span><br><span class="line">    <span class="keyword">this</span>.boosted = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">assert</span> traits.allSimple();</span><br><span class="line">    computeBestCost(cluster.getPlanner()); <span class="comment">//note: 计算 best</span></span><br><span class="line">    recomputeDigest(); <span class="comment">//note: 计算 digest</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个 RelSubset 都将会记录其最佳 plan（<code>best</code>）和最佳 plan 的 cost（<code>bestCost</code>）信息。</p>
<h4 id="RuleMatch"><a href="#RuleMatch" class="headerlink" title="RuleMatch"></a>RuleMatch</h4><p>RuleMatch 是这里对 Rule 和 RelSubset 关系的一个抽象，它会记录这两者的信息。</p>
<blockquote>
<p>A match of a rule to a particular set of target relational expressions, frozen in time.</p>
</blockquote>
<h4 id="importance"><a href="#importance" class="headerlink" title="importance"></a>importance</h4><p>importance 决定了在进行 Rule 优化时 Rule 应用的顺序，它是一个相对概念，在 VolcanoPlanner 中有两个 importance，分别是 RelSubset 和 RuleMatch 的 importance，这里先提前介绍一下。</p>
<h5 id="RelSubset-的-importance"><a href="#RelSubset-的-importance" class="headerlink" title="RelSubset 的 importance"></a>RelSubset 的 importance</h5><p>RelSubset importance 计算方法见其 api 定义（<strong>图中的 sum 改成 Math.max{}</strong>这个地方有误）：</p>
<p><img src="/images/calcite/13-compute.png" alt="computeImportance"> </p>
<p>举个例子：假设一个 RelSubset（记为 $s_0$） 的 cost 是3，对应的 importance 是0.5，这个 RelNode 有两个输入（inputs），对应的 RelSubset 记为 $s_1$、$s_2$（假设 $s_1$、$s_2$ 不再有输入 RelNode），其 cost 分别为 2和5，那么 $s_1$ 的 importance 为</p>
<p>Importance of $s_1$ = $\frac{2}{3+2+5}$ $\cdot$ 0.5 = 0.1</p>
<p>Importance of $s_2$ = $\frac{5}{3+2+5}$ $\cdot$ 0.5 = 0.25</p>
<p>其中，2代表的是 $s_1$ 的 cost，$3+2+5$ 代表的是 $s_0$ 的 cost（本节点的 cost 加上其所有 input 的 cost）。下面看下其具体的代码实现（调用 RuleQueue 中的 <code>recompute()</code> 计算其 importance）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.RuleQueue</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Recomputes the importance of the given RelSubset.</span></span><br><span class="line"><span class="comment"> * note：重新计算指定的 RelSubset 的 importance</span></span><br><span class="line"><span class="comment"> * note：如果为 true，即使 subset 没有注册，也会强制 importance 更新</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> subset RelSubset whose importance is to be recomputed</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> force  if true, forces an importance update even if the subset has</span></span><br><span class="line"><span class="comment"> *               not been registered</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">recompute</span><span class="params">(RelSubset subset, <span class="keyword">boolean</span> force)</span> </span>&#123;</span><br><span class="line">  Double previousImportance = subsetImportances.get(subset);</span><br><span class="line">  <span class="keyword">if</span> (previousImportance == <span class="keyword">null</span>) &#123; <span class="comment">//note: subset 还没有注册的情况下</span></span><br><span class="line">    <span class="keyword">if</span> (!force) &#123; <span class="comment">//note: 如果不是强制，可以直接先返回</span></span><br><span class="line">      <span class="comment">// Subset has not been registered yet. Don't worry about it.</span></span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    previousImportance = Double.NEGATIVE_INFINITY;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 计算器 importance 值</span></span><br><span class="line">  <span class="keyword">double</span> importance = computeImportance(subset);</span><br><span class="line">  <span class="keyword">if</span> (previousImportance == importance) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 缓存中更新其 importance</span></span><br><span class="line">  updateImportance(subset, importance);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算一个节点的 importance</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">computeImportance</span><span class="params">(RelSubset subset)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">double</span> importance;</span><br><span class="line">  <span class="keyword">if</span> (subset == planner.root) &#123;</span><br><span class="line">    <span class="comment">// The root always has importance = 1</span></span><br><span class="line">    <span class="comment">//note: root RelSubset 的 importance 为1</span></span><br><span class="line">    importance = <span class="number">1.0</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> RelMetadataQuery mq = subset.getCluster().getMetadataQuery();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The importance of a subset is the max of its importance to its</span></span><br><span class="line">    <span class="comment">// parents</span></span><br><span class="line">    <span class="comment">//note: 计算其相对于 parent 的最大 importance，多个 parent 的情况下，选择一个最大值</span></span><br><span class="line">    importance = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (RelSubset parent : subset.getParentSubsets(planner)) &#123;</span><br><span class="line">      <span class="comment">//note: 计算这个 RelSubset 相对于 parent 的 importance</span></span><br><span class="line">      <span class="keyword">final</span> <span class="keyword">double</span> childImportance =</span><br><span class="line">          computeImportanceOfChild(mq, subset, parent);</span><br><span class="line">      <span class="comment">//note: 选择最大的 importance</span></span><br><span class="line">      importance = Math.max(importance, childImportance);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  LOGGER.trace(<span class="string">"Importance of [&#123;&#125;] is &#123;&#125;"</span>, subset, importance);</span><br><span class="line">  <span class="keyword">return</span> importance;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note：根据 cost 计算 child 相对于 parent 的 importance（这是个相对值）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">double</span> <span class="title">computeImportanceOfChild</span><span class="params">(RelMetadataQuery mq, RelSubset child,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelSubset parent)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 获取 parent 的 importance</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> parentImportance = getImportance(parent);</span><br><span class="line">  <span class="comment">//note: 获取对应的 cost 信息</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> childCost = toDouble(planner.getCost(child, mq));</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> parentCost = toDouble(planner.getCost(parent, mq));</span><br><span class="line">  <span class="keyword">double</span> alpha = childCost / parentCost;</span><br><span class="line">  <span class="keyword">if</span> (alpha &gt;= <span class="number">1.0</span>) &#123;</span><br><span class="line">    <span class="comment">// child is always less important than parent</span></span><br><span class="line">    alpha = <span class="number">0.99</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 根据 cost 比列计算其 importance</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">double</span> importance = parentImportance * alpha;</span><br><span class="line">  LOGGER.trace(<span class="string">"Importance of [&#123;&#125;] to its parent [&#123;&#125;] is &#123;&#125; (parent importance=&#123;&#125;, child cost=&#123;&#125;,"</span></span><br><span class="line">      + <span class="string">" parent cost=&#123;&#125;)"</span>, child, parent, importance, parentImportance, childCost, parentCost);</span><br><span class="line">  <span class="keyword">return</span> importance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>computeImportanceOfChild()</code> 中计算 RelSubset 相对于 parent RelSubset 的 importance 时，一个比较重要的地方就是如何计算 cost，关于 cost 的计算见：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">//note: Computes the cost of a RelNode.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelOptCost <span class="title">getCost</span><span class="params">(RelNode rel, RelMetadataQuery mq)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> rel != <span class="keyword">null</span> : <span class="string">"pre-condition: rel != null"</span>;</span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> RelSubset) &#123; <span class="comment">//note: 如果是 RelSubset，证明是已经计算 cost 的 subset</span></span><br><span class="line">    <span class="keyword">return</span> ((RelSubset) rel).bestCost;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (rel.getTraitSet().getTrait(ConventionTraitDef.INSTANCE)</span><br><span class="line">      == Convention.NONE) &#123;</span><br><span class="line">    <span class="keyword">return</span> costFactory.makeInfiniteCost(); <span class="comment">//note: 这种情况下也会返回 infinite Cost</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 计算其 cost</span></span><br><span class="line">  RelOptCost cost = mq.getNonCumulativeCost(rel);</span><br><span class="line">  <span class="keyword">if</span> (!zeroCost.isLt(cost)) &#123; <span class="comment">//note: cost 比0还小的情况</span></span><br><span class="line">    <span class="comment">// cost must be positive, so nudge it</span></span><br><span class="line">    cost = costFactory.makeTinyCost();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: RelNode 的 cost 会把其 input 全部加上</span></span><br><span class="line">  <span class="keyword">for</span> (RelNode input : rel.getInputs()) &#123;</span><br><span class="line">    cost = cost.plus(getCost(input, mq));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> cost;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面就是 RelSubset importance 计算的代码实现，从实现中可以发现这个特点：</p>
<ol>
<li>越靠近 root 的 RelSubset，其 importance 越大，这个带来的好处就是在优化时，会尽量先优化靠近 root 的 RelNode，这样带来的收益也会最大。</li>
</ol>
<h5 id="RuleMatch-的-importance"><a href="#RuleMatch-的-importance" class="headerlink" title="RuleMatch 的 importance"></a>RuleMatch 的 importance</h5><p>RuleMatch 的 importance 定义为以下两个中比较大的一个（如果对应的 RelSubset 有 importance 的情况下）：</p>
<ol>
<li>这个 RuleMatch 对应 RelSubset（这个 rule match 的 RelSubset）的 importance；</li>
<li>输出的 RelSubset（taget RelSubset）的 importance（如果这个 RelSubset 在 VolcanoPlanner 的缓存中存在的话）。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoRuleMatch</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Computes the importance of this rule match.</span></span><br><span class="line"><span class="comment"> * note：计算 rule match 的 importance</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> importance of this rule match</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">computeImportance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> rels[<span class="number">0</span>] != <span class="keyword">null</span>; <span class="comment">//note: rels[0] 这个 Rule Match 对应的 RelSubset</span></span><br><span class="line">  RelSubset subset = volcanoPlanner.getSubset(rels[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">double</span> importance = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (subset != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">//note: 获取 RelSubset 的 importance</span></span><br><span class="line">    importance = volcanoPlanner.ruleQueue.getImportance(subset);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: Returns a guess as to which subset the result of this rule will belong to.</span></span><br><span class="line">  <span class="keyword">final</span> RelSubset targetSubset = guessSubset();</span><br><span class="line">  <span class="keyword">if</span> ((targetSubset != <span class="keyword">null</span>) &amp;&amp; (targetSubset != subset)) &#123;</span><br><span class="line">    <span class="comment">// If this rule will generate a member of an equivalence class</span></span><br><span class="line">    <span class="comment">// which is more important, use that importance.</span></span><br><span class="line">    <span class="comment">//note: 获取 targetSubset 的 importance</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">double</span> targetImportance =</span><br><span class="line">        volcanoPlanner.ruleQueue.getImportance(targetSubset);</span><br><span class="line">    <span class="keyword">if</span> (targetImportance &gt; importance) &#123;</span><br><span class="line">      importance = targetImportance;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If the equivalence class is cheaper than the target, bump up</span></span><br><span class="line">      <span class="comment">// the importance of the rule. A converter is an easy way to</span></span><br><span class="line">      <span class="comment">// make the plan cheaper, so we'd hate to miss this opportunity.</span></span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="comment">// REVIEW: jhyde, 2007/12/21: This rule seems to make sense, but</span></span><br><span class="line">      <span class="comment">// is disabled until it has been proven.</span></span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">      <span class="comment">// CHECKSTYLE: IGNORE 3</span></span><br><span class="line">      <span class="keyword">if</span> ((subset != <span class="keyword">null</span>)</span><br><span class="line">          &amp;&amp; subset.bestCost.isLt(targetSubset.bestCost)</span><br><span class="line">          &amp;&amp; <span class="keyword">false</span>) &#123; <span class="comment">//note: 肯定不会进入</span></span><br><span class="line">        importance *=</span><br><span class="line">            targetSubset.bestCost.divideBy(subset.bestCost);</span><br><span class="line">        importance = Math.min(importance, <span class="number">0.99</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> importance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>RuleMatch 的 importance 主要是决定了在选择 RuleMatch 时，应该先处理哪一个？它本质上还是直接用的 RelSubset 的 importance。</p>
<h3 id="VolcanoPlanner-处理流程"><a href="#VolcanoPlanner-处理流程" class="headerlink" title="VolcanoPlanner 处理流程"></a>VolcanoPlanner 处理流程</h3><p>还是以前面的示例，只不过这里把优化器换成 VolcanoPlanner 来实现，通过这个示例来详细看下 VolcanoPlanner 内部的实现逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1. 初始化 VolcanoPlanner 对象，并添加相应的 Rule</span></span><br><span class="line">VolcanoPlanner planner = <span class="keyword">new</span> VolcanoPlanner();</span><br><span class="line">planner.addRelTraitDef(ConventionTraitDef.INSTANCE);</span><br><span class="line">planner.addRelTraitDef(RelDistributionTraitDef.INSTANCE);</span><br><span class="line"><span class="comment">// 添加相应的 rule</span></span><br><span class="line">planner.addRule(FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN);</span><br><span class="line">planner.addRule(ReduceExpressionsRule.PROJECT_INSTANCE);</span><br><span class="line">planner.addRule(PruneEmptyRules.PROJECT_INSTANCE);</span><br><span class="line"><span class="comment">// 添加相应的 ConverterRule</span></span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_MERGE_JOIN_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_SORT_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_VALUES_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_PROJECT_RULE);</span><br><span class="line">planner.addRule(EnumerableRules.ENUMERABLE_FILTER_RULE);</span><br><span class="line"><span class="comment">//2. Changes a relational expression to an equivalent one with a different set of traits.</span></span><br><span class="line">RelTraitSet desiredTraits =</span><br><span class="line">    relNode.getCluster().traitSet().replace(EnumerableConvention.INSTANCE);</span><br><span class="line">relNode = planner.changeTraits(relNode, desiredTraits);</span><br><span class="line"><span class="comment">//3. 通过 VolcanoPlanner 的 setRoot 方法注册相应的 RelNode，并进行相应的初始化操作</span></span><br><span class="line">planner.setRoot(relNode);</span><br><span class="line"><span class="comment">//4. 通过动态规划算法找到 cost 最小的 plan</span></span><br><span class="line">relNode = planner.findBestExp();</span><br></pre></td></tr></table></figure>
<p>优化后的结果为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EnumerableSort(sort0=[<span class="variable">$0</span>], dir0=[ASC])</span><br><span class="line">  EnumerableProject(USER_ID=[<span class="variable">$0</span>], USER_NAME=[<span class="variable">$1</span>], USER_COMPANY=[<span class="variable">$5</span>], USER_AGE=[<span class="variable">$2</span>])</span><br><span class="line">    EnumerableMergeJoin(condition=[=(<span class="variable">$0</span>, <span class="variable">$3</span>)], joinType=[inner])</span><br><span class="line">      EnumerableFilter(condition=[&gt;(<span class="variable">$2</span>, 30)])</span><br><span class="line">        EnumerableTableScan(table=[[USERS]])</span><br><span class="line">      EnumerableFilter(condition=[&gt;(<span class="variable">$0</span>, 10)])</span><br><span class="line">        EnumerableTableScan(table=[[JOBS]])</span><br></pre></td></tr></table></figure>
<p>在应用 VolcanoPlanner 时，整体分为以下四步：</p>
<ol>
<li>初始化 VolcanoPlanner，并添加相应的 Rule（包括 ConverterRule）；</li>
<li>对 RelNode 做等价转换，这里只是改变其物理属性（<code>Convention</code>）；</li>
<li>通过 VolcanoPlanner 的 <code>setRoot()</code> 方法注册相应的 RelNode，并进行相应的初始化操作；</li>
<li>通过动态规划算法找到 cost 最小的 plan；</li>
</ol>
<p>下面来分享一下上面的详细流程。</p>
<h4 id="1-VolcanoPlanner-初始化"><a href="#1-VolcanoPlanner-初始化" class="headerlink" title="1. VolcanoPlanner 初始化"></a>1. VolcanoPlanner 初始化</h4><p>在这里总共有三步，分别是 VolcanoPlanner 初始化，<code>addRelTraitDef()</code> 添加 RelTraitDef，<code>addRule()</code> 添加 rule，先看下 VolcanoPlanner 的初始化：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a uninitialized &lt;code&gt;VolcanoPlanner&lt;/code&gt;. To fully initialize it, the caller must register the desired set of relations, rules, and calling conventions.</span></span><br><span class="line"><span class="comment"> * note: 创建一个没有初始化的 VolcanoPlanner，如果要进行初始化，调用者必须注册 set of relations、rules、calling conventions.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">VolcanoPlanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(<span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a &#123;<span class="doctag">@code</span> VolcanoPlanner&#125; with a given cost factory.</span></span><br><span class="line"><span class="comment"> * note: 创建 VolcanoPlanner 实例，并制定 costFactory（默认为 VolcanoCost.FACTORY）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">VolcanoPlanner</span><span class="params">(RelOptCostFactory costFactory, //</span></span></span><br><span class="line"><span class="function"><span class="params">    Context externalContext)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">super</span>(costFactory == <span class="keyword">null</span> ? VolcanoCost.FACTORY : costFactory, <span class="comment">//</span></span><br><span class="line">      externalContext);</span><br><span class="line">  <span class="keyword">this</span>.zeroCost = <span class="keyword">this</span>.costFactory.makeZeroCost();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里其实并没有做什么，只是做了一些简单的初始化，如果要想设置相应 RelTraitDef 的话，需要调用 <code>addRelTraitDef()</code> 进行添加，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">//note: 添加 RelTraitDef</span></span><br><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addRelTraitDef</span><span class="params">(RelTraitDef relTraitDef)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> !traitDefs.contains(relTraitDef) &amp;&amp; traitDefs.add(relTraitDef);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果要给 VolcanoPlanner 添加 Rule 的话，需要调用 <code>addRule()</code> 进行添加，<strong>在这个方法里重点做的一步是将具体的 RelNode 与 RelOptRuleOperand 之间的关系记录下来，记录到 <code>classOperands</code> 中</strong>，相当于在优化时，哪个 RelNode 可以应用哪些 Rule 都是记录在这个缓存里的。其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">//note: 添加 rule</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">addRule</span><span class="params">(RelOptRule rule)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (locked) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (ruleSet.contains(rule)) &#123;</span><br><span class="line">    <span class="comment">// Rule already exists.</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">boolean</span> added = ruleSet.add(rule);</span><br><span class="line">  <span class="keyword">assert</span> added;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> String ruleName = rule.toString();</span><br><span class="line">  <span class="comment">//note: 这里的 ruleNames 允许重复的 key 值，但是这里还是要求 rule description 保持唯一的，与 rule 一一对应</span></span><br><span class="line">  <span class="keyword">if</span> (ruleNames.put(ruleName, rule.getClass())) &#123;</span><br><span class="line">    Set&lt;Class&gt; x = ruleNames.get(ruleName);</span><br><span class="line">    <span class="keyword">if</span> (x.size() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Rule description '"</span> + ruleName</span><br><span class="line">          + <span class="string">"' is not unique; classes: "</span> + x);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 注册一个 rule 的 description（保存在 mapDescToRule 中）</span></span><br><span class="line">  mapRuleDescription(rule);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Each of this rule's operands is an 'entry point' for a rule call. Register each operand against all concrete sub-classes that could match it.</span></span><br><span class="line">  <span class="comment">//note: 记录每个 sub-classes 与 operand 的关系（如果能 match 的话，就记录一次）。一个 RelOptRuleOperand 只会有一个 class 与之对应，这里找的是 subclass</span></span><br><span class="line">  <span class="keyword">for</span> (RelOptRuleOperand operand : rule.getOperands()) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Class&lt;? extends RelNode&gt; subClass</span><br><span class="line">        : subClasses(operand.getMatchedClass())) &#123;</span><br><span class="line">      classOperands.put(subClass, operand);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If this is a converter rule, check that it operates on one of the</span></span><br><span class="line">  <span class="comment">// kinds of trait we are interested in, and if so, register the rule</span></span><br><span class="line">  <span class="comment">// with the trait.</span></span><br><span class="line">  <span class="comment">//note: 对于 ConverterRule 的操作，如果其 ruleTraitDef 类型包含在我们初始化的 traitDefs 中，</span></span><br><span class="line">  <span class="comment">//note: 就注册这个 converterRule 到 ruleTraitDef 中</span></span><br><span class="line">  <span class="comment">//note: 如果不包含 ruleTraitDef，这个 ConverterRule 在本次优化的过程中是用不到的</span></span><br><span class="line">  <span class="keyword">if</span> (rule <span class="keyword">instanceof</span> ConverterRule) &#123;</span><br><span class="line">    ConverterRule converterRule = (ConverterRule) rule;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> RelTrait ruleTrait = converterRule.getInTrait();</span><br><span class="line">    <span class="keyword">final</span> RelTraitDef ruleTraitDef = ruleTrait.getTraitDef();</span><br><span class="line">    <span class="keyword">if</span> (traitDefs.contains(ruleTraitDef)) &#123; <span class="comment">//note: 这里注册好像也没有用到</span></span><br><span class="line">      ruleTraitDef.registerConverterRule(<span class="keyword">this</span>, converterRule);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-RelNode-changeTraits"><a href="#2-RelNode-changeTraits" class="headerlink" title="2. RelNode changeTraits"></a>2. RelNode changeTraits</h4><p>这里分为两步：</p>
<ol>
<li>通过 RelTraitSet 的 <code>replace()</code> 方法，将 RelTraitSet 中对应的 RelTraitDef 做对应的更新，其他的 RelTrait 不变；</li>
<li>这一步简单来说就是：Changes a relational expression to an equivalent one with a different set of traits，对相应的 RelNode 做 converter 操作，这里实际上也会做很多的内容，这部分会放在第三步讲解，主要是 <code>registerImpl()</code> 方法的实现。</li>
</ol>
<h4 id="3-VolcanoPlanner-setRoot"><a href="#3-VolcanoPlanner-setRoot" class="headerlink" title="3. VolcanoPlanner setRoot"></a>3. VolcanoPlanner setRoot</h4><p>VolcanoPlanner 会调用 <code>setRoot()</code> 方法注册相应的 Root RelNode，并进行一系列 Volcano 必须的初始化操作，很多的操作都是在这里实现的，这里来详细看下其实现。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRoot</span><span class="params">(RelNode rel)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// We're registered all the rules, and therefore RelNode classes,</span></span><br><span class="line">  <span class="comment">// we're interested in, and have not yet started calling metadata providers.</span></span><br><span class="line">  <span class="comment">// So now is a good time to tell the metadata layer what to expect.</span></span><br><span class="line">  registerMetadataRels();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 注册相应的 RelNode，会做一系列的初始化操作, RelNode 会有对应的 RelSubset</span></span><br><span class="line">  <span class="keyword">this</span>.root = registerImpl(rel, <span class="keyword">null</span>);</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>.originalRoot == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>.originalRoot = rel;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Making a node the root changes its importance.</span></span><br><span class="line">  <span class="comment">//note: 重新计算 root subset 的 importance</span></span><br><span class="line">  <span class="keyword">this</span>.ruleQueue.recompute(<span class="keyword">this</span>.root);</span><br><span class="line">  <span class="comment">//Ensures that the subset that is the root relational expression contains converters to all other subsets in its equivalence set.</span></span><br><span class="line">  ensureRootConverters();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于 <code>setRoot()</code> 方法来说，核心的处理流程是在 <code>registerImpl()</code> 方法中，在这个方法会进行相应的初始化操作（包括 RelNode 到 RelSubset 的转换、计算 RelSubset 的 importance 等），其他的方法在上面有相应的备注，这里我们看下 <code>registerImpl()</code> 具体做了哪些事情：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Registers a new expression &lt;code&gt;exp&lt;/code&gt; and queues up rule matches.</span></span><br><span class="line"><span class="comment"> * If &lt;code&gt;set&lt;/code&gt; is not null, makes the expression part of that</span></span><br><span class="line"><span class="comment"> * equivalence set. If an identical expression is already registered, we</span></span><br><span class="line"><span class="comment"> * don't need to register this one and nor should we queue up rule matches.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：注册一个新的 expression；对 rule match 进行排队；</span></span><br><span class="line"><span class="comment"> * note：如果 set 不为 null，那么就使 expression 成为等价集合（RelSet）的一部分</span></span><br><span class="line"><span class="comment"> * note：rel：必须是 RelSubset 或者未注册的 RelNode</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rel relational expression to register. Must be either a</span></span><br><span class="line"><span class="comment"> *         &#123;<span class="doctag">@link</span> RelSubset&#125;, or an unregistered &#123;<span class="doctag">@link</span> RelNode&#125;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> set set that rel belongs to, or &lt;code&gt;null&lt;/code&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the equivalence-set</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> RelSubset <span class="title">registerImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    RelNode rel,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelSet set)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> RelSubset) &#123; <span class="comment">//note: 如果是 RelSubset 类型，已经注册过了</span></span><br><span class="line">    <span class="keyword">return</span> registerSubset(set, (RelSubset) rel); <span class="comment">//note: 做相应的 merge</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">assert</span> !isRegistered(rel) : <span class="string">"already been registered: "</span> + rel;</span><br><span class="line">  <span class="keyword">if</span> (rel.getCluster().getPlanner() != <span class="keyword">this</span>) &#123; <span class="comment">//note: cluster 中 planner 与这里不同</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Relational expression "</span> + rel</span><br><span class="line">        + <span class="string">" belongs to a different planner than is currently being used."</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Now is a good time to ensure that the relational expression</span></span><br><span class="line">  <span class="comment">// implements the interface required by its calling convention.</span></span><br><span class="line">  <span class="comment">//note: 确保 relational expression 可以实施其 calling convention 所需的接口</span></span><br><span class="line">  <span class="comment">//note: 获取 RelNode 的 RelTraitSet</span></span><br><span class="line">  <span class="keyword">final</span> RelTraitSet traits = rel.getTraitSet();</span><br><span class="line">  <span class="comment">//note: 获取其 ConventionTraitDef</span></span><br><span class="line">  <span class="keyword">final</span> Convention convention = traits.getTrait(ConventionTraitDef.INSTANCE);</span><br><span class="line">  <span class="keyword">assert</span> convention != <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">if</span> (!convention.getInterface().isInstance(rel)</span><br><span class="line">      &amp;&amp; !(rel <span class="keyword">instanceof</span> Converter)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Relational expression "</span> + rel</span><br><span class="line">        + <span class="string">" has calling-convention "</span> + convention</span><br><span class="line">        + <span class="string">" but does not implement the required interface '"</span></span><br><span class="line">        + convention.getInterface() + <span class="string">"' of that convention"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (traits.size() != traitDefs.size()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Relational expression "</span> + rel</span><br><span class="line">        + <span class="string">" does not have the correct number of traits: "</span> + traits.size()</span><br><span class="line">        + <span class="string">" != "</span> + traitDefs.size());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Ensure that its sub-expressions are registered.</span></span><br><span class="line">  <span class="comment">//note: 其实现在 AbstractRelNode 对应的方法中，实际上调用的还是 ensureRegistered 方法进行注册</span></span><br><span class="line">  <span class="comment">//note: 将 RelNode 的所有 inputs 注册到 planner 中</span></span><br><span class="line">  <span class="comment">//note: 这里会递归调用 registerImpl 注册 relNode 与 RelSet，直到其 inputs 全部注册</span></span><br><span class="line">  <span class="comment">//note: 返回的是一个 RelSubset 类型</span></span><br><span class="line">  rel = rel.onRegister(<span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Record its provenance. (Rule call may be null.)</span></span><br><span class="line">  <span class="comment">//note: 记录 RelNode 的来源</span></span><br><span class="line">  <span class="keyword">if</span> (ruleCallStack.isEmpty()) &#123; <span class="comment">//note: 不知道来源时</span></span><br><span class="line">    provenanceMap.put(rel, Provenance.EMPTY);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 来自 rule 触发的情况</span></span><br><span class="line">    <span class="keyword">final</span> VolcanoRuleCall ruleCall = ruleCallStack.peek();</span><br><span class="line">    provenanceMap.put(</span><br><span class="line">        rel,</span><br><span class="line">        <span class="keyword">new</span> RuleProvenance(</span><br><span class="line">            ruleCall.rule,</span><br><span class="line">            ImmutableList.copyOf(ruleCall.rels),</span><br><span class="line">            ruleCall.id));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If it is equivalent to an existing expression, return the set that</span></span><br><span class="line">  <span class="comment">// the equivalent expression belongs to.</span></span><br><span class="line">  <span class="comment">//note: 根据 RelNode 的 digest（摘要，全局唯一）判断其是否已经有对应的 RelSubset，有的话直接放回</span></span><br><span class="line">  String key = rel.getDigest();</span><br><span class="line">  RelNode equivExp = mapDigestToRel.get(key);</span><br><span class="line">  <span class="keyword">if</span> (equivExp == <span class="keyword">null</span>) &#123; <span class="comment">//note: 还没注册的情况</span></span><br><span class="line">    <span class="comment">// do nothing</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (equivExp == rel) &#123;<span class="comment">//note: 已经有其缓存信息</span></span><br><span class="line">    <span class="keyword">return</span> getSubset(rel);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">assert</span> RelOptUtil.equal(</span><br><span class="line">        <span class="string">"left"</span>, equivExp.getRowType(),</span><br><span class="line">        <span class="string">"right"</span>, rel.getRowType(),</span><br><span class="line">        Litmus.THROW);</span><br><span class="line">    RelSet equivSet = getSet(equivExp); <span class="comment">//note: 有 RelSubset 但对应的 RelNode 不同时，这里对其 RelSet 做下 merge</span></span><br><span class="line">    <span class="keyword">if</span> (equivSet != <span class="keyword">null</span>) &#123;</span><br><span class="line">      LOGGER.trace(</span><br><span class="line">          <span class="string">"Register: rel#&#123;&#125; is equivalent to &#123;&#125;"</span>, rel.getId(), equivExp.getDescription());</span><br><span class="line">      <span class="keyword">return</span> registerSubset(set, getSubset(equivExp));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note： Converters are in the same set as their children.</span></span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> Converter) &#123;</span><br><span class="line">    <span class="keyword">final</span> RelNode input = ((Converter) rel).getInput();</span><br><span class="line">    <span class="keyword">final</span> RelSet childSet = getSet(input);</span><br><span class="line">    <span class="keyword">if</span> ((set != <span class="keyword">null</span>)</span><br><span class="line">        &amp;&amp; (set != childSet)</span><br><span class="line">        &amp;&amp; (set.equivalentSet == <span class="keyword">null</span>)) &#123;</span><br><span class="line">      LOGGER.trace(</span><br><span class="line">          <span class="string">"Register #&#123;&#125; &#123;&#125; (and merge sets, because it is a conversion)"</span>,</span><br><span class="line">          rel.getId(), rel.getDigest());</span><br><span class="line">      merge(set, childSet);</span><br><span class="line">      registerCount++;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// During the mergers, the child set may have changed, and since</span></span><br><span class="line">      <span class="comment">// we're not registered yet, we won't have been informed. So</span></span><br><span class="line">      <span class="comment">// check whether we are now equivalent to an existing</span></span><br><span class="line">      <span class="comment">// expression.</span></span><br><span class="line">      <span class="keyword">if</span> (fixUpInputs(rel)) &#123;</span><br><span class="line">        rel.recomputeDigest();</span><br><span class="line">        key = rel.getDigest();</span><br><span class="line">        RelNode equivRel = mapDigestToRel.get(key);</span><br><span class="line">        <span class="keyword">if</span> ((equivRel != rel) &amp;&amp; (equivRel != <span class="keyword">null</span>)) &#123;</span><br><span class="line">          <span class="keyword">assert</span> RelOptUtil.equal(</span><br><span class="line">              <span class="string">"rel rowtype"</span>,</span><br><span class="line">              rel.getRowType(),</span><br><span class="line">              <span class="string">"equivRel rowtype"</span>,</span><br><span class="line">              equivRel.getRowType(),</span><br><span class="line">              Litmus.THROW);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// make sure this bad rel didn't get into the</span></span><br><span class="line">          <span class="comment">// set in any way (fixupInputs will do this but it</span></span><br><span class="line">          <span class="comment">// doesn't know if it should so it does it anyway)</span></span><br><span class="line">          set.obliterateRelNode(rel);</span><br><span class="line"></span><br><span class="line">          <span class="comment">// There is already an equivalent expression. Use that</span></span><br><span class="line">          <span class="comment">// one, and forget about this one.</span></span><br><span class="line">          <span class="keyword">return</span> getSubset(equivRel);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      set = childSet;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Place the expression in the appropriate equivalence set.</span></span><br><span class="line">  <span class="comment">//note: 把 expression 放到合适的 等价集 中</span></span><br><span class="line">  <span class="comment">//note: 如果 RelSet 不存在，这里会初始化一个 RelSet</span></span><br><span class="line">  <span class="keyword">if</span> (set == <span class="keyword">null</span>) &#123;</span><br><span class="line">    set = <span class="keyword">new</span> RelSet(</span><br><span class="line">        nextSetId++,</span><br><span class="line">        Util.minus(</span><br><span class="line">            RelOptUtil.getVariablesSet(rel),</span><br><span class="line">            rel.getVariablesSet()),</span><br><span class="line">        RelOptUtil.getVariablesUsed(rel));</span><br><span class="line">    <span class="keyword">this</span>.allSets.add(set);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Chain to find 'live' equivalent set, just in case several sets are</span></span><br><span class="line">  <span class="comment">// merging at the same time.</span></span><br><span class="line">  <span class="comment">//note: 递归查询，一直找到最开始的 语义相等的集合，防止不同集合同时被 merge</span></span><br><span class="line">  <span class="keyword">while</span> (set.equivalentSet != <span class="keyword">null</span>) &#123;</span><br><span class="line">    set = set.equivalentSet;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Allow each rel to register its own rules.</span></span><br><span class="line">  registerClass(rel);</span><br><span class="line"></span><br><span class="line">  registerCount++;</span><br><span class="line">  <span class="comment">//note: 初始时是 0</span></span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">int</span> subsetBeforeCount = set.subsets.size();</span><br><span class="line">  <span class="comment">//note: 向等价集中添加相应的 RelNode，并更新其 best 信息</span></span><br><span class="line">  RelSubset subset = addRelToSet(rel, set);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 缓存相关信息，返回的 key 之前对应的 value</span></span><br><span class="line">  <span class="keyword">final</span> RelNode xx = mapDigestToRel.put(key, rel);</span><br><span class="line">  <span class="keyword">assert</span> xx == <span class="keyword">null</span> || xx == rel : rel.getDigest();</span><br><span class="line"></span><br><span class="line">  LOGGER.trace(<span class="string">"Register &#123;&#125; in &#123;&#125;"</span>, rel.getDescription(), subset.getDescription());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This relational expression may have been registered while we</span></span><br><span class="line">  <span class="comment">// recursively registered its children. If this is the case, we're done.</span></span><br><span class="line">  <span class="keyword">if</span> (xx != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> subset;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create back-links from its children, which makes children more</span></span><br><span class="line">  <span class="comment">// important.</span></span><br><span class="line">  <span class="comment">//note: 如果是 root，初始化其 importance 为 1.0</span></span><br><span class="line">  <span class="keyword">if</span> (rel == <span class="keyword">this</span>.root) &#123;</span><br><span class="line">    ruleQueue.subsetImportances.put(</span><br><span class="line">        subset,</span><br><span class="line">        <span class="number">1.0</span>); <span class="comment">// todo: remove</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 将 Rel 的 input 对应的 RelSubset 的 parents 设置为当前的 Rel</span></span><br><span class="line">  <span class="comment">//note: 也就是说，一个 RelNode 的 input 为其对应 RelSubset 的 children 节点</span></span><br><span class="line">  <span class="keyword">for</span> (RelNode input : rel.getInputs()) &#123;</span><br><span class="line">    RelSubset childSubset = (RelSubset) input;</span><br><span class="line">    childSubset.set.parents.add(rel);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Child subset is more important now a new parent uses it.</span></span><br><span class="line">    <span class="comment">//note: 重新计算 RelSubset 的 importance</span></span><br><span class="line">    ruleQueue.recompute(childSubset);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (rel == <span class="keyword">this</span>.root) &#123;<span class="comment">// <span class="doctag">TODO:</span> 2019-03-11 这里为什么要删除呢？</span></span><br><span class="line">    ruleQueue.subsetImportances.remove(subset);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Remember abstract converters until they're satisfied</span></span><br><span class="line">  <span class="comment">//note: 如果是 AbstractConverter 示例，添加到 abstractConverters 集合中</span></span><br><span class="line">  <span class="keyword">if</span> (rel <span class="keyword">instanceof</span> AbstractConverter) &#123;</span><br><span class="line">    set.abstractConverters.add((AbstractConverter) rel);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If this set has any unsatisfied converters, try to satisfy them.</span></span><br><span class="line">  <span class="comment">//note: check set.abstractConverters</span></span><br><span class="line">  checkForSatisfiedConverters(set, rel);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make sure this rel's subset importance is updated</span></span><br><span class="line">  <span class="comment">//note: 强制更新（重新计算） subset 的 importance</span></span><br><span class="line">  ruleQueue.recompute(subset, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 触发所有匹配的 rule，这里是添加到对应的 RuleQueue 中</span></span><br><span class="line">  <span class="comment">// Queue up all rules triggered by this relexp's creation.</span></span><br><span class="line">  fireRules(rel, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// It's a new subset.</span></span><br><span class="line">  <span class="comment">//note: 如果是一个 new subset，再做一次触发</span></span><br><span class="line">  <span class="keyword">if</span> (set.subsets.size() &gt; subsetBeforeCount) &#123;</span><br><span class="line">    fireRules(subset, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> subset;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>registerImpl()</code> 处理流程比较复杂，其方法实现，可以简单总结为以下几步：</p>
<ol>
<li>在经过最上面的一些验证之后，会通过 <code>rel.onRegister(this)</code> 这步操作，递归地调用 VolcanoPlanner 的 <code>ensureRegistered()</code> 方法对其 <code>inputs</code> RelNode 进行注册，最后还是调用 <code>registerImpl()</code> 方法先注册叶子节点，然后再父节点，最后到根节点；</li>
<li>根据 RelNode 的 digest 信息（一般这个对于 RelNode 来说是全局唯一的），判断其是否已经存在 <code>mapDigestToRel</code> 缓存中，如果存在的话，那么判断会 RelNode 是否相同，如果相同的话，证明之前已经注册过，直接通过 <code>getSubset()</code> 返回其对应的 RelSubset 信息，否则就对其 RelSubset 做下 merge；</li>
<li>如果 RelNode 对应的 RelSet 为 null，这里会新建一个 RelSet，并通过 <code>addRelToSet()</code> 将 RelNode 添加到 RelSet 中，并且更新 VolcanoPlanner 的 <code>mapRel2Subset</code> 缓存记录（RelNode 与 RelSubset 的对应关系），在 <code>addRelToSet()</code> 的最后还会更新 RelSubset 的 best plan 和 best cost（每当往一个 RelSubset 添加相应的 RelNode 时，都会判断这个 RelNode 是否代表了 best plan，如果是的话，就更新）；</li>
<li>将这个 RelNode 的 inputs 设置为其对应 RelSubset 的 children 节点（实际的操作时，是在 RelSet 的 <code>parents</code> 中记录其父节点）；</li>
<li>强制重新计算当前 RelNode 对应 RelSubset 的 importance；</li>
<li>如果这个 RelSubset 是新建的，会再触发一次 <code>fireRules()</code> 方法（会先对 RelNode 触发一次），遍历找到所有可以 match 的 Rule，对每个 Rule 都会创建一个 VolcanoRuleMatch 对象（会记录 RelNode、RelOptRuleOperand 等信息，RelOptRuleOperand 中又会记录 Rule 的信息），并将这个 VolcanoRuleMatch 添加到对应的 RuleQueue 中（就是前面图中的那个 RuleQueue）。</li>
</ol>
<p>这里，来看下 <code>fireRules()</code> 方法的实现，它的目的是把配置的 RuleMatch 添加到 RuleQueue 中，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Fires all rules matched by a relational expression.</span></span><br><span class="line"><span class="comment"> * note： 触发满足这个 relational expression 的所有 rules</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rel      Relational expression which has just been created (or maybe</span></span><br><span class="line"><span class="comment"> *                 from the queue)</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> deferred If true, each time a rule matches, just add an entry to</span></span><br><span class="line"><span class="comment"> *                 the queue.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fireRules</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    RelNode rel,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">boolean</span> deferred)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (RelOptRuleOperand operand : classOperands.get(rel.getClass())) &#123;</span><br><span class="line">    <span class="keyword">if</span> (operand.matches(rel)) &#123; <span class="comment">//note: rule 匹配的情况</span></span><br><span class="line">      <span class="keyword">final</span> VolcanoRuleCall ruleCall;</span><br><span class="line">      <span class="keyword">if</span> (deferred) &#123; <span class="comment">//note: 这里默认都是 true，会把 RuleMatch 添加到 queue 中</span></span><br><span class="line">        ruleCall = <span class="keyword">new</span> DeferringRuleCall(<span class="keyword">this</span>, operand);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ruleCall = <span class="keyword">new</span> VolcanoRuleCall(<span class="keyword">this</span>, operand);</span><br><span class="line">      &#125;</span><br><span class="line">      ruleCall.match(rel);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A rule call which defers its actions. Whereas &#123;<span class="doctag">@link</span> RelOptRuleCall&#125;</span></span><br><span class="line"><span class="comment"> * invokes the rule when it finds a match, a &lt;code&gt;DeferringRuleCall&lt;/code&gt;</span></span><br><span class="line"><span class="comment"> * creates a &#123;<span class="doctag">@link</span> VolcanoRuleMatch&#125; which can be invoked later.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DeferringRuleCall</span> <span class="keyword">extends</span> <span class="title">VolcanoRuleCall</span> </span>&#123;</span><br><span class="line">  DeferringRuleCall(</span><br><span class="line">      VolcanoPlanner planner,</span><br><span class="line">      RelOptRuleOperand operand) &#123;</span><br><span class="line">    <span class="keyword">super</span>(planner, operand);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Rather than invoking the rule (as the base method does), creates a</span></span><br><span class="line"><span class="comment">   * &#123;<span class="doctag">@link</span> VolcanoRuleMatch&#125; which can be invoked later.</span></span><br><span class="line"><span class="comment">   * note：不是直接触发 rule，而是创建一个后续可以被触发的 VolcanoRuleMatch</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onMatch</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> VolcanoRuleMatch match =</span><br><span class="line">        <span class="keyword">new</span> VolcanoRuleMatch(</span><br><span class="line">            volcanoPlanner,</span><br><span class="line">            getOperand0(), <span class="comment">//note: 其实就是 operand</span></span><br><span class="line">            rels,</span><br><span class="line">            nodeInputs);</span><br><span class="line">    volcanoPlanner.ruleQueue.addMatch(match);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的方法中，对于匹配的 Rule，将会创建一个 VolcanoRuleMatch 对象，之后再把这个 VolcanoRuleMatch 对象添加到对应的 RuleQueue 中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.RuleQueue</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Adds a rule match. The rule-matches are automatically added to all</span></span><br><span class="line"><span class="comment"> * existing &#123;<span class="doctag">@link</span> PhaseMatchList per-phase rule-match lists&#125; which allow</span></span><br><span class="line"><span class="comment"> * the rule referenced by the match.</span></span><br><span class="line"><span class="comment"> * note：添加一个 rule match（添加到所有现存的 match phase 中）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addMatch</span><span class="params">(VolcanoRuleMatch match)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> String matchName = match.toString();</span><br><span class="line">  <span class="keyword">for</span> (PhaseMatchList matchList : matchListMap.values()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!matchList.names.add(matchName)) &#123;</span><br><span class="line">      <span class="comment">// Identical match has already been added.</span></span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    String ruleClassName = match.getRule().getClass().getSimpleName();</span><br><span class="line"></span><br><span class="line">    Set&lt;String&gt; phaseRuleSet = phaseRuleMapping.get(matchList.phase);</span><br><span class="line">    <span class="comment">//note: 如果 phaseRuleSet 不为 ALL_RULES，并且 phaseRuleSet 不包含这个 ruleClassName 时，就跳过(其他三个阶段都属于这个情况)</span></span><br><span class="line">    <span class="comment">//note: 在添加 rule match 时，phaseRuleSet 可以控制哪些 match 可以添加、哪些不能添加</span></span><br><span class="line">    <span class="comment">//note: 这里的话，默认只有处在 OPTIMIZE 阶段的 PhaseMatchList 可以添加相应的 rule match</span></span><br><span class="line">    <span class="keyword">if</span> (phaseRuleSet != ALL_RULES) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!phaseRuleSet.contains(ruleClassName)) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOGGER.trace(<span class="string">"&#123;&#125; Rule-match queued: &#123;&#125;"</span>, matchList.phase.toString(), matchName);</span><br><span class="line"></span><br><span class="line">    matchList.list.add(match);</span><br><span class="line"></span><br><span class="line">    matchList.matchMap.put(</span><br><span class="line">        planner.getSubset(match.rels[<span class="number">0</span>]), match);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里 VolcanoPlanner 需要初始化的内容都初始化完成了，下面就到了具体的优化部分。</p>
<h4 id="4-VolcanoPlanner-findBestExp"><a href="#4-VolcanoPlanner-findBestExp" class="headerlink" title="4. VolcanoPlanner findBestExp"></a>4. VolcanoPlanner findBestExp</h4><p>VolcanoPlanner 的 <code>findBestExp()</code> 是具体进行优化的地方，先介绍一下这里的优化策略（每进行一次迭代，<code>cumulativeTicks</code> 加1，它记录了总的迭代次数）：</p>
<ol>
<li>第一次找到可执行计划的迭代次数记为 <code>firstFiniteTick</code>，其对应的 Cost 暂时记为 BestCost；</li>
<li>制定下一次优化要达到的目标为 <code>BestCost*0.9</code>，再根据 <code>firstFiniteTick</code> 及当前的迭代次数计算 <code>giveUpTick</code>，这个值代表的意思是：如果迭代次数超过这个值还没有达到优化目标，那么将会放弃迭代，认为当前的 plan 就是 best plan；</li>
<li>如果 RuleQueue 中 RuleMatch 为空，那么也会退出迭代，认为当前的 plan 就是 best plan；</li>
<li>在每次迭代时都会从 RuleQueue 中选择一个 RuleMatch，策略是选择一个最高 importance 的 RuleMatch，可以保证在每次规则优化时都是选择当前优化效果最好的 Rule 去优化；</li>
<li>最后根据 best plan，构建其对应的 RelNode。</li>
</ol>
<p>上面就是 <code>findBestExp()</code> 主要设计理念，这里来看其具体的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.VolcanoPlanner</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Finds the most efficient expression to implement the query given via</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> org.apache.calcite.plan.RelOptPlanner#setRoot(org.apache.calcite.rel.RelNode)&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：找到最有效率的 relational expression，这个算法包含一系列阶段，每个阶段被触发的 rules 可能不同</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The algorithm executes repeatedly in a series of phases. In each phase</span></span><br><span class="line"><span class="comment"> * the exact rules that may be fired varies. The mapping of phases to rule</span></span><br><span class="line"><span class="comment"> * sets is maintained in the &#123;<span class="doctag">@link</span> #ruleQueue&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：在每个阶段，planner 都会初始化这个 RelSubset 的 importance，planner 会遍历 rule queue 中 rules 直到：</span></span><br><span class="line"><span class="comment"> * note：1. rule queue 变为空；</span></span><br><span class="line"><span class="comment"> * note：2. 对于 ambitious planner，最近 cost 不再提高时（具体来说，第一次找到一个可执行计划时，需要达到需要迭代总数的10%或更大）；</span></span><br><span class="line"><span class="comment"> * note：3. 对于 non-ambitious planner，当找到一个可执行的计划就行；</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;In each phase, the planner sets the initial importance of the existing</span></span><br><span class="line"><span class="comment"> * RelSubSets (&#123;<span class="doctag">@link</span> #setInitialImportance()&#125;). The planner then iterates</span></span><br><span class="line"><span class="comment"> * over the rule matches presented by the rule queue until:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;The rule queue becomes empty.&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;For ambitious planners: No improvements to the plan have been made</span></span><br><span class="line"><span class="comment"> * recently (specifically within a number of iterations that is 10% of the</span></span><br><span class="line"><span class="comment"> * number of iterations necessary to first reach an implementable plan or 25</span></span><br><span class="line"><span class="comment"> * iterations whichever is larger).&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;For non-ambitious planners: When an implementable plan is found.&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：此外，如果每10次迭代之后，没有一个可实现的计划，包含 logical RelNode 的 RelSubSets 将会通过 injectImportanceBoost 给一个 importance；</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Furthermore, after every 10 iterations without an implementable plan,</span></span><br><span class="line"><span class="comment"> * RelSubSets that contain only logical RelNodes are given an importance</span></span><br><span class="line"><span class="comment"> * boost via &#123;<span class="doctag">@link</span> #injectImportanceBoost()&#125;. Once an implementable plan is</span></span><br><span class="line"><span class="comment"> * found, the artificially raised importance values are cleared (see</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> #clearImportanceBoost()&#125;).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the most efficient RelNode tree found for implementing the given</span></span><br><span class="line"><span class="comment"> * query</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">findBestExp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 确保 root relational expression 的 subset（RelSubset）在它的等价集（RelSet）中包含所有 RelSubset 的 converter</span></span><br><span class="line">  <span class="comment">//note: 来保证 planner 从其他的 subsets 找到的实现方案可以转换为 root，否则可能因为 convention 不同，无法实施</span></span><br><span class="line">  ensureRootConverters();</span><br><span class="line">  <span class="comment">//note: materialized views 相关，这里可以先忽略~</span></span><br><span class="line">  registerMaterializations();</span><br><span class="line">  <span class="keyword">int</span> cumulativeTicks = <span class="number">0</span>; <span class="comment">//note: 四个阶段通用的变量</span></span><br><span class="line">  <span class="comment">//note: 不同的阶段，总共四个阶段，实际上只有 OPTIMIZE 这个阶段有效，因为其他阶段不会有 RuleMatch</span></span><br><span class="line">  <span class="keyword">for</span> (VolcanoPlannerPhase phase : VolcanoPlannerPhase.values()) &#123;</span><br><span class="line">    <span class="comment">//note: 在不同的阶段，初始化 RelSubSets 相应的 importance</span></span><br><span class="line">    <span class="comment">//note: root 节点往下子节点的 importance 都会被初始化</span></span><br><span class="line">    setInitialImportance();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 默认是 VolcanoCost</span></span><br><span class="line">    RelOptCost targetCost = costFactory.makeHugeCost();</span><br><span class="line">    <span class="keyword">int</span> tick = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> firstFiniteTick = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> splitCount = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> giveUpTick = Integer.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      ++tick;</span><br><span class="line">      ++cumulativeTicks;</span><br><span class="line">      <span class="comment">//note: 第一次运行是 false，两个不是一个对象，一个是 costFactory.makeHugeCost， 一个是 costFactory.makeInfiniteCost</span></span><br><span class="line">      <span class="comment">//note: 如果低于目标 cost，这里再重新设置一个新目标、新的 giveUpTick</span></span><br><span class="line">      <span class="keyword">if</span> (root.bestCost.isLe(targetCost)) &#123;</span><br><span class="line">        <span class="comment">//note: 本阶段第一次运行，目的是为了调用 clearImportanceBoost 方法，清除相应的 importance 信息</span></span><br><span class="line">        <span class="keyword">if</span> (firstFiniteTick &lt; <span class="number">0</span>) &#123;</span><br><span class="line">          firstFiniteTick = cumulativeTicks;</span><br><span class="line"></span><br><span class="line">          <span class="comment">//note: 对于那些手动提高 importance 的 RelSubset 进行重新计算</span></span><br><span class="line">          clearImportanceBoost();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ambitious) &#123;</span><br><span class="line">          <span class="comment">// Choose a slightly more ambitious target cost, and</span></span><br><span class="line">          <span class="comment">// try again. If it took us 1000 iterations to find our</span></span><br><span class="line">          <span class="comment">// first finite plan, give ourselves another 100</span></span><br><span class="line">          <span class="comment">// iterations to reduce the cost by 10%.</span></span><br><span class="line">          <span class="comment">//note: 设置 target 为当前 best cost 的 0.9，调整相应的目标，再进行优化</span></span><br><span class="line">          targetCost = root.bestCost.multiplyBy(<span class="number">0.9</span>);</span><br><span class="line">          ++splitCount;</span><br><span class="line">          <span class="keyword">if</span> (impatient) &#123;</span><br><span class="line">            <span class="keyword">if</span> (firstFiniteTick &lt; <span class="number">10</span>) &#123;</span><br><span class="line">              <span class="comment">// It's possible pre-processing can create</span></span><br><span class="line">              <span class="comment">// an implementable plan -- give us some time</span></span><br><span class="line">              <span class="comment">// to actually optimize it.</span></span><br><span class="line">              <span class="comment">//note: 有可能在 pre-processing 阶段就实现一个 implementable plan，所以先设置一个值，后面再去优化</span></span><br><span class="line">              giveUpTick = cumulativeTicks + <span class="number">25</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              giveUpTick =</span><br><span class="line">                  cumulativeTicks</span><br><span class="line">                      + Math.max(firstFiniteTick / <span class="number">10</span>, <span class="number">25</span>);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">//note: 最近没有任何进步（超过 giveUpTick 限制，还没达到目标值），直接采用当前的 best plan</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cumulativeTicks &gt; giveUpTick) &#123;</span><br><span class="line">        <span class="comment">// We haven't made progress recently. Take the current best.</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (root.bestCost.isInfinite() &amp;&amp; ((tick % <span class="number">10</span>) == <span class="number">0</span>)) &#123;</span><br><span class="line">        injectImportanceBoost();</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      LOGGER.debug(<span class="string">"PLANNER = &#123;&#125;; TICK = &#123;&#125;/&#123;&#125;; PHASE = &#123;&#125;; COST = &#123;&#125;"</span>,</span><br><span class="line">          <span class="keyword">this</span>, cumulativeTicks, tick, phase.toString(), root.bestCost);</span><br><span class="line"></span><br><span class="line">      VolcanoRuleMatch match = ruleQueue.popMatch(phase);</span><br><span class="line">      <span class="comment">//note: 如果没有规则，会直接退出当前的阶段</span></span><br><span class="line">      <span class="keyword">if</span> (match == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">assert</span> match.getRule().matches(match);</span><br><span class="line">      <span class="comment">//note: 做相应的规则匹配</span></span><br><span class="line">      match.onMatch();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// The root may have been merged with another</span></span><br><span class="line">      <span class="comment">// subset. Find the new root subset.</span></span><br><span class="line">      root = canonize(root);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 当期阶段完成，移除 ruleQueue 中记录的 rule-match list</span></span><br><span class="line">    ruleQueue.phaseCompleted(phase);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (LOGGER.isTraceEnabled()) &#123;</span><br><span class="line">    StringWriter sw = <span class="keyword">new</span> StringWriter();</span><br><span class="line">    <span class="keyword">final</span> PrintWriter pw = <span class="keyword">new</span> PrintWriter(sw);</span><br><span class="line">    dump(pw);</span><br><span class="line">    pw.flush();</span><br><span class="line">    LOGGER.trace(sw.toString());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 根据 plan 构建其 RelNode 树</span></span><br><span class="line">  RelNode cheapest = root.buildCheapestPlan(<span class="keyword">this</span>);</span><br><span class="line">  <span class="keyword">if</span> (LOGGER.isDebugEnabled()) &#123;</span><br><span class="line">    LOGGER.debug(</span><br><span class="line">        <span class="string">"Cheapest plan:\n&#123;&#125;"</span>, RelOptUtil.toString(cheapest, SqlExplainLevel.ALL_ATTRIBUTES));</span><br><span class="line"></span><br><span class="line">    LOGGER.debug(<span class="string">"Provenance:\n&#123;&#125;"</span>, provenance(cheapest));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> cheapest;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整体的流程正如前面所述，这里来看下 RuleQueue 中 <code>popMatch()</code> 方法的实现，它的目的是选择 the highest importance 的 RuleMatch，这个方法的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.plan.volcano.RuleQueue</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Removes the rule match with the highest importance, and returns it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * note：返回最高 importance 的 rule，并从 Rule Match 中移除（处理过后的就移除）</span></span><br><span class="line"><span class="comment"> * note：如果集合为空，就返回 null</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Returns &#123;<span class="doctag">@code</span> null&#125; if there are no more matches.&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Note that the VolcanoPlanner may still decide to reject rule matches</span></span><br><span class="line"><span class="comment"> * which have become invalid, say if one of their operands belongs to an</span></span><br><span class="line"><span class="comment"> * obsolete set or has importance=0.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> java.lang.AssertionError if this method is called with a phase</span></span><br><span class="line"><span class="comment"> *                              previously marked as completed via</span></span><br><span class="line"><span class="comment"> *                              &#123;<span class="doctag">@link</span> #phaseCompleted(VolcanoPlannerPhase)&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">VolcanoRuleMatch <span class="title">popMatch</span><span class="params">(VolcanoPlannerPhase phase)</span> </span>&#123;</span><br><span class="line">  dump();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 选择当前阶段对应的 PhaseMatchList</span></span><br><span class="line">  PhaseMatchList phaseMatchList = matchListMap.get(phase);</span><br><span class="line">  <span class="keyword">if</span> (phaseMatchList == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Used match list for phase "</span> + phase</span><br><span class="line">        + <span class="string">" after phase complete"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> List&lt;VolcanoRuleMatch&gt; matchList = phaseMatchList.list;</span><br><span class="line">  VolcanoRuleMatch match;</span><br><span class="line">  <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="comment">//note: 按照前面的逻辑只有在 OPTIMIZE 阶段，PhaseMatchList 才不为空，其他阶段都是空</span></span><br><span class="line">    <span class="comment">// 参考 addMatch 方法</span></span><br><span class="line">    <span class="keyword">if</span> (matchList.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (LOGGER.isTraceEnabled()) &#123;</span><br><span class="line">      matchList.sort(MATCH_COMPARATOR);</span><br><span class="line">      match = matchList.remove(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">      StringBuilder b = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">      b.append(<span class="string">"Sorted rule queue:"</span>);</span><br><span class="line">      <span class="keyword">for</span> (VolcanoRuleMatch match2 : matchList) &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">double</span> importance = match2.computeImportance();</span><br><span class="line">        b.append(<span class="string">"\n"</span>);</span><br><span class="line">        b.append(match2);</span><br><span class="line">        b.append(<span class="string">" importance "</span>);</span><br><span class="line">        b.append(importance);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      LOGGER.trace(b.toString());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 直接遍历找到 importance 最大的 match（上面先做排序，是为了输出日志）</span></span><br><span class="line">      <span class="comment">// If we're not tracing, it's not worth the effort of sorting the</span></span><br><span class="line">      <span class="comment">// list to find the minimum.</span></span><br><span class="line">      match = <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">int</span> bestPos = -<span class="number">1</span>;</span><br><span class="line">      <span class="keyword">int</span> i = -<span class="number">1</span>;</span><br><span class="line">      <span class="keyword">for</span> (VolcanoRuleMatch match2 : matchList) &#123;</span><br><span class="line">        ++i;</span><br><span class="line">        <span class="keyword">if</span> (match == <span class="keyword">null</span></span><br><span class="line">            || MATCH_COMPARATOR.compare(match2, match) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">          bestPos = i;</span><br><span class="line">          match = match2;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      match = matchList.remove(bestPos);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (skipMatch(match)) &#123;</span><br><span class="line">      LOGGER.debug(<span class="string">"Skip match: &#123;&#125;"</span>, match);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A rule match's digest is composed of the operand RelNodes' digests,</span></span><br><span class="line">  <span class="comment">// which may have changed if sets have merged since the rule match was</span></span><br><span class="line">  <span class="comment">// enqueued.</span></span><br><span class="line">  <span class="comment">//note: 重新计算一下这个 RuleMatch 的 digest</span></span><br><span class="line">  match.recomputeDigest();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 从 phaseMatchList 移除这个 RuleMatch</span></span><br><span class="line">  phaseMatchList.matchMap.remove(</span><br><span class="line">      planner.getSubset(match.rels[<span class="number">0</span>]), match);</span><br><span class="line"></span><br><span class="line">  LOGGER.debug(<span class="string">"Pop match: &#123;&#125;"</span>, match);</span><br><span class="line">  <span class="keyword">return</span> match;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里，我们就把 VolcanoPlanner 的优化讲述完了，当然并没有面面俱到所有的细节，VolcanoPlanner 的整体处理图如下：</p>
<p><img src="/images/calcite/14-volcano.png" alt="VolcanoPlanner 整体处理流程"> </p>
<h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><h4 id="1-初始化-RuleQueue-时，添加的-one-useless-rule-name-有什么用？"><a href="#1-初始化-RuleQueue-时，添加的-one-useless-rule-name-有什么用？" class="headerlink" title="1. 初始化 RuleQueue 时，添加的 one useless rule name 有什么用？"></a>1. 初始化 RuleQueue 时，添加的 one useless rule name 有什么用？</h4><p>在初始化 RuleQueue 时，会给 VolcanoPlanner 的四个阶段 <code>PRE_PROCESS_MDR, PRE_PROCESS, OPTIMIZE, CLEANUP</code> 都初始化一个 PhaseMatchList 对象（记录这个阶段对应的 RuleMatch），这时候会给其中的三个阶段添加一个 useless rule，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> VolcanoPlannerPhaseRuleMappingInitializer</span><br><span class="line">    getPhaseRuleMappingInitializer() &#123;</span><br><span class="line">  <span class="keyword">return</span> phaseRuleMap -&gt; &#123;</span><br><span class="line">    <span class="comment">// Disable all phases except OPTIMIZE by adding one useless rule name.</span></span><br><span class="line">    <span class="comment">//note: 通过添加一个无用的 rule name 来 disable 优化器的其他三个阶段</span></span><br><span class="line">    phaseRuleMap.get(VolcanoPlannerPhase.PRE_PROCESS_MDR).add(<span class="string">"xxx"</span>);</span><br><span class="line">    phaseRuleMap.get(VolcanoPlannerPhase.PRE_PROCESS).add(<span class="string">"xxx"</span>);</span><br><span class="line">    phaseRuleMap.get(VolcanoPlannerPhase.CLEANUP).add(<span class="string">"xxx"</span>);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>开始时还困惑这个什么用？后来看到下面的代码基本就明白了</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (VolcanoPlannerPhase phase : VolcanoPlannerPhase.values()) &#123;</span><br><span class="line">  <span class="comment">// empty phases get converted to "all rules"</span></span><br><span class="line">  <span class="comment">//note: 如果阶段对应的 rule set 为空，那么就给这个阶段对应的 rule set 添加一个 【ALL_RULES】</span></span><br><span class="line">  <span class="comment">//也就是只有 OPTIMIZE 这个阶段对应的会添加 ALL_RULES</span></span><br><span class="line">  <span class="keyword">if</span> (phaseRuleMapping.get(phase).isEmpty()) &#123;</span><br><span class="line">    phaseRuleMapping.put(phase, ALL_RULES);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>后面在调用 RuleQueue 的 <code>addMatch()</code> 方法会做相应的判断，如果 phaseRuleSet 不为 ALL_RULES，并且 phaseRuleSet 不包含这个 ruleClassName 时，那么就跳过这个 RuleMatch，也就是说实际上只有 <strong>OPTIMIZE</strong> 这个阶段是发挥作用的，其他阶段没有添加任何 RuleMatch。</p>
<h4 id="2-四个-phase-实际上只用了-1个阶段，为什么要设置4个阶段？"><a href="#2-四个-phase-实际上只用了-1个阶段，为什么要设置4个阶段？" class="headerlink" title="2. 四个 phase 实际上只用了 1个阶段，为什么要设置4个阶段？"></a>2. 四个 phase 实际上只用了 1个阶段，为什么要设置4个阶段？</h4><p>VolcanoPlanner 的四个阶段 <code>PRE_PROCESS_MDR, PRE_PROCESS, OPTIMIZE, CLEANUP</code>，实际只有 <code>OPTIMIZE</code> 进行真正的优化操作，其他阶段并没有，这里自己是有一些困惑的：</p>
<ol>
<li>为什么要分为4个阶段，在添加 RuleMatch 时，是向四个阶段同时添加，这个设计有什么好处？为什么要优化四次？</li>
<li>设计了4个阶段，为什么默认只用了1个？</li>
</ol>
<p>这两个问题，暂时也没有头绪，有想法的，欢迎交流。</p>
<p>这部分的内容比较多，到这里 Calcite 主要处理流程的文章也终于梳理完了，因为是初次接触，文章理解有误的地方，欢迎各位指教~</p>
<p>附上上一篇文章：<a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a>。</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://blog.csdn.net/wangxingxing2006/article/details/78907278" target="_blank" rel="external">HepPlanner源码分析——Calcite</a>；</li>
<li><a href="https://zhuanlan.zhihu.com/p/48735419" target="_blank" rel="external">SQL 查询优化原理与 Volcano Optimizer 介绍</a>；</li>
<li><a href="https://blog.csdn.net/u013007900/article/details/78993101" target="_blank" rel="external">高级数据库十六：查询优化器（二）</a>；</li>
<li><a href="http://rann.cc/2018/08/23/sql-optimized-principles.html" target="_blank" rel="external">【SQL】SQL优化器原理——查询优化器综述</a>；</li>
<li><a href="http://hbasefly.com/2017/03/01/sparksql-catalyst/" target="_blank" rel="external">SparkSQL – 从0到1认识Catalyst</a>；</li>
<li><a href="http://hbasefly.com/2017/05/04/bigdata%EF%BC%8Dcbo/" target="_blank" rel="external">BigData－‘基于代价优化’究竟是怎么一回事？</a>；</li>
<li><a href="https://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-phoenix-using-apache-calcite?qid=b7a1ca0f-e7bf-49ad-bc51-0615ec8a4971&amp;v=&amp;b=&amp;from_search=4" target="_blank" rel="external">Cost-based Query Optimization in Apache Phoenix using Apache Calcite</a>；</li>
<li><a href="https://cs.uwaterloo.ca/~david/cs848/volcano.pdf" target="_blank" rel="external">The Volcano Optimizer Generator: Extensibility and Efficient Search</a>：Volcano 模型的经典论文；</li>
<li><a href="https://pdfs.semanticscholar.org/c1a3/9da04a072f695e9a7f36bf397fba5c19b93c.pdf?_ga=2.162106044.1003201390.1552806109-329306565.1552806109" target="_blank" rel="external">The Cascades Framework for Query Optimization</a>：Cascades 模型的经典论文。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;紧接上篇文章&lt;a href=&quot;http://matt33.com/2019/03/07/apache-calcite-process-flow/&quot;&gt;Apache Calcite 处理流程详解（一）&lt;/a&gt;，这里是 Calcite 系列文章的第二篇，后面还会有文章讲述 Cal
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="calcite" scheme="http://matt33.com/tags/calcite/"/>
    
  </entry>
  
  <entry>
    <title>Apache Calcite 处理流程详解（一）</title>
    <link href="http://matt33.com/2019/03/07/apache-calcite-process-flow/"/>
    <id>http://matt33.com/2019/03/07/apache-calcite-process-flow/</id>
    <published>2019-03-07T12:40:38.000Z</published>
    <updated>2019-03-17T07:47:39.645Z</updated>
    
    <content type="html"><![CDATA[<p>关于 Apache Calcite 的简单介绍可以参考 <a href="https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite" target="_blank" rel="external">Apache Calcite：Hadoop 中新型大数据查询引擎</a> 这篇文章，Calcite 一开始设计的目标就是 <strong>one size fits all</strong>，它希望能为不同计算存储引擎提供统一的 SQL 查询引擎，当然 Calcite 并不仅仅是一个简单的 SQL 查询引擎，在论文 <a href="https://arxiv.org/pdf/1802.10233.pdf" target="_blank" rel="external">Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</a> 的摘要（摘要见下面）部分，关于 Calcite 的核心点有简单的介绍，Calcite 的架构有三个特点：flexible, embeddable, and extensible，就是灵活性、组件可插拔、可扩展，它的 SQL Parser 层、Optimizer 层等都可以单独使用，这也是 Calcite 受总多开源框架欢迎的原因之一。</p>
<blockquote>
<p>Apache Calcite is a foundational software framework that provides <strong>query processing, optimization, and query language</strong> support to many popular open-source data processing systems such as Apache Hive, Apache Storm, Apache Flink, Druid, and MapD. Calcite’s architecture consists of </p>
<ol>
<li>a modular and extensible query optimizer with hundreds of built-in optimization rules, </li>
<li>a query processor capable of processing a variety of query languages, </li>
<li>an adapter architecture designed for extensibility, </li>
<li>and support for heterogeneous data models and stores (relational, semi-structured, streaming, and geospatial).<br><strong>This flexible, embeddable, and extensible architecture</strong> is what makes Calcite an attractive choice for adoption in bigdata frameworks. It is an active project that continues to introduce support for the new types of data sources, query languages, and approaches to query processing and optimization.</li>
</ol>
</blockquote>
<h1 id="Calcite-概念"><a href="#Calcite-概念" class="headerlink" title="Calcite 概念"></a>Calcite 概念</h1><p>在介绍 Calcite 架构之前，先来看下与 Calcite 相关的基础性内容。</p>
<h2 id="关系代数的基本知识"><a href="#关系代数的基本知识" class="headerlink" title="关系代数的基本知识"></a>关系代数的基本知识</h2><p>关系代数是关系型数据库操作的理论基础，关系代数支持并、差、笛卡尔积、投影和选择等基本运算。关系代数也是 Calcite 的核心，任何一个查询都可以表示成由关系运算符组成的树。在 Calcite 中，它会先将 SQL 转换成关系表达式（relational expression），然后通过规则匹配（rules match）进行相应的优化，优化会有一个成本（cost）模型为参考。</p>
<p>这里先看下关系代数相关内容，这对于理解 Calcite 很有帮助，特别是 Calcite Optimizer 这块的内容，关系代数的基础可以参考这篇文章 <a href="https://blog.csdn.net/QuinnNorris/article/details/70739094" target="_blank" rel="external">SQL 形式化语言——关系代数</a>，简单总结如下：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>英文</th>
<th>符号</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>选择</td>
<td>select</td>
<td>σ</td>
<td>类似于 SQL 中的 where</td>
</tr>
<tr>
<td>投影</td>
<td>project</td>
<td>Π</td>
<td>类似于 SQL 中的 select</td>
</tr>
<tr>
<td>并</td>
<td>union</td>
<td>∪</td>
<td>类似于 SQL 中的 union</td>
</tr>
<tr>
<td>集合差</td>
<td>set-difference</td>
<td>-</td>
<td>SQL中没有对应的操作符</td>
</tr>
<tr>
<td>笛卡儿积</td>
<td>Cartesian-product</td>
<td>×</td>
<td>类似于 SQL 中不带 on 条件的 inner join</td>
</tr>
<tr>
<td>重命名</td>
<td>rename</td>
<td>ρ</td>
<td>类似于 SQL 中的 as</td>
</tr>
<tr>
<td>集合交</td>
<td>intersection</td>
<td>∩</td>
<td>SQL中没有对应的操作符</td>
</tr>
<tr>
<td>自然连接</td>
<td>natural join</td>
<td>⋈</td>
<td>类似于 SQL 中的 inner join</td>
</tr>
<tr>
<td>赋值</td>
<td>assignment</td>
<td>←</td>
</tr>
</tbody>
</table>
<h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><p>查询优化主要是围绕着 <strong>等价交换</strong> 的原则做相应的转换，这部分可以参考【《数据库系统概念（中文第六版）》第13章——查询优化】，关于查询优化理论知识，这里就不再详述，列出一些个人不错不错的博客，大家可以参考一下：</p>
<ol>
<li><a href="https://www.jianshu.com/p/edf503a2a1e7" target="_blank" rel="external">数据库查询优化入门: 代数与物理优化基础</a>；</li>
<li><a href="https://blog.csdn.net/u013007900/article/details/78978271" target="_blank" rel="external">高级数据库十五：查询优化器（一）</a>；</li>
<li><a href="https://blog.csdn.net/u013007900/article/details/78993101" target="_blank" rel="external">高级数据库十六：查询优化器（二）</a>；</li>
<li><a href="http://www.ptbird.cn/optimization-of-relational-algebraic-expression.html" target="_blank" rel="external">「 数据库原理 」查询优化（关系代数表达式优化）</a>；</li>
<li><a href="http://book.51cto.com/art/201306/400084.htm" target="_blank" rel="external">4.1.3 关系数据库系统的查询优化（1）</a>；</li>
<li><a href="http://book.51cto.com/art/201306/400085.htm" target="_blank" rel="external">4.1.3 关系数据库系统的查询优化（10）</a>；</li>
</ol>
<h2 id="Calcite-中的一些概念"><a href="#Calcite-中的一些概念" class="headerlink" title="Calcite 中的一些概念"></a>Calcite 中的一些概念</h2><p>Calcite 抛出的概念非常多，笔者最开始在看代码时就被这些概念绕得云里雾里，这时候先从代码的细节里跳出来，先把这些概念理清楚、归归类后再去看代码，思路就清晰很多，因此，在介绍 Calcite 整体实现前，先把这些概念梳理一下，需要对这些概念有个基本的理解，相关的概念如下图所示：</p>
<p><img src="/images/calcite/0-calcite.png" alt="calcite 基本概念"></p>
<p>整理如下表所示：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>RelOptRule</td>
<td>transforms an expression into another。对 expression 做等价转换</td>
<td>根据传递给它的 RelOptRuleOperand 来对目标 RelNode 树进行规则匹配，匹配成功后，会再次调用 <code>matches()</code> 方法（默认返回真）进行进一步检查。如果 <code>mathes()</code> 结果为真，则调用 <code>onMatch()</code> 进行转换。</td>
</tr>
<tr>
<td>ConverterRule</td>
<td>Abstract base class for a rule which converts from one calling convention to another without changing semantics.</td>
<td>它是 RelOptRule 的子类，专门用来做数据源之间的转换（Calling convention），<strong>ConverterRule 一般会调用对应的 Converter 来完成工作</strong>，比如说：JdbcToSparkConverterRule 调用 JdbcToSparkConverter 来完成对 JDBC Table 到 Spark RDD 的转换。</td>
</tr>
<tr>
<td>RelNode</td>
<td>relational expression，RelNode 会标识其 input RelNode 信息，这样就构成了一棵 RelNode 树</td>
<td>代表了<strong>对数据的一个处理操作</strong>，常见的操作有 Sort、Join、Project、Filter、Scan 等。它蕴含的是对整个 Relation 的操作，而不是对具体数据的处理逻辑。</td>
</tr>
<tr>
<td>Converter</td>
<td>A relational expression implements the interface <code>Converter</code> to indicate that it converts a physical attribute, or RelTrait of a relational expression from one value to another.</td>
<td><strong>用来把一种 RelTrait 转换为另一种 RelTrait 的 RelNode</strong>。如 JdbcToSparkConverter 可以把 JDBC 里的 table 转换为 Spark RDD。如果需要在一个 RelNode 中处理来源于异构系统的逻辑表，Calcite 要求先用 Converter 把异构系统的逻辑表转换为同一种 Convention。</td>
</tr>
<tr>
<td>RexNode</td>
<td>Row-level expression</td>
<td>行表达式（标量表达式），蕴含的是对一行数据的处理逻辑。每个行表达式都有数据的类型。这是因为在 Valdiation 的过程中，编译器会推导出表达式的结果类型。常见的行表达式包括字面量 RexLiteral， 变量 RexVariable， 函数或操作符调用 RexCal l等。 RexNode 通过 RexBuilder 进行构建。</td>
</tr>
<tr>
<td>RelTrait</td>
<td>RelTrait represents the manifestation of a relational expression trait within a trait definition.</td>
<td>用来定义逻辑表的物理相关属性（physical property），三种主要的 trait 类型是：Convention、RelCollation、RelDistribution；</td>
</tr>
<tr>
<td>Convention</td>
<td>Calling convention used to repressent a single data source, inputs must be in the same convention</td>
<td>继承自 RelTrait，类型很少，代表一个单一的数据源，一个  relational expression 必须在同一个 convention 中；</td>
</tr>
<tr>
<td>RelTraitDef</td>
<td></td>
<td>主要有三种：ConventionTraitDef：用来代表数据源 RelCollationTraitDef：用来定义参与排序的字段；RelDistributionTraitDef：用来定义数据在物理存储上的分布方式（比如：single、hash、range、random 等）；</td>
</tr>
<tr>
<td>RelOptCluster</td>
<td>An environment for related relational expressions during the optimization of a query.</td>
<td>palnner 运行时的环境，保存上下文信息；</td>
</tr>
<tr>
<td>RelOptPlanner</td>
<td>A RelOptPlanner is a query optimizer: it transforms a relational expression into a semantically equivalent relational expression, according to a given set of rules and a cost model.</td>
<td>也就是<strong>优化器</strong>，Calcite 支持RBO（Rule-Based Optimizer） 和 CBO（Cost-Based Optimizer）。Calcite 的 RBO （HepPlanner）称为启发式优化器（heuristic implementation ），它简单地按 AST 树结构匹配所有已知规则，直到没有规则能够匹配为止；Calcite 的 CBO 称为火山式优化器（VolcanoPlanner）成本优化器也会匹配并应用规则，当整棵树的成本降低趋于稳定后，优化完成，成本优化器依赖于比较准确的成本估算。RelOptCost 和 Statistic 与成本估算相关；</td>
</tr>
<tr>
<td>RelOptCost</td>
<td>defines an interface for optimizer cost in terms of number of rows processed, CPU cost, and I/O cost.</td>
<td>优化器成本模型会依赖；</td>
</tr>
</tbody>
</table>
<h1 id="Calcite-架构"><a href="#Calcite-架构" class="headerlink" title="Calcite 架构"></a>Calcite 架构</h1><p>关于 Calcite 的架构，可以参考下图（图片来自前面那篇论文），它与传统数据库管理系统有一些相似之处，相比而言，它将数据存储、数据处理算法和元数据存储这些部分忽略掉了，这样设计带来的好处是：对于涉及多种数据源和多种计算引擎的应用而言，Calcite 因为可以兼容多种存储和计算引擎，使得 Calcite 可以提供统一查询服务，Calcite 将会是这些应用的最佳选择。</p>
<p><img src="/images/calcite/1-calcite.png" alt="Calcite Architecture，图片来自论文"></p>
<p>在 Calcite 架构中，最核心地方就是 Optimizer，也就是优化器，一个 Optimization Engine 包含三个组成部分：</p>
<ol>
<li>rules：也就是匹配规则，Calcite 内置上百种 Rules 来优化 relational expression，当然也支持自定义 rules；</li>
<li>metadata providers：主要是向优化器提供信息，这些信息会有助于指导优化器向着目标（减少整体 cost）进行优化，信息可以包括行数、table 哪一列是唯一列等，也包括计算 RelNode 树中执行 subexpression cost 的函数；</li>
<li>planner engines：它的主要目标是进行触发 rules 来达到指定目标，比如像 cost-based optimizer（CBO）的目标是减少cost（Cost 包括处理的数据行数、CPU cost、IO cost 等）。</li>
</ol>
<h1 id="Calcite-处理流程"><a href="#Calcite-处理流程" class="headerlink" title="Calcite 处理流程"></a>Calcite 处理流程</h1><p>Sql 的执行过程一般可以分为下图中的四个阶段，Calcite 同样也是这样：</p>
<p><img src="/images/calcite/dataflow.png" alt="Sql 执行过程"></p>
<p>但这里为了讲述方便，把 SQL 的执行分为下面五个阶段（跟上面比比又独立出了一个阶段）：</p>
<ol>
<li>解析 SQL， 把 SQL 转换成为 AST （抽象语法树），在 Calcite 中用 SqlNode 来表示；</li>
<li>语法检查，根据数据库的元数据信息进行语法验证，验证之后还是用 SqlNode 表示 AST 语法树；</li>
<li>语义分析，根据 SqlNode 及元信息构建 RelNode 树，也就是最初版本的逻辑计划（Logical Plan）；</li>
<li>逻辑计划优化，优化器的核心，根据前面生成的逻辑计划按照相应的规则（Rule）进行优化；</li>
<li>物理执行，生成物理计划，物理执行计划执行。</li>
</ol>
<p>这里我们只关注前四步的内容，会配合源码实现以及一个示例来讲解。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>示例 SQL 如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> u.id <span class="keyword">as</span> user_id, u.name <span class="keyword">as</span> user_name, j.company <span class="keyword">as</span> user_company, u.age <span class="keyword">as</span> user_age </span><br><span class="line"><span class="keyword">from</span> <span class="keyword">users</span> u <span class="keyword">join</span> jobs j <span class="keyword">on</span> u.name=j.name</span><br><span class="line"><span class="keyword">where</span> u.age &gt; <span class="number">30</span> <span class="keyword">and</span> j.id&gt;<span class="number">10</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> user_id</span><br></pre></td></tr></table></figure>
<p>这里有两张表，其表各个字段及类型定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">SchemaPlus rootSchema = Frameworks.createRootSchema(<span class="keyword">true</span>);</span><br><span class="line">rootSchema.add(<span class="string">"USERS"</span>, <span class="keyword">new</span> AbstractTable() &#123; <span class="comment">//note: add a table</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RelDataType <span class="title">getRowType</span><span class="params">(<span class="keyword">final</span> RelDataTypeFactory typeFactory)</span> </span>&#123;</span><br><span class="line">        RelDataTypeFactory.Builder builder = typeFactory.builder();</span><br><span class="line"></span><br><span class="line">        builder.add(<span class="string">"ID"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.INTEGER));</span><br><span class="line">        builder.add(<span class="string">"NAME"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.CHAR));</span><br><span class="line">        builder.add(<span class="string">"AGE"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.INTEGER));</span><br><span class="line">        <span class="keyword">return</span> builder.build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">rootSchema.add(<span class="string">"JOBS"</span>, <span class="keyword">new</span> AbstractTable() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RelDataType <span class="title">getRowType</span><span class="params">(<span class="keyword">final</span> RelDataTypeFactory typeFactory)</span> </span>&#123;</span><br><span class="line">        RelDataTypeFactory.Builder builder = typeFactory.builder();</span><br><span class="line"></span><br><span class="line">        builder.add(<span class="string">"ID"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.INTEGER));</span><br><span class="line">        builder.add(<span class="string">"NAME"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.CHAR));</span><br><span class="line">        builder.add(<span class="string">"COMPANY"</span>, <span class="keyword">new</span> BasicSqlType(<span class="keyword">new</span> RelDataTypeSystemImpl() &#123;&#125;, SqlTypeName.CHAR));</span><br><span class="line">        <span class="keyword">return</span> builder.build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h2 id="Step1-SQL-解析阶段（SQL–-gt-SqlNode）"><a href="#Step1-SQL-解析阶段（SQL–-gt-SqlNode）" class="headerlink" title="Step1: SQL 解析阶段（SQL–&gt;SqlNode）"></a>Step1: SQL 解析阶段（SQL–&gt;SqlNode）</h2><p>使用 Calcite 进行 Sql 解析的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SqlParser parser = SqlParser.create(sql, SqlParser.Config.DEFAULT);</span><br><span class="line">SqlNode sqlNode = parser.parseStmt();</span><br></pre></td></tr></table></figure>
<p>Calcite 使用 JavaCC 做 SQL 解析，JavaCC 根据 Calcite 中定义的 <a href="https://github.com/apache/calcite/blob/master/core/src/main/codegen/templates/Parser.jj" target="_blank" rel="external">Parser.jj</a> 文件，生成一系列的 java 代码，生成的 Java 代码会把 SQL 转换成 AST 的数据结构（这里是 SqlNode 类型）。</p>
<blockquote>
<p>与 Javacc 相似的工具还有 ANTLR，JavaCC 中的 jj 文件也跟 ANTLR 中的 G4文件类似，Apache Spark 中使用这个工具做类似的事情。</p>
</blockquote>
<h3 id="Javacc"><a href="#Javacc" class="headerlink" title="Javacc"></a>Javacc</h3><p>关于 Javacc 内容可以参考下面这几篇文章，这里就不再详细展开，可以通过下面文章的例子把 JavaCC 的语法了解一下，这样我们也可以自己设计一个 DSL（Doomain Specific Language）。</p>
<ul>
<li><a href="https://www.cnblogs.com/Gavin_Liu/archive/2009/03/07/1405029.html" target="_blank" rel="external">JavaCC 研究与应用( 8000字 心得 源程序)</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/xml/x-javacc/part1/index.html" target="_blank" rel="external">JavaCC、解析树和 XQuery 语法，第 1 部分</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/xml/x-javacc/part2/index.html" target="_blank" rel="external">JavaCC、解析树和 XQuery 语法，第 2 部分</a>；</li>
<li><a href="https://www.yangguo.info/2014/12/13/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-Javacc%E4%BD%BF%E7%94%A8/" target="_blank" rel="external">编译原理之Javacc使用</a>；</li>
<li><a href="http://www.engr.mun.ca/~theo/JavaCC-Tutorial/javacc-tutorial.pdf" target="_blank" rel="external">javacc tutorial</a>；</li>
</ul>
<p>回到 Calcite，Javacc 这里要实现一个 SQL Parser，它的功能有以下两个，这里都是需要在 jj 文件中定义的。</p>
<ol>
<li>设计词法和语义，定义 SQL 中具体的元素；</li>
<li>实现词法分析器（Lexer）和语法分析器（Parser），完成对 SQL 的解析，完成相应的转换。</li>
</ol>
<h3 id="SQL-Parser-流程"><a href="#SQL-Parser-流程" class="headerlink" title="SQL Parser 流程"></a>SQL Parser 流程</h3><p>当 SqlParser 调用 <code>parseStmt()</code> 方法后，其相应的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.calcite.sql.parser.SqlParser</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseStmt</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> parseQuery();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseQuery</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> parser.parseSqlStmtEof(); <span class="comment">//note: 解析sql语句</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable ex) &#123;</span><br><span class="line">    <span class="keyword">if</span> (ex <span class="keyword">instanceof</span> CalciteContextException) &#123;</span><br><span class="line">      <span class="keyword">final</span> String originalSql = parser.getOriginalSql();</span><br><span class="line">      <span class="keyword">if</span> (originalSql != <span class="keyword">null</span>) &#123;</span><br><span class="line">        ((CalciteContextException) ex).setOriginalStatement(originalSql);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> parser.normalizeException(ex);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 SqlParser 中 parser 指的是 <code>SqlParserImpl</code> 类（<code>SqlParser.Config.DEFAULT</code> 指定的），它就是由 JJ 文件生成的解析类，其处理流程如下，具体解析逻辑还是要看 JJ 文件中的定义。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.sql.parser.impl.SqlParserImpl</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseSqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> SqlStmtEof();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parses an SQL statement followed by the end-of-file symbol.</span></span><br><span class="line"><span class="comment"> * note:解析SQL语句(后面有文件结束符号)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">public</span> SqlNode <span class="title">SqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">  SqlNode stmt;</span><br><span class="line">  stmt = SqlStmt();</span><br><span class="line">  jj_consume_token(<span class="number">0</span>);</span><br><span class="line">      &#123;<span class="keyword">if</span> (<span class="keyword">true</span>) <span class="keyword">return</span> stmt;&#125;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Missing return statement in function"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">//note: 解析 SQL statement</span></span><br><span class="line"><span class="function"><span class="keyword">final</span> <span class="keyword">public</span> SqlNode <span class="title">SqlStmt</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">  SqlNode stmt;</span><br><span class="line">  <span class="keyword">if</span> (jj_2_34(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlSetOption(Span.of(), <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_35(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlAlter();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_36(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = OrderedQueryOrExpr(ExprContext.ACCEPT_QUERY);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_37(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlExplain();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_38(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlDescribe();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_39(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlInsert();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_40(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlDelete();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_41(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlUpdate();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_42(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlMerge();</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (jj_2_43(<span class="number">2</span>)) &#123;</span><br><span class="line">    stmt = SqlProcedureCall();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    jj_consume_token(-<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ParseException();</span><br><span class="line">  &#125;</span><br><span class="line">      &#123;<span class="keyword">if</span> (<span class="keyword">true</span>) <span class="keyword">return</span> stmt;&#125;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> Error(<span class="string">"Missing return statement in function"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>示例中 SQL 经过前面的解析之后，会生成一个 SqlNode，这个 SqlNode 是一个 SqlOrder 类型，DEBUG 后的 SqlOrder 对象如下图所示。</p>
<p><img src="/images/calcite/2-calciter.jpg" alt="SqlNode 结果"> </p>
<h2 id="Step2-SqlNode-验证（SqlNode–-gt-SqlNode）"><a href="#Step2-SqlNode-验证（SqlNode–-gt-SqlNode）" class="headerlink" title="Step2: SqlNode 验证（SqlNode–&gt;SqlNode）"></a>Step2: SqlNode 验证（SqlNode–&gt;SqlNode）</h2><p>经过上面的第一步，会生成一个 SqlNode 对象，它是一个<strong>未经验证</strong>的抽象语法树，下面就进入了一个<strong>语法检查</strong>阶段，语法检查前需要知道元数据信息，这个检查会包括表名、字段名、函数名、数据类型的检查。进行语法检查的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 二、sql validate（会先通过Catalog读取获取相应的metadata和namespace）</span></span><br><span class="line"><span class="comment">//note: get metadata and namespace</span></span><br><span class="line">SqlTypeFactoryImpl factory = <span class="keyword">new</span> SqlTypeFactoryImpl(RelDataTypeSystem.DEFAULT);</span><br><span class="line">CalciteCatalogReader calciteCatalogReader = <span class="keyword">new</span> CalciteCatalogReader(</span><br><span class="line">    CalciteSchema.from(rootScheme),</span><br><span class="line">    CalciteSchema.from(rootScheme).path(<span class="keyword">null</span>),</span><br><span class="line">    factory,</span><br><span class="line">    <span class="keyword">new</span> CalciteConnectionConfigImpl(<span class="keyword">new</span> Properties()));</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 校验（包括对表名，字段名，函数名，字段类型的校验。）</span></span><br><span class="line">SqlValidator validator = SqlValidatorUtil.newValidator(SqlStdOperatorTable.instance(), calciteCatalogReader, factory,</span><br><span class="line">    conformance(frameworkConfig));</span><br><span class="line">SqlNode validateSqlNode = validator.validate(sqlNode);</span><br></pre></td></tr></table></figure>
<p>我们知道 Calcite 本身是不管理和存储元数据的，在检查之前，需要先把元信息注册到 Calcite 中，一般的操作方法是实现 SchemaFactory，由它去创建相应的 Schema，在 Schema 中可以注册相应的元数据信息（如：通过 <code>getTableMap()</code> 方法注册表信息），如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.schema.impl.AbstractSchema</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns a map of tables in this schema by name.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The implementations of &#123;<span class="doctag">@link</span> #getTableNames()&#125;</span></span><br><span class="line"><span class="comment"> * and &#123;<span class="doctag">@link</span> #getTable(String)&#125; depend on this map.</span></span><br><span class="line"><span class="comment"> * The default implementation of this method returns the empty map.</span></span><br><span class="line"><span class="comment"> * Override this method to change their behavior.&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Map of tables in this schema by name</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Map&lt;String, Table&gt; <span class="title">getTableMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> ImmutableMap.of();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//org.apache.calcite.adapter.csvorg.apache.calcite.adapter.csv.CsvSchemasvSchema</span></span><br><span class="line"><span class="comment">//note: 创建表</span></span><br><span class="line"><span class="meta">@Override</span> <span class="function"><span class="keyword">protected</span> Map&lt;String, Table&gt; <span class="title">getTableMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (tableMap == <span class="keyword">null</span>) &#123;</span><br><span class="line">    tableMap = createTableMap();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> tableMap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CsvSchemasvSchema 中的 <code>getTableMap()</code> 方法通过 <code>createTableMap()</code> 来注册相应的表信息。</p>
<p>结合前面的例子再来分析，在前面定义了 CalciteCatalogReader 实例，该实例就是用来读取 Schema 中的元数据信息的。真正检查的逻辑是在 <code>SqlValidatorImpl</code> 类中实现的，这个 check 的逻辑比较复杂，在看代码时通过两种手段来看：</p>
<ol>
<li>DEBUG 的方式，可以看到其方法调用的过程；</li>
<li>测试程序中故意构造一些 Case，观察其异常栈。</li>
</ol>
<p>比如，在示例中 SQL 中，如果把一个字段名写错，写成 ids，其报错信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">org.apache.calcite.runtime.CalciteContextException: From line 1, column 156 to line 1, column 158: Column <span class="string">'IDS'</span> not found <span class="keyword">in</span> table <span class="string">'J'</span></span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">    at org.apache.calcite.runtime.Resources<span class="variable">$ExInstWithCause</span>.ex(Resources.java:463)</span><br><span class="line">    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:787)</span><br><span class="line">    at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:772)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:4788)</span><br><span class="line">    at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visit(SqlValidatorImpl.java:5683)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visit(SqlValidatorImpl.java:5665)</span><br><span class="line">    at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:334)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:134)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:101)</span><br><span class="line">    at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:865)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visitScoped(SqlValidatorImpl.java:5701)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:50)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:33)</span><br><span class="line">    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:138)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:134)</span><br><span class="line">    at org.apache.calcite.sql.util.SqlShuttle<span class="variable">$CallCopyingArgHandler</span>.visitChild(SqlShuttle.java:101)</span><br><span class="line">    at org.apache.calcite.sql.SqlOperator.acceptCall(SqlOperator.java:865)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl<span class="variable">$Expander</span>.visitScoped(SqlValidatorImpl.java:5701)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:50)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlScopedShuttle.visit(SqlScopedShuttle.java:33)</span><br><span class="line">    at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:138)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5272)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateWhereClause(SqlValidatorImpl.java:3977)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3305)</span><br><span class="line">    at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)</span><br><span class="line">    at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:977)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:953)</span><br><span class="line">    at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:216)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:928)</span><br><span class="line">    at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:632)</span><br><span class="line">    at com.matt.test.calcite.test.SqlTest3.sqlToRelNode(SqlTest3.java:200)</span><br><span class="line">    at com.matt.test.calcite.test.SqlTest3.main(SqlTest3.java:117)</span><br><span class="line">Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Column <span class="string">'IDS'</span> not found <span class="keyword">in</span> table <span class="string">'J'</span></span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">    at org.apache.calcite.runtime.Resources<span class="variable">$ExInstWithCause</span>.ex(Resources.java:463)</span><br><span class="line">    at org.apache.calcite.runtime.Resources<span class="variable">$ExInst</span>.ex(Resources.java:572)</span><br><span class="line">    ... 33 more</span><br><span class="line">java.lang.NullPointerException</span><br><span class="line">    at org.apache.calcite.plan.hep.HepPlanner.addRelToGraph(HepPlanner.java:806)</span><br><span class="line">    at org.apache.calcite.plan.hep.HepPlanner.setRoot(HepPlanner.java:152)</span><br><span class="line">    at com.matt.test.calcite.test.SqlTest3.main(SqlTest3.java:124)</span><br></pre></td></tr></table></figure>
<h3 id="SqlValidatorImpl-检查过程"><a href="#SqlValidatorImpl-检查过程" class="headerlink" title="SqlValidatorImpl 检查过程"></a>SqlValidatorImpl 检查过程</h3><p>语法检查验证是通过 SqlValidatorImpl 的 <code>validate()</code> 方法进行操作的，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">org.apache.calcite.sql.validate.SqlValidatorImpl</span><br><span class="line"><span class="comment">//note: 做相应的语法树校验</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">validate</span><span class="params">(SqlNode topNode)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: root 对应的 Scope</span></span><br><span class="line">  SqlValidatorScope scope = <span class="keyword">new</span> EmptyScope(<span class="keyword">this</span>);</span><br><span class="line">  scope = <span class="keyword">new</span> CatalogScope(scope, ImmutableList.of(<span class="string">"CATALOG"</span>));</span><br><span class="line">  <span class="comment">//note: 1.rewrite expression</span></span><br><span class="line">  <span class="comment">//note: 2.做相应的语法检查</span></span><br><span class="line">  <span class="keyword">final</span> SqlNode topNode2 = validateScopedExpression(topNode, scope); <span class="comment">//note: 验证</span></span><br><span class="line">  <span class="keyword">final</span> RelDataType type = getValidatedNodeType(topNode2);</span><br><span class="line">  Util.discard(type);</span><br><span class="line">  <span class="keyword">return</span> topNode2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的实现是在 <code>validateScopedExpression()</code> 方法中，其实现如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> SqlNode <span class="title">validateScopedExpression</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlNode topNode,</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlValidatorScope scope)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: 1. rewrite expression，将其标准化，便于后面的逻辑计划优化</span></span><br><span class="line">  SqlNode outermostNode = performUnconditionalRewrites(topNode, <span class="keyword">false</span>);</span><br><span class="line">  cursorSet.add(outermostNode);</span><br><span class="line">  top = outermostNode;</span><br><span class="line">  TRACER.trace(<span class="string">"After unconditional rewrite: &#123;&#125;"</span>, outermostNode);</span><br><span class="line">  <span class="comment">//note: 2. Registers a query in a parent scope.</span></span><br><span class="line">  <span class="comment">//note: register scopes and namespaces implied a relational expression</span></span><br><span class="line">  <span class="keyword">if</span> (outermostNode.isA(SqlKind.TOP_LEVEL)) &#123;</span><br><span class="line">    registerQuery(scope, <span class="keyword">null</span>, outermostNode, outermostNode, <span class="keyword">null</span>, <span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 3. catalog 验证，调用 SqlNode 的 validate 方法，</span></span><br><span class="line">  outermostNode.validate(<span class="keyword">this</span>, scope);</span><br><span class="line">  <span class="keyword">if</span> (!outermostNode.isA(SqlKind.TOP_LEVEL)) &#123;</span><br><span class="line">    <span class="comment">// force type derivation so that we can provide it to the</span></span><br><span class="line">    <span class="comment">// caller later without needing the scope</span></span><br><span class="line">    deriveType(scope, outermostNode);</span><br><span class="line">  &#125;</span><br><span class="line">  TRACER.trace(<span class="string">"After validation: &#123;&#125;"</span>, outermostNode);</span><br><span class="line">  <span class="keyword">return</span> outermostNode;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它的处理逻辑主要分为三步：</p>
<ol>
<li>rewrite expression，将其标准化，便于后面的逻辑计划优化；</li>
<li>注册这个 relational expression 的 scopes 和 namespaces（这两个对象代表了其元信息）；</li>
<li>进行相应的验证，这里会依赖第二步注册的 scopes 和 namespaces 信息。</li>
</ol>
<h4 id="Rewrite"><a href="#Rewrite" class="headerlink" title="Rewrite"></a>Rewrite</h4><p>关于 Rewrite 这一步，一直困惑比较，因为根据 <code>After unconditional rewrite:</code> 这条日志的结果看，其实前后 SqlNode 并没有太大变化，看 <code>performUnconditionalRewrites()</code> 这部分代码时，看得不是很明白，不过还是注意到了 SqlOrderBy 的注释（注释如下），它的意思是 SqlOrderBy 通过 <code>performUnconditionalRewrites()</code> 方法已经被 SqlSelect 对象中的 <code>ORDER_OPERAND</code> 取代了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Parse tree node that represents an &#123;<span class="doctag">@code</span> ORDER BY&#125; on a query other than a</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@code</span> SELECT&#125; (e.g. &#123;<span class="doctag">@code</span> VALUES&#125; or &#123;<span class="doctag">@code</span> UNION&#125;).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;It is a purely syntactic operator, and is eliminated by</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> org.apache.calcite.sql.validate.SqlValidatorImpl#performUnconditionalRewrites&#125;</span></span><br><span class="line"><span class="comment"> * and replaced with the ORDER_OPERAND of SqlSelect.&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlOrderBy</span> <span class="keyword">extends</span> <span class="title">SqlCall</span> </span>&#123;</span><br></pre></td></tr></table></figure>
<p>注意到 SqlOrderBy 的原因是因为在 <code>performUnconditionalRewrites()</code> 方法前面都是递归对每个对象进行处理，在后面进行真正的 ransform 时，主要在围绕着 ORDER_BY 这个类型做处理，而且从代码中可以看出，将其类型从 SqlOrderBy 转换成了 SqlSelect，BUDEG 前面的示例，发现 outermostNode 与 topNode 的类型确实发生了变化，如下图所示。</p>
<p><img src="/images/calcite/3-calcite.png" alt="Rewrite 前后的对比"></p>
<p>这个方法有个好的地方就是，在不改变原有 SQL Parser 的逻辑的情况下，可以在这个方法里做一些改动，当然如果 SQL Parser 的结果如果直接可用当然是最好的，就不需要再进行一次 Rewrite 了。</p>
<h4 id="registerQuery"><a href="#registerQuery" class="headerlink" title="registerQuery"></a>registerQuery</h4><p>这里的功能主要就是将[元数据]转换成 SqlValidator 内部的 对象 进行表示，也就是 SqlValidatorScope 和 SqlValidatorNamespace 两种类型的对象：</p>
<ol>
<li>SqlValidatorNamespace：a description of a data source used in a query，它代表了 SQL 查询的数据源，它是一个逻辑上数据源，可以是一张表，也可以是一个子查询；</li>
<li>SqlValidatorScope：describes the tables and columns accessible at a particular point in the query，代表了在某一个程序运行点，当前可见的字段名和表名。</li>
</ol>
<p>这个理解起来并不是那么容易，在 SelectScope 类中有一个示例讲述，这个示例对这两个概念的理解很有帮助。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * &lt;h3&gt;Scopes&lt;/h3&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;In the query&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;blockquote&gt;</span></span><br><span class="line"><span class="comment"> * &lt;pre&gt;</span></span><br><span class="line"><span class="comment"> * SELECT expr1</span></span><br><span class="line"><span class="comment"> * FROM t1,</span></span><br><span class="line"><span class="comment"> *     t2,</span></span><br><span class="line"><span class="comment"> *     (SELECT expr2 FROM t3) AS q3</span></span><br><span class="line"><span class="comment"> * WHERE c1 IN (SELECT expr3 FROM t4)</span></span><br><span class="line"><span class="comment"> * ORDER BY expr4&lt;/pre&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/blockquote&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The scopes available at various points of the query are as follows:&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr1 can see t1, t2, q3&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr2 can see t3&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr3 can see t4, t1, t2&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;expr4 can see t1, t2, q3, plus (depending upon the dialect) any aliases</span></span><br><span class="line"><span class="comment"> * defined in the SELECT clause&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ul&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;h3&gt;Namespaces&lt;/h3&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;In the above query, there are 4 namespaces:&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;t1&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;t2&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;(SELECT expr2 FROM t3) AS q3&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;(SELECT expr3 FROM t4)&lt;/li&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<h4 id="validate-验证"><a href="#validate-验证" class="headerlink" title="validate 验证"></a>validate 验证</h4><p>接着回到最复杂的一步，就是 outermostNode 实例调用 <code>validate(this, scope)</code> 方法进行验证的部分，对于我们这个示例，这里最后调用的是 SqlSelect 的 <code>validate()</code> 方法，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">validate</span><span class="params">(SqlValidator validator, SqlValidatorScope scope)</span> </span>&#123;</span><br><span class="line">  validator.validateQuery(<span class="keyword">this</span>, scope, validator.getUnknownType());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它调用的是 SqlValidatorImpl 的 <code>validateQuery()</code> 方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">validateQuery</span><span class="params">(SqlNode node, SqlValidatorScope scope,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlValidatorNamespace ns = getNamespace(node, scope);</span><br><span class="line">  <span class="keyword">if</span> (node.getKind() == SqlKind.TABLESAMPLE) &#123;</span><br><span class="line">    List&lt;SqlNode&gt; operands = ((SqlCall) node).getOperandList();</span><br><span class="line">    SqlSampleSpec sampleSpec = SqlLiteral.sampleValue(operands.get(<span class="number">1</span>));</span><br><span class="line">    <span class="keyword">if</span> (sampleSpec <span class="keyword">instanceof</span> SqlSampleSpec.SqlTableSampleSpec) &#123;</span><br><span class="line">      validateFeature(RESOURCE.sQLFeature_T613(), node.getParserPosition());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sampleSpec</span><br><span class="line">        <span class="keyword">instanceof</span> SqlSampleSpec.SqlSubstitutionSampleSpec) &#123;</span><br><span class="line">      validateFeature(RESOURCE.sQLFeatureExt_T613_Substitution(),</span><br><span class="line">          node.getParserPosition());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  validateNamespace(ns, targetRowType);<span class="comment">//note: 检查</span></span><br><span class="line">  <span class="keyword">switch</span> (node.getKind()) &#123;</span><br><span class="line">  <span class="keyword">case</span> EXTEND:</span><br><span class="line">    <span class="comment">// Until we have a dedicated namespace for EXTEND</span></span><br><span class="line">    deriveType(scope, node);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (node == top) &#123;</span><br><span class="line">    validateModality(node);</span><br><span class="line">  &#125;</span><br><span class="line">  validateAccess(</span><br><span class="line">      node,</span><br><span class="line">      ns.getTable(),</span><br><span class="line">      SqlAccessEnum.SELECT);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Validates a namespace.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> namespace Namespace</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> targetRowType Desired row type, must not be null, may be the data</span></span><br><span class="line"><span class="comment"> *                      type 'unknown'.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">validateNamespace</span><span class="params">(<span class="keyword">final</span> SqlValidatorNamespace namespace,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  namespace.validate(targetRowType);<span class="comment">//note: 验证</span></span><br><span class="line">  <span class="keyword">if</span> (namespace.getNode() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    setValidatedNodeType(namespace.getNode(), namespace.getType());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这部分的调用逻辑非常复杂，主要的语法验证是 SqlValidatorScope 部分（它里面有相应的表名、字段名等信息），而 namespace 表示需要进行验证的数据源，最开始的这个 SqlNode 有一个 root namespace，上面的 <code>validateNamespace()</code> 方法会首先调用其 namespace 的 <code>validate()</code> 方法进行验证，以前面的示例为例，这里是 SelectNamespace，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//org.apache.calcite.sql.validate.AbstractNamespace</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">validate</span><span class="params">(RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">switch</span> (status) &#123;</span><br><span class="line">  <span class="keyword">case</span> UNVALIDATED: <span class="comment">//note: 还没开始 check</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      status = SqlValidatorImpl.Status.IN_PROGRESS; <span class="comment">//note: 更新当前 namespace 的状态</span></span><br><span class="line">      Preconditions.checkArgument(rowType == <span class="keyword">null</span>,</span><br><span class="line">          <span class="string">"Namespace.rowType must be null before validate has been called"</span>);</span><br><span class="line">      RelDataType type = validateImpl(targetRowType); <span class="comment">//note: 检查验证</span></span><br><span class="line">      Preconditions.checkArgument(type != <span class="keyword">null</span>,</span><br><span class="line">          <span class="string">"validateImpl() returned null"</span>);</span><br><span class="line">      setType(type);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      status = SqlValidatorImpl.Status.VALID;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IN_PROGRESS: <span class="comment">//note: 已经开始 check 了，死循环了</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"Cycle detected during type-checking"</span>);</span><br><span class="line">  <span class="keyword">case</span> VALID:<span class="comment">//note: 检查结束</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">throw</span> Util.unexpected(status);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//org.apache.calcite.sql.validate.SelectNamespace</span></span><br><span class="line"><span class="comment">//note: 检查，还是调用 SqlValidatorImpl 的方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelDataType <span class="title">validateImpl</span><span class="params">(RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  validator.validateSelect(select, targetRowType);</span><br><span class="line">  <span class="keyword">return</span> rowType;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后验证方法的实现是 SqlValidatorImpl 的 <code>validateSelect()</code> 方法（对本示例而言），其调用过程如下图所示：</p>
<p><img src="/images/calcite/4-sqlvalidator.png" alt="验证部分的处理流程"> </p>
<h2 id="Step3-语义分析（SqlNode–-gt-RelNode-RexNode）"><a href="#Step3-语义分析（SqlNode–-gt-RelNode-RexNode）" class="headerlink" title="Step3: 语义分析（SqlNode–&gt;RelNode/RexNode）"></a>Step3: 语义分析（SqlNode–&gt;RelNode/RexNode）</h2><p>经过第二步之后，这里的 SqlNode 就是经过语法校验的 SqlNode 树，接下来这一步就是将 SqlNode 转换成 RelNode/RexNode，也就是生成相应的逻辑计划（Logical Plan），示例的代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create the rexBuilder</span></span><br><span class="line"><span class="keyword">final</span> RexBuilder rexBuilder =  <span class="keyword">new</span> RexBuilder(factory);</span><br><span class="line"><span class="comment">// init the planner</span></span><br><span class="line"><span class="comment">// 这里也可以注册 VolcanoPlanner，这一步 planner 并没有使用</span></span><br><span class="line">HepProgramBuilder builder = <span class="keyword">new</span> HepProgramBuilder();</span><br><span class="line">RelOptPlanner planner = <span class="keyword">new</span> HepPlanner(builder.build());</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: init cluster: An environment for related relational expressions during the optimization of a query.</span></span><br><span class="line"><span class="keyword">final</span> RelOptCluster cluster = RelOptCluster.create(planner, rexBuilder);</span><br><span class="line"><span class="comment">//note: init SqlToRelConverter</span></span><br><span class="line"><span class="keyword">final</span> SqlToRelConverter.Config config = SqlToRelConverter.configBuilder()</span><br><span class="line">    .withConfig(frameworkConfig.getSqlToRelConverterConfig())</span><br><span class="line">    .withTrimUnusedFields(<span class="keyword">false</span>)</span><br><span class="line">    .withConvertTableAccess(<span class="keyword">false</span>)</span><br><span class="line">    .build(); <span class="comment">//note: config</span></span><br><span class="line"><span class="comment">// 创建 SqlToRelConverter 实例，cluster、calciteCatalogReader、validator 都传进去了，SqlToRelConverter 会缓存这些对象</span></span><br><span class="line"><span class="keyword">final</span> SqlToRelConverter sqlToRelConverter = <span class="keyword">new</span> SqlToRelConverter(<span class="keyword">new</span> DogView(), validator, calciteCatalogReader, cluster, StandardConvertletTable.INSTANCE, config);</span><br><span class="line"><span class="comment">// convert to RelNode</span></span><br><span class="line">RelRoot root = sqlToRelConverter.convertQuery(validateSqlNode, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">root = root.withRel(sqlToRelConverter.flattenTypes(root.rel, <span class="keyword">true</span>));</span><br><span class="line"><span class="keyword">final</span> RelBuilder relBuilder = config.getRelBuilderFactory().create(cluster, <span class="keyword">null</span>);</span><br><span class="line">root = root.withRel(RelDecorrelator.decorrelateQuery(root.rel, relBuilder));</span><br><span class="line"></span><br><span class="line">RelNode relNode = root.rel;</span><br><span class="line"></span><br><span class="line"><span class="comment">//DogView 的实现</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">DogView</span> <span class="keyword">implements</span> <span class="title">RelOptTable</span>.<span class="title">ViewExpander</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DogView</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RelRoot <span class="title">expandView</span><span class="params">(RelDataType rowType, String queryString, List&lt;String&gt; schemaPath,</span></span></span><br><span class="line"><span class="function"><span class="params">                              List&lt;String&gt; viewPath)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了方便分析，这里也把上面的过程分为以下几步：</p>
<ol>
<li>初始化 RexBuilder；</li>
<li>初始化 RelOptPlanner;</li>
<li>初始化 RelOptCluster；</li>
<li>初始化 SqlToRelConverter；</li>
<li>进行转换；</li>
</ol>
<p>第1、2、4步在上述代码已经有相应的注释，这里不再介绍，下面从第三步开始讲述。</p>
<h3 id="初始化-RelOptCluster"><a href="#初始化-RelOptCluster" class="headerlink" title="初始化 RelOptCluster"></a>初始化 RelOptCluster</h3><p>RelOptCluster 初始化的代码如下，这里基本都走默认的参数配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">org.apache.calcite.plan.RelOptCluster</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Creates a cluster. */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> RelOptCluster <span class="title">create</span><span class="params">(RelOptPlanner planner,</span></span></span><br><span class="line"><span class="function"><span class="params">    RexBuilder rexBuilder)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> RelOptCluster(planner, rexBuilder.getTypeFactory(),</span><br><span class="line">      rexBuilder, <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>), <span class="keyword">new</span> HashMap&lt;&gt;());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Creates a cluster.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For use only from &#123;<span class="doctag">@link</span> #create&#125; and &#123;<span class="doctag">@link</span> RelOptQuery&#125;.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">RelOptCluster(RelOptPlanner planner, RelDataTypeFactory typeFactory,</span><br><span class="line">    RexBuilder rexBuilder, AtomicInteger nextCorrel,</span><br><span class="line">    Map&lt;String, RelNode&gt; mapCorrelToRel) &#123;</span><br><span class="line">  <span class="keyword">this</span>.nextCorrel = nextCorrel;</span><br><span class="line">  <span class="keyword">this</span>.mapCorrelToRel = mapCorrelToRel;</span><br><span class="line">  <span class="keyword">this</span>.planner = Objects.requireNonNull(planner);</span><br><span class="line">  <span class="keyword">this</span>.typeFactory = Objects.requireNonNull(typeFactory);</span><br><span class="line">  <span class="keyword">this</span>.rexBuilder = rexBuilder;</span><br><span class="line">  <span class="keyword">this</span>.originalExpression = rexBuilder.makeLiteral(<span class="string">"?"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set up a default rel metadata provider,</span></span><br><span class="line">  <span class="comment">// giving the planner first crack at everything</span></span><br><span class="line">  <span class="comment">//note: 默认的 metadata provider</span></span><br><span class="line">  setMetadataProvider(DefaultRelMetadataProvider.INSTANCE);</span><br><span class="line">  <span class="comment">//note: trait（对于 HepPlaner 和 VolcanoPlanner 不一样)</span></span><br><span class="line">  <span class="keyword">this</span>.emptyTraitSet = planner.emptyTraitSet();</span><br><span class="line">  <span class="keyword">assert</span> emptyTraitSet.size() == planner.getRelTraitDefs().size();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="SqlToRelConverter-转换"><a href="#SqlToRelConverter-转换" class="headerlink" title="SqlToRelConverter 转换"></a>SqlToRelConverter 转换</h3><p>SqlToRelConverter 中的 <code>convertQuery()</code> 将 SqlNode 转换为 RelRoot，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Converts an unvalidated query's parse tree into a relational expression.</span></span><br><span class="line"><span class="comment"> * note：把一个 parser tree 转换为 relational expression</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> query           Query to convert</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> needsValidation Whether to validate the query before converting;</span></span><br><span class="line"><span class="comment"> *                        &lt;code&gt;false&lt;/code&gt; if the query has already been</span></span><br><span class="line"><span class="comment"> *                        validated.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> top             Whether the query is top-level, say if its result</span></span><br><span class="line"><span class="comment"> *                        will become a JDBC result set; &lt;code&gt;false&lt;/code&gt; if</span></span><br><span class="line"><span class="comment"> *                        the query will be part of a view.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelRoot <span class="title">convertQuery</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlNode query,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">boolean</span> needsValidation,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> <span class="keyword">boolean</span> top)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (needsValidation) &#123; <span class="comment">//note: 是否需要做相应的校验（如果校验过了，这里就不需要了）</span></span><br><span class="line">    query = validator.validate(query);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 设置 MetadataProvider</span></span><br><span class="line">  RelMetadataQuery.THREAD_PROVIDERS.set(</span><br><span class="line">      JaninoRelMetadataProvider.of(cluster.getMetadataProvider()));</span><br><span class="line">  <span class="comment">//note: 得到 RelNode(relational expression)</span></span><br><span class="line">  RelNode result = convertQueryRecursive(query, top, <span class="keyword">null</span>).rel;</span><br><span class="line">  <span class="keyword">if</span> (top) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isStream(query)) &#123;<span class="comment">//note: 如果 stream 的话</span></span><br><span class="line">      result = <span class="keyword">new</span> LogicalDelta(cluster, result.getTraitSet(), result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  RelCollation collation = RelCollations.EMPTY;</span><br><span class="line">  <span class="keyword">if</span> (!query.isA(SqlKind.DML)) &#123; <span class="comment">//note: 如果是 DML 语句</span></span><br><span class="line">    <span class="keyword">if</span> (isOrdered(query)) &#123; <span class="comment">//note: 如果需要做排序的话</span></span><br><span class="line">      collation = requiredCollation(result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 对转换前后的 RelDataType 做验证</span></span><br><span class="line">  checkConvertedType(query, result);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (SQL2REL_LOGGER.isDebugEnabled()) &#123;</span><br><span class="line">    SQL2REL_LOGGER.debug(</span><br><span class="line">        RelOptUtil.dumpPlan(<span class="string">"Plan after converting SqlNode to RelNode"</span>,</span><br><span class="line">            result, SqlExplainFormat.TEXT,</span><br><span class="line">            SqlExplainLevel.EXPPLAN_ATTRIBUTES));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> RelDataType validatedRowType = validator.getValidatedNodeType(query);</span><br><span class="line">  <span class="keyword">return</span> RelRoot.of(result, validatedRowType, query.getKind())</span><br><span class="line">      .withCollation(collation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>真正的实现是在 <code>convertQueryRecursive()</code> 方法中完成的，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Recursively converts a query to a relational expression.</span></span><br><span class="line"><span class="comment"> * note：递归地讲一个 query 转换为 relational expression</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> query         Query</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> top           Whether this query is the top-level query of the</span></span><br><span class="line"><span class="comment"> *                      statement</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> targetRowType Target row type, or null</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Relational expression</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> RelRoot <span class="title">convertQueryRecursive</span><span class="params">(SqlNode query, <span class="keyword">boolean</span> top,</span></span></span><br><span class="line"><span class="function"><span class="params">    RelDataType targetRowType)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlKind kind = query.getKind();</span><br><span class="line">  <span class="keyword">switch</span> (kind) &#123;</span><br><span class="line">  <span class="keyword">case</span> SELECT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertSelect((SqlSelect) query, top), kind);</span><br><span class="line">  <span class="keyword">case</span> INSERT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertInsert((SqlInsert) query), kind);</span><br><span class="line">  <span class="keyword">case</span> DELETE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertDelete((SqlDelete) query), kind);</span><br><span class="line">  <span class="keyword">case</span> UPDATE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertUpdate((SqlUpdate) query), kind);</span><br><span class="line">  <span class="keyword">case</span> MERGE:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertMerge((SqlMerge) query), kind);</span><br><span class="line">  <span class="keyword">case</span> UNION:</span><br><span class="line">  <span class="keyword">case</span> INTERSECT:</span><br><span class="line">  <span class="keyword">case</span> EXCEPT:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertSetOp((SqlCall) query), kind);</span><br><span class="line">  <span class="keyword">case</span> WITH:</span><br><span class="line">    <span class="keyword">return</span> convertWith((SqlWith) query, top);</span><br><span class="line">  <span class="keyword">case</span> VALUES:</span><br><span class="line">    <span class="keyword">return</span> RelRoot.of(convertValues((SqlCall) query, targetRowType), kind);</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError(<span class="string">"not a query: "</span> + query);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>依然以前面的示例为例，因为是 SqlSelect 类型，这里会调用下面的方法做相应的转换：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Converts a SELECT statement's parse tree into a relational expression.</span></span><br><span class="line"><span class="comment"> * note：将一个 Select parse tree 转换成一个关系表达式</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RelNode <span class="title">convertSelect</span><span class="params">(SqlSelect select, <span class="keyword">boolean</span> top)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> SqlValidatorScope selectScope = validator.getWhereScope(select);</span><br><span class="line">  <span class="keyword">final</span> Blackboard bb = createBlackboard(selectScope, <span class="keyword">null</span>, top);</span><br><span class="line">  convertSelectImpl(bb, select);<span class="comment">//note: 做相应的转换</span></span><br><span class="line">  <span class="keyword">return</span> bb.root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>convertSelectImpl()</code> 方法中会依次对 SqlSelect 的各个部分做相应转换，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implementation of &#123;<span class="doctag">@link</span> #convertSelect(SqlSelect, boolean)&#125;;</span></span><br><span class="line"><span class="comment"> * derived class may override.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">convertSelectImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">final</span> Blackboard bb,</span></span></span><br><span class="line"><span class="function"><span class="params">    SqlSelect select)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//note: convertFrom</span></span><br><span class="line">  convertFrom(</span><br><span class="line">      bb,</span><br><span class="line">      select.getFrom());</span><br><span class="line">  <span class="comment">//note: convertWhere</span></span><br><span class="line">  convertWhere(</span><br><span class="line">      bb,</span><br><span class="line">      select.getWhere());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> List&lt;SqlNode&gt; orderExprList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="keyword">final</span> List&lt;RelFieldCollation&gt; collationList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  <span class="comment">//note: 有 order by 操作时</span></span><br><span class="line">  gatherOrderExprs(</span><br><span class="line">      bb,</span><br><span class="line">      select,</span><br><span class="line">      select.getOrderList(),</span><br><span class="line">      orderExprList,</span><br><span class="line">      collationList);</span><br><span class="line">  <span class="keyword">final</span> RelCollation collation =</span><br><span class="line">      cluster.traitSet().canonize(RelCollations.of(collationList));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (validator.isAggregate(select)) &#123;</span><br><span class="line">    <span class="comment">//note: 当有聚合操作时，也就是含有 group by、having 或者 Select 和 order by 中含有聚合函数</span></span><br><span class="line">    convertAgg(</span><br><span class="line">        bb,</span><br><span class="line">        select,</span><br><span class="line">        orderExprList);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 对 select list 部分的处理</span></span><br><span class="line">    convertSelectList(</span><br><span class="line">        bb,</span><br><span class="line">        select,</span><br><span class="line">        orderExprList);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (select.isDistinct()) &#123; <span class="comment">//note: select 后面含有 DISTINCT 关键字时（去重）</span></span><br><span class="line">    distinctify(bb, <span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: Converts a query's ORDER BY clause, if any.</span></span><br><span class="line">  convertOrder(</span><br><span class="line">      select, bb, collation, orderExprList, select.getOffset(),</span><br><span class="line">      select.getFetch());</span><br><span class="line">  bb.setRoot(bb.root, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里以示例中的 From 部分为例介绍 SqlNode 到 RelNode 的逻辑，按照示例 DEUBG 后的结果如下图所示，因为 form 部分是一个 join 操作，会进入 join 相关的处理中。</p>
<p><img src="/images/calcite/5-calcite.jpg" alt="convertFrom 之 Join 的情况"> </p>
<p>这部分方法调用过程是：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">convertQuery --&gt;</span><br><span class="line">convertQueryRecursive --&gt;</span><br><span class="line">convertSelect --&gt;</span><br><span class="line">convertSelectImpl --&gt;</span><br><span class="line">convertFrom &amp; convertWhere &amp; convertSelectList</span><br></pre></td></tr></table></figure>
<p>到这里 SqlNode 到 RelNode 过程就完成了，生成的逻辑计划如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LogicalSort(sort0=[<span class="variable">$0</span>], dir0=[ASC])</span><br><span class="line">  LogicalProject(USER_ID=[<span class="variable">$0</span>], USER_NAME=[<span class="variable">$1</span>], USER_COMPANY=[<span class="variable">$5</span>], USER_AGE=[<span class="variable">$2</span>])</span><br><span class="line">    LogicalFilter(condition=[AND(&gt;(<span class="variable">$2</span>, 30), &gt;(<span class="variable">$3</span>, 10))])</span><br><span class="line">      LogicalJoin(condition=[=(<span class="variable">$1</span>, <span class="variable">$4</span>)], joinType=[inner])</span><br><span class="line">        LogicalTableScan(table=[[USERS]])</span><br><span class="line">        LogicalTableScan(table=[[JOBS]])</span><br></pre></td></tr></table></figure>
<p>到这里前三步就算全部完成了。</p>
<h2 id="Step4-优化阶段（RelNode–-gt-RelNode）"><a href="#Step4-优化阶段（RelNode–-gt-RelNode）" class="headerlink" title="Step4: 优化阶段（RelNode–&gt;RelNode）"></a>Step4: 优化阶段（RelNode–&gt;RelNode）</h2><p>终于来来到了第四阶段，也就是 Calcite 的核心所在，优化器进行优化的地方，前面 sql 中有一个明显可以优化的地方就是过滤条件的下压（push down），在进行 join 操作前，先进行 filter 操作，这样的话就不需要在 join 时进行全量 join，减少参与 join 的数据量。</p>
<p>关于filter 操作下压，在 Calcite 中已经有相应的 Rule 实现，就是 <code>FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN</code>，这里使用 HepPlanner 作为示例的 planer，并注册 FilterIntoJoinRule 规则进行相应的优化，其代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HepProgramBuilder builder = <span class="keyword">new</span> HepProgramBuilder();</span><br><span class="line">builder.addRuleInstance(FilterJoinRule.FilterIntoJoinRule.FILTER_ON_JOIN); <span class="comment">//note: 添加 rule</span></span><br><span class="line">HepPlanner hepPlanner = <span class="keyword">new</span> HepPlanner(builder.build());</span><br><span class="line">hepPlanner.setRoot(relNode);</span><br><span class="line">relNode = hepPlanner.findBestExp();</span><br></pre></td></tr></table></figure>
<p>在 Calcite 中，提供了两种 planner：HepPlanner 和 VolcanoPlanner，关于这块内容可以参考【Drill/Calcite查询优化系列】这几篇文章（讲述得非常详细，赞），这里先简单介绍一下 HepPlanner 和 VolcanoPlanner，后面会关于这两个 planner 的代码实现做深入的讲述。</p>
<h3 id="HepPlanner"><a href="#HepPlanner" class="headerlink" title="HepPlanner"></a>HepPlanner</h3><p>特点（来自 <a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite" target="_blank" rel="external">Apache Calcite介绍</a>）：</p>
<ol>
<li>HepPlanner is a heuristic optimizer similar to Spark’s optimizer，与 spark 的优化器相似，HepPlanner 是一个 heuristic 优化器；</li>
<li>Applies all matching rules until none can be applied：将会匹配所有的 rules 直到一个 rule 被满足；</li>
<li>Heuristic optimization is faster than cost- based optimization：它比 CBO 更快；</li>
<li>Risk of infinite recursion if rules make opposing changes to the plan：如果没有每次都不匹配规则，可能会有无限递归风险；</li>
</ol>
<h3 id="VolcanoPlanner"><a href="#VolcanoPlanner" class="headerlink" title="VolcanoPlanner"></a>VolcanoPlanner</h3><p>特点（来自 <a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite" target="_blank" rel="external">Apache Calcite介绍</a>）：</p>
<ol>
<li>VolcanoPlanner is a cost-based optimizer：VolcanoPlanner是一个CBO优化器；</li>
<li>Applies matching rules iteratively, selecting the plan with the cheapest cost on each iteration：迭代地应用 rules，直到找到cost最小的plan；</li>
<li>Costs are provided by relational expressions；</li>
<li>Not all possible plans can be computed：不会计算所有可能的计划；</li>
<li>Stops optimization when the cost does not significantly improve through a determinable number of iterations：根据已知的情况，如果下面的迭代不能带来提升时，这些计划将会停止优化；</li>
</ol>
<h3 id="示例运行结果"><a href="#示例运行结果" class="headerlink" title="示例运行结果"></a>示例运行结果</h3><p>经过 HepPlanner 优化后的逻辑计划为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LogicalSort(sort0=[<span class="variable">$0</span>], dir0=[ASC])</span><br><span class="line">  LogicalProject(USER_ID=[<span class="variable">$0</span>], USER_NAME=[<span class="variable">$1</span>], USER_COMPANY=[<span class="variable">$5</span>], USER_AGE=[<span class="variable">$2</span>])</span><br><span class="line">    LogicalJoin(condition=[=(<span class="variable">$1</span>, <span class="variable">$4</span>)], joinType=[inner])</span><br><span class="line">      LogicalFilter(condition=[&gt;(<span class="variable">$2</span>, 30)])</span><br><span class="line">        EnumerableTableScan(table=[[USERS]])</span><br><span class="line">      LogicalFilter(condition=[&gt;(<span class="variable">$0</span>, 10)])</span><br><span class="line">        EnumerableTableScan(table=[[JOBS]])</span><br></pre></td></tr></table></figure>
<p>可以看到优化的结果是符合我们预期的，HepPlanner 和 VolcanoPlanner 详细流程比较复杂，后面会有单独的文章进行讲述。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Calcite 本身的架构比较好理解，但是具体到代码层面就不是那么好理解了，它抛出了很多的概念，如果不把这些概念搞明白，代码基本看得也是云里雾里，特别是之前没有接触过这块内容的同学（我最开始看 Calcite 代码时是真的头大），入门的门槛确实高一些，但是当这些流程梳理清楚之后，其实再回头看，也没有多少东西，在生产中用的时候主要也是针对具体的业务场景扩展相应的 SQL 语法、进行具体的规则优化。</p>
<p>Calcite 架构设计得比较好，其中各个组件都可以单独使用，Rule（规则）扩展性很强，用户可以根据业务场景自定义相应的优化规则，它支持标准的 SQL，支持不同的存储和计算引擎，目前在业界应用也比较广泛，这也证明其牛叉之处。</p>
<blockquote>
<p>本文只是个人理解的总结，由于本人也是刚接触这块，理解有偏差的地方，欢迎指正~</p>
</blockquote>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite" target="_blank" rel="external">Apache Calcite：Hadoop 中新型大数据查询引擎</a>；</li>
<li><a href="https://www.slideshare.net/JordanHalterman/introduction-to-apache-calcite" target="_blank" rel="external">Apache Calcite介绍</a>；</li>
<li><a href="https://arxiv.org/pdf/1802.10233.pdf" target="_blank" rel="external">Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</a>；</li>
<li><a href="http://calcite.apache.org/docs/tutorial.html" target="_blank" rel="external">Calcite Tutorial</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于 Apache Calcite 的简单介绍可以参考 &lt;a href=&quot;https://www.infoq.cn/article/new-big-data-hadoop-query-engine-apache-calcite&quot; target=&quot;_blank&quot; rel=&quot;e
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="calcite" scheme="http://matt33.com/tags/calcite/"/>
    
  </entry>
  
  <entry>
    <title>如何高效学习</title>
    <link href="http://matt33.com/2018/11/21/effective-learning/"/>
    <id>http://matt33.com/2018/11/21/effective-learning/</id>
    <published>2018-11-21T02:12:51.000Z</published>
    <updated>2019-02-24T02:29:01.556Z</updated>
    
    <content type="html"><![CDATA[<p>在这个知识爆炸、科技日新月异的时代，技术的变化远比我们想象的要快很多，这就对工程师的要求就提高了很多，特别是对于那些在技术上有所追求的工程师而言。对于一些互联网大厂，学习能力也成了面试中重点考察的内容。如何快速学习、掌握一门新的技术，如何提高自己的学习效率，对于有一定工作经验的人来说，可能每个人都有一个自己的学习方法论，但是我们也需要去学习借鉴别人（特别是那些有一定技术影响力的技术大咖）的经验，来不断更新和完善自己的方法轮。今天这篇《高效学习》，就是与大家一起探讨技术学习的方法论，本文的内容主要来自耗子叔的《左耳听风 —— 高效学习篇》，中间会穿插个人的一些经验，算是对这个系列的一个总结。如果想看原文内容，欢迎订阅耗子叔的这个专栏，这个专栏质量还是非常高的，耗子叔推荐了很多优秀的学习资源（通过文章末尾处的二维码链接购买）。</p>
<h2 id="端正学习态度"><a href="#端正学习态度" class="headerlink" title="端正学习态度"></a>端正学习态度</h2><p>对于大多数人来说，我们并不是那种天赋异禀的天才，所以那些速成的学习方法并不适合我们，因为，<strong>对于非天才的我们来说，学习是不可能速成的</strong>，学习本来就是一件【逆人性】的事，就像锻炼身体一样，<strong>需要人持续付出，会让人感到痛苦，并随时想找理由放弃，实际上，痛苦是成长的必经阶段</strong>。</p>
<p>大部分人都认为自己热爱学习，但是有多少能真正付出实践、并一直坚持下去，能做到实践和坚持的人，一般运气都不会太差。如果我们去研究一下古今中外的成功人士，就会发现，他们基本上都是非常自律的，也都是非常热爱学习的，他们可以沉得下心来不断学习，在学习中不断地思考、探索和实践。懒，是人类的天性，如果不能克服自己 DNA 中的弱点，不能端正自己的态度，不能自律，不能坚持，不能举一反三，不能不断追问等，那么，无论多好的方法，你都不可能学好。所以，<strong>有正确的态度非常重要</strong>。</p>
<blockquote>
<p>当然只做到上面说的，并不一定能保证能够实现所谓的成功，但是完全可以让你在某个领域做到足够优秀。</p>
</blockquote>
<h3 id="主动学习和被动学习"><a href="#主动学习和被动学习" class="headerlink" title="主动学习和被动学习"></a>主动学习和被动学习</h3><p>下面这张图，大部分人应该都见过，这张图又称为学习金字塔：</p>
<p><img src="/images/share/learn.png" alt="学习效率"></p>
<p>人的学习，可以分为【被动学习】和【主动学习】两个层次：</p>
<ul>
<li><strong>被动学习</strong>：如听讲、阅读、视听、演示，学习内容的平均留存率为 5%、10%、20% 和 30%；</li>
<li><strong>主动学习</strong>：如通过讨论、实践、教授给他人，会将原来被动学习的内容留存率从 5% 提升到 50%、75%、90%。</li>
</ul>
<p>关于这个，我是深有体会的，如果我们只是看书或听一下别人的分享，不去实践，可能不到半个月，能记住 10% 的内容就不错了，我认为最好的学习方法是 <strong>实践，总结，教授给别人（要让别人听明白，教授的过程要有深度的讨论，而不是 PPT 走一遍）</strong> 。</p>
<p>过去一年多，很幸运的是，遇到了几个热爱学习的小伙伴，我们经常周末一起组织分享，每次分享只涉及很少的一块内容，分享过程中我们以讨论为主，这对分享者的能力锻炼有很好的效果（通过讨论听众也能收获很多），首先他需要自己能够理解这个问题，其次他需要把自己的理解给别人讲清楚，还需要回答其他人提出的问题（这些问题可能是分享者压根没注意的问题）。我也一直想在团队内部推广这种学习方法（这种方法人数太多的话就不太适合了），但是在团队内部去推，效果没有想象中得那么好，而且在团队内部反而很难坚持下去（大家的时间都比较有限，如果占据了别人的工作时间，别人可能需要加班才能完成自己的工作，所以大家兴趣并没有那么高昂）。相反，如果能找几个愿意一起学习的小伙伴一同学习、成长，这样反而效果好很多，如果你能找到这样的一群小伙伴，我是非常推荐这种学习方式，把自己学习的内容分享给其他人（大家一起学习、讨论这种学习效果，考虑问题的深度要比自己独自学习高出很多）。</p>
<h3 id="浅度学习和深度学习"><a href="#浅度学习和深度学习" class="headerlink" title="浅度学习和深度学习"></a>浅度学习和深度学习</h3><p><strong>学习并不是努力读更多的书，盲目追求阅读的速度和数量，这会让人产生低层次的勤奋和成长的感觉，这只是在使蛮力。要思辩，要践行，要总结和归纳，否则，你只是在机械地重复某件事，而不会有质的成长。</strong></p>
<p>在知识的领域其实也有阶层之分（类似于富人和穷人在财富方面的阶层之分，阶层的跨越非常难，但不是没有可能），那么长期在底层知识阶层的人，需要等着高层的人来喂养，他们长期陷入各种谣言和不准确的信息环境中，于是就导致错误和幼稚的认知，并习惯于哪些不费劲儿的轻度学习方式，<strong>从而一点点地丧失了深度学习的独立思考能力，从而再也没有能力打破知识阶层的限制，被困在认知底层翻不了身</strong>（就像我们经常说的，美国那些在穷人区生活的人们，他们在没有受到很好教育的前提下想突破自己的阶层，真的很难）。</p>
<p>对于知识的学习，我们应该如何进行深度学习呢？下面几点是关键：</p>
<ol>
<li><strong>高质量的信息源和第一手的知识</strong>；</li>
<li><strong>把知识连成地图，将自己的理解反述出来</strong>；</li>
<li><strong>不断地反思和思辩，与不同年龄段的人讨论</strong>：讨论、交流很多情况下，比自己看书、看代码收获要多很多；</li>
<li><strong>举一反三，并践行之，把知识转换成技能</strong>。</li>
</ol>
<p>学习有三个步骤：</p>
<ol>
<li><strong>知识采集</strong>：信息源是非常重要的，<strong>获取信息源头、破解表面信息的内在本质、多方数据印证</strong>，是这个步骤的关键；</li>
<li><strong>知识缝合</strong>：所谓缝合就是把信息组织起来，成为结构体的知识，这里，<strong>连接记忆，逻辑推理，知识梳理</strong> 是很重要的三部分；</li>
<li><strong>技能转换</strong>：通过 <strong>举一反三、实践和练习</strong>，以及<strong>教授传导</strong>，把知识转换成自己的技能，这种技能可以让你进入更高的阶层；</li>
</ol>
<h3 id="学习的目的"><a href="#学习的目的" class="headerlink" title="学习的目的"></a>学习的目的</h3><p>学习目的是什么呢？</p>
<ol>
<li><strong>学习是为了找到方法</strong>：学习不仅仅是为了找到答案，而更是为了<strong>找到方法</strong>，掌握了通往答案的路径和方法之后，便拥有了无师自通的能力；</li>
<li><strong>学习是为了找到原理</strong>：学习不仅仅是为了知道，而更是为了<strong>思考和理解</strong>（真正的学习，从来都不是轻松的，而是那种你知道得越多，你的问题就会越多，你的问题越多，你就会思考得越多，你思考得越多，你就会觉得自己直到越少，于是你就会想要了解更多，这是一种螺旋式上升上下求索的状态），一旦掌握了这些本质的东西，你就会发现，整个复杂多变的世界在变得越来越简单；</li>
<li><strong>学习是为了了解自己</strong>：学习不仅仅是为了开拓眼界，而更是为了找到自己的未知，为了了解自己，开拓眼界的目的就是为了发现自己的不足和上升空间，从而才能让自己成长；</li>
<li><strong>学习是为了改变自己</strong>：学习不仅仅是为了成长，而更是为了改变自己（改变自己的思考方式和思维方式，改变自己与生俱来的那些垃圾和低效的算法）。</li>
</ol>
<h2 id="源头、原理和知识地图"><a href="#源头、原理和知识地图" class="headerlink" title="源头、原理和知识地图"></a>源头、原理和知识地图</h2><h3 id="挑选知识和信息源"><a href="#挑选知识和信息源" class="headerlink" title="挑选知识和信息源"></a>挑选知识和信息源</h3><p>对于计算机知识来说，<strong>学习英文</strong>是是否能够成长的关键，如果我们能用 Google 英文关键词就可以找到自己想要的知识，那么我们只是算得上能跟得上这个时代，但如果能在社区里跟社区里的大牛交流得到答案，这样才算是领先于这个时代。</p>
<p>信息源应该有以下几个特质：</p>
<ol>
<li><strong>第一手的资料</strong>，不是被别人理解过、消化过的二手资料，尤其对于知识性的东西来说，更是这样；</li>
<li>应该是有佐证、有数据、有引用的，或是有权威人士或大公司生产系统背书的资料，应该是被时间和实践检验过的，或是小心求证过的，不是拍脑袋野路子或是道听途说的资料；</li>
<li>应该是加入了一些自己的经验和思考，可以引发人深思的，是所谓信息的密集很大的文章。</li>
</ol>
<p>耗子叔比较推荐 Medium 上的文章，这个上面的文章质量比较高。</p>
<h3 id="注重基础和原理"><a href="#注重基础和原理" class="headerlink" title="注重基础和原理"></a>注重基础和原理</h3><p><strong>基础知识和原理性的东西是无比重要的</strong>，无论是 JVM 还是 Node，或者是 Python 解释器里干了什么，它都无法逾越底层操作系统 API 对 『物理世界』的限制。</p>
<p>比如，当学习一门新的语言时，除了看每个语言都有的 if-else、for/while-loop、function 等东西外，还需要重点看的就是：</p>
<ul>
<li>出错处理是怎么玩的？</li>
<li>内存管理是怎么玩的？</li>
<li>数据封装和扩展是怎么玩的？</li>
<li>多态和泛型是怎么搞的？</li>
<li>运行时识别和反射是怎么玩的？</li>
<li>并发编程是怎么玩的？</li>
<li>…</li>
</ul>
<p>所以，最关键的是，<strong>这些基础知识和原理性的东西和技术，都是经历过长时间的考验的，这些基础技术也有很多人类历史上的智慧结晶，会给你很多启示和帮助</strong>（基础知识虽然很枯燥不实用、工作上用不到，学习这些知识是为了学得更快，基础打牢，学什么都快，而学得快就会学得多，学得多，就会思考得多，思考得多，就会学得更快…）。</p>
<h3 id="使用知识图"><a href="#使用知识图" class="headerlink" title="使用知识图"></a>使用知识图</h3><p>耗子叔在这里介绍一个<strong>知识图</strong>的学习方式，通过这种方式可以让我们从一个技术最重要的主干的地方开始出发遍历所有的技术细节，以 C++ 为例，分为三部分：</p>
<ol>
<li>C++ 是用来解决 C 语言问题的，那么 C 语言有什么问题呢？指针、宏、错误处理、数据拷贝…C++是用什么技术来解决这些问题的？</li>
<li>C++ 的面向对象特性：封装、继承、多态。封装，让我想起了构造函数、析构函数等。析构函数让我想起了初始化列表，想到了默认构造函数，想到了拷贝构造函数，想到了 new…多态，让我想到了虚函数，想到了 RTTI，RTTI 让我想起了 <code>dynamic_cast</code> 和 <code>typeid</code> 等；</li>
<li>C++ 的泛型编程，我想到了 <code>templete</code>，想到了操作符重载，想到了函数对象，想到了 STL，想到数据容器，想到了 iterator，想到了通用算法等等。</li>
</ol>
<p>有了这样一颗知识树之后，当出现一些不知道的知识点时，可以往这棵知识树上挂，而这样一来，也使得我们的学习更为系统和全面。</p>
<h2 id="深度、归纳和坚持实践"><a href="#深度、归纳和坚持实践" class="headerlink" title="深度、归纳和坚持实践"></a>深度、归纳和坚持实践</h2><h3 id="系统地学习"><a href="#系统地学习" class="headerlink" title="系统地学习"></a>系统地学习</h3><p>在系统性地学习一项技术时，耗子叔总结了一个<strong>学习模板</strong>，模板内容如下：</p>
<ol>
<li><strong>这个技术出现的背景、初衷和要达到什么样的目标或是要解决什么样的问题</strong>，这是这个技术的成因和目标（设计理念），也是这个技术的灵魂；</li>
<li><strong>这个技术的优势和劣势分别是什么，或者说，这个技术的 tradeoff 是什么</strong>，任何技术都有其好坏，在解决一个问题的时候，也会带来新的问题，一般来说，任何设计都有 tradeoff，所以，需要知道这个技术的优势和劣势，以及带来的挑战；</li>
<li><strong>这个技术的适用场景</strong>，要注意没有一个技术是普适的，每个技术都其特别适合的场景，所谓的场景一般分为两个：一个是业务场景，一个是技术场景；</li>
<li><strong>技术的组成部分和关键点</strong>，这是技术的核心思想，也是这个技术的灵魂所在，学习技术的核心部分是快速掌握的关键；</li>
<li><strong>技术的底层原理和关键实现</strong>，任何一个技术都有其底层的关键基础技术，学习这些关键的底层技术，可以让我们未来很快地掌握其他技术；</li>
<li><strong>已有的实现和它之间的对比</strong>，一般来说，任何一个技术都会有不同的实现，不同的实现都会有不同的侧重，学习不同的实现，可以让你得到不同的想法和思路，对于开阔思维、深入细节是非常重要的。</li>
</ol>
<h3 id="举一反三"><a href="#举一反三" class="headerlink" title="举一反三"></a>举一反三</h3><p>重点是如何才能让自己拥有举一反三的能力，在这方面，耗子叔对自己训练如下：</p>
<ol>
<li>对于一个场景，制造出各种不同的问题或难题；</li>
<li>对于一个问题，努力寻找尽可能多的解，并比较这些解的优劣；</li>
<li>对于一个解，努力寻找各种不同的测试案例，以图让其健壮。</li>
</ol>
<p>举一反三的能力，可以分解为：</p>
<ol>
<li><strong>联想能力</strong>：这种能力的锻炼需要你平时就在不停地思考同一个事物的不同的用法，或是联想与之有关的别的事物。对于软件开发和技术学习也一样；</li>
<li><strong>抽象能力</strong>：抽象能力是举一反三的基本技能。平时你解决问题的时候，如果你能对这个问题进行抽象，你就可以获得更多的表现形式。抽象能力需要找到解决问题的通用模型，比如数学就是对现实世界的一种抽象。只要我们能把现实世界的各种问题建立成数据模型（如，建立各种维度的向量），我们就可以用数学来求解，这也是机器学习的本质；</li>
<li><strong>自省能力</strong>：所谓自省能力就是自己找自己的难看。当你得到一个解的时候，要站在自己的对立面来找这个解的漏洞。有点像左右手互博。这种自己和自己辩论的能力又叫思辨能力。将自己分裂成正反方，左右方，甚至多方，站在不同的立场上来和自己辩论，从而做到不漏过一个 case，从而获得完整全面的问题分析能力。</li>
</ol>
<p>如果要获得这三种能力，除了你要很喜欢思考和找其它人来辩论或讨论以外，还要看你自己是否真的善于思考，是否有好奇心，是否喜欢打破沙锅问到底，是否喜欢关注细节，做事是否认真，是否严谨……</p>
<h3 id="总结和归纳"><a href="#总结和归纳" class="headerlink" title="总结和归纳"></a>总结和归纳</h3><p>我们把学到的东西用自己的语言和理解重新组织并表达出来，本质上是对信息进行消化和再加工的过程，这个过程可能会有信息损失，但也可能会有新信息加入，本质上是信息重构的过程。<strong>我们积累的知识越多，在知识间进行联系和区辨的能力就越强，对知识进行总结和归纳也就越轻松</strong>。而想要提高总结归纳的能力，首先要多阅读，多积累素材，扩大自己的知识面，多和别人讨论，多思辨，从而见多识广。</p>
<p>不过，我们需要注意的是，如果只学了部分知识或者还没有学透，就开始对知识进行总结归纳，那么总结归纳出来的知识结构也只能是混乱和幼稚的。因此，学习的开始阶段，可以不急于总结归纳，不急于下判断，做结论，而应该<strong>保留部分知识的不确定性，保持对知识的开放状态</strong>。当对整个知识的理解更深入，自己站的位置更高以后，总结和归纳才会更有条理。总结归纳更多是在复习中对知识的回顾和重组，而不是一边学习一边就总结归纳。</p>
<p>最后再总结一下做总结归纳的方法：<strong>把你看到和学习到的信息，归整好，排列好，关联好，总之把信息碎片给结构化掉，然后在结构化的信息中，找到规律，找到相通之处，找到共同之处，进行简化、归纳和总结，最终形成一种套路，一种模式，一种通用方法</strong>。</p>
<h3 id="实践出真知"><a href="#实践出真知" class="headerlink" title="实践出真知"></a>实践出真知</h3><p><strong>实践是很累很痛苦的事，但只有痛苦才会让人反思，而反思则是学习和改变自己的动力。Grow up through the pain，是非常有道理的。</strong></p>
<h3 id="坚持不懈"><a href="#坚持不懈" class="headerlink" title="坚持不懈"></a>坚持不懈</h3><p>坚持本来也是一件反人性的事情，关于坚持的问题，大家应该都见过很多相似的文章，总之，坚持是一件看似简单、但是完成率非常低的事情。如果想要让自己能够坚持下去，最好能够让自己处于一个<strong>正反馈的循环</strong>中，比如，学习一个技术之后，与大家去分享自己的经验，或者整理出一篇博客让其他学习，都是一种很好的学习方法。</p>
<h2 id="如何学习和阅读代码"><a href="#如何学习和阅读代码" class="headerlink" title="如何学习和阅读代码"></a>如何学习和阅读代码</h2><h3 id="读书还是读代码？"><a href="#读书还是读代码？" class="headerlink" title="读书还是读代码？"></a>读书还是读代码？</h3><p>关于书/文档和代码的关系：</p>
<ul>
<li>代码：What、How &amp; Details；</li>
<li>书/文档：What、How &amp; Why；</li>
</ul>
<p>代码是具体的实现，但是并不能告诉你为什么？<strong>书和文档是人对人说的话，代码是人对机器说的话</strong>：</p>
<ol>
<li><strong>如果想知道为什么要这么搞，应该去看书、看文档</strong>：特别当我们想了解一种思想、一种方法、一种原理、一种经验时，书和文档是最佳的方式、更有效率一些；</li>
<li><strong>如果想知道是怎么实现的，实现的细节，应该去看代码</strong>：对于具体的实现，比如：某协程的实现、某模块的性能、某个算法的实现，这时候最好的方式就是去读代码；</li>
</ol>
<p>至于从代码中收获大还是从书中收获大，不同的场景、不同的目的下，会有不同的答案，我个人对这部分的想法是：</p>
<ol>
<li>工作的前几年，更多的时候应该关注代码、关注细节的实现、多写代码（当然不是说完全不看书，书是必须要看的，特别是当有了相关实战经验之后再去看书看，效果会更好），这个阶段，Google、Stack Overflow、Github 将会是最好的学习渠道，如果在过程中，还能获得一些技术影响力，那将再好不过了；</li>
<li>有一定经验之后，这时候需要更多的【理性认识】，在这个阶段，我们的想法不再是实现某个功能，可能是想做出更牛逼的东西来，这时候应该多读那些大牛的书、与大牛交流、关注国际顶级会议的论文，应该让自己往技术 leader 这个方向发展。</li>
</ol>
<h3 id="如何阅读源代码"><a href="#如何阅读源代码" class="headerlink" title="如何阅读源代码"></a>如何阅读源代码</h3><p>关于如何阅读源代码，耗子叔分享了一些干货，我这里简单总结一下</p>
<p>首先是阅读代码之前，最好先有以下了解：</p>
<ol>
<li>基础知识：相关的语言和基础技术的知识；</li>
<li>软件功能：需要知道这个软件是做什么的、有哪些特性、哪些配置项，最好能够读一遍用户手册，然后让软件跑起来，自己先用一下感受一下；</li>
<li>相关文档：读一下相关的内部文档；</li>
<li>代码的组织结构：先简单看下源码的组织结构。</li>
</ol>
<p>接下来，就是详细地看代码的实现，这里耗子叔分享了一个源代码阅读的经验：</p>
<ol>
<li><strong>接口抽象定义</strong>：任何代码都会有很多接口或抽象定义，其描述了代码需要处理的数据结构或者业务实体，以及它们之间的关系，理清楚这些关系是非常重要的；</li>
<li><strong>模块粘合层</strong>：我们的代码有很多都是用来粘合代码的，比如中间件（middleware）、Promises 模式、回调（Callback）、代理委托、依赖注入等。这些代码模块间的粘合技术是非常重要的，因为它们会把本来平铺直述的代码给分裂开来，让你不容易看明白它们的关系；</li>
<li><strong>业务流程</strong>：这是代码运行的过程。<strong>一开始，我们不要进入细节，但需要在高层搞清楚整个业务的流程是什么样的</strong>，在这个流程中，数据是怎么被传递和处理的。一般来说，我们需要<strong>画程序流程图或者时序处理图</strong>；</li>
<li><strong>具体实现</strong>：了解上述的三个方面的内容，相信你对整个代码的框架和逻辑已经有了总体认识。这个时候，你就可以深入细节，开始阅读具体实现的代码了。对于代码的具体实现，一般来说，你需要知道下面一些事实，这样有助于你在阅读代码时找到重点。<ul>
<li><strong>代码逻辑</strong>：代码有两种逻辑，一种是<strong>业务逻辑</strong>，这种逻辑是真正的业务处理逻辑；另一种是<strong>控制逻辑</strong>，这种逻辑只是用控制程序流转的，不是业务逻辑。比如：flag 之类的控制变量，多线程处理的代码，异步控制的代码，远程通讯的代码，对象序列化反序列化的代码等。这两种逻辑你要分开，很多代码之所以混乱就是把这两种逻辑混在一起了；</li>
<li><strong>出错处理</strong>：根据 2：8 原则，20% 的代码是正常的逻辑，80% 的代码是在处理各种错误，所以，你在读代码的时候，完全可以把处理错误的代码全部删除掉，这样就会留下比较干净和简单的正常逻辑的代码。排除干扰因素，可以更高效地读代码；</li>
<li><strong>数据处理</strong>：只要你认真观察，就会发现，我们好多代码就是在那里倒腾数据。比如 DAO、DTO，比如 JSON、XML，这些代码冗长无聊，不是主要逻辑，可以不理；</li>
<li><strong>重要的算法</strong>：一般来说，我们的代码里会有很多重要的算法，我说的并不一定是什么排序或是搜索算法，可能会是一些其它的核心算法，比如一些索引表的算法，全局唯一 ID 的算法，信息推荐的算法、统计算法、通读算法（如 Gossip）等。这些比较核心的算法可能会非常难读，但它们往往是最有技术含量的部分；</li>
<li><strong>底层交互</strong>：有一些代码是和底层系统的交互，一般来说是和操作系统或是 JVM 的交互。因此，读这些代码通常需要一定的底层技术知识，不然，很难读懂；</li>
</ul>
</li>
<li><strong>运行时调试</strong>：很多时候，代码只有运行起来了，才能知道具体发生了什么事，所以，我们让代码运行进来，然后用日志也好，debug 设置断点跟踪也好。实际看一下代码的运行过程，是了解代码的一种很好的方式。</li>
</ol>
<p>总结一下，阅读代码的方法如下。</p>
<ul>
<li>一般采用自顶向下，从总体到细节的【剥洋葱皮】的读法；</li>
<li>画图是必要的，程序流程图，调用时序图，模块组织图；</li>
<li>代码逻辑归一下类，排除杂音，主要逻辑才会更清楚；</li>
<li>debug 跟踪一下代码是了解代码在执行中发生了什么的最好方式。</li>
</ul>
<h2 id="面对枯燥和量大的知识"><a href="#面对枯燥和量大的知识" class="headerlink" title="面对枯燥和量大的知识"></a>面对枯燥和量大的知识</h2><p>知识很多，在学习的时候要<strong>抓住本质，关注本质和原理</strong>，这些才是不容易改变的，是经得住时间考验的。<strong>带着问题去学习</strong>也是一种非常好的学习方式，耗子叔根据自己经验在专栏中分享以下几个 tips：</p>
<ol>
<li><strong>认真阅读文档</strong>：使用前之前看文档，跟遇到问题之后再看一遍使用文档，收获可能会完全不一样；</li>
<li><strong>用不同的方式学习同一个东西</strong>：比如，看书、听课、写博客、讲课等；</li>
<li><strong>不要被打断</strong>：被打断简直是学习天敌，保持自己注意力的集中；</li>
<li><strong>总结压缩信息</strong>：面对太多的信息时，用一个自己的【压缩算法】抓住问题的关键点；</li>
<li><strong>把未知关联到已知</strong>：把新学的知识关联到已知的事物上来；</li>
<li><strong>用教的方式学习</strong>：这种方式对自己的能力会是一个极大的提升；</li>
<li><strong>学以致用</strong>：把学到的东西用起来，在实践中深化自己的学习效果；</li>
<li><strong>不要记忆</strong>：聪明的人不会记忆知识的，他们会找方法，那些可以推导出知识或答案的方法；</li>
<li><strong>多犯错误</strong>：犯错会让你学到更多，通过错误总结教训。</li>
</ol>
<blockquote>
<p>这里有一个 TED 的演讲，<a href="https://weibo.com/tv/v/Gj1tol62s?fid=1034:3e9bbb1d315b62c5deb90e13efd09981" target="_blank" rel="external">TED演讲：只需20个小时，你就能学会任何事情！</a>，保证自己全身心投入、不受外界打扰的情况下，只要20个小时，我们就能达到这里 <a href="http://matt33.com/2018/08/01/system-learn-summary/#3-%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6">如何学习开源项目-第三步</a>，当然这20个小时要求是一个非常专注的20个小时，我还没有尝试过这种学习方法，近期准备尝试一次这种学习方法，到时候会写一篇文章来总结一下自己的经验。</p>
</blockquote>
<p>最后，以矮大紧的一句话作为结束：【时代变来变去，确实有一些新的东西，但是在这样一个时代里，有一样东西没有变，就是有这样一群人，然后我们都读了一点书，受过不错的教育，然后对自己的心灵能长出什么东西，虽然不知道具体会长什么东西，但是拒绝全部种玉米、拒绝全部长土豆，希望心里有一亩田，有一天能长出一朵不知道是什么的花。（—来自《晓说》）】（这段话好像跟文章的主题没有什么关系，但不知为何突然想到了这段话，这里就列了出来）。</p>
<p><img src="/images/share/zuoer-tingfeng.jpeg" alt="《极客时间-左耳听风》专栏，这里订阅有优惠"></p>
<hr>
<p>参考：</p>
<ul>
<li>极客时间-左耳听风-《高效学习》系列整理；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在这个知识爆炸、科技日新月异的时代，技术的变化远比我们想象的要快很多，这就对工程师的要求就提高了很多，特别是对于那些在技术上有所追求的工程师而言。对于一些互联网大厂，学习能力也成了面试中重点考察的内容。如何快速学习、掌握一门新的技术，如何提高自己的学习效率，对于有一定工作经
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="思考" scheme="http://matt33.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Exactly-Once 之事务性实现</title>
    <link href="http://matt33.com/2018/11/04/kafka-transaction/"/>
    <id>http://matt33.com/2018/11/04/kafka-transaction/</id>
    <published>2018-11-04T12:36:34.000Z</published>
    <updated>2019-02-24T02:29:01.556Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempotent Producer 不提供跨多个 Partition 和跨会话场景下的保证，因此，我们是需要一种更强的事务保证，能够原子处理多个 Partition 的写入操作，数据要么全部写入成功，要么全部失败，不期望出现中间状态。这就是 Kafka Transactions 希望解决的问题，简单来说就是能够实现 <code>atomic writes across partitions</code>，本文以 Apache Kafka 2.0.0 代码实现为例，深入分析一下 Kafka 是如何实现这一机制的。</p>
<p>Apache Kafka 在 Exactly-Once Semantics（EOS）上三种粒度的保证如下（来自 <a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="external">Exactly-once Semantics in Apache Kafka</a>）：</p>
<ol>
<li>Idempotent Producer：Exactly-once，in-order，delivery per partition；</li>
<li>Transactions：Atomic writes across partitions；</li>
<li>Exactly-Once stream processing across read-process-write tasks；</li>
</ol>
<p>第二种情况就是本文讲述的主要内容，在讲述整个事务处理流程时，也顺便分析第三种情况。</p>
<h2 id="Kafka-Transactions"><a href="#Kafka-Transactions" class="headerlink" title="Kafka Transactions"></a>Kafka Transactions</h2><p>Kafka 事务性最开始的出发点是为了在 Kafka Streams 中实现 Exactly-Once 语义的数据处理，这个问题提出之后，在真正的方案讨论阶段，社区又挖掘了更多的应用场景，也为了尽可能覆盖更多的应用场景，在真正的实现中，在很多地方做了相应的 tradeoffs，后面会写篇文章对比一下 RocketMQ 事务性的实现，就能明白 Kafka 事务性实现及应用场景的复杂性了。</p>
<p>Kafka 的事务处理，主要是允许应用可以把消费和生产的 batch 处理（涉及多个 Partition）在一个原子单元内完成，操作要么全部完成、要么全部失败。为了实现这种机制，我们需要应用能提供一个唯一 id，即使故障恢复后也不会改变，这个 id 就是 TransactionnalId（也叫 txn.id，后面会详细讲述），txn.id 可以跟内部的 PID 1:1 分配，它们不同的是 txn.id 是用户提供的，而 PID 是 Producer 内部自动生成的（并且故障恢复后这个 PID 会变化），有了 txn.id 这个机制，就可以实现多 partition、跨会话的 EOS 语义。</p>
<p>当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：</p>
<ol>
<li>跨会话的幂等性写入：即使中间故障，恢复后依然可以保持幂等性；</li>
<li>跨会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；</li>
<li>跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。</li>
</ol>
<p>上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer 端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：</p>
<ol>
<li>对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；</li>
<li>一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；</li>
<li>Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；</li>
<li>Consumer 可能没有订阅这个事务涉及的全部 Partition。</li>
</ol>
<p>简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：</p>
<ol>
<li>Atomic writes across multiple partitions.</li>
<li>All messages in a transaction are made visible together, or none are.</li>
<li>Consumers must be configured to skip uncommitted messages.</li>
</ol>
<h2 id="事务性示例"><a href="#事务性示例" class="headerlink" title="事务性示例"></a>事务性示例</h2><p>Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 <code>transactional.id</code>，通过 <code>initTransactions()</code> 初始化事务状态信息，再通过 <code>beginTransaction()</code> 标识一个事务的开始，然后通过 <code>commitTransaction()</code> 或 <code>abortTransaction()</code> 对事务进行 commit 或 abort，示例如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"client.id"</span>, <span class="string">"ProducerTranscationnalExample"</span>);</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"test-transactional"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line">producer.initTransactions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    String msg = <span class="string">"matt test"</span>;</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"0"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"1"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"2"</span>, msg.toString()));</span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException e1) &#123;</span><br><span class="line">    e1.printStackTrace();</span><br><span class="line">    producer.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e2) &#123;</span><br><span class="line">    e2.printStackTrace();</span><br><span class="line">    producer.abortTransaction();</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>
<p>事务性的 API 也同样保持了 Kafka 一直以来的简洁性，使用起来是非常方便的。</p>
<h2 id="事务性要解决的问题"><a href="#事务性要解决的问题" class="headerlink" title="事务性要解决的问题"></a>事务性要解决的问题</h2><p>回想一下，前面一篇文章中关于幂等性要解决的问题（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98">幂等性要解决的问题</a>），事务性其实更多的是解决幂等性中没有解决的问题，比如：</p>
<ol>
<li>在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），这相当于出现了中间的状态，这并不是我们期望的结果；</li>
<li>Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；</li>
</ol>
<p>再来分析一下，Kafka 提供的事务性是如何解决上面两个问题的：</p>
<ol>
<li>如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；</li>
<li>其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactly-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactly-Once（这只是设计的思想，具体的实现后续会有文章详细解揭秘）。</li>
</ol>
<h2 id="事务性实现的关键"><a href="#事务性实现的关键" class="headerlink" title="事务性实现的关键"></a>事务性实现的关键</h2><p>对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。对于一个事务操作而言，其会涉及到多个 Topic-Partition 数据的写入，如果是一个 long transaction 操作，可能会涉及到非常多的数据，如何才能保证这个事务操作的原子性（要么全部完成，要么全部失败）呢？</p>
<ol>
<li>关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，如果大家一致认为可以 commit，那么就执行 commit，否则执行 abort：<ul>
<li>我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；</li>
<li>Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；</li>
<li>这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 <strong>TransactionCoordinator</strong> 角色；</li>
</ul>
</li>
<li>有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？<ul>
<li>TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 <strong>事务状态信息如何恢复？</strong> 也就是事务的状态信息需要<strong>很强的容错性、一致性</strong>；</li>
<li>关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），那就是 <code>min.isr + ack</code> 机制；</li>
<li>分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 <code>__consumer_offset</code> 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，而对应事务数据（transaction log）就是 <code>__transaction_state</code> 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator 在做故障恢复也是从这个 topic 中恢复数据；</li>
</ul>
</li>
<li>有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：<ul>
<li>Kafka 在 Producer 端引入了一个 <strong>TransactionalId</strong> 来解决这个问题，这个 txn.id 是由应用来配置的；</li>
<li>TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方（同一个 TransactionCoordinator）进行处理；</li>
</ul>
</li>
<li>再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：<ul>
<li>这个就是<strong>事务状态转移</strong>，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态（前提是这个状态转移是有效的）。</li>
</ul>
</li>
</ol>
<blockquote>
<p>上面的分析都是个人见解，有问题欢迎指正~</p>
</blockquote>
<p>下面这节就讲述一下事务性实现的一些关键的实现机制（对这些细节不太感兴趣或者之前没有深入接触过 Kafka，可以直接跳过，直接去看下一节的事务流程处理，先去了解一下一个事务操作的主要流程步骤）。</p>
<h3 id="TransactionCoordinator"><a href="#TransactionCoordinator" class="headerlink" title="TransactionCoordinator"></a>TransactionCoordinator</h3><p>TransactionCoordinator 与 GroupCoordinator 有一些相似之处，它主要是处理来自 Transactional Producer 的一些与事务相关的请求，涉及的请求如下表所示（关于这些请求处理的详细过程会在下篇文章详细讲述，这里先有个大概的认识即可）：</p>
<table>
<thead>
<tr>
<th>请求类型</th>
<th>用途说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>ApiKeys.FIND_COORDINATOR</td>
<td>Transaction Producer 会发送这个 FindCoordinatorRequest 请求，来查询当前事务（txn.id）对应的 TransactionCoordinator，这个与 GroupCoordinator 查询类似，是根据 txn.id 的 hash 值取模找到对应 Partition 的 leader，这个 leader 就是该事务对应的 TransactionCoordinator</td>
</tr>
<tr>
<td>ApiKeys.INIT_PRODUCER_ID</td>
<td>Producer 初始化时，会发送一个 InitProducerIdRequest 请求，来获取其分配的 PID 信息，对于幂等性的 Producer，会随机选择一台 broker 发送请求，而对于 Transaction Producer 会选择向其对应的 TransactionCoordinator 发送该请求（目的是为了根据 txn.id 对应的事务状态做一些判断）</td>
</tr>
<tr>
<td>ApiKeys.ADD_PARTITIONS_TO_TXN</td>
<td>将这个事务涉及到的 topic-partition 列表添加到事务的 meta 信息中（通过 AddPartitionsToTxnRequest 请求），事务 meta 信息需要知道当前的事务操作涉及到了哪些 Topic-Partition 的写入</td>
</tr>
<tr>
<td>ApiKeys.ADD_OFFSETS_TO_TXN</td>
<td>Transaction Producer 的这个 AddOffsetsToTxnRequest 请求是由 <code>sendOffsetsToTransaction()</code> 接口触发的，它主要是用在 consume-process-produce 的场景中，这时候 consumer 也是整个事务的一部分，只有这个事务 commit 时，offset 才会被真正 commit（主要还是用于 Failover）</td>
</tr>
<tr>
<td>ApiKeys.END_TXN</td>
<td>当提交事务时， Transaction Producer 会向 TransactionCoordinator 发送一个 EndTxnRequest 请求，来 commit 或者 abort 事务</td>
</tr>
</tbody>
</table>
<p>TransactionCoordinator 对象中还有两个关键的对象，分别是:</p>
<ol>
<li>TransactionStateManager：这个对象，从名字应该就能大概明白其作用是关于事务的状态管理，它会维护分配到这个 TransactionCoordinator 的所有事务的 meta 信息；</li>
<li>TransactionMarkerChannelManager：这个主要是用于向其他的 Broker 发送 Transaction Marker 数据，关于 Transaction Marker，第一次接触的人，可能会有一些困惑，什么是 Transaction Marker，Transaction Marker 是用来解决什么问题的呢？这里先留一个疑问，后面会来解密。</li>
</ol>
<p>总结一下，TransactionCoordinator 主要的功能有三个，分别是：</p>
<ol>
<li>处理事务相关的请求；</li>
<li>维护事务的状态信息；</li>
<li>向其他 Broker 发送 Transaction Marker 数据。</li>
</ol>
<h3 id="Transaction-Log（-transaction-state）"><a href="#Transaction-Log（-transaction-state）" class="headerlink" title="Transaction Log（__transaction_state）"></a>Transaction Log（__transaction_state）</h3><p>在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 topic <code>__transaction_state</code>，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 <code>__transaction_state</code> 的 partition 数取模得到，<code>__transaction_state</code> Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 <code>__transaction_state</code> Partition 2 的 leader 来处理。</p>
<p>对于 <code>__transaction_state</code> 这个 topic 默认是由 Server 端的 <code>transaction.state.log.replication.factor</code> 参数来配置，默认是3，如果当前 leader 故障，需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？</p>
<p>正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 <code>__transaction_state</code> 中读取之前事务的日志信息，来恢复其状态信息，前提是要求事务日志写入做相应的不丢配置。这也是 <code>__transaction_state</code> 一个重要作用之一，用于 TransactionCoordinator 的恢复，<code>__transaction_state</code>  与 <code>__consumer_offsets</code> 一样是 compact 类型的 topic，其 scheme 如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Key =&gt; Version TransactionalId</span><br><span class="line">    Version =&gt; 0 (int16)</span><br><span class="line">    TransactionalId =&gt; String</span><br><span class="line"></span><br><span class="line">Value =&gt; Version ProducerId ProducerEpoch TxnTimeoutDuration TxnStatus [TxnPartitions] TxnEntryLastUpdateTime TxnStartTime</span><br><span class="line">    Version =&gt; 0 (int16)</span><br><span class="line">    ProducerId =&gt; int64</span><br><span class="line">    ProducerEpoch =&gt; int16</span><br><span class="line">    TxnTimeoutDuration =&gt; int32</span><br><span class="line">    TxnStatus =&gt; int8</span><br><span class="line">    TxnPartitions =&gt; [Topic [Partition]]</span><br><span class="line">        Topic =&gt; String</span><br><span class="line">        Partition =&gt; int32</span><br><span class="line">    TxnLastUpdateTime =&gt; int64</span><br><span class="line">    TxnStartTime =&gt; int64</span><br></pre></td></tr></table></figure>
<h3 id="Transaction-Marker"><a href="#Transaction-Marker" class="headerlink" title="Transaction Marker"></a>Transaction Marker</h3><p>终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？</p>
<p>Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）</p>
<ol>
<li>Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；</li>
<li><p>在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？</p>
<ul>
<li>这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；</li>
</ul>
<p>再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？</p>
<ol>
<li>首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，是可以共享这个 TCP 连接的；</li>
<li>有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 信息做相应的处理。</li>
</ol>
</li>
</ol>
<p>Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ControlMessageKey =&gt; Version ControlMessageType</span><br><span class="line">    Version =&gt; int16</span><br><span class="line">    ControlMessageType =&gt; int16</span><br><span class="line"></span><br><span class="line">TransactionControlMessageValue =&gt; Version CoordinatorEpoch</span><br><span class="line">    Version =&gt; int16</span><br><span class="line">    CoordinatorEpoch =&gt; int32</span><br></pre></td></tr></table></figure>
<p>这里再讲一个额外的内容，对于事务写入的数据，为了给消息添加一个标识（标识这条消息是不是来自事务写入的），<strong>数据格式（消息协议）发生了变化</strong>，这个改动主要是在 Attribute 字段，对于 MessageSet，Attribute 是16位，新的格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">| Unused (6-15) | Control (5) | Transactional (4) | Timestamp Type (3) | Compression Type (0-2) |</span><br></pre></td></tr></table></figure>
<p>对于 Message，也就是单条数据存储时（其中 Marker 数据都是单条存储的），在 Kafka 中，只有 MessageSet 才可以做压缩，所以 Message 就没必要设置压缩字段，其格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">| Unused (1-7) | Control Flag(0) |</span><br></pre></td></tr></table></figure>
<h3 id="Server-端事务状态管理"><a href="#Server-端事务状态管理" class="headerlink" title="Server 端事务状态管理"></a>Server 端事务状态管理</h3><p>TransactionCoordinator 会维护相应的事务的状态信息（也就是 TxnStatus），对于一个事务，总共有以下几种状态：</p>
<table>
<thead>
<tr>
<th>状态</th>
<th>状态码</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Empty</td>
<td>0</td>
<td>Transaction has not existed yet</td>
</tr>
<tr>
<td>Ongoing</td>
<td>1</td>
<td>Transaction has started and ongoing</td>
</tr>
<tr>
<td>PrepareCommit</td>
<td>2</td>
<td>Group is preparing to commit</td>
</tr>
<tr>
<td>PrepareAbort</td>
<td>3</td>
<td>Group is preparing to abort</td>
</tr>
<tr>
<td>CompleteCommit</td>
<td>4</td>
<td>Group has completed commit</td>
</tr>
<tr>
<td>CompleteAbort</td>
<td>5</td>
<td>Group has completed abort</td>
</tr>
<tr>
<td>Dead</td>
<td>6</td>
<td>TransactionalId has expired and is about to be removed from the transaction cache</td>
</tr>
<tr>
<td>PrepareEpochFence</td>
<td>7</td>
<td>We are in the middle of bumping the epoch and fencing out older producers</td>
</tr>
</tbody>
</table>
<p>其相应有效的状态转移图如下：</p>
<p><img src="/images/kafka/server-txn.png" alt="Server 端 Transaction 的状态转移图"></p>
<p>正常情况下，对于一个事务而言，其状态状态流程应该是 Empty –&gt; Ongoing –&gt; PrepareCommit –&gt; CompleteCommit –&gt; Empty 或者是 Empty –&gt; Ongoing –&gt; PrepareAbort –&gt; CompleteAbort –&gt; Empty。</p>
<h3 id="Client-端事务状态管理"><a href="#Client-端事务状态管理" class="headerlink" title="Client 端事务状态管理"></a>Client 端事务状态管理</h3><p>Client 的事务状态信息主要记录本地事务的状态，当然跟其他的系统类似，本地的状态信息与 Server 端的状态信息并不完全一致（状态的设置，就像 GroupCoodinator 会维护一个 Group 的状态，每个 Consumer 也会维护本地的 Consumer 对象的状态一样）。Client 端的事务状态信息主要用于 Client 端的事务状态处理，其主要有以下几种：</p>
<ol>
<li>UNINITIALIZED：Transactional Producer 初始化时的状态，此时还没有事务处理；</li>
<li>INITIALIZING：Transactional Producer 调用 <code>initTransactions()</code> 方法初始化事务相关的内容，比如发送 InitProducerIdRequest 请求；</li>
<li>READY：对于新建的事务，Transactional Producer 收到来自 TransactionCoordinator 的 InitProducerIdResponse 后，其状态会置为 READY（对于已有的事务而言，是当前事务完成后 Client 的状态会转移为 READY）；</li>
<li>IN_TRANSACTION：Transactional Producer 调用 <code>beginTransaction()</code> 方法，开始一个事务，标志着一个事务开始初始化；</li>
<li>COMMITTING_TRANSACTION：Transactional Producer 调用 <code>commitTransaction()</code> 方法时，会先更新本地的状态信息；</li>
<li>ABORTING_TRANSACTION：Transactional Producer 调用 <code>abortTransaction()</code> 方法时，会先更新本地的状态信息；</li>
<li>ABORTABLE_ERROR：在一个事务操作中，如果有数据发送失败，本地状态会转移到这个状态，之后再自动 abort 事务；</li>
<li>FATAL_ERROR：转移到这个状态之后，再进行状态转移时，会抛出异常；</li>
</ol>
<p>Client 端状态如下图：</p>
<p><img src="/images/kafka/client-txn.png" alt="Client 端 Transaction 的状态转移图"></p>
<h2 id="事务性的整体流程"><a href="#事务性的整体流程" class="headerlink" title="事务性的整体流程"></a>事务性的整体流程</h2><p>有了前面对 Kafka 事务性关键实现的讲述之后，这里详细讲述一个事务操作的处理流程，当然这里只是重点讲述事务性相关的内容，官方版的流程图可参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-DataFlow" target="_blank" rel="external">Kafka Exactly-Once Data Flow</a>，这里我做了一些改动，其流程图如下：</p>
<p><img src="/images/kafka/txn-data-flow.png" alt="consume-process-produce 事务的处理流程"></p>
<p>这个流程是以 consume-process-produce 场景为例（主要是 kafka streams 的场景），图中红虚框及 4.3a 部分是关于 consumer 的操作，去掉这部分的话，就是只考虑写入情况的场景。这种只考虑写入场景的事务操作目前在业内应用也是非常广泛的，比如 Flink + Kafka 端到端的 Exactly-Once 实现就是这种场景，下面来详细讲述一下整个流程。</p>
<h3 id="1-Finding-a-TransactionCoordinator"><a href="#1-Finding-a-TransactionCoordinator" class="headerlink" title="1. Finding a TransactionCoordinator"></a>1. Finding a TransactionCoordinator</h3><p>对于事务性的处理，第一步首先需要做的就是找到这个事务 txn.id 对应的 TransactionCoordinator，Transaction Producer 会向 Broker （随机选择一台 broker，一般选择本地连接最少的这台 broker）发送 FindCoordinatorRequest 请求，获取其 TransactionCoordinator。</p>
<p>怎么找到对应的 TransactionCoordinator 呢？这个前面已经讲过了，主要是通过下面的方法获取 <code>__transaction_state</code> 的 Partition，该 Partition 对应的 leader 就是这个 txn.id 对应的 TransactionCoordinator。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionFor</span></span>(transactionalId: <span class="type">String</span>): <span class="type">Int</span> = <span class="type">Utils</span>.abs(transactionalId.hashCode) % transactionTopicPartitionCount</span><br></pre></td></tr></table></figure>
<h3 id="2-Getting-a-PID"><a href="#2-Getting-a-PID" class="headerlink" title="2. Getting a PID"></a>2. Getting a PID</h3><p>PID 这里就不再介绍了，不了解的可以看前面那篇文章（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#PID">Producer ID</a>）。</p>
<p>Transaction Producer 在 <code>initializeTransactions()</code> 方法中会向 TransactionCoordinator 发送 InitPidRequest 请求获取其分配的 PID，有了 PID，事务写入时可以保证幂等性，PID 如何分配可以参考 <a href="http://matt33.com/2018/10/24/kafka-idempotent/#Producer-PID-%E7%94%B3%E8%AF%B7">PID 分配</a>，但是 TransactionCoordinator 在给事务 Producer 分配 PID 会做一些判断，主要的内容是：</p>
<ol>
<li>如果这个 txn.id 之前没有相应的事务状态（new txn.id），那么会初始化其事务 meta 信息 TransactionMetadata（会给其分配一个 PID，初始的 epoch 为-1），如果有事务状态，获取之前的状态；</li>
<li>校验其 TransactionMetadata 的状态信息（参考下面代码中 <code>prepareInitProduceIdTransit()</code> 方法）：<ol>
<li>如果前面还有状态转移正在进行，直接返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果此时的状态为 PrepareAbort 或 PrepareCommit，返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果之前的状态为 CompleteAbort、CompleteCommit 或 Empty，那么先将状态转移为 Empty，然后更新一下 epoch 值；</li>
<li>如果之前的状态为 Ongoing，状态会转移成 PrepareEpochFence，然后再 abort 当前的事务，并向 client 返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果状态为 Dead 或 PrepareEpochFence，直接抛出相应的 FATAL 异常；</li>
</ol>
</li>
<li>将 txn.id 与相应的 TransactionMetadata 持久化到事务日志中，对于 new txn.id，这个持久化的数据主要时 txn.id 与 pid 关系信息，如图中的 3a 所示。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: producer 启用事务性的情况下，检测此时事务的状态信息</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareInitProduceIdTransit</span></span>(transactionalId: <span class="type">String</span>,</span><br><span class="line">                                        transactionTimeoutMs: <span class="type">Int</span>,</span><br><span class="line">                                        coordinatorEpoch: <span class="type">Int</span>,</span><br><span class="line">                                        txnMetadata: <span class="type">TransactionMetadata</span>): <span class="type">ApiResult</span>[(<span class="type">Int</span>, <span class="type">TxnTransitMetadata</span>)] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (txnMetadata.pendingTransitionInProgress) &#123;</span><br><span class="line">    <span class="comment">// return a retriable exception to let the client backoff and retry</span></span><br><span class="line">    <span class="type">Left</span>(<span class="type">Errors</span>.<span class="type">CONCURRENT_TRANSACTIONS</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// caller should have synchronized on txnMetadata already</span></span><br><span class="line">    txnMetadata.state <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">PrepareAbort</span> | <span class="type">PrepareCommit</span> =&gt;</span><br><span class="line">        <span class="comment">// reply to client and let it backoff and retry</span></span><br><span class="line">        <span class="type">Left</span>(<span class="type">Errors</span>.<span class="type">CONCURRENT_TRANSACTIONS</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">CompleteAbort</span> | <span class="type">CompleteCommit</span> | <span class="type">Empty</span> =&gt; <span class="comment">//note: 此时需要将状态转移到 Empty（此时状态并没有转移，只是在 PendingState 记录了将要转移的状态）</span></span><br><span class="line">        <span class="keyword">val</span> transitMetadata = <span class="keyword">if</span> (txnMetadata.isProducerEpochExhausted) &#123;</span><br><span class="line">          <span class="keyword">val</span> newProducerId = producerIdManager.generateProducerId()</span><br><span class="line">          txnMetadata.prepareProducerIdRotation(newProducerId, transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 增加 producer 的 epoch 值</span></span><br><span class="line">          txnMetadata.prepareIncrementProducerEpoch(transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">Right</span>(coordinatorEpoch, transitMetadata)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Ongoing</span> =&gt; <span class="comment">//note: abort 当前的事务，并返回一个 CONCURRENT_TRANSACTIONS 异常，强制 client 去重试</span></span><br><span class="line">        <span class="comment">// indicate to abort the current ongoing txn first. Note that this epoch is never returned to the</span></span><br><span class="line">        <span class="comment">// user. We will abort the ongoing transaction and return CONCURRENT_TRANSACTIONS to the client.</span></span><br><span class="line">        <span class="comment">// This forces the client to retry, which will ensure that the epoch is bumped a second time. In</span></span><br><span class="line">        <span class="comment">// particular, if fencing the current producer exhausts the available epochs for the current producerId,</span></span><br><span class="line">        <span class="comment">// then when the client retries, we will generate a new producerId.</span></span><br><span class="line">        <span class="type">Right</span>(coordinatorEpoch, txnMetadata.prepareFenceProducerEpoch())</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Dead</span> | <span class="type">PrepareEpochFence</span> =&gt; <span class="comment">//note: 返回错误</span></span><br><span class="line">        <span class="keyword">val</span> errorMsg = <span class="string">s"Found transactionalId <span class="subst">$transactionalId</span> with state <span class="subst">$&#123;txnMetadata.state&#125;</span>. "</span> +</span><br><span class="line">          <span class="string">s"This is illegal as we should never have transitioned to this state."</span></span><br><span class="line">        fatal(errorMsg)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(errorMsg)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-Starting-a-Transaction"><a href="#3-Starting-a-Transaction" class="headerlink" title="3. Starting a Transaction"></a>3. Starting a Transaction</h3><p>前面两步都是 Transaction Producer 调用 <code>initTransactions()</code> 部分，到这里，Producer 可以调用 <code>beginTransaction()</code> 开始一个事务操作，其实现方法如下面所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 应该在一个事务操作之前进行调用</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    transactionManager.beginTransaction();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TransactionManager</span></span><br><span class="line"><span class="comment">//note: 在一个事务开始之前进行调用，这里实际上只是转换了状态（只在 producer 本地记录了状态的开始）</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.IN_TRANSACTION);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里只是将本地事务状态转移成 IN_TRANSACTION，并没有与 Server 端进行交互，所以在流程图中没有体现出来（TransactionManager 初始化时，其状态为 UNINITIALIZED，Producer 调用 <code>initializeTransactions()</code> 方法，其状态转移成 INITIALIZING）。</p>
<h3 id="4-Consume-Porcess-Produce-Loop"><a href="#4-Consume-Porcess-Produce-Loop" class="headerlink" title="4. Consume-Porcess-Produce Loop"></a>4. Consume-Porcess-Produce Loop</h3><p>在这个阶段，Transaction Producer 会做相应的处理，主要包括：从 consumer 拉取数据、对数据做相应的处理、通过 Producer 写入到下游系统中（对于只有写入场景，忽略前面那一步即可），下面有一个示例（start 和 end 中间的部分），是一个典型的 consume-process-produce 场景：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords records = consumer.poll(Long.MAX_VALUE);</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    <span class="comment">//start</span></span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord record : records)&#123;</span><br><span class="line">        producer.send(producerRecord(“outputTopic1”, record));</span><br><span class="line">        producer.send(producerRecord(“outputTopic2”, record));</span><br><span class="line">    &#125;</span><br><span class="line">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</span><br><span class="line">    <span class="comment">//end</span></span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面来结合前面的流程图来讲述一下这部分的实现。</p>
<h4 id="4-1-AddPartitionsToTxnRequest"><a href="#4-1-AddPartitionsToTxnRequest" class="headerlink" title="4.1. AddPartitionsToTxnRequest"></a>4.1. AddPartitionsToTxnRequest</h4><p>Producer 在调用 <code>send()</code> 方法时，Producer 会将这个对应的 Topic—Partition 添加到 TransactionManager 的记录中，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如何开启了幂等性或事务性，需要做一些处理</span></span><br><span class="line"><span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">    transactionManager.maybeAddPartitionToTransaction(tp);</span><br></pre></td></tr></table></figure>
<p>如果这个 Topic-Partition 之前不存在，那么就添加到 newPartitionsInTransaction 集合中，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 tp 添加到 newPartitionsInTransaction 中，记录当前进行事务操作的 tp</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">maybeAddPartitionToTransaction</span><span class="params">(TopicPartition topicPartition)</span> </span>&#123;</span><br><span class="line">    failIfNotReadyForSend();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 如果 partition 已经添加到 partitionsInTransaction、pendingPartitionsInTransaction、newPartitionsInTransaction中</span></span><br><span class="line">    <span class="keyword">if</span> (isPartitionAdded(topicPartition) || isPartitionPendingAdd(topicPartition))</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Begin adding new partition &#123;&#125; to transaction"</span>, topicPartition);</span><br><span class="line">    newPartitionsInTransaction.add(topicPartition);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Producer 端的 Sender 线程会将这个信息通过 AddPartitionsToTxnRequest 请求发送给 TransactionCoordinator，也就是图中的 4.1 过程，TransactionCoordinator 会将这个 Topic-Partition 列表更新到 txn.id 对应的 TransactionMetadata 中，并且会持久化到事务日志中，也就是图中的 4.1 a 部分，这里持久化的数据主要是 txn.id 与其涉及到的 Topic-Partition 信息。</p>
<h4 id="4-2-ProduceRequest"><a href="#4-2-ProduceRequest" class="headerlink" title="4.2. ProduceRequest"></a>4.2. ProduceRequest</h4><p>这一步与正常 Producer 写入基本上一样，就是相应的 Leader 在持久化数据时会在头信息中标识这条数据是不是来自事务 Producer 的写入（主要是数据协议有变动，Server 处理并不需要做额外的处理）。</p>
<h4 id="4-3-AddOffsetsToTxnRequest"><a href="#4-3-AddOffsetsToTxnRequest" class="headerlink" title="4.3. AddOffsetsToTxnRequest"></a>4.3. AddOffsetsToTxnRequest</h4><p>Producer 在调用 <code>sendOffsetsToTransaction()</code> 方法时，第一步会首先向 TransactionCoordinator 发送相应的 AddOffsetsToTxnRequest 请求，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProcducer</span></span><br><span class="line"><span class="comment">//note: 当你需要 batch 的消费-处理-写入消息，这个方法需要被使用</span></span><br><span class="line"><span class="comment">//note: 发送指定的 offset 给 group coordinator，用来标记这些 offset 是作为当前事务的一部分，只有这次事务成功时</span></span><br><span class="line"><span class="comment">//note: 这些 offset 才会被认为 commit 了</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     String consumerGroupId)</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 发送 AddOffsetsToTxRequest</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                                        String consumerGroupId)</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.IN_TRANSACTION)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Cannot send offsets to transaction either because the producer is not in an "</span> +</span><br><span class="line">                <span class="string">"active transaction"</span>);</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Begin adding offsets &#123;&#125; for consumer group &#123;&#125; to transaction"</span>, offsets, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnRequest.Builder builder = <span class="keyword">new</span> AddOffsetsToTxnRequest.Builder(transactionalId,</span><br><span class="line">            producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnHandler handler = <span class="keyword">new</span> AddOffsetsToTxnHandler(builder, offsets);</span><br><span class="line">    enqueueRequest(handler);</span><br><span class="line">    <span class="keyword">return</span> handler.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TransactionCoordinator 在收到这个请求时，处理方法与 4.1 中的一样，把这个 group.id 对应的 <code>__consumer_offsets</code> 的 Partition （与写入涉及的 Topic-Partition 一样）保存到事务对应的 meta 中，之后会持久化相应的事务日志，如图中 4.3a 所示。</p>
<h4 id="4-4-TxnOffsetsCommitRequest"><a href="#4-4-TxnOffsetsCommitRequest" class="headerlink" title="4.4. TxnOffsetsCommitRequest"></a>4.4. TxnOffsetsCommitRequest</h4><p>Producer 在收到 TransactionCoordinator 关于 AddOffsetsToTxnRequest 请求的结果后，后再次发送 TxnOffsetsCommitRequest 请求给对应的 GroupCoordinator，AddOffsetsToTxnHandler 的 <code>handleResponse()</code> 的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleResponse</span><span class="params">(AbstractResponse response)</span> </span>&#123;</span><br><span class="line">    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;</span><br><span class="line">    Errors error = addOffsetsToTxnResponse.error();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (error == Errors.NONE) &#123;</span><br><span class="line">        log.debug(<span class="string">"Successfully added partition for consumer group &#123;&#125; to transaction"</span>, builder.consumerGroupId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// note the result is not completed until the TxnOffsetCommit returns</span></span><br><span class="line">        <span class="comment">//note: AddOffsetsToTnxRequest 之后，还会再发送 TxnOffsetCommitRequest</span></span><br><span class="line">        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));</span><br><span class="line">        transactionStarted = <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123;</span><br><span class="line">        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.CONCURRENT_TRANSACTIONS) &#123;</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.INVALID_PRODUCER_EPOCH) &#123;</span><br><span class="line">        fatalError(error.exception());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">        fatalError(error.exception());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</span><br><span class="line">        abortableError(<span class="keyword">new</span> GroupAuthorizationException(builder.consumerGroupId()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fatalError(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error in AddOffsetsToTxnResponse: "</span> + error.message()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>GroupCoordinator 在收到相应的请求后，会将 offset 信息持久化到 consumer offsets log 中（包含对应的 PID 信息），但是<strong>不会更新到缓存</strong>中，除非这个事务 commit 了，这样的话就可以保证这个 offset 信息对 consumer 是不可见的（没有更新到缓存中的数据是不可见的，通过接口是获取的，这是 GroupCoordinator 本身来保证的）。</p>
<h3 id="5-Committing-or-Aborting-a-Transaction"><a href="#5-Committing-or-Aborting-a-Transaction" class="headerlink" title="5.Committing or Aborting a Transaction"></a>5.Committing or Aborting a Transaction</h3><p>在一个事务操作处理完成之后，Producer 需要调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p>
<h4 id="5-1-EndTxnRequest"><a href="#5-1-EndTxnRequest" class="headerlink" title="5.1. EndTxnRequest"></a>5.1. EndTxnRequest</h4><p>无论是 Commit 还是 Abort，对于 Producer 而言，都是向 TransactionCoordinator 发送 EndTxnRequest 请求，这个请求的内容里会标识是 commit 操作还是 abort 操作，Producer 的 <code>commitTransaction()</code> 方法实现如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: commit 正在进行的事务操作，这个方法在真正发送 commit 之后将会 flush 所有未发送的数据</span></span><br><span class="line"><span class="comment">//note: 如果在发送中遇到任何一个不能修复的错误，这个方法抛出异常，事务也不会被提交，所有 send 必须成功，这个事务才能 commit 成功</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginCommit();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 开始 commit，转移本地本地保存的状态以及发送相应的请求</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">beginCommit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.COMMITTING_TRANSACTION);</span><br><span class="line">    <span class="keyword">return</span> beginCompletingTransaction(TransactionResult.COMMIT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Producer 的 <code>abortTransaction()</code> 方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 取消正在进行事务，任何没有 flush 的数据都会被丢弃</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginAbort();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">beginAbort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.ABORTABLE_ERROR)</span><br><span class="line">        maybeFailWithError();</span><br><span class="line">    transitionTo(State.ABORTING_TRANSACTION);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We're aborting the transaction, so there should be no need to add new partitions</span></span><br><span class="line">    newPartitionsInTransaction.clear();</span><br><span class="line">    <span class="keyword">return</span> beginCompletingTransaction(TransactionResult.ABORT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>它们最终都是调用了 TransactionManager 的 <code>beginCompletingTransaction()</code> 方法，这个方法会向其 待发送请求列表 中添加 EndTxnRequest 请求，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 发送 EndTxnRequest 请求，添加到 pending 队列中</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> TransactionalRequestResult <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult transactionResult)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!newPartitionsInTransaction.isEmpty())</span><br><span class="line">        enqueueRequest(addPartitionsToTransactionHandler());</span><br><span class="line">    EndTxnRequest.Builder builder = <span class="keyword">new</span> EndTxnRequest.Builder(transactionalId, producerIdAndEpoch.producerId,</span><br><span class="line">            producerIdAndEpoch.epoch, transactionResult);</span><br><span class="line">    EndTxnHandler handler = <span class="keyword">new</span> EndTxnHandler(builder);</span><br><span class="line">    enqueueRequest(handler);</span><br><span class="line">    <span class="keyword">return</span> handler.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TransactionCoordinator 在收到 EndTxnRequest 请求后，会做以下处理：</p>
<ol>
<li>更新事务的 meta 信息，状态转移成 PREPARE_COMMIT 或 PREPARE_ABORT，并将事务状态信息持久化到事务日志中；</li>
<li>根据事务 meta 信息，向其涉及到的所有 Topic-Partition 的 leader 发送 Transaction Marker 信息（也就是 WriteTxnMarkerRquest 请求，见下面的 5.2 分析）；</li>
<li>最后将事务状态更新为 COMMIT 或者 ABORT，并将事务的 meta 持久化到事务日志中，也就是 5.3 步骤。</li>
</ol>
<h4 id="5-2-WriteTxnMarkerRquest"><a href="#5-2-WriteTxnMarkerRquest" class="headerlink" title="5.2. WriteTxnMarkerRquest"></a>5.2. WriteTxnMarkerRquest</h4><p>WriteTxnMarkerRquest 是 TransactionCoordinator 收到 Producer 的 EndTxnRequest 请求后向其他 Broker 发送的请求，主要是告诉它们事务已经完成。不论是普通的 Topic-Partition 还是 <code>__consumer_offsets</code>，在收到这个请求后，都会把事务结果（Transaction Marker 的格数据式见前面）持久化到对应的日志文件中，这样下游 Consumer 在消费这个数据时，就知道这个事务是 commit 还是 abort。</p>
<h4 id="5-3-Writing-the-Final-Commit-or-Abort-Message"><a href="#5-3-Writing-the-Final-Commit-or-Abort-Message" class="headerlink" title="5.3. Writing the Final Commit or Abort Message"></a>5.3. Writing the Final Commit or Abort Message</h4><p>当这个事务涉及到所有 Topic-Partition 都已经把这个 marker 信息持久化到日志文件之后，TransactionCoordinator 会将这个事务的状态置为 COMMIT 或 ABORT，并持久化到事务日志文件中，到这里，这个事务操作就算真正完成了，TransactionCoordinator 缓存的很多关于这个事务的数据可以被清除了。</p>
<h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>在上面讲述完 Kafka 事务性处理之后，我们来思考一下以下这些问题，上面的流程可能会出现下面这些问题或者很多人可能会有下面的疑问：</p>
<ol>
<li>txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？</li>
<li>TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？</li>
<li>对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？</li>
<li>对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？</li>
<li>如果 txn.id 长期不使用，server 端怎么处理？</li>
<li>PID Snapshot 是做什么的？是用来解决什么问题？</li>
</ol>
<p>下面，来详细分析一下上面提到的这些问题。</p>
<h3 id="如果多个-Producer-使用同一个-txn-id-会出现什么情况？"><a href="#如果多个-Producer-使用同一个-txn-id-会出现什么情况？" class="headerlink" title="如果多个 Producer 使用同一个 txn.id 会出现什么情况？"></a>如果多个 Producer 使用同一个 txn.id 会出现什么情况？</h3><p>对于这个情况，我们这里直接做了一个相应的实验，两个 Producer 示例都使用了同一个 txn.id（为 test-transactional-matt），Producer 1 先启动，然后过一会再启动 Producer 2，这时候会发现一个现象，那就是 Producer 1 进程会抛出异常退出进程，其异常信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are <span class="keyword">in</span> an error state</span><br><span class="line">	at org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:784)</span><br><span class="line">	at org.apache.kafka.clients.producer.internals.TransactionManager.beginTransaction(TransactionManager.java:215)</span><br><span class="line">	at org.apache.kafka.clients.producer.KafkaProducer.beginTransaction(KafkaProducer.java:606)</span><br><span class="line">	at com.matt.test.kafka.producer.ProducerTransactionExample.main(ProducerTransactionExample.java:68)</span><br><span class="line">Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></span><br></pre></td></tr></table></figure>
<p>这里抛出了 ProducerFencedException 异常，如果打开相应的 Debug 日志，在 Producer 1 的日志文件会看到下面的日志信息</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[2018-11-03 12:48:52,495] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Transition from state COMMITTING_TRANSACTION to error state FATAL_ERROR (org.apache.kafka.clients.producer.internals.TransactionManager)</span><br><span class="line">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></span><br><span class="line"><span class="string">[2018-11-03 12:48:52,498] ERROR [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)</span></span><br><span class="line"><span class="string">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer'</span>s transaction has been expired by the broker.</span><br><span class="line">[2018-11-03 12:48:52,599] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[2018-11-03 12:48:52,599] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Beginning shutdown of Kafka producer I/O thread, sending remaining records. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[2018-11-03 12:48:52,601] DEBUG Removed sensor with name connections-closed: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,601] DEBUG Removed sensor with name connections-created: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name successful-authentication: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name failed-authentication: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name bytes-sent-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,603] DEBUG Removed sensor with name bytes-sent: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,603] DEBUG Removed sensor with name bytes-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name select-time: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name io-time: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name node--1.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node--1.bytes-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node--1.latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node-33.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-33.bytes-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-33.latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.bytes-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[2018-11-03 12:48:52,607] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Shutdown of Kafka producer I/O thread has completed. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[2018-11-03 12:48:52,607] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[2018-11-03 12:48:52,808] ERROR Forcing producer close! (com.matt.test.kafka.producer.ProducerTransactionExample)</span><br><span class="line">[2018-11-03 12:48:52,808] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[2018-11-03 12:48:52,808] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br></pre></td></tr></table></figure>
<p>Producer 1 本地事务状态从 COMMITTING_TRANSACTION 变成了 FATAL_ERROR 状态，导致 Producer 进程直接退出了，出现这个异常的原因，就是抛出的 ProducerFencedException 异常，简单来说 Producer 1 被 Fencing 了（这是 Producer Fencing 的情况）。因此，这个问题的答案就很清除了，如果多个 Producer 共用一个 txn.id，那么最后启动的 Producer 会成功运行，会它之前启动的 Producer 都 Fencing 掉（至于为什么会 Fencing 下一小节会做分析）。</p>
<h3 id="Fencing"><a href="#Fencing" class="headerlink" title="Fencing"></a>Fencing</h3><p>关于 Fencing 这个机制，在分布式系统还是很常见的，我第一个见到这个机制是在 HDFS 中，可以参考我之前总结的一篇文章 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98">HDFS NN 脑裂问题</a>，Fencing 机制解决的主要也是这种类型的问题 —— 脑裂问题，简单来说就是，本来系统这个组件在某个时刻应该只有一个处于 active 状态的，但是在实际生产环境中，特别是切换期间，可能会同时出现两个组件处于 active 状态，这就是脑裂问题，在 Kafka 的事务场景下，用到 Fencing 机制有两个地方：</p>
<ol>
<li>TransactionCoordinator Fencing；</li>
<li>Producer Fencing；</li>
</ol>
<h4 id="TransactionCoordinator-Fencing"><a href="#TransactionCoordinator-Fencing" class="headerlink" title="TransactionCoordinator Fencing"></a>TransactionCoordinator Fencing</h4><p>TransactionCoordinator 在遇到上 long FGC 时，可能会导致 脑裂 问题，FGC 时会 stop-the-world，这时候可能会与 zk 连接超时导致临时节点消失进而触发 leader 选举，如果 <code>__transaction_state</code> 发生了 leader 选举，TransactionCoordinator 就会切换，如果此时旧的 TransactionCoordinator FGC 完成，在还没来得及同步到最细 meta 之前，会有一个短暂的时刻，对于一个 txn.id 而言就是这个时刻可能出现了两个 TransactionCoordinator。</p>
<p>相应的解决方案就是 TransactionCoordinator Fencing，这里 Fencing 策略不像离线场景 HDFS 这种直接 Kill 旧的 NN 进程或者强制切换状态这么暴力，而是通过 CoordinatorEpoch 来判断，每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 <code>__transaction_state</code> Partition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。</p>
<p>明白了 TransactionCoordinator 脑裂问题发生情况及解决方案之后，来分析下，Fencing 机制会在哪里发挥作用？仔细想想，是可以推断出来的，只可能是 TransactionCoordinator 向别人发请求时影响才会比较严重（特别是乱发 admin 命令）。有了 CoordinatorEpoch 之后，其他 Server 在收到请求时做相应的判断，如果发现 CoordinatorEpoch 值比缓存的最新的值小，那么 Fencing 就生效，拒绝这个请求，也就是 TransactionCoordinator 发送 WriteTxnMarkerRequest 时可能会触发这一机制。</p>
<h4 id="Producer-Fencing"><a href="#Producer-Fencing" class="headerlink" title="Producer Fencing"></a>Producer Fencing</h4><p>Producer Fencing 与前面的类似，如果对于相同 PID 和 txn.id 的 Producer，Server 端会记录最新的 Epoch 值，拒绝来自 zombie Producer （Epoch 值小的 Producer）的请求。前面第一个问题的情况，Producer 2 在启动时，会向 TransactionCoordinator 发送 InitPIDRequest 请求，此时 TransactionCoordinator 已经有了这个 txn.id 对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样 Producer 2 就被认为是最新的 Producer，而 Producer 1 就会被认为是 zombie Producer，因此，TransactionCoordinator 在处理 Producer 1 的事务请求时，会返回相应的异常信息。</p>
<h3 id="Consumer-端如何消费事务数据"><a href="#Consumer-端如何消费事务数据" class="headerlink" title="Consumer 端如何消费事务数据"></a>Consumer 端如何消费事务数据</h3><p>在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 <code>isolation.level</code> 配置，这个是配置对于事务性数据的消费策略，有以下两种可选配置：</p>
<ol>
<li><code>read_committed</code>: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.</li>
<li><code>read_uncommitted</code>: consume all available messages in offset ordering. This is the <strong>default value</strong>.</li>
</ol>
<p>简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），这部分的难点在于 read_committed 机制的实现。</p>
<h4 id="Last-Stable-Offset（LSO）"><a href="#Last-Stable-Offset（LSO）" class="headerlink" title="Last Stable Offset（LSO）"></a>Last Stable Offset（LSO）</h4><p>在事务机制的实现中，Kafka 又设置了一个新的 offset 概念，那就是 Last Stable Offset，简称 LSO（其他的 Offset 概念可参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Kafka Offset 那些事</a>），先看下 LSO 的定义：</p>
<blockquote>
<p>The LSO is defined as the latest offset such that the status of all transactional messages at lower offsets have been determined (i.e. committed or aborted).</p>
</blockquote>
<p>对于一个 Partition 而言，offset 小于 LSO 的数据，全都是已经确定的数据，这个主要是对于事务操作而言，在这个 offset 之前的事务操作都是已经完成的事务（已经 commit 或 abort），如果这个 Partition 没有涉及到事务数据，那么 LSO 就是其 HW（水位）。</p>
<h4 id="Server-处理-read-committed-类型的-Fetch-请求"><a href="#Server-处理-read-committed-类型的-Fetch-请求" class="headerlink" title="Server 处理 read_committed 类型的 Fetch 请求"></a>Server 处理 read_committed 类型的 Fetch 请求</h4><p>如果 Consumer 的消费策略设置的是 read_committed，其在向 Server 发送 Fetch 请求时，Server 端<strong>只会返回 LSO 之前的数据</strong>，在 LSO 之后的数据不会返回。</p>
<p>这种机制有没有什么问题呢？我现在能想到的就是如果有一个 long transaction，比如其 first offset 是 1000，另外有几个已经完成的小事务操作，比如：txn1（offset：1100~1200）、txn2（offset：1400~1500），假设此时的 LSO 是 1000，也就是说这个 long transaction 还没有完成，那么已经完成的 txn1、txn2 也会对 consumer 不可见（假设都是 commit 操作），此时<strong>受 long transaction 的影响可能会导致数据有延迟</strong>。</p>
<p>那么我们再来想一下，如果不设计 LSO，又会有什么问题呢？可能分两种情况：</p>
<ol>
<li>允许读未完成的事务：那么 Consumer 可以直接读取到 Partition 的 HW 位置，对于未完成的事务，因为设置的是 read_committed 机制，所以不能对用户可见，需要在 Consumer 端做缓存，这个缓存应该设置多大？（不限制肯定会出现 OOM 的情况，当然也可以现在 client 端持久化到硬盘，这样的设计太过于复杂，还需要考虑 client 端 IO、磁盘故障等风险），明显这种设计方案是不可行的；</li>
<li>如果不允许读未完成的事务：相当于还是在 Server 端处理，与前面的区别是，这里需要先把示例中的 txn1、txn2 的数据发送给 Consumer，这样的设计会带来什么问题呢？<ol>
<li>假设这个 long transaction commit 了，其 end offset 是 2000，这时候有两种方案：第一种是把 1000-2000 的数据全部读出来（可能是磁盘读），把这个 long transaction 的数据过滤出来返回给 Consumer；第二种是随机读，只读这个 long transaction 的数据，无论哪种都有多触发一次磁盘读的风险，可能影响影响 Server 端的性能；</li>
<li>Server 端需要维护每个 consumer group 有哪些事务读了、哪些事务没读的 meta 信息，因为 consumer 是随机可能挂掉，需要接上次消费的，这样实现就复杂很多了；</li>
<li>还有一个问题是，消费的顺序性无法保证，两次消费其读取到的数据顺序可能是不同的（两次消费启动时间不一样）；</li>
</ol>
</li>
</ol>
<p>从这些分析来看，个人认为 LSO 机制还是一种相当来说 实现起来比较简单、而且不影响原来 server 端性能、还能保证顺序性的一种设计方案，它不一定是最好的，但也不会差太多。在实际的生产场景中，尽量避免 long transaction 这种操作，而且 long transaction可能也会容易触发事务超时。</p>
<h4 id="Consumer-如何过滤-abort-的事务数据"><a href="#Consumer-如何过滤-abort-的事务数据" class="headerlink" title="Consumer 如何过滤 abort 的事务数据"></a>Consumer 如何过滤 abort 的事务数据</h4><p>Consumer 在拉取到相应的数据之后，后面该怎么处理呢？它拉取到的这批数据并不能保证都是完整的事务数据，很有可能是拉取到一个事务的部分数据（marker 数据还没有拉取到），这时候应该怎么办？难道 Consumer 先把这部分数据缓存下来，等后面的 marker 数据到来时再确认数据应该不应该丢弃？（还是又 OOM 的风险）有没有更好的实现方案？</p>
<p>Kafka 的设计总是不会让我们失望，这部分做的优化也是非常高明，Broker 会追踪每个 Partition 涉及到的 abort transactions，Partition 的每个 log segment 都会有一个单独只写的文件（append-only file）来存储 abort transaction 信息，因为 abort transaction 并不是很多，所以这个开销是可以可以接受的，之所以要持久化到磁盘，主要是为了故障后快速恢复，要不然 Broker 需要把这个 Partition 的所有数据都读一遍，才能直到哪些事务是 abort 的，这样的话，开销太大（如果这个 Partition 没有事务操作，就不会生成这个文件）。这个持久化的文件是以 <code>.txnindex</code> 做后缀，前面依然是这个 log segment 的 offset 信息，存储的数据格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TransactionEntry =&gt;</span><br><span class="line">    Version =&gt; int16</span><br><span class="line">    PID =&gt; int64</span><br><span class="line">    FirstOffset =&gt; int64</span><br><span class="line">    LastOffset =&gt; int64</span><br><span class="line">    LastStableOffset =&gt; int64</span><br></pre></td></tr></table></figure>
<p>有了这个设计，Consumer 在拉取数据时，Broker 会把这批数据涉及到的所有 abort transaction 信息都返回给 Consumer，Server 端会根据拉取的 offset 范围与 abort transaction 的 offset 做对比，返回涉及到的 abort transaction 集合，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAbortedTxns</span></span>(fetchOffset: <span class="type">Long</span>, upperBoundOffset: <span class="type">Long</span>): <span class="type">TxnIndexSearchResult</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> abortedTransactions = <span class="type">ListBuffer</span>.empty[<span class="type">AbortedTxn</span>]</span><br><span class="line">  <span class="keyword">for</span> ((abortedTxn, _) &lt;- iterator()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastOffset &gt;= fetchOffset &amp;&amp; abortedTxn.firstOffset &lt; upperBoundOffset)</span><br><span class="line">      abortedTransactions += abortedTxn <span class="comment">//note: 这个 abort 的事务有在在这个范围内，就返回</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastStableOffset &gt;= upperBoundOffset)</span><br><span class="line">      <span class="keyword">return</span> <span class="type">TxnIndexSearchResult</span>(abortedTransactions.toList, isComplete = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">TxnIndexSearchResult</span>(abortedTransactions.toList, isComplete = <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Consumer 在拿到这些数据之后，会进行相应的过滤，大概的判断逻辑如下（Server 端返回的 abort transaction 列表就保存在 <code>abortedTransactions</code>  集合中，<code>abortedProducerIds</code>  最开始时是为空的）：</p>
<ol>
<li>如果这个数据是 control msg（也即是 marker 数据），是 ABORT 的话，那么与这个事务相关的 PID 信息从 <code>abortedProducerIds</code> 集合删掉，是 COMMIT 的话，就忽略（每个这个 PID 对应的 marker 数据收到之后，就从 <code>abortedProducerIds</code> 中清除这个 PID 信息）；</li>
<li>如果这个数据是正常的数据，把它的 PID 和 offset 信息与 <code>abortedTransactions</code> 队列（有序队列，头部 transaction 的 first offset 最小）第一个 transaction 做比较，如果 PID 相同，并且 offset 大于等于这个 transaction 的 first offset，就将这个 PID 信息添加到 <code>abortedProducerIds</code> 集合中，同时从 <code>abortedTransactions</code> 队列中删除这个 transaction，最后再丢掉这个 batch（它是 abort transaction 的数据）；</li>
<li>检查这个 batch 的 PID 是否在 <code>abortedProducerIds</code> 集合中，在的话，就丢弃，不在的话就返回上层应用。</li>
</ol>
<p>这部分的实现确实有些绕（有兴趣的可以慢慢咀嚼一下），它严重依赖了 Kafka 提供的下面两种保证：</p>
<ol>
<li>Consumer 拉取到的数据，在处理时，其 offset 是严格有序的；</li>
<li>同一个 txn.id（PID 相同）在某一个时刻最多只能有一个事务正在进行；</li>
</ol>
<p>这部分代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Record <span class="title">nextFetchedRecord</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (records == <span class="keyword">null</span> || !records.hasNext()) &#123; <span class="comment">//note: records 为空（数据全部丢掉了），records 没有数据（是 control msg）</span></span><br><span class="line">            maybeCloseRecordStream();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!batches.hasNext()) &#123;</span><br><span class="line">                <span class="comment">// Message format v2 preserves the last offset in a batch even if the last record is removed</span></span><br><span class="line">                <span class="comment">// through compaction. By using the next offset computed from the last offset in the batch,</span></span><br><span class="line">                <span class="comment">// we ensure that the offset of the next fetch will point to the next batch, which avoids</span></span><br><span class="line">                <span class="comment">// unnecessary re-fetching of the same batch (in the worst case, the consumer could get stuck</span></span><br><span class="line">                <span class="comment">// fetching the same batch repeatedly).</span></span><br><span class="line">                <span class="keyword">if</span> (currentBatch != <span class="keyword">null</span>)</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                drain();</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            currentBatch = batches.next();</span><br><span class="line">            maybeEnsureValid(currentBatch);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isolationLevel == IsolationLevel.READ_COMMITTED &amp;&amp; currentBatch.hasProducerId()) &#123;</span><br><span class="line">                <span class="comment">//note: 需要做相应的判断</span></span><br><span class="line">                <span class="comment">// remove from the aborted transaction queue all aborted transactions which have begun</span></span><br><span class="line">                <span class="comment">// before the current batch's last offset and add the associated producerIds to the</span></span><br><span class="line">                <span class="comment">// aborted producer set</span></span><br><span class="line">                <span class="comment">//note: 如果这个 batch 的 offset 已经大于等于 abortedTransactions 中第一事务的 first offset</span></span><br><span class="line">                <span class="comment">//note: 那就证明下个 abort transaction 的数据已经开始到来，将 PID 添加到 abortedProducerIds 中</span></span><br><span class="line">                consumeAbortedTransactionsUpTo(currentBatch.lastOffset());</span><br><span class="line"></span><br><span class="line">                <span class="keyword">long</span> producerId = currentBatch.producerId();</span><br><span class="line">                <span class="keyword">if</span> (containsAbortMarker(currentBatch)) &#123;</span><br><span class="line">                    abortedProducerIds.remove(producerId); <span class="comment">//note: 这个 PID（当前事务）涉及到的数据已经处理完</span></span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isBatchAborted(currentBatch)) &#123; <span class="comment">//note: 丢弃这个数据</span></span><br><span class="line">                    log.debug(<span class="string">"Skipping aborted record batch from partition &#123;&#125; with producerId &#123;&#125; and "</span> +</span><br><span class="line">                                  <span class="string">"offsets &#123;&#125; to &#123;&#125;"</span>,</span><br><span class="line">                              partition, producerId, currentBatch.baseOffset(), currentBatch.lastOffset());</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            records = currentBatch.streamingIterator(decompressionBufferSupplier);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            Record record = records.next();</span><br><span class="line">            <span class="comment">// skip any records out of range</span></span><br><span class="line">            <span class="keyword">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</span><br><span class="line">                <span class="comment">// we only do validation when the message should not be skipped.</span></span><br><span class="line">                maybeEnsureValid(record);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// control records are not returned to the user</span></span><br><span class="line">                <span class="keyword">if</span> (!currentBatch.isControlBatch()) &#123; <span class="comment">//note: 过滤掉 marker 数据</span></span><br><span class="line">                    <span class="keyword">return</span> record;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// Increment the next fetch offset when we skip a control batch.</span></span><br><span class="line">                    nextFetchOffset = record.offset() + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Consumer-消费数据时，其顺序如何保证"><a href="#Consumer-消费数据时，其顺序如何保证" class="headerlink" title="Consumer 消费数据时，其顺序如何保证"></a>Consumer 消费数据时，其顺序如何保证</h3><p>有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。</p>
<h3 id="如果-txn-id-长期不使用，server-端怎么处理？"><a href="#如果-txn-id-长期不使用，server-端怎么处理？" class="headerlink" title="如果 txn.id 长期不使用，server 端怎么处理？"></a>如果 txn.id 长期不使用，server 端怎么处理？</h3><p>Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 <code>transaction.timeout.ms</code>，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间（参数是 <code>transaction.timeout.ms</code>，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。</p>
<p>上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，Server 端提供了一个 <code>transaction.id.expiration.ms</code> 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，那么 TransactionCoordinator 将会使这个 txn.id 过期。</p>
<h3 id="PID-Snapshot-是做什么的？用来解决什么问题？"><a href="#PID-Snapshot-是做什么的？用来解决什么问题？" class="headerlink" title="PID Snapshot 是做什么的？用来解决什么问题？"></a>PID Snapshot 是做什么的？用来解决什么问题？</h3><p>对于每个 Topic-Partition，Broker 都会在内存中维护其 PID 与 sequence number（最后成功写入的 msg 的 sequence number）的对应关系（这个在上面幂等性文章应讲述过，主要是为了不丢补充的实现）。</p>
<p>Broker 重启时，如果想恢复上面的状态信息，那么它读取所有的 log 文件。相比于之下，定期对这个 state 信息做 checkpoint（Snapshot），明显收益是非常大的，此时如果 Broker 重启，只需要读取最近一个 Snapshot 文件，之后的数据再从 log 文件中恢复即可。</p>
<p>这个 PID Snapshot 样式如 00000000000235947656.snapshot，以 <code>.snapshot</code> 作为后缀，其数据格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX-35 app.matt_test_transaction_json_3-2]$ /usr/<span class="built_in">local</span>/java18/bin/java -Djava.ext.dirs=/XXX/kafka/libs kafka.tools.DumpLogSegments --files 00000000000235947656.snapshot</span><br><span class="line">Dumping 00000000000235947656.snapshot</span><br><span class="line">producerId: 2000 producerEpoch: 1 coordinatorEpoch: 4 currentTxnFirstOffset: None firstSequence: 95769510 lastSequence: 95769511 lastOffset: 235947654 offsetDelta: 1 timestamp: 1541325156503</span><br><span class="line">producerId: 3000 producerEpoch: 5 coordinatorEpoch: 6 currentTxnFirstOffset: None firstSequence: 91669662 lastSequence: 91669666 lastOffset: 235947651 offsetDelta: 4 timestamp: 1541325156454</span><br></pre></td></tr></table></figure>
<p>在实际的使用中，这个 snapshot 文件一般只会保存最近的两个文件。</p>
<h3 id="中间流程故障如何恢复"><a href="#中间流程故障如何恢复" class="headerlink" title="中间流程故障如何恢复"></a>中间流程故障如何恢复</h3><p>对于上面所讲述的一个事务操作流程，实际生产环境中，任何一个地方都有可能出现的失败：</p>
<ol>
<li>Producer 在发送 <code>beginTransaction()</code> 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li>
<li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li>
<li>Producer 发送 <code>commitTransaction()</code> 时出现 timeout 或者错误：Producer 应该重试这个请求；</li>
<li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li>
</ol>
<p>陆陆续续写了几天，终于把这篇文章总结完了。</p>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="external">Idempotent Producer</a>；</li>
<li><a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="external">Exactly-once Semantics in Apache Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka" target="_blank" rel="external">Transactional Messaging in Kafka</a>；</li>
<li><a href="https://www.confluent.io/blog/transactions-apache-kafka/" target="_blank" rel="external">Transactions in Apache Kafka</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempo
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 事务性之幂等性实现</title>
    <link href="http://matt33.com/2018/10/24/kafka-idempotent/"/>
    <id>http://matt33.com/2018/10/24/kafka-idempotent/</id>
    <published>2018-10-24T06:11:25.000Z</published>
    <updated>2019-02-24T02:29:01.555Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 Kafka 事务性系列的文章我们只重点关注前两种层面上的事务性，与 Kafka Streams 相关的内容暂时不做讨论。社区从开始讨论事务性，前后持续近半年时间，相关的设计文档有六十几页（参考 <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>）。事务性这部分的实现也是非常复杂的，之前 Producer 端的代码实现其实是非常简单的，增加事务性的逻辑之后，这部分代码复杂度提高了很多，本篇及后面几篇关于事务性的文章会以 2.0.0 版的代码实现为例，对这部分做了一下分析，计划分为五篇文章：</p>
<ol>
<li>第一篇：Kafka 幂等性实现；</li>
<li>第二篇：Kafka 事务性实现；</li>
<li>第三篇：Kafka 事务性相关处理请求在 Server 端如何处理及其实现细节；</li>
<li>第四篇：关于 Kafka 事务性实现的一些思考，也会简单介绍一下 RocketMQ 事务性的实现，做一下对比；</li>
<li>第五篇：Flink + Kafka 如何实现 Exactly Once；</li>
</ol>
<p>这篇是 Kafka 事务性系列的第一篇文章，主要讲述幂等性实现的整体流程，幂等性的实现相对于事务性的实现简单很多，也是事务性实现的基础。</p>
<h2 id="Producer-幂等性"><a href="#Producer-幂等性" class="headerlink" title="Producer 幂等性"></a>Producer 幂等性</h2><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p>
<ul>
<li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li>
<li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li>
</ul>
<p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p>
<h2 id="幂等性示例"><a href="#幂等性示例" class="headerlink" title="幂等性示例"></a>幂等性示例</h2><p>Producer 使用幂等性的示例非常简单，与正常情况下 Producer 使用相比变化不大，只需要把 Producer 的配置 enable.idempotence 设置为 true 即可，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>); <span class="comment">// 当 enable.idempotence 为 true，这里默认为 all</span></span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line"></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"test"</span>);</span><br></pre></td></tr></table></figure>
<p>Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好。</p>
<h2 id="幂等性要解决的问题"><a href="#幂等性要解决的问题" class="headerlink" title="幂等性要解决的问题"></a>幂等性要解决的问题</h2><p>在看 Producer 是如何实现幂等性之前，首先先考虑一个问题：<strong>幂等性是来解决什么问题的？</strong> 在 0.11.0 之前，Kafka 通过 Producer 端和 Server 端的相关配置可以做到<strong>数据不丢</strong>，也就是 at least once，但是在一些情况下，可能会导致数据重复，比如：网络请求延迟等导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。</p>
<p>对于大多数应用而言，数据保证不丢是可以满足其需求的，但是对于一些其他的应用场景（比如支付数据等），它们是要求精确计数的，这时候如果上游数据有重复，下游应用只能在消费数据时进行相应的去重操作，应用在去重时，最常用的手段就是根据唯一 id 键做 check 去重。</p>
<p>在这种场景下，因为上游生产导致的数据重复问题，会导致所有有精确计数需求的下游应用都需要做这种复杂的、重复的去重处理。试想一下：如果在发送时，系统就能保证 exactly once，这对下游将是多么大的解脱。这就是幂等性要解决的问题，主要是解决数据重复的问题，正如前面所述，数据重复问题，通用的解决方案就是加唯一 id，然后根据 id 判断数据是否重复，Producer 的幂等性也是这样实现的，这一小节就让我们看下 Kafka 的 Producer 如何保证数据的 exactly once 的。</p>
<h2 id="幂等性的实现原理"><a href="#幂等性的实现原理" class="headerlink" title="幂等性的实现原理"></a>幂等性的实现原理</h2><p>在讲述幂等性处理流程之前，先看下 Producer 是如何来保证幂等性的，正如前面所述，幂等性要解决的问题是：Producer 设置 at least once 时，由于异常触发重试机制导致数据重复，幂等性的目的就是为了解决这个数据重复的问题，简单来说就是：</p>
<p><strong>at least once + 幂等 = exactly once</strong></p>
<p>通过在 al least once 的基础上加上 幂等性来坐到 exactly once，当然这个层面的 exactly once 是有限制的，比如它会要求单会话内有效或者跨会话使用事务性有效等。这里我们先分析最简单的情况，那就是在单会话内如何做到幂等性，进而保证 exactly once。</p>
<p>要做到幂等性，要解决下面的问题：</p>
<ol>
<li>系统需要有能力鉴别一条数据到底是不是重复的数据？常用的手段是通过 <strong>唯一键/唯一 id</strong> 来判断，这时候系统一般是需要缓存已经处理的唯一键记录，这样才能更有效率地判断一条数据是不是重复；</li>
<li>唯一键应该选择什么粒度？对于分布式存储系统来说，肯定不能用全局唯一键（全局是针对集群级别），核心的解决思路依然是 <strong>分而治之</strong>，数据密集型系统为了实现分布式都是有分区概念的，而分区之间是有相应的隔离，对于 Kafka 而言，这里的解决方案就是在分区的维度上去做，重复数据的判断让 partition 的 leader 去判断处理，前提是 Produce 请求需要把唯一键值告诉 leader；</li>
<li>分区粒度实现唯一键会不会有其他问题？这里需要考虑的问题是当一个 Partition 有来自多个 client 写入的情况，这些 client 之间是很难做到使用同一个唯一键（一个是它们之间很难做到唯一键的实时感知，另一个是这样实现是否有必要）。而如果系统在实现时做到了  <strong>client + partition</strong> 粒度，这样实现的好处是每个 client 都是完全独立的（它们之间不需要有任何的联系，这是非常大的优点），只是在 Server 端对不同的 client 做好相应的区分即可，当然同一个 client 在处理多个 Topic-Partition 时是完全可以使用同一个 PID 的。</li>
</ol>
<p>有了上面的分析（都是个人见解，如果有误，欢迎指教），就不难理解 Producer 幂等性的实现原理，Kafka Producer 在实现时有以下两个重要机制：</p>
<ol>
<li>PID（Producer ID），用来标识每个 producer client；</li>
<li>sequence numbers，client 发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复。</li>
</ol>
<p>下面详细讲述这两个实现机制。</p>
<h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>每个 Producer 在初始化时都会被分配一个唯一的 PID，这个 PID 对应用是透明的，完全没有暴露给用户。对于一个给定的 PID，sequence number 将会从0开始自增，每个 Topic-Partition 都会有一个独立的 sequence number。Producer 在发送数据时，将会给每条 msg 标识一个 sequence number，Server 也就是通过这个来验证数据是否重复。这里的 PID 是全局唯一的，Producer 故障后重新启动后会被分配一个新的 PID，这也是幂等性无法做到跨会话的一个原因。</p>
<h4 id="Producer-PID-申请"><a href="#Producer-PID-申请" class="headerlink" title="Producer PID 申请"></a>Producer PID 申请</h4><p>这里看下 PID 在 Server 端是如何分配的？Client 通过向 Server 发送一个 InitProducerIdRequest 请求获取 PID（幂等性时，是选择一台连接数最少的 Broker 发送这个请求），这里看下 Server 端是如何处理这个请求的？KafkaApis 中 <code>handleInitProducerIdRequest()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleInitProducerIdRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> initProducerIdRequest = request.body[<span class="type">InitProducerIdRequest</span>]</span><br><span class="line">  <span class="keyword">val</span> transactionalId = initProducerIdRequest.transactionalId</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId != <span class="literal">null</span>) &#123; <span class="comment">//note: 设置 txn.id 时，验证对 txn.id 的权限</span></span><br><span class="line">    <span class="keyword">if</span> (!authorize(request.session, <span class="type">Write</span>, <span class="type">Resource</span>(<span class="type">TransactionalId</span>, transactionalId, <span class="type">LITERAL</span>))) &#123;</span><br><span class="line">      sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">TRANSACTIONAL_ID_AUTHORIZATION_FAILED</span>.exception)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!authorize(request.session, <span class="type">IdempotentWrite</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123; <span class="comment">//note: 没有设置 txn.id 时，验证对集群是否有幂等性权限</span></span><br><span class="line">    sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.exception)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(result: <span class="type">InitProducerIdResult</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createResponse</span></span>(requestThrottleMs: <span class="type">Int</span>): <span class="type">AbstractResponse</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> responseBody = <span class="keyword">new</span> <span class="type">InitProducerIdResponse</span>(requestThrottleMs, result.error, result.producerId, result.producerEpoch)</span><br><span class="line">      trace(<span class="string">s"Completed <span class="subst">$transactionalId</span>'s InitProducerIdRequest with result <span class="subst">$result</span> from client <span class="subst">$&#123;request.header.clientId&#125;</span>."</span>)</span><br><span class="line">      responseBody</span><br><span class="line">    &#125;</span><br><span class="line">    sendResponseMaybeThrottle(request, createResponse)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 生成相应的了 pid，返回给 producer</span></span><br><span class="line">  txnCoordinator.handleInitProducerId(transactionalId, initProducerIdRequest.transactionTimeoutMs, sendResponseCallback)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里实际上是调用了 TransactionCoordinator （Broker 在启动 server 服务时都会初始化这个实例）的 <code>handleInitProducerId()</code> 方法做了相应的处理，其实现如下（这里只关注幂等性的处理）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleInitProducerId</span></span>(transactionalId: <span class="type">String</span>,</span><br><span class="line">                         transactionTimeoutMs: <span class="type">Int</span>,</span><br><span class="line">                         responseCallback: <span class="type">InitProducerIdCallback</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId == <span class="literal">null</span>) &#123; <span class="comment">//note: 只设置幂等性时，直接分配 pid 并返回</span></span><br><span class="line">    <span class="comment">// if the transactional id is null, then always blindly accept the request</span></span><br><span class="line">    <span class="comment">// and return a new producerId from the producerId manager</span></span><br><span class="line">    <span class="keyword">val</span> producerId = producerIdManager.generateProducerId()</span><br><span class="line">    responseCallback(<span class="type">InitProducerIdResult</span>(producerId, producerEpoch = <span class="number">0</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Server 在给一个 client 初始化 PID 时，实际上是通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID。</p>
<h4 id="Server-PID-管理"><a href="#Server-PID-管理" class="headerlink" title="Server PID 管理"></a>Server PID 管理</h4><p>如前面所述，在幂等性的情况下，直接通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID，其中 ProducerIdManager 是在 TransactionCoordinator 对象初始化时初始化的，这个对象主要是用来管理 PID 信息：</p>
<ul>
<li>在本地的 PID 端用完了或者处于新建状态时，申请 PID 段（默认情况下，每次申请 1000 个 PID）；</li>
<li>TransactionCoordinator 对象通过 <code>generateProducerId()</code> 方法获取下一个可以使用的 PID；</li>
</ul>
<p><strong>PID 端申请是向 ZooKeeper 申请</strong>，zk 中有一个 <code>/latest_producer_id_block</code> 节点，每个 Broker 向 zk 申请一个 PID 段后，都会把自己申请的 PID 段信息写入到这个节点，这样当其他 Broker 再申请 PID 段时，会首先读写这个节点的信息，然后根据 block_end 选择一个 PID 段，最后再把信息写会到 zk 的这个节点，这个节点信息格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,<span class="attr">"broker"</span>:<span class="number">35</span>,<span class="attr">"block_start"</span>:<span class="string">"4000"</span>,<span class="attr">"block_end"</span>:<span class="string">"4999"</span>&#125;</span><br></pre></td></tr></table></figure>
<p>ProducerIdManager 向 zk 申请 PID 段的方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getNewProducerIdBlock</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> zkWriteComplete = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">while</span> (!zkWriteComplete) &#123; <span class="comment">//note: 直到从 zk 拿取到分配的 PID 段</span></span><br><span class="line">    <span class="comment">// refresh current producerId block from zookeeper again</span></span><br><span class="line">    <span class="keyword">val</span> (dataOpt, zkVersion) = zkClient.getDataAndVersion(<span class="type">ProducerIdBlockZNode</span>.path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// generate the new producerId block</span></span><br><span class="line">    currentProducerIdBlock = dataOpt <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(data) =&gt;</span><br><span class="line">        <span class="comment">//note: 从 zk 获取当前最新的 pid 信息，如果后面更新失败，这里也会重新从 zk 获取</span></span><br><span class="line">        <span class="keyword">val</span> currProducerIdBlock = <span class="type">ProducerIdManager</span>.parseProducerIdBlockData(data)</span><br><span class="line">        debug(<span class="string">s"Read current producerId block <span class="subst">$currProducerIdBlock</span>, Zk path version <span class="subst">$zkVersion</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currProducerIdBlock.blockEndId &gt; <span class="type">Long</span>.<span class="type">MaxValue</span> - <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span>) &#123;<span class="comment">//note: 不足以分配1000个 PID</span></span><br><span class="line">          <span class="comment">// we have exhausted all producerIds (wow!), treat it as a fatal error</span></span><br><span class="line">          <span class="comment">//note: 当 PID 分配超过限制时，直接报错了（每秒分配1个，够用2百亿年了）</span></span><br><span class="line">          fatal(<span class="string">s"Exhausted all producerIds as the next block's end producerId is will has exceeded long type limit (current block end producerId is <span class="subst">$&#123;currProducerIdBlock.blockEndId&#125;</span>)"</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Have exhausted all producerIds."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">ProducerIdBlock</span>(brokerId, currProducerIdBlock.blockEndId + <span class="number">1</span>L, currProducerIdBlock.blockEndId + <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: 该节点还不存在，第一次初始化</span></span><br><span class="line">        debug(<span class="string">s"There is no producerId block yet (Zk path version <span class="subst">$zkVersion</span>), creating the first block"</span>)</span><br><span class="line">        <span class="type">ProducerIdBlock</span>(brokerId, <span class="number">0</span>L, <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span> - <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> newProducerIdBlockData = <span class="type">ProducerIdManager</span>.generateProducerIdBlockJson(currentProducerIdBlock)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try to write the new producerId block into zookeeper</span></span><br><span class="line">    <span class="comment">//note: 将新的 pid 信息写入到 zk，如果写入失败（写入之前会比对 zkVersion，如果这个有变动，证明这期间有别的 Broker 在操作，那么写入失败），重新申请</span></span><br><span class="line">    <span class="keyword">val</span> (succeeded, version) = zkClient.conditionalUpdatePath(<span class="type">ProducerIdBlockZNode</span>.path,</span><br><span class="line">      newProducerIdBlockData, zkVersion, <span class="type">Some</span>(checkProducerIdBlockZkData))</span><br><span class="line">    zkWriteComplete = succeeded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (zkWriteComplete)</span><br><span class="line">      info(<span class="string">s"Acquired new producerId block <span class="subst">$currentProducerIdBlock</span> by writing to Zk with path version <span class="subst">$version</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ProducerIdManager 申请 PID 段的流程如下：</p>
<ol>
<li>先从 zk 的 <code>/latest_producer_id_block</code> 节点读取最新已经分配的 PID 段信息；</li>
<li>如果该节点不存在，直接从 0 开始分配，选择 0~1000 的 PID 段（ProducerIdManager 的 PidBlockSize 默认为 1000，即是每次申请的 PID 段大小）；</li>
<li>如果该节点存在，读取其中数据，根据 block_end 选择 <block_end+1, block_end+1000> 这个 PID 段（如果 PID 段超过 Long 类型的最大值，这里会直接返回一个异常）；</block_end+1,></li>
<li>在选择了相应的 PID 段后，将这个 PID 段信息写回到 zk 的这个节点中，如果写入成功，那么 PID 段就证明申请成功，如果写入失败（写入时会判断当前节点的 zkVersion 是否与步骤1获取的 zkVersion 相同，如果相同，那么可以成功写入，否则写入就会失败，证明这个节点被修改过），证明此时可能其他的 Broker 已经更新了这个节点（当前的 PID 段可能已经被其他 Broker 申请），那么从步骤 1 重新开始，直到写入成功。</li>
</ol>
<p>明白了 ProducerIdManager 如何申请 PID 段之后，再看 <code>generateProducerId()</code> 这个方法就简单很多了，这个方法在每次调用时，都会更新 nextProducerId 值（下一次可以使用 PID 值），如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateProducerId</span></span>(): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">this</span> synchronized &#123;</span><br><span class="line">    <span class="comment">// grab a new block of producerIds if this block has been exhausted</span></span><br><span class="line">    <span class="keyword">if</span> (nextProducerId &gt; currentProducerIdBlock.blockEndId) &#123;</span><br><span class="line">      <span class="comment">//note: 如果分配的 pid 用完了，重新再向 zk 申请一批</span></span><br><span class="line">      getNewProducerIdBlock()</span><br><span class="line">      nextProducerId = currentProducerIdBlock.blockStartId + <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      nextProducerId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    nextProducerId - <span class="number">1</span> <span class="comment">//note: 返回当前分配的 pid</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里就是 Producer PID 如何申请（事务性情况下 PID 的申请会复杂一些，下篇文章再讲述）以及 Server 端如何管理 PID 的。</p>
<h3 id="sequence-numbers"><a href="#sequence-numbers" class="headerlink" title="sequence numbers"></a>sequence numbers</h3><p>再有了 PID 之后，在 PID + Topic-Partition 级别上添加一个 sequence numbers 信息，就可以实现 Producer 的幂等性了。ProducerBatch 也提供了一个 <code>setProducerState()</code> 方法，它可以给一个 batch 添加一些 meta 信息（pid、baseSequence、isTransactional），这些信息是会伴随着 ProduceRequest 发到 Server 端，Server 端也正是通过这些 meta 来做相应的判断，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ProducerBatch</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(ProducerIdAndEpoch producerIdAndEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemoryRecordsBuilder</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(<span class="keyword">long</span> producerId, <span class="keyword">short</span> producerEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (isClosed()) &#123;</span><br><span class="line">        <span class="comment">// Sequence numbers are assigned when the batch is closed while the accumulator is being drained.</span></span><br><span class="line">        <span class="comment">// If the resulting ProduceRequest to the partition leader failed for a retriable error, the batch will</span></span><br><span class="line">        <span class="comment">// be re queued. In this case, we should not attempt to set the state again, since changing the producerId and sequence</span></span><br><span class="line">        <span class="comment">// once a batch has been sent to the broker risks introducing duplicates.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Trying to set producer state of an already closed batch. This indicates a bug on the client."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.producerId = producerId;</span><br><span class="line">    <span class="keyword">this</span>.producerEpoch = producerEpoch;</span><br><span class="line">    <span class="keyword">this</span>.baseSequence = baseSequence;</span><br><span class="line">    <span class="keyword">this</span>.isTransactional = isTransactional;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="幂等性实现整体流程"><a href="#幂等性实现整体流程" class="headerlink" title="幂等性实现整体流程"></a>幂等性实现整体流程</h2><p>在前面讲述完 Kafka 幂等性的两个实现机制（PID+sequence numbers）之后，这里详细讲述一下，幂等性时其整体的处理流程，主要讲述幂等性相关的内容，其他的部分会简单介绍（可以参考前面【Kafka 源码分析系列文章】了解 Producer 端处理流程以及 Server 端关于 ProduceRequest 请求的处理流程），其流程如下图所示：</p>
<p><img src="/images/kafka/kafka-idemoptent.png" alt="Producer 幂等性时处理流程"></p>
<p>这个图只展示了幂等性情况下，Producer 的大概流程，很多部分在前面的文章中做过分析，本文不再讲述，这里重点关注与幂等性相关的内容（事务性实现更加复杂，后面的文章再讲述），首先 KafkaProducer 在初始化时会初始化一个 TransactionManager 实例，它的作用有以下几个部分：</p>
<ol>
<li>记录本地的事务状态（事务性时必须）；</li>
<li>记录一些状态信息以保证幂等性，比如：每个 topic-partition 对应的下一个 sequence numbers 和 last acked batch（最近一个已经确认的 batch）的最大的 sequence number 等；</li>
<li>记录 ProducerIdAndEpoch 信息（PID 信息）。</li>
</ol>
<h3 id="Client-幂等性时发送流程"><a href="#Client-幂等性时发送流程" class="headerlink" title="Client 幂等性时发送流程"></a>Client 幂等性时发送流程</h3><p>如前面图中所示，幂等性时，Producer 的发送流程如下：</p>
<ol>
<li>应用通过 KafkaProducer 的 <code>send()</code> 方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的；</li>
<li>Producer 后台发送线程 Sender，在 <code>run()</code> 方法中，会先根据 TransactionManager 的 <code>shouldResetProducerStateAfterResolvingSequences()</code> 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有 topic-partition 的 batch 重试多次失败最后因为超时而被移除，这时 sequence number 将无法做到连续，因为 sequence number 有部分已经分配出去，这时系统依赖自身的机制无法继续进行下去（因为幂等性是要保证不丢不重的），相当于程序遇到了一个 fatal 异常，PID 会进行重置，TransactionManager 相关的缓存信息被清空（Producer 不会重启），只是保存状态信息的 TransactionManager 做了 <code>clear+new</code> 操作，遇到这个问题时是无法保证 exactly once 的（有数据已经发送失败了，并且超过了重试次数）；</li>
<li>Sender 线程通过 <code>maybeWaitForProducerId()</code> 方法判断是否需要申请 PID，如果需要的话，这里会阻塞直到获取到相应的 PID 信息；</li>
<li>Sender 线程通过 <code>sendProducerData()</code> 方法发送数据，整体流程与之前的 Producer 流程相似，不同的地方是在 RecordAccumulator 的 <code>drain()</code> 方法中，在加了幂等性之后，<code>drain()</code> 方法多了如下几步判断：<ol>
<li>常规的判断：判断这个 topic-partition 是否可以继续发送（如果出现前面2中的情况是不允许发送的）、判断 PID 是否有效、如果这个 batch 是重试的 batch，那么需要判断这个 batch 之前是否还有 batch 没有发送完成，如果有，这里会先跳过这个 Topic-Partition 的发送，直到前面的 batch 发送完成，<strong>最坏情况下，这个 Topic-Partition 的 in-flight request 将会减少到1</strong>（这个涉及也是考虑到 server 端的一个设置，文章下面会详细分析）；</li>
<li>如果这个 ProducerBatch 还没有这个相应的 PID 和 sequence number 信息，会在这里进行相应的设置；</li>
</ol>
</li>
<li>最后 Sender 线程再调用 <code>sendProduceRequests()</code> 方法发送 ProduceRequest 请求，后面的就跟之前正常的流程保持一致了。</li>
</ol>
<p>这里看下几个关键方法的实现，首先是 Sender 线程获取 PID 信息的方法  <code>maybeWaitForProducerId()</code> ，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 等待直到 Producer 获取到相应的 PID 和 epoch 信息</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">maybeWaitForProducerId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!transactionManager.hasProducerId() &amp;&amp; !transactionManager.hasError()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Node node = awaitLeastLoadedNodeReady(requestTimeoutMs); <span class="comment">//note: 选取 node（本地连接数最少的 node）</span></span><br><span class="line">            <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">                ClientResponse response = sendAndAwaitInitProducerIdRequest(node); <span class="comment">//note: 发送 InitPidRequest</span></span><br><span class="line">                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();</span><br><span class="line">                Errors error = initProducerIdResponse.error();</span><br><span class="line">                <span class="keyword">if</span> (error == Errors.NONE) &#123; <span class="comment">//note: 更新 Producer 的 PID 和 epoch 信息</span></span><br><span class="line">                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">new</span> ProducerIdAndEpoch(</span><br><span class="line">                            initProducerIdResponse.producerId(), initProducerIdResponse.epoch());</span><br><span class="line">                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error.exception() <span class="keyword">instanceof</span> RetriableException) &#123;</span><br><span class="line">                    log.debug(<span class="string">"Retriable error from InitProducerId response"</span>, error.message());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    transactionManager.transitionToFatalError(error.exception());</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.debug(<span class="string">"Could not find an available broker to send InitProducerIdRequest to. "</span> +</span><br><span class="line">                        <span class="string">"We will back off and try again."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedVersionException e) &#123;</span><br><span class="line">            transactionManager.transitionToFatalError(e);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.debug(<span class="string">"Broker &#123;&#125; disconnected while awaiting InitProducerId response"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        log.trace(<span class="string">"Retry InitProducerIdRequest in &#123;&#125;ms."</span>, retryBackoffMs);</span><br><span class="line">        time.sleep(retryBackoffMs);</span><br><span class="line">        metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再看下 RecordAccumulator 的 <code>drain()</code> 方法，重点需要关注的是关于幂等性和事务性相关的处理，具体如下所示，这里面关于事务性相关的判断在上面的流程中已经讲述。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Drain all the data for the given nodes and collate them into a list of batches that will fit within the specified</span></span><br><span class="line"><span class="comment"> * size on a per-node basis. This method attempts to avoid choosing the same topic-node over and over.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nodes The list of node to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxSize The maximum number of bytes to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current unix time in milliseconds</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A list of &#123;<span class="doctag">@link</span> ProducerBatch&#125; for each node specified with total size less than the requested maxSize.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(Cluster cluster,</span><br><span class="line">                                               Set&lt;Node&gt; nodes,</span><br><span class="line">                                               <span class="keyword">int</span> maxSize,</span><br><span class="line">                                               <span class="keyword">long</span> now) &#123;</span><br><span class="line">    <span class="keyword">if</span> (nodes.isEmpty())</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line"></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodes) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</span><br><span class="line">        List&lt;ProducerBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">/* to make starvation less likely this loop doesn't start at 0 */</span></span><br><span class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            PartitionInfo part = parts.get(drainIndex);</span><br><span class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</span><br><span class="line">            <span class="comment">// Only proceed if the partition has no in-flight batches.</span></span><br><span class="line">            <span class="keyword">if</span> (!isMuted(tp, now)) &#123;</span><br><span class="line">                Deque&lt;ProducerBatch&gt; deque = getDeque(tp);</span><br><span class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (deque) &#123; <span class="comment">//note: 先判断有没有数据，然后后面真正处理时再加锁处理</span></span><br><span class="line">                        ProducerBatch first = deque.peekFirst();</span><br><span class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            <span class="keyword">boolean</span> backoff = first.attempts() &gt; <span class="number">0</span> &amp;&amp; first.waitedTimeMs(now) &lt; retryBackoffMs;</span><br><span class="line">                            <span class="comment">// Only drain the batch if it is not during backoff period.</span></span><br><span class="line">                            <span class="keyword">if</span> (!backoff) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (size + first.estimatedSizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) &#123;</span><br><span class="line">                                    <span class="comment">// there is a rare case that a single batch size is larger than the request size due</span></span><br><span class="line">                                    <span class="comment">// to compression; in this case we will still eventually send this batch in a single</span></span><br><span class="line">                                    <span class="comment">// request</span></span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">null</span>;</span><br><span class="line">                                    <span class="keyword">boolean</span> isTransactional = <span class="keyword">false</span>;</span><br><span class="line">                                    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123; <span class="comment">//note: 幂等性或事务性时， 做一些检查判断</span></span><br><span class="line">                                        <span class="keyword">if</span> (!transactionManager.isSendToPartitionAllowed(tp))</span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        producerIdAndEpoch = transactionManager.producerIdAndEpoch();</span><br><span class="line">                                        <span class="keyword">if</span> (!producerIdAndEpoch.isValid()) <span class="comment">//note: pid 是否有效</span></span><br><span class="line">                                            <span class="comment">// we cannot send the batch until we have refreshed the producer id</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        isTransactional = transactionManager.isTransactional();</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">if</span> (!first.hasSequence() &amp;&amp; transactionManager.hasUnresolvedSequence(first.topicPartition))</span><br><span class="line">                                            <span class="comment">//note: 当前这个 topic-partition 的数据出现过超时,不能发送,如果是新的 batch 数据直接跳过（没有 seq  number 信息）</span></span><br><span class="line">                                            <span class="comment">// Don't drain any new batches while the state of previous sequence numbers</span></span><br><span class="line">                                            <span class="comment">// is unknown. The previous batches would be unknown if they were aborted</span></span><br><span class="line">                                            <span class="comment">// on the client after being sent to the broker at least once.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line">                                        <span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">                                                &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">                                            <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">                                            <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">                                            <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">                                            <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">                                            <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">                                            <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">                                            <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">                                            <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">                                            <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">                                            <span class="comment">// in flight request count to 1.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line">                                    &#125;</span><br><span class="line"></span><br><span class="line">                                    ProducerBatch batch = deque.pollFirst();</span><br><span class="line">                                    <span class="keyword">if</span> (producerIdAndEpoch != <span class="keyword">null</span> &amp;&amp; !batch.hasSequence()) &#123;<span class="comment">//note: batch 的相关信息（seq id）是在这里设置的</span></span><br><span class="line">                                        <span class="comment">//note: 这个 batch 还没有 seq number 信息</span></span><br><span class="line">                                        <span class="comment">// If the batch already has an assigned sequence, then we should not change the producer id and</span></span><br><span class="line">                                        <span class="comment">// sequence number, since this may introduce duplicates. In particular,</span></span><br><span class="line">                                        <span class="comment">// the previous attempt may actually have been accepted, and if we change</span></span><br><span class="line">                                        <span class="comment">// the producer id and sequence here, this attempt will also be accepted,</span></span><br><span class="line">                                        <span class="comment">// causing a duplicate.</span></span><br><span class="line">                                        <span class="comment">//</span></span><br><span class="line">                                        <span class="comment">// Additionally, we update the next sequence number bound for the partition,</span></span><br><span class="line">                                        <span class="comment">// and also have the transaction manager track the batch so as to ensure</span></span><br><span class="line">                                        <span class="comment">// that sequence ordering is maintained even if we receive out of order</span></span><br><span class="line">                                        <span class="comment">// responses.</span></span><br><span class="line">                                        <span class="comment">//note: 给这个 batch 设置相应的 pid、seq id 等信息</span></span><br><span class="line">                                        batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);</span><br><span class="line">                                        transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount); <span class="comment">//note: 增加 partition 对应的下一个 seq id 值</span></span><br><span class="line">                                        log.debug(<span class="string">"Assigned producerId &#123;&#125; and producerEpoch &#123;&#125; to batch with base sequence "</span> +</span><br><span class="line">                                                        <span class="string">"&#123;&#125; being sent to partition &#123;&#125;"</span>, producerIdAndEpoch.producerId,</span><br><span class="line">                                                producerIdAndEpoch.epoch, batch.baseSequence(), tp);</span><br><span class="line"></span><br><span class="line">                                        transactionManager.addInFlightBatch(batch);</span><br><span class="line">                                    &#125;</span><br><span class="line">                                    batch.close();</span><br><span class="line">                                    size += batch.records().sizeInBytes();</span><br><span class="line">                                    ready.add(batch);</span><br><span class="line">                                    batch.drained(now);</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</span><br><span class="line">        &#125; <span class="keyword">while</span> (start != drainIndex);</span><br><span class="line">        batches.put(node.id(), ready);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="幂等性时-Server-端如何处理-ProduceRequest-请求"><a href="#幂等性时-Server-端如何处理-ProduceRequest-请求" class="headerlink" title="幂等性时 Server 端如何处理 ProduceRequest 请求"></a>幂等性时 Server 端如何处理 ProduceRequest 请求</h3><p>如前面途中所示，当 Broker 收到 ProduceRequest 请求之后，会通过 <code>handleProduceRequest()</code> 做相应的处理，其处理流程如下（这里只讲述关于幂等性相关的内容）：</p>
<ol>
<li>如果请求是事务请求，检查是否对 TXN.id 有 Write 权限，没有的话返回 TRANSACTIONAL_ID_AUTHORIZATION_FAILED；</li>
<li>如果请求设置了幂等性，检查是否对 ClusterResource 有 IdempotentWrite 权限，没有的话返回 CLUSTER_AUTHORIZATION_FAILED；</li>
<li>验证对 topic 是否有 Write 权限以及 Topic 是否存在，否则返回 TOPIC_AUTHORIZATION_FAILED 或 UNKNOWN_TOPIC_OR_PARTITION 异常；</li>
<li>检查是否有 PID 信息，没有的话走正常的写入流程；</li>
<li>LOG 对象会在 <code>analyzeAndValidateProducerState()</code> 方法先根据 batch 的 sequence number 信息检查这个 batch 是否重复（server 端会缓存 PID 对应这个 Topic-Partition 的最近5个 batch 信息），如果有重复，这里当做写入成功返回（不更新 LOG 对象中相应的状态信息，比如这个 replica 的 the end offset 等）；</li>
<li>有了 PID 信息，并且不是重复 batch 时，在更新 producer 信息时，会做以下校验：<ol>
<li>检查该 PID 是否已经缓存中存在（主要是在 ProducerStateManager 对象中检查）；</li>
<li>如果不存在，那么判断 sequence number 是否 从0 开始，是的话，在缓存中记录 PID 的 meta（PID，epoch， sequence number），并执行写入操作，否则返回 UnknownProducerIdException（PID 在 server 端已经过期或者这个 PID 写的数据都已经过期了，但是 Client 还在接着上次的 sequence number 发送数据）；</li>
<li>如果该 PID 存在，先检查 PID epoch 与 server 端记录的是否相同；</li>
<li>如果不同并且 sequence number 不从 0 开始，那么返回 OutOfOrderSequenceException 异常；</li>
<li>如果不同并且 sequence number 从 0 开始，那么正常写入；</li>
<li>如果相同，那么根据缓存中记录的最近一次 sequence number（currentLastSeq）检查是否为连续（会区分为 0、Int.MaxValue 等情况），不连续的情况下返回 OutOfOrderSequenceException 异常。</li>
</ol>
</li>
<li>下面与正常写入相同。</li>
</ol>
<p>幂等性时，Broker 在处理 ProduceRequest 请求时，多了一些校验操作，这里重点看一下其中一些重要实现，先看下 <code>analyzeAndValidateProducerState()</code> 方法的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">analyzeAndValidateProducerState</span></span>(records: <span class="type">MemoryRecords</span>, isFromClient: <span class="type">Boolean</span>): (mutable.<span class="type">Map</span>[<span class="type">Long</span>, <span class="type">ProducerAppendInfo</span>], <span class="type">List</span>[<span class="type">CompletedTxn</span>], <span class="type">Option</span>[<span class="type">BatchMetadata</span>]) = &#123;</span><br><span class="line">  <span class="keyword">val</span> updatedProducers = mutable.<span class="type">Map</span>.empty[<span class="type">Long</span>, <span class="type">ProducerAppendInfo</span>]</span><br><span class="line">  <span class="keyword">val</span> completedTxns = <span class="type">ListBuffer</span>.empty[<span class="type">CompletedTxn</span>]</span><br><span class="line">  <span class="keyword">for</span> (batch &lt;- records.batches.asScala <span class="keyword">if</span> batch.hasProducerId) &#123; <span class="comment">//note: 有 pid 时,才会做相应的判断</span></span><br><span class="line">    <span class="keyword">val</span> maybeLastEntry = producerStateManager.lastEntry(batch.producerId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if this is a client produce request, there will be up to 5 batches which could have been duplicated.</span></span><br><span class="line">    <span class="comment">// If we find a duplicate, we return the metadata of the appended batch to the client.</span></span><br><span class="line">    <span class="keyword">if</span> (isFromClient) &#123;</span><br><span class="line">      maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach &#123; duplicate =&gt;</span><br><span class="line">        <span class="keyword">return</span> (updatedProducers, completedTxns.toList, <span class="type">Some</span>(duplicate)) <span class="comment">//note: 如果这个 batch 已经收到过，这里直接返回</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> maybeCompletedTxn = updateProducers(batch, updatedProducers, isFromClient = isFromClient) <span class="comment">//note: 这里</span></span><br><span class="line">    maybeCompletedTxn.foreach(completedTxns += _)</span><br><span class="line">  &#125;</span><br><span class="line">  (updatedProducers, completedTxns.toList, <span class="type">None</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果这个 batch 有 PID 信息，会首先检查这个 batch 是否为重复的 batch 数据，其实现如下，batchMetadata 会缓存最新 5个 batch 的数据（如果超过5个，添加时会进行删除，这个也是幂等性要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5 的原因，与这个值的设置有关），根据 batchMetadata 缓存的 batch 数据来判断这个 batch 是否为重复的数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findDuplicateBatch</span></span>(batch: <span class="type">RecordBatch</span>): <span class="type">Option</span>[<span class="type">BatchMetadata</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (batch.producerEpoch != producerEpoch)</span><br><span class="line">     <span class="type">None</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    batchWithSequenceRange(batch.baseSequence, batch.lastSequence)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Return the batch metadata of the cached batch having the exact sequence range, if any.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchWithSequenceRange</span></span>(firstSeq: <span class="type">Int</span>, lastSeq: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">BatchMetadata</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> duplicate = batchMetadata.filter &#123; metadata =&gt;</span><br><span class="line">    firstSeq == metadata.firstSeq &amp;&amp; lastSeq == metadata.lastSeq</span><br><span class="line">  &#125;</span><br><span class="line">  duplicate.headOption</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatchMetadata</span></span>(batch: <span class="type">BatchMetadata</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (batchMetadata.size == <span class="type">ProducerStateEntry</span>.<span class="type">NumBatchesToRetain</span>)</span><br><span class="line">    batchMetadata.dequeue() <span class="comment">//note: 只会保留最近 5 个 batch 的记录</span></span><br><span class="line">  batchMetadata.enqueue(batch) <span class="comment">//note: 添加到 batchMetadata 中记录，便于后续根据 seq id 判断是否重复</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果 batch 不是重复的数据，<code>analyzeAndValidateProducerState()</code> 会通过 <code>updateProducers()</code> 更新 producer 的相应记录，在更新的过程中，会做一步校验，校验方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 检查 seq number</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkSequence</span></span>(producerEpoch: <span class="type">Short</span>, appendFirstSeq: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (producerEpoch != updatedEntry.producerEpoch) &#123; <span class="comment">//note: epoch 不同时</span></span><br><span class="line">    <span class="keyword">if</span> (appendFirstSeq != <span class="number">0</span>) &#123; <span class="comment">//note: 此时要求 seq number 必须从0开始（如果不是的话，pid 可能是新建的或者 PID 在 Server 端已经过期）</span></span><br><span class="line">      <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 不是-1，证明时原来的 pid 过期了）</span></span><br><span class="line">      <span class="keyword">if</span> (updatedEntry.producerEpoch != <span class="type">RecordBatch</span>.<span class="type">NO_PRODUCER_EPOCH</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Invalid sequence number for new epoch: <span class="subst">$producerEpoch</span> "</span> +</span><br><span class="line">          <span class="string">s"(request epoch), <span class="subst">$appendFirstSeq</span> (seq. number)"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 为-1，证明 server 端 meta 新建的，PID 在 server 端已经过期，client 还在接着上次的 seq 发数据）</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownProducerIdException</span>(<span class="string">s"Found no record of producerId=<span class="subst">$producerId</span> on the broker. It is possible "</span> +</span><br><span class="line">          <span class="string">s"that the last message with t（）he producerId=<span class="subst">$producerId</span> has been removed due to hitting the retention limit."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> currentLastSeq = <span class="keyword">if</span> (!updatedEntry.isEmpty)</span><br><span class="line">      updatedEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (producerEpoch == currentEntry.producerEpoch)</span><br><span class="line">      currentEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="type">RecordBatch</span>.<span class="type">NO_SEQUENCE</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (currentLastSeq == <span class="type">RecordBatch</span>.<span class="type">NO_SEQUENCE</span> &amp;&amp; appendFirstSeq != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">//note: 此时期望的 seq number 是从 0 开始,因为 currentLastSeq 是 -1,也就意味着这个 pid 还没有写入过数据</span></span><br><span class="line">      <span class="comment">// the epoch was bumped by a control record, so we expect the sequence number to be reset</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Out of order sequence number for producerId <span class="subst">$producerId</span>: found <span class="subst">$appendFirstSeq</span> "</span> +</span><br><span class="line">        <span class="string">s"(incoming seq. number), but expected 0"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inSequence(currentLastSeq, appendFirstSeq)) &#123;</span><br><span class="line">      <span class="comment">//note: 判断是否连续</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Out of order sequence number for producerId <span class="subst">$producerId</span>: <span class="subst">$appendFirstSeq</span> "</span> +</span><br><span class="line">        <span class="string">s"(incoming seq. number), <span class="subst">$currentLastSeq</span> (current end sequence number)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其校验逻辑如前面流程中所述。</p>
<h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>这里主要思考两个问题：</p>
<ol>
<li>Producer 在设置幂等性时，为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5，如果设置大于 5（不考虑 Producer 端参数校验的报错），会带来什么后果？</li>
<li>Producer 在设置幂等性时，如果我们设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，那么是否可以保证有序，如果可以，是怎么做到的？</li>
</ol>
<p>先说一下结论，问题 1 的这个设置要求其实上面分析的时候已经讲述过了，主要跟 server 端只会缓存最近 5 个 batch 的机制有关；问题 2，即使 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，幂等性时依然可以做到有序，下面来详细分析一下这两个问题。</p>
<h3 id="为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5"><a href="#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5" class="headerlink" title="为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档，忘记在哪了），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p>
<p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（相当于client 狂发错误请求）。</p>
<p>那有没有更好的方案呢？我认为是有的，那就是对于 OutOfOrderSequenceException 异常，再进行细分，区分这个 sequence number 是大于 nextSeq （期望的下次 sequence number  值）还是小于 nextSeq，如果是小于，那么肯定是重复的数据。</p>
<h3 id="当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序"><a href="#当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序" class="headerlink" title="当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序"></a>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序</h3><p>先来分析一下，在什么情况下 Producer 会出现乱序的问题？没有幂等性时，乱序的问题是在重试时出现的，举个例子：client 依然发送了 6 个请求 1、2、3、4、5、6（它们分别对应了一个 batch），这 6 个请求只有 2-6 成功 ack 了，1 失败了，这时候需要重试，重试时就会把 batch 1 的数据添加到待发送的数据列队中），那么下次再发送时，batch 1 的数据将会被发送，这时候数据就已经出现了乱序，因为 batch 1 的数据已经晚于了 batch 2-6。</p>
<p>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 1 时，是可以解决这个为题，因为同时只允许一个请求正在发送，只有当前的请求发送完成（成功 ack 后），才能继续下一条请求的发送，类似单线程处理这种模式，每次请求发送时都会等待上次的完成，效率非常差，但是可以解决乱序的问题（当然这里有序只是针对单 client 情况，多 client 并发写是无法做到的）。</p>
<p>系统能提供的方案，基本上就是有序性与性能之间二选一，无法做到兼容，实际上系统出现请求重试的几率是很小的（一般都是网络问题触发的），可能连 0.1% 的时间都不到，但是就是为了这 0.1% 时间都不到的情况，应用需要牺牲性能问题来解决，在大数据场景下，我们是希望有更友好的方式来解决这个问题。简单来说，就是当出现重试时，max-in-flight-request 可以动态减少到 1，在正常情况下还是按 5 （5是举例说明）来处理，这有点类似于分布式系统 CAP 理论中关于 P 的考虑，当出现问题时，可以容忍性能变差，但是其他的情况下，我们希望的是能拥有原来的性能，而不是一刀切。令人高兴的，在 Kafka 2.0.0 版本中，如果 Producer 开始了幂等性，Kafka 是可以做到这一点的，如果不开启幂等性，是无法做到的，因为它的实现是依赖了 sequence number。</p>
<p>当请求出现重试时，batch 会重新添加到队列中，这时候是根据 sequence number 添加到队列的合适位置（有些 batch 如果还没有 sequence number，那么就保持其相对位置不变），也就是队列中排在这个 batch 前面的 batch，其 sequence number 都比这个 batch 的 sequence number 小，其实现如下，这个方法保证了在重试时，其 batch 会被放到合适的位置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Re-enqueue the given record batch in the accumulator to retry</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reenqueue</span><span class="params">(ProducerBatch batch, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    batch.reenqueued(now); <span class="comment">//note: 重试,更新相应的 meta</span></span><br><span class="line">    Deque&lt;ProducerBatch&gt; deque = getOrCreateDeque(batch.topicPartition);</span><br><span class="line">    <span class="keyword">synchronized</span> (deque) &#123;</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>)</span><br><span class="line">            insertInSequenceOrder(deque, batch); <span class="comment">//note: 将 batch 添加到队列的合适位置（根据 seq num 信息）</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            deque.addFirst(batch);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外 Sender 在发送请求时，会首先通过 RecordAccumulator 的 <code>drain()</code> 方法获取其发送的数据，在遍历 Topic-Partition 对应的 queue 中的 batch 时，如果发现 batch 已经有了 sequence number 的话，则证明这个 batch 是重试的 batch，因为没有重试的 batch 其 sequence number 还没有设置，这时候会做一个判断，会等待其 in-flight-requests 中请求发送完成，才允许再次发送这个 Topic-Partition 的数据，其判断实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 获取 inFlightBatches 中第一个 batch 的 baseSequence, inFlightBatches 为 null 的话返回 RecordBatch.NO_SEQUENCE</span></span><br><span class="line"><span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line"><span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">        &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">    <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">    <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">    <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">    <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">    <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">    <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">    <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">    <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">    <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">    <span class="comment">// in flight request count to 1.</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>仅有 client 端这两个机制还不够，Server 端在处理 ProduceRequest 请求时，还会检查 batch 的 sequence number 值，它会要求这个值必须是连续的，如果不连续都会返回异常，Client 会进行相应的重试，举个栗子：假设 Client 发送的请求顺序是 1、2、3、4、5（分别对应了一个 batch），如果中间的请求 2 出现了异常，那么会导致 3、4、5 都返回异常进行重试（因为 sequence number 不连续），也就是说此时 2、3、4、5 都会进行重试操作添加到对应的 queue 中。</p>
<p>Producer 的 TransactionManager 实例的 inflightBatchesBySequence 成员变量会维护这个 Topic-Partition 与目前正在发送的 batch 的对应关系（通过 <code>addInFlightBatch()</code> 方法添加 batch 记录），只有这个 batch 成功 ack 后，才会通过 <code>removeInFlightBatch()</code> 方法将这个 batch 从 inflightBatchesBySequence 中移除。接着前面的例子，此时 inflightBatchesBySequence 中还有 2、3、4、5 这几个 batch（有顺序的，2 在前面），根据前面的 RecordAccumulator 的 <code>drain()</code> 方法可以知道只有这个 Topic-Partition 下次要发送的 batch 是 batch 2（跟 transactionManager 的这个 <code>firstInFlightSequence()</code> 方法获取 inFlightBatches 中第一个 batch 的 baseSequence 来判断） 时，才可以发送，否则会直接 break，跳过这个 Topic-Partition 的数据发送。这里相当于有一个等待，等待 batch 2 重新加入到 queue 中，才可以发送，不能跳过 batch 2，直接重试 batch 3、4、5，这是不允许的。</p>
<p>简单来说，其实现机制概括为：</p>
<ol>
<li>Server 端验证 batch 的 sequence number 值，不连续时，直接返回异常；</li>
<li>Client 端请求重试时，batch 在 reenqueue 时会根据 sequence number 值放到合适的位置（有序保证之一）；</li>
<li>Sender 线程发送时，在遍历 queue 中的 batch 时，会检查这个 batch 是否是重试的 batch，如果是的话，只有这个 batch 是最旧的那个需要重试的 batch，才允许发送，否则本次发送跳过这个 Topic-Partition 数据的发送等待下次发送。</li>
</ol>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="external">Idempotent Producer</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 K
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>BookKeeper 集群搭建及使用</title>
    <link href="http://matt33.com/2018/10/19/bk-cluster-install-and-use/"/>
    <id>http://matt33.com/2018/10/19/bk-cluster-install-and-use/</id>
    <published>2018-10-19T15:23:35.000Z</published>
    <updated>2019-02-24T02:29:01.555Z</updated>
    
    <content type="html"><![CDATA[<p>随着 Apache Pulsar 成为 Apache 的顶级开源项目，其存储层的解决方案 Apache BookKeeper 再次受到业界广泛关注。BookKeeper 在 Pulsar 之前也有很多成功的应用，比如使用 BookKeeper 实现了 HDFS NameNode 的 HA 机制（可能大部分公司使用的还是 Quorum Journal Manage 方案）、Twitter 开源的 DistributedLog 系统（可参考<a href="http://www.infoq.com/cn/news/2016/05/Twitter-Github-DistributedLog" target="_blank" rel="external">Twitter开源分布式高性能日志复制服务</a>），BookKeeper 作为一个高扩展、强容错、低延迟的存储服务（A scalable, fault-tolerant, and low-latency storage service optimized for real-time workloads），它相当于把底层的存储层系统服务化（BookKeeper 是更底层的存储服务，类似于 Kafka 的存储层）。这样可以使得依赖于 BookKeeper 实现的分布式存储系统（包括分布式消息队列）在设计时可以只关注其应用层和功能层的内容，存储层比较难解决的问题像一致性、容错等，BookKeeper 已经实现了，从这个层面看，BookKeeper 确实解决业内的一些问题，而且 BookKeeper （Ledger 化，Ledger 相当于 Kafka segment）天生适合云上部署，未来还是有很大潜力的。近段对 BookKeeper 做了一些相应的调研，做了一些总结，本文将会主要从集群部署和使用角度来介绍一下 Apache BookKeeper，后面准备再写一篇文章来深入讲述其架构设计及实现原理。</p>
<h2 id="BookKeeper-简介"><a href="#BookKeeper-简介" class="headerlink" title="BookKeeper 简介"></a>BookKeeper 简介</h2><p>这里先对 BookKeeper 的基本概念做一下介绍，下图是 BookKeeper 的架构图（图片来自 <a href="https://www.slideshare.net/streamlio/introduction-to-apache-bookkeeper-distributed-storage?qid=3cbd6bbf-9e04-4e38-9ab6-4619e4d8f61e&amp;v=&amp;b=&amp;from_search=1" target="_blank" rel="external">Introduction to Apache BookKeeper</a>）：</p>
<p><img src="/images/bookkeeper/bookkeeper.png" alt="Apache BookKeeper 架构图"></p>
<p>在 BookKeeper 中节点（Server）被称作 Bookie（类似于 Kafka 中 Broker，HDFS 中的 DN，但是 BookKeeper 没有 Master 节点，它是典型 Slave/Slave 架构），数据在 Bookie 上以 Ledger 的形式存储（类似 Kafka 中的 Segment，HDFS 中的 Block）， BookKeeper 相关的基本概念如下：</p>
<ol>
<li>Cluster: 所有的 Bookie 组成一个集群（连接到同一个 zk 地址的 Bookie 集合）；</li>
<li>Bookie：BookKeeper 的存储节点，也即 Server 节点；</li>
<li>Ledger：Ledger 是对一个 log 文件的抽象，它本质上是由一系列 Entry （类似与 Kafka 每条 msg）组成的，client 在向 BookKeeper 写数据时也是往 Ledger 中写的；</li>
<li>Entry：entry 本质上就是一条数据，它会有一个 id 做标识；</li>
<li>Journal: Write ahead log，数据是先写到 Journal 中，这个也是 BookKeeper 读写分离实现机制的一部分，后续会详细分析；</li>
<li>Ensemble: Set of Bookies across which a ledger is striped，一个 Ledger 所涉及的 Bookie 集合，初始化 Ledger 时，需要指定这个 Ledger 可以在几台 Bookie 上存储；</li>
<li>Write Quorum Size: Number of replicas，要写入的副本数；</li>
<li>Ack Quorum Size: Number of responses needed before client’s write is satisfied，当这么多副本写入成功后才会向 client 返回成功，比如副本数设置了 3，这个设置了2，client 会同时向三副本写入数据，当收到两个成功响应后，会认为数据已经写入成功；</li>
<li>LAC: Last Add Confirmed，Ledger 中已经确认的最近一条数据的 entry id。</li>
</ol>
<h2 id="BookKeeper-集群搭建"><a href="#BookKeeper-集群搭建" class="headerlink" title="BookKeeper 集群搭建"></a>BookKeeper 集群搭建</h2><p>关于 BookKeeper 集群的搭建可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/deployment/manual/#starting-up-bookies" target="_blank" rel="external">Apache BookKeeper Manual deployment</a> 这篇文章。</p>
<h3 id="集群搭建前准备"><a href="#集群搭建前准备" class="headerlink" title="集群搭建前准备"></a>集群搭建前准备</h3><p>BookKeeper 集群搭建需要：</p>
<ol>
<li>ZooKeeper 集群；</li>
<li>一些 Bookie 节点（在集群的模式下最好是选取三台）；</li>
<li>JDK 版本要求是 JDK8；</li>
</ol>
<p>这里先看下 BookKeeper 的目录结构，跟其他分布式系统也类似，命令在 bin 目录下，配置文件在 conf 目录下，lib 是其依赖的相关 jar 包，如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX2 bookkeeper]$ ll</span><br><span class="line">total 64</span><br><span class="line">drwxr-xr-x 2 matt matt  4096 Sep 20 18:35 bin</span><br><span class="line">drwxr-xr-x 2 matt matt  4096 Sep 20 18:35 conf</span><br><span class="line">drwxrwxr-x 9 matt matt  4096 Oct  9 21:41 deps</span><br><span class="line">drwxrwxr-x 2 matt matt 12288 Oct  9 21:41 lib</span><br><span class="line">-rw-r--r-- 1 matt matt 24184 Sep 20 18:35 LICENSE</span><br><span class="line">-rw-r--r-- 1 matt matt  5114 Sep 20 18:35 NOTICE</span><br><span class="line">-rw-r--r-- 1 matt matt  4267 Sep 20 18:35 README.md</span><br></pre></td></tr></table></figure>
<p>bin 目录下提供了 BookKeeper 相应的操作命令，这里用的命令主要是 <code>bin/bookkeeper*</code>（<code>bookkeeper-daemon.sh</code> 可以让 Bookie 进程在后台自动运行），可以在 <code>bin/common.sh</code> 配置一些通用的配置（比如 JAVA_HOME），关于 bookkeeper 命令的使用方法见 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/cli/#bookkeeper" target="_blank" rel="external">bookkeeper cli</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX2 bookkeeper]$ ll bin/</span><br><span class="line">total 56</span><br><span class="line">-rwxr-xr-x 1 matt matt 2319 Sep 20 18:35 bkctl</span><br><span class="line">-rwxr-xr-x 1 matt matt 5874 Sep 20 18:35 bookkeeper</span><br><span class="line">-rwxr-xr-x 1 matt matt 2869 Sep 20 18:35 bookkeeper-cluster.sh</span><br><span class="line">-rwxr-xr-x 1 matt matt 4590 Sep 20 18:35 bookkeeper-daemon.sh</span><br><span class="line">-rwxr-xr-x 1 matt matt 7785 Sep 20 18:35 common.sh</span><br><span class="line">-rwxr-xr-x 1 matt matt 4575 Sep 20 18:35 dlog</span><br><span class="line">-rwxr-xr-x 1 matt matt 1738 Sep 20 18:35 standalone</span><br><span class="line">-rwxr-xr-x 1 matt matt 5128 Sep 20 18:35 standalone.docker-compose</span><br><span class="line">-rwxr-xr-x 1 matt matt 1854 Sep 20 18:35 standalone.process</span><br></pre></td></tr></table></figure>
<p>在 bookkeper 命令中，又提供了 shell 的相关命令，这里提供的命令非常丰富，可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/cli/#the-bookkeeper-shell" target="_blank" rel="external">BookKeeper Shell</a>，如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX2 bookkeeper]$ bin/bookkeeper shell</span><br><span class="line">Usage: bookkeeper shell [-localbookie [&lt;host:port&gt;]] [-ledgeridformat &lt;hex/long/uuid&gt;] [-entryformat &lt;hex/string&gt;] [-conf configuration] &lt;<span class="built_in">command</span>&gt;</span><br><span class="line"><span class="built_in">where</span> <span class="built_in">command</span> is one of:</span><br><span class="line">       autorecovery [-<span class="built_in">enable</span>|-<span class="built_in">disable</span>]</span><br><span class="line">       bookieformat [-nonInteractive] [-force] [-deleteCookie]</span><br><span class="line">       bookieinfo</span><br><span class="line">       bookiesanity [-entries N] [-timeout N]</span><br><span class="line">       convert-to-db-storage</span><br><span class="line">       convert-to-interleaved-storage</span><br><span class="line">       decommissionbookie [-bookieid &lt;bookieaddress&gt;]</span><br><span class="line">       deleteledger -ledgerid &lt;ledgerid&gt; [-force]</span><br><span class="line">       <span class="built_in">help</span>         [COMMAND]</span><br><span class="line">       initbookie</span><br><span class="line">       initnewcluster</span><br><span class="line">       lastmark</span><br><span class="line">       ledger       [-m] &lt;ledger_id&gt;</span><br><span class="line">       ledgermetadata -ledgerid &lt;ledgerid&gt;</span><br><span class="line">       listbookies  [-readwrite|-<span class="built_in">readonly</span>] [-hostnames]</span><br><span class="line">       listfilesondisc  [-journal|-entrylog|-index]</span><br><span class="line">       listledgers  [-meta] [-bookieid &lt;bookieaddress&gt;]</span><br><span class="line">       listunderreplicated [[-missingreplica &lt;bookieaddress&gt;] [-excludingmissingreplica &lt;bookieaddress&gt;]] [-printmissingreplica] [-printreplicationworkerid]</span><br><span class="line">       lostbookierecoverydelay [-get|-<span class="built_in">set</span> &lt;value&gt;]</span><br><span class="line">       metaformat   [-nonInteractive] [-force]</span><br><span class="line">       nukeexistingcluster -zkledgersrootpath &lt;zkledgersrootpath&gt; [-instanceid &lt;instanceid&gt; | -force]</span><br><span class="line">       readjournal [-dir] [-msg] &lt;journal_id | journal_file_name&gt;</span><br><span class="line">       readledger  [-bookie &lt;address:port&gt;]  [-msg] -ledgerid &lt;ledgerid&gt; [-firstentryid &lt;firstentryid&gt; [-lastentryid &lt;lastentryid&gt;]] [-force-recovery]</span><br><span class="line">       readlog      [-msg] &lt;entry_log_id | entry_log_file_name&gt; [-ledgerid &lt;ledgerid&gt; [-entryid &lt;entryid&gt;]] [-startpos &lt;startEntryLogBytePos&gt; [-endpos &lt;endEntryLogBytePos&gt;]]</span><br><span class="line">       readlogmetadata &lt;entry_log_id | entry_log_file_name&gt;</span><br><span class="line">       rebuild-db-ledger-locations-index</span><br><span class="line">       recover [-deleteCookie] &lt;bookieSrc[:bookieSrc]&gt;</span><br><span class="line">       simpletest   [-ensemble N] [-writeQuorum N] [-ackQuorum N] [-numEntries N]</span><br><span class="line">       triggeraudit</span><br><span class="line">       updatecookie [-bookieId &lt;hostname|ip&gt;] [-expandstorage] [-list] [-delete &lt;force&gt;]</span><br><span class="line">       updateledgers -bookieId &lt;hostname|ip&gt; [-updatespersec N] [-<span class="built_in">limit</span> N] [-verbose <span class="literal">true</span>/<span class="literal">false</span>] [-printprogress N]</span><br><span class="line">       whatisinstanceid</span><br><span class="line">       whoisauditor</span><br></pre></td></tr></table></figure>
<p>conf 目录下是关于 BookKeeper 的相关配置，如下所示，主要配置在 <code>bk_server.conf</code> 中，这里可以提供的配置非常多，具体可配置的参数可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/config/" target="_blank" rel="external">BookKeeper Config</a>，</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX2 bookkeeper]$ ll conf/</span><br><span class="line">total 84</span><br><span class="line">-rw-r--r-- 1 matt matt  1804 Sep 20 18:35 bk_cli_env.sh</span><br><span class="line">-rw-r--r-- 1 matt matt  2448 Sep 20 18:35 bkenv.sh</span><br><span class="line">-rwxr-xr-x 1 matt matt 42269 Sep 20 18:35 bk_server.conf</span><br><span class="line">-rw-r--r-- 1 matt matt  1211 Sep 20 18:35 jaas_example.conf</span><br><span class="line">-rw-r--r-- 1 matt matt  2311 Sep 20 18:35 log4j.cli.properties</span><br><span class="line">-rw-r--r-- 1 matt matt  2881 Sep 20 18:35 log4j.properties</span><br><span class="line">-rw-r--r-- 1 matt matt  1810 Sep 20 18:35 log4j.shell.properties</span><br><span class="line">-rw-r--r-- 1 matt matt  1117 Sep 20 18:35 nettyenv.sh</span><br><span class="line">-rwxr-xr-x 1 matt matt  1300 Sep 20 18:35 standalone.conf</span><br><span class="line">-rw-r--r-- 1 matt matt  3275 Sep 20 18:35 zookeeper.conf</span><br><span class="line">-rw-r--r-- 1 matt matt   843 Sep 20 18:35 zookeeper.conf.dynamic</span><br></pre></td></tr></table></figure>
<h3 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h3><p>在 <a href="https://bookkeeper.apache.org/releases/" target="_blank" rel="external">Apache BookKeeper Releases</a> 中下载 BookKeeper 最新的安装包（这里以 bookkeeper-server-4.8.0-bin.tar.gz 为例）。</p>
<p>将安装包在指定目录下解压后，启动的操作分为以下几步：</p>
<ol>
<li>修改相关配置（<code>zkServers</code>、<code>bookiePort</code>、<code>journalDir</code>、<code>ledgerDir</code> 等）；</li>
<li>在相应的机器上启动 Bookie 进程（使用 <code>./bin/bookkeeper-daemon.sh start bookie</code> 启动 Bookie）；</li>
<li>当所有的 Bookie 启动完成后，随便选择一台，初始化集群 meta 信息（使用 <code>bookkeeper-server/bin/bookkeeper shell metaformat</code> 命令初始化集群的 meta 信息，这里只需要初始化一次）。</li>
</ol>
<p>如果启动成功的话（如果有异常日志，即使 Bookie 进程存在，也可能没有启动成功），启动正常的情况下，在日志中，可以看到类似下面的信息：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2018-10-15 11:24:49,549 - INFO  [main:ComponentStarter@81] - Started component bookie-server.</span><br></pre></td></tr></table></figure>
<h3 id="Admin-REST-API"><a href="#Admin-REST-API" class="headerlink" title="Admin REST API"></a>Admin REST API</h3><p>BookKeeper 服务提供了相应的 Rest API，可供管理员使用，具体可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/admin/http/" target="_blank" rel="external">BookKeeper Admin REST API</a>，如果想要使用这个功能，首先需要 Bookie 服务将 bk_server.conf 中的 <code>httpServerEnabled</code> 配置设置为 true ，相关的配置参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/config/#http-server-settings" target="_blank" rel="external">Http server settings</a>。</p>
<h3 id="安装时踩的坑"><a href="#安装时踩的坑" class="headerlink" title="安装时踩的坑"></a>安装时踩的坑</h3><p>在搭建 BookKeeper 集群中，并没有想象中那么顺畅，遇到了一些小问题，记录如下：</p>
<h4 id="问题1：修改配置后重新启动失败"><a href="#问题1：修改配置后重新启动失败" class="headerlink" title="问题1：修改配置后重新启动失败"></a>问题1：修改配置后重新启动失败</h4><p>在使用  <code>./bin/bookkeeper-daemon.sh  stop bookie</code> 命令关闭 Bookie 进程，当关闭完 Bookie 进程后，再次启动时，发现无法启动，报出了下面的错误：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2018-10-13 21:05:40,674 - ERROR [main:Main@221] - Failed to build bookie server</span><br><span class="line">org.apache.bookkeeper.bookie.BookieException<span class="variable">$InvalidCookieException</span>: instanceId 406a08e5-911e-4ab6-b97b-40e4a56279a8 is not matching with null</span><br><span class="line">	at org.apache.bookkeeper.bookie.Cookie.verifyInternal(Cookie.java:142)</span><br><span class="line">	at org.apache.bookkeeper.bookie.Cookie.verify(Cookie.java:147)</span><br><span class="line">	at org.apache.bookkeeper.bookie.Bookie.verifyAndGetMissingDirs(Bookie.java:381)</span><br><span class="line">	at org.apache.bookkeeper.bookie.Bookie.checkEnvironmentWithStorageExpansion(Bookie.java:444)</span><br><span class="line">	at org.apache.bookkeeper.bookie.Bookie.checkEnvironment(Bookie.java:262)</span><br><span class="line">	at org.apache.bookkeeper.bookie.Bookie.&lt;init&gt;(Bookie.java:646)</span><br><span class="line">	at org.apache.bookkeeper.proto.BookieServer.newBookie(BookieServer.java:133)</span><br><span class="line">	at org.apache.bookkeeper.proto.BookieServer.&lt;init&gt;(BookieServer.java:102)</span><br><span class="line">	at org.apache.bookkeeper.server.service.BookieService.&lt;init&gt;(BookieService.java:43)</span><br><span class="line">	at org.apache.bookkeeper.server.Main.buildBookieServer(Main.java:299)</span><br><span class="line">	at org.apache.bookkeeper.server.Main.doMain(Main.java:219)</span><br><span class="line">	at org.apache.bookkeeper.server.Main.main(Main.java:201)</span><br></pre></td></tr></table></figure>
<p>大概的意思就是说现在 zk 上的 instanceId 是 <code>406a08e5-911e-4ab6-b97b-40e4a56279a8</code>，而期望的 instanceId 是 null，索引因为验证失败导致进程无法启动，instanceId 是搭建集群第三步（初始化集群 meta 信息的地方）中初始化的。此时如果我们启动测试的 client 程序，会抛出以下异常，这是因为目前集群只有2台 Bookie 处在可用状态，而 ensSize 默认是 3，writeQuorumSize 是 2，ackQuorumSize 是2。在 client 的测试程序中，新建一个 Ledger 时，由于集群当前可用的 Bookie 为2，不满足相应的条件，所以抛出了一下的异常：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">org.apache.bookkeeper.client.BKException<span class="variable">$BKNotEnoughBookiesException</span>: Not enough non-faulty bookies available</span><br><span class="line">	at org.apache.bookkeeper.client.SyncCallbackUtils.finish(SyncCallbackUtils.java:83)</span><br><span class="line">	at org.apache.bookkeeper.client.SyncCallbackUtils<span class="variable">$SyncCreateCallback</span>.createComplete(SyncCallbackUtils.java:106)</span><br><span class="line">	at org.apache.bookkeeper.client.LedgerCreateOp.createComplete(LedgerCreateOp.java:238)</span><br><span class="line">	at org.apache.bookkeeper.client.LedgerCreateOp.initiate(LedgerCreateOp.java:142)</span><br><span class="line">	at org.apache.bookkeeper.client.BookKeeper.asyncCreateLedger(BookKeeper.java:891)</span><br><span class="line">	at org.apache.bookkeeper.client.BookKeeper.createLedger(BookKeeper.java:975)</span><br><span class="line">	at org.apache.bookkeeper.client.BookKeeper.createLedger(BookKeeper.java:930)</span><br><span class="line">	at org.apache.bookkeeper.client.BookKeeper.createLedger(BookKeeper.java:911)</span><br><span class="line">	at com.matt.test.bookkeeper.ledger.LedgerTest.createLedgerSync(LedgerTest.java:110)</span><br><span class="line">	at com.matt.test.bookkeeper.ledger.LedgerTest.main(LedgerTest.java:25)</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.NullPointerException</span><br><span class="line">	at com.matt.test.bookkeeper.ledger.LedgerTest.main(LedgerTest.java:26)</span><br></pre></td></tr></table></figure>
<p>关于这个 BookieException$InvalidCookieException 异常，google 了一下并没有找到相应的解决办法，所以就直接看了相应的代码，抛出异常的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">verifyInternal</span><span class="params">(Cookie c, <span class="keyword">boolean</span> checkIfSuperSet)</span> <span class="keyword">throws</span> BookieException.InvalidCookieException </span>&#123;</span><br><span class="line">    String errMsg;</span><br><span class="line">    <span class="keyword">if</span> (c.layoutVersion &lt; <span class="number">3</span> &amp;&amp; c.layoutVersion != layoutVersion) &#123;</span><br><span class="line">        errMsg = <span class="string">"Cookie is of too old version "</span> + c.layoutVersion;</span><br><span class="line">        LOG.error(errMsg);</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BookieException.InvalidCookieException(errMsg);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!(c.layoutVersion &gt;= <span class="number">3</span> &amp;&amp; c.bookieHost.equals(bookieHost)</span><br><span class="line">        &amp;&amp; c.journalDirs.equals(journalDirs) &amp;&amp; verifyLedgerDirs(c, checkIfSuperSet))) &#123;</span><br><span class="line">        errMsg = <span class="string">"Cookie ["</span> + <span class="keyword">this</span> + <span class="string">"] is not matching with ["</span> + c + <span class="string">"]"</span>;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BookieException.InvalidCookieException(errMsg);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((instanceId == <span class="keyword">null</span> &amp;&amp; c.instanceId != <span class="keyword">null</span>)</span><br><span class="line">            || (instanceId != <span class="keyword">null</span> &amp;&amp; !instanceId.equals(c.instanceId))) &#123;</span><br><span class="line">        <span class="comment">// instanceId should be same in both cookies</span></span><br><span class="line">        errMsg = <span class="string">"instanceId "</span> + instanceId</span><br><span class="line">                + <span class="string">" is not matching with "</span> + c.instanceId;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BookieException.InvalidCookieException(errMsg); <span class="comment">// 由于 instanceId 不匹配，抛出了相应的异常</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里可以看到的是从 zk 上拿到的 instanceId 是 <code>406a08e5-911e-4ab6-b97b-40e4a56279a8</code>，而 Cookie 实例 c 中的 instanceId 为 null，那么 这个 Cookie 是如何初始化的呢？往上追一下代码，发现是在初始化 Bookie 时，会检查一下相应的运行环境，此时会从 journalDirectories 和 ledgerDirectories 中 <code>current/VERSION</code> 中初始化相应的 Cookie 对象，由于这个台机器之前启动过，所以这个文件已经创建了，文件的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[matt@XXX2 bookkeeper]$ cat /tmp/bk-data/current/VERSION</span><br><span class="line">4</span><br><span class="line">bookieHost: &quot;XXX:3181&quot;</span><br><span class="line">journalDir: &quot;/tmp/bk-txn&quot;</span><br><span class="line">ledgerDirs: &quot;1\t/tmp/bk-data&quot;</span><br><span class="line">[matt@XXX2 bookkeeper]$ cat /tmp/bk-txn/current/VERSION</span><br><span class="line">4</span><br><span class="line">bookieHost: &quot;XXX:3181&quot;</span><br><span class="line">journalDir: &quot;/tmp/bk-txn&quot;</span><br><span class="line">ledgerDirs: &quot;1\t/tmp/bk-data&quot;</span><br></pre></td></tr></table></figure>
<p>Cookie 从文件加载相应文件，并初始化对象的实现方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Read cookie from registration manager for a given bookie &lt;i&gt;address&lt;/i&gt;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> rm registration manager</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> address bookie address</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> versioned cookie object</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> BookieException when fail to read cookie</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Versioned&lt;Cookie&gt; <span class="title">readFromRegistrationManager</span><span class="params">(RegistrationManager rm,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                     BookieSocketAddress address)</span> <span class="keyword">throws</span> BookieException </span>&#123;</span><br><span class="line">    Versioned&lt;<span class="keyword">byte</span>[]&gt; cookieData = rm.readCookie(address.toString());</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> (BufferedReader reader = <span class="keyword">new</span> BufferedReader(</span><br><span class="line">                <span class="keyword">new</span> StringReader(<span class="keyword">new</span> String(cookieData.getValue(), UTF_8)))) &#123;</span><br><span class="line">            Builder builder = parse(reader);</span><br><span class="line">            Cookie cookie = builder.build();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Versioned&lt;Cookie&gt;(cookie, cookieData.getVersion());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> InvalidCookieException(ioe);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Builder <span class="title">parse</span><span class="params">(BufferedReader reader)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Builder cBuilder = Cookie.newBuilder();</span><br><span class="line">    <span class="keyword">int</span> layoutVersion = <span class="number">0</span>;</span><br><span class="line">    String line = reader.readLine();</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == line) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> EOFException(<span class="string">"Exception in parsing cookie"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        layoutVersion = Integer.parseInt(line.trim());</span><br><span class="line">        cBuilder.setLayoutVersion(layoutVersion);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NumberFormatException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Invalid string '"</span> + line.trim()</span><br><span class="line">                + <span class="string">"', cannot parse cookie."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (layoutVersion == <span class="number">3</span>) &#123;</span><br><span class="line">        cBuilder.setBookieHost(reader.readLine());</span><br><span class="line">        cBuilder.setJournalDirs(reader.readLine());</span><br><span class="line">        cBuilder.setLedgerDirs(reader.readLine());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (layoutVersion &gt;= <span class="number">4</span>) &#123; <span class="comment">//这里的版本默认为 4</span></span><br><span class="line">        CookieFormat.Builder cfBuilder = CookieFormat.newBuilder();</span><br><span class="line">        TextFormat.merge(reader, cfBuilder);</span><br><span class="line">        CookieFormat data = cfBuilder.build();</span><br><span class="line">        cBuilder.setBookieHost(data.getBookieHost());</span><br><span class="line">        cBuilder.setJournalDirs(data.getJournalDir());</span><br><span class="line">        cBuilder.setLedgerDirs(data.getLedgerDirs());</span><br><span class="line">        <span class="comment">// Since InstanceId is optional</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != data.getInstanceId() &amp;&amp; !data.getInstanceId().isEmpty()) &#123; <span class="comment">//如果文件中没有 instanceId 字段，这里就不会初始化到 Cookie 中</span></span><br><span class="line">            cBuilder.setInstanceId(data.getInstanceId());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cBuilder;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解决的方法很简单，在 <code>current/VERSION</code> 文件中添加相应的 instanceId 字段后，Bookie 便可启动成功。但是这里还需要考虑的问题是：</p>
<ul>
<li>instanceId 在这里的作用是什么？instanceId 是在集群初始化时设置的，关于这个值的含义，我推测它的目的是对节点的上线做一个简单的认证，也就是说如果打算在集群中新添加一台 Bookie，需要知道当前的 instanceId 值，这样才能加入到这个集群中；</li>
<li>Bookie 服务的启动流程是什么样的？这里就需要看下代码的具体实现，追一下 Bookie 的启动流程了。</li>
</ul>
<h2 id="BookKeeper-API-使用"><a href="#BookKeeper-API-使用" class="headerlink" title="BookKeeper API 使用"></a>BookKeeper API 使用</h2><p>关于 BookKeeper API，总共提供了以下三种 API：</p>
<ol>
<li>The ledger API is a lower-level API that enables you to interact with ledgers directly，第一种是一种较为底层的 API 接口，直接与 Ledger 交互，见 <a href="https://bookkeeper.apache.org/docs/4.8.0/api/ledger-api/" target="_blank" rel="external">The Ledger API</a>；</li>
<li>The Ledger Advanced API is an advanced extension to Ledger API to provide more flexibilities to applications，第二种较高级的 API，提供了一些较高级的功能，见 <a href="https://bookkeeper.apache.org/docs/4.8.0/api/ledger-adv-apiThe Advanced Ledger API/" target="_blank" rel="external">The Advanced Ledger API</a>；</li>
<li>The DistributedLog API is a higher-level API that provides convenient abstractions，这种是关于 DistributedLog 的一些操作 API，见 <a href="https://bookkeeper.apache.org/docs/4.8.0/api/distributedlog-api/" target="_blank" rel="external">DistributedLog</a>。</li>
</ol>
<p>在这节，我们主要看下第一种的实现，会简单讲述一下第二种，第三种这里不再介绍。</p>
<h3 id="The-Ledger-API"><a href="#The-Ledger-API" class="headerlink" title="The Ledger API"></a>The Ledger API</h3><p>关于 Ledger API 基本操作主要有以下几种：</p>
<ol>
<li>创建 Ledger；</li>
<li>向 Ledger 写入数据（Entry）；</li>
<li>关闭 Ledger，Ledger 关闭后数据就不能再写入，Ledger 一旦关闭它的数据就是不可变的；</li>
<li>从 Ledger 中读取数据；</li>
<li>删除 Ledger。</li>
</ol>
<p>当然实现上述操作的前提是，需要先初始化一个 BookKeeper Client，下面开始慢慢讲述。</p>
<h4 id="初始化-BookKeeper-Client"><a href="#初始化-BookKeeper-Client" class="headerlink" title="初始化 BookKeeper Client"></a>初始化 BookKeeper Client</h4><p>BK Client 的初始化需要指定 zk 地址，BK Client 通过 zk 来连接到 BK 集群，具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一种初始化 BookKeeper Client 的方法</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    String connectionString = zkAddr; <span class="comment">// For a single-node, local ZooKeeper cluster</span></span><br><span class="line">    BookKeeper bkClient = <span class="keyword">new</span> BookKeeper(connectionString);</span><br><span class="line">    logger.info(<span class="string">"BookKeeper client init success."</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException | IOException | BKException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(</span><br><span class="line">            <span class="string">"There is an exception throw while creating the BookKeeper client."</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种初始化 BookKeeper Client 的方法</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    ClientConfiguration config = <span class="keyword">new</span> ClientConfiguration();</span><br><span class="line">    config.setZkServers(zkAddr);</span><br><span class="line">    config.setAddEntryTimeout(<span class="number">2000</span>);</span><br><span class="line">    BookKeeper bkClient = <span class="keyword">new</span> BookKeeper(config);</span><br><span class="line">    logger.info(<span class="string">"BookKeeper client init success."</span>);</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException | IOException | BKException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(</span><br><span class="line">            <span class="string">"There is an exception throw while creating the BookKeeper client."</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="新建一个-Ledger"><a href="#新建一个-Ledger" class="headerlink" title="新建一个 Ledger"></a>新建一个 Ledger</h4><p>Ledger 的创建有两种，一种是同步创建，一种是异步创建（创建时需要指定相应的 password），其实现分别如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create the ledger, default ensemble size is 3, write quorum size is 2, ack quorum size is 2</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> pw password</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> LedgerHandle</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> LedgerHandle <span class="title">createLedgerSync</span><span class="params">(String pw)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] password = pw.getBytes();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        LedgerHandle handle = bkClient.createLedger(BookKeeper.DigestType.MAC, password);</span><br><span class="line">        <span class="keyword">return</span> handle;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create the ledger</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> pw password</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createLedgerAsync</span><span class="params">(String pw)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">LedgerCreationCallback</span> <span class="keyword">implements</span> <span class="title">AsyncCallback</span>.<span class="title">CreateCallback</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createComplete</span><span class="params">(<span class="keyword">int</span> returnCode, LedgerHandle handle, Object ctx)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"Ledger successfully created"</span>);</span><br><span class="line">            logger.info(<span class="string">"Ledger successfully created async."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bkClient.asyncCreateLedger(</span><br><span class="line">            <span class="number">3</span>, <span class="comment">// ensSize</span></span><br><span class="line">            <span class="number">2</span>, <span class="comment">// writeQuorumSize and ackQuorumSize</span></span><br><span class="line">            BookKeeper.DigestType.MAC,</span><br><span class="line">            pw.getBytes(),</span><br><span class="line">            <span class="keyword">new</span> LedgerCreationCallback(),</span><br><span class="line">            <span class="string">"some context"</span></span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>新建好 Ledger 之后，会返回一个 LedgerHandle 实例，对于 Ledger 的操作都是通过这个实例对象完成的，也可以通过 <code>LedgerHandle.getId()</code> 方法获取 Ledger 的 id，有了这个 id 就可以映射到具体的 Ledger，当需要读取数据时，通过 ledger id 初始化相应的 LedgerHandle 实例即可。</p>
<h4 id="向-Ledger-写入数据"><a href="#向-Ledger-写入数据" class="headerlink" title="向 Ledger 写入数据"></a>向 Ledger 写入数据</h4><p>有了 Ledger 对应的 LedgerHandle 实例之后，可以通过 <code>addEntry()</code> 方法直接向 Ledger 写数据，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">addEntry</span><span class="params">(LedgerHandle ledgerHandle, String msg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ledgerHandle.addEntry(msg.getBytes());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="从-Ledger-读取数据"><a href="#从-Ledger-读取数据" class="headerlink" title="从 Ledger 读取数据"></a>从 Ledger 读取数据</h4><p>从 Ledger 读取数据时，也是通过 LedgerHandle 实例的方法实现，提供了以下三种方法：</p>
<ol>
<li>指定读取的 entry.id 范围消费；</li>
<li>从某一个 entry.id 一直读取到 LAC （LastAddConfirmed，该 Ledger 中最近的已经确认的数据）位置；</li>
<li>从某一个 entry.id 一直读取到 lastEntryIdExpectedToRead 位置，该位置可以比 LAC 大，前提是需要该值已经有对应的数据；</li>
</ol>
<p>方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * read entry from startId to endId</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ledgerHandle the ledger</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> startId      start entry id</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> endId        end entry id</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the entries, if occur exception, return null</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Enumeration&lt;LedgerEntry&gt; <span class="title">readEntry</span><span class="params">(LedgerHandle ledgerHandle, <span class="keyword">int</span> startId, <span class="keyword">int</span> endId)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ledgerHandle.readEntries(startId, endId);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * read entry from 0 to the LAC</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ledgerHandle the ledger</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the entries, if occur exception, return null</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Enumeration&lt;LedgerEntry&gt; <span class="title">readEntry</span><span class="params">(LedgerHandle ledgerHandle)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ledgerHandle.readEntries(<span class="number">0</span>, ledgerHandle.getLastAddConfirmed());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * read entry form 0 to lastEntryIdExpectedToRead which can larger than the LastAddConfirmed range</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ledgerHandle              the handle</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> lastEntryIdExpectedToRead the last entry id</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> the entries, if occur exception, return null</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Enumeration&lt;LedgerEntry&gt; <span class="title">readEntry</span><span class="params">(LedgerHandle ledgerHandle,</span></span></span><br><span class="line"><span class="function"><span class="params">                                          <span class="keyword">long</span> lastEntryIdExpectedToRead)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ledgerHandle.readUnconfirmedEntries(<span class="number">0</span>, lastEntryIdExpectedToRead);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="删除-Ledger"><a href="#删除-Ledger" class="headerlink" title="删除 Ledger"></a>删除 Ledger</h4><p>Ledger 的删除实现也很简洁，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * delete the ledger</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ledgerId the ledger id</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> if occur exception, return false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deleteLedger</span><span class="params">(<span class="keyword">long</span> ledgerId)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        bkClient.deleteLedger(ledgerId);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="The-Ledger-Advanced-API"><a href="#The-Ledger-Advanced-API" class="headerlink" title="The Ledger Advanced API"></a>The Ledger Advanced API</h3><p>Ledger 的 Advanced API 在用法上与上面的实现差异不大，它向应用提供了更大的灵活性，比如：在创建 Ledger 时，应用可以指定 LedgerId，写入 Entry 时，应用也可以指定相应的 EntryID。</p>
<h4 id="新建-Ledger"><a href="#新建-Ledger" class="headerlink" title="新建 Ledger"></a>新建 Ledger</h4><p>在新建 Ledger 这部分，Advanced API 可以指定 LedgerId 创建相应的 Ledger，如下面示例的第三种实现。</p>
<p>假设当前 BK 集群的 LedgerId 已经到了5，这时候在新建 Ledger 时如果不指定 LedgerId，下一个被使用的 LedgerId 就是6，如果应用指定了 7，新建的 Leader 的 id 将会是设置的 7，id 6 会等待下次再被使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create the ledger</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> password pw</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> LedgerHandleAdv</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> LedgerHandleAdv <span class="title">createLedger</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] passwd = password.getBytes();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        LedgerHandleAdv handle = (LedgerHandleAdv) bkClient.createLedgerAdv(</span><br><span class="line">                <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="comment">// replica settings</span></span><br><span class="line">                BookKeeper.DigestType.CRC32,</span><br><span class="line">                passwd);</span><br><span class="line">        <span class="keyword">return</span> handle;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create the ledger async</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> password</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createLedgerAsync</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">LedgerCreationCallback</span> <span class="keyword">implements</span> <span class="title">AsyncCallback</span>.<span class="title">CreateCallback</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createComplete</span><span class="params">(<span class="keyword">int</span> returnCode, LedgerHandle handle, Object ctx)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"Ledger successfully created"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    bkClient.asyncCreateLedgerAdv(</span><br><span class="line">            <span class="number">3</span>, <span class="comment">// ensemble size</span></span><br><span class="line">            <span class="number">3</span>, <span class="comment">// write quorum size</span></span><br><span class="line">            <span class="number">2</span>, <span class="comment">// ack quorum size</span></span><br><span class="line">            BookKeeper.DigestType.CRC32,</span><br><span class="line">            password.getBytes(),</span><br><span class="line">            <span class="keyword">new</span> LedgerCreationCallback(),</span><br><span class="line">            <span class="string">"some context"</span>,</span><br><span class="line">            <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create the ledger on special ledgerId</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> password pw</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ledgerId the ledger id, if the ledger id exist, it will return BKLedgerExistException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> LedgerHandleAdv</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> LedgerHandleAdv <span class="title">createLedger</span><span class="params">(String password, <span class="keyword">long</span> ledgerId)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] passwd = password.getBytes();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        LedgerHandleAdv handle = (LedgerHandleAdv) bkClient.createLedgerAdv(</span><br><span class="line">                ledgerId,</span><br><span class="line">                <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="comment">// replica settings</span></span><br><span class="line">                BookKeeper.DigestType.CRC32,</span><br><span class="line">                passwd,</span><br><span class="line">                <span class="keyword">null</span>);</span><br><span class="line">        <span class="keyword">return</span> handle;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="向-Ledger-添加-Entry"><a href="#向-Ledger-添加-Entry" class="headerlink" title="向 Ledger 添加 Entry"></a>向 Ledger 添加 Entry</h4><p>向 Ledger 添加 Entry API 中，最吸引我的是可以指定 EntryId 写入（熟悉 Kafka 的同学知道，向 Kafka 写入数据是可以指定 Partition，但是不能指定 offset，如果可以指定 offset 写入，那么在做容灾时就可以实现 topic 的完全同步，下游可以根据 commit offset 随时切换数据源），其示例如下（注意，Advanced API 在写数据时是强制要指定 entryId 的）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * add the msg to the ledger on the special entryId</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> ledgerHandleAdv ledgerHandleAdv</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> entryId         the entry id</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> msg             msg</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addEntry</span><span class="params">(LedgerHandleAdv ledgerHandleAdv, <span class="keyword">long</span> entryId, String msg)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ledgerHandleAdv.addEntry(entryId, msg.getBytes());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于这个 API，社区官方文档有如下介绍：</p>
<ol>
<li>The entry id has to be non-negative.</li>
<li>Clients are okay to add entries out of order.</li>
<li>However, the entries are only acknowledged in a monotonic order starting from 0.</li>
</ol>
<p>首先，说下我对上面的理解：entry.id 要求是非负的，client 在添加 entry 时可以乱序，但是 entry 只有 0 开始单调顺序增加时才会被 ack。最开始，我以为是只要 entry.id 单调递增就可以，跑了一个测试用例，第一个 entry 的 id 设置为 0，第二个设置为 2，然后程序直接 hang 在那里了，相应日志信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:662 ] - [ DEBUG ]  Got Add response from bookie:XXX.230:3181 rc:EOK, ledger:8:entry:0</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:663 ] - [ DEBUG ]  Got Add response from bookie:XXX.247:3181 rc:EOK, ledger:8:entry:0</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:663 ] - [ DEBUG ]  Submit callback (lid:8, eid: 0). rc:0</span><br><span class="line">2018-10-19 16:58:34  [ main:663 ] - [ DEBUG ]  Adding entry [50, 32, 109, 97, 116, 116, 32, 116, 101, 115, 116]</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Got Add response from bookie:XXX.247:3181 rc:EOK, ledger:8:entry:2</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Got Add response from bookie:XXX.230:3181 rc:EOK, ledger:8:entry:2</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Head of the queue entryId: 2 is not the expected value: 1</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Got Add response from bookie:XXX.146:3181 rc:EOK, ledger:8:entry:0</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Head of the queue entryId: 2 is not the expected value: 1</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:681 ] - [ DEBUG ]  Got Add response from bookie:XXX.146:3181 rc:EOK, ledger:8:entry:2</span><br><span class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:681 ] - [ DEBUG ]  Head of the queue entryId: 2 is not the expected value: 1</span><br><span class="line">2018-10-19 16:58:37  [ main-SendThread(zk01:2181):3702 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</span><br><span class="line">2018-10-19 16:58:40  [ main-SendThread(zk01:2181):7039 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</span><br><span class="line">2018-10-19 16:58:43  [ main-SendThread(zk01:2181):10374 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</span><br><span class="line">2018-10-19 16:58:47  [ main-SendThread(zk01:2181):13710 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</span><br><span class="line">2018-10-19 16:58:50  [ main-SendThread(zk01:2181):17043 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</span><br></pre></td></tr></table></figure>
<p>可以看到有这样的异常日志 <code>Head of the queue entryId: 2 is not the expected value: 1</code>，期望的 entry id 是 1，这里是 2，乱序了，导致程序直接 hang 住（hang 住的原因推测是这个 Entry 没有被 ack），该异常信息出现地方如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendAddSuccessCallbacks</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Start from the head of the queue and proceed while there are</span></span><br><span class="line">    <span class="comment">// entries that have had all their responses come back</span></span><br><span class="line">    PendingAddOp pendingAddOp;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> ((pendingAddOp = pendingAddOps.peek()) != <span class="keyword">null</span></span><br><span class="line">           &amp;&amp; blockAddCompletions.get() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!pendingAddOp.completed) &#123;</span><br><span class="line">            <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">                LOG.debug(<span class="string">"pending add not completed: &#123;&#125;"</span>, pendingAddOp);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Check if it is the next entry in the sequence.</span></span><br><span class="line">        <span class="keyword">if</span> (pendingAddOp.entryId != <span class="number">0</span> &amp;&amp; pendingAddOp.entryId != pendingAddsSequenceHead + <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">                LOG.debug(<span class="string">"Head of the queue entryId: &#123;&#125; is not the expected value: &#123;&#125;"</span>, pendingAddOp.entryId,</span><br><span class="line">                           pendingAddsSequenceHead + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        pendingAddOps.remove();</span><br><span class="line">        explicitLacFlushPolicy.updatePiggyBackedLac(lastAddConfirmed);</span><br><span class="line">        pendingAddsSequenceHead = pendingAddOp.entryId;</span><br><span class="line">        <span class="keyword">if</span> (!writeFlags.contains(WriteFlag.DEFERRED_SYNC)) &#123;</span><br><span class="line">            <span class="keyword">this</span>.lastAddConfirmed = pendingAddsSequenceHead;</span><br><span class="line">        &#125;</span><br><span class="line">        pendingAddOp.submitCallback(BKException.Code.OK);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果 entry id 出现了乱序，会导致这个 add 操作没有正常处理。但是如果这里强制要求 entry.id 从 0，而还有序，那么这个 API 跟前面的 API 有什么区别？这点没有搞懂，也向社区发一封邮件咨询，还在等待社区的响应。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着 Apache Pulsar 成为 Apache 的顶级开源项目，其存储层的解决方案 Apache BookKeeper 再次受到业界广泛关注。BookKeeper 在 Pulsar 之前也有很多成功的应用，比如使用 BookKeeper 实现了 HDFS NameNo
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="bk" scheme="http://matt33.com/tags/bk/"/>
    
  </entry>
  
  <entry>
    <title>YARN 架构学习总结</title>
    <link href="http://matt33.com/2018/09/01/yarn-architecture-learn/"/>
    <id>http://matt33.com/2018/09/01/yarn-architecture-learn/</id>
    <published>2018-09-01T14:39:47.000Z</published>
    <updated>2019-02-24T02:29:01.554Z</updated>
    
    <content type="html"><![CDATA[<p>关于 Hadoop 的介绍，这里就不再多说，可以简答来说 Hadoop 的出现真正让更多的互联网公司开始有能力解决大数据场景下的问题，其中的 HDFS 和 YARN 已经成为大数据场景下存储和资源调度的统一解决方案（MR 现在正在被 Spark 所取代，Spark 在计算这块的地位也开始受到其他框架的冲击，流计算上有 Flink，AI 上有 Tensorflow，两面夹击，但是 Spark 的生态建设得很好，其他框架想要在生产环境立马取代还有很长的路要走）。本片文章就是关于 YARN 框架学习的简单总结，目的是希望自己能对分布式调度这块有更深入的了解，当然也希望也这篇文章能够对初学者有所帮助，文章的主要内容来自 <a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a> 和 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>。</p>
<h1 id="Yarn-背景"><a href="#Yarn-背景" class="headerlink" title="Yarn 背景"></a>Yarn 背景</h1><p>关于 YARN 出现的背景，还是得从 Hadoop1.0 说起，在 Hadoop1.0 中，MR 作业的调度还是有两个重要的组件：JobTracker 和 TaskTracker，其基础的架构如下图所示，从下图中可以大概看出原 MR 作业启动流程：</p>
<ol>
<li>首先用户程序 (Client) 提交了一个 job，job 的信息会发送到 JobTracker 中，JobTracker 是 Map-Reduce 框架的中心，它需要与集群中的机器定时通信 (心跳：heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理<strong>所有 job</strong> 失败、重启等操作；</li>
<li>TaskTracker 是 Map-Reduce 集群中每台机器都有的一个组件，它做的事情主要是监视自己所在机器的资源使用情况；</li>
<li>TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以便处理新提交的 job，来决定其应该分配运行在哪些机器上。</li>
</ol>
<p><img src="/images/hadoop/yarn10.png" alt="Hadoop 1.0 调度的架构图"></p>
<p>可以看出原来的调度框架实现非常简答明了，在 Hadoop 推出的最初几年，也获得业界的认可，但是随着集群规模的增大，很多的弊端开始显露出来，主要有以下几点：</p>
<ol>
<li>JobTracker 是 Map-Reduce 的集中处理点，存在<strong>单点故障</strong>；</li>
<li>JobTracker 赋予的功能太多，导致负载过重，1.0 时未将资源管理与作业控制（包括：作业监控、容错等）分开，导致负载重而且无法支撑更多的计算框架，当集群的作业非常多时，会有很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出 Hadoop1.0 的 Map-Reduce 只能支持 4000 节点主机上限的原因；</li>
<li>在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一个节点上，很容易出现 OOM；</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
</ol>
<p>Hadoop 2.0 中下一代 MR 框架的基本设计思想就是将 JobTracker 的两个主要功能，资源管理和作业控制（包括作业监控、容错等），分拆成两个独立的进程。资源管理与具体的应用程序无关，它负责整个集群的资源（内存、CPU、磁盘等）管理，而作业控制进程则是直接与应用程序相关的模块，且每个作业控制进程只负责管理一个作业，这样就是 YARN 诞生的背景，它是在 MapReduce 框架上衍生出的一个资源统一的管理平台。</p>
<h1 id="Yarn-架构"><a href="#Yarn-架构" class="headerlink" title="Yarn 架构"></a>Yarn 架构</h1><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：</p>
<p><img src="/images/hadoop/yarn20.gif" alt="YARN 基本架构"></p>
<h2 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h2><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：</p>
<ol>
<li>调度器：Scheduler；</li>
<li>应用程序管理器：Applications Manager，ASM。</li>
</ol>
<h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><p>调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 <strong>资源容器(Resource Container，也即 Container)</strong>，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。</p>
<h3 id="应用程序管理器"><a href="#应用程序管理器" class="headerlink" title="应用程序管理器"></a>应用程序管理器</h3><p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p>
<h2 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h2><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p>
<h2 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h2><p>提交的每个作业都会包含一个 AM，主要功能包括：</p>
<ol>
<li>与 RM 协商以获取资源（用 container 表示）；</li>
<li>将得到的任务进一步分配给内部的任务；</li>
<li>与 NM 通信以启动/停止任务；</li>
<li>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。</li>
</ol>
<p>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。</p>
<h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>
<h1 id="YARN-作业提交流程"><a href="#YARN-作业提交流程" class="headerlink" title="YARN 作业提交流程"></a>YARN 作业提交流程</h1><p>当用户向 YARN 中提交一个应用程序后，YARN 将分两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成，如下图所示（此图来自<a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a>）：</p>
<p><img src="/images/hadoop/yarn-flow.png" alt="YARN 工作流程"></p>
<p>上图所示的 YARN 工作流程分为以下几个步骤：</p>
<ol>
<li>用户向 YARN 提交应用程序，其中包括 ApplicationMaster 程序，启动 ApplicationMaster 命令、用户程序等；</li>
<li>RM 为该应用程序分配第一个 Container，并与对应的 NM 通信，要求它在这个 Container 中启动应用程序的 ApplicationMaster；</li>
<li>ApplicationMaster 首先向 RM 注册，这样用户可以直接通过 NM 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，一直重复下面的 4-7 步；</li>
<li>ApplicationMaster 采用轮询的方式通过 RPC 协议向 RM 申请和领取资源；</li>
<li>一旦 ApplicationMaster 申请到资源后，便与对应的 NM 通信，要求它启动任务；</li>
<li>NM 为任务设置好运行环境（包括环境变量、jar 包等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</li>
<li>应用程序运行完成后，ApplicationMaster 向 RM 注销并关闭自己（当然像 Storm、Flink 这种常驻应用程序列外）。</li>
</ol>
<h1 id="调度器-1"><a href="#调度器-1" class="headerlink" title="调度器"></a>调度器</h1><p>YARN 的调度器是一个可插拔的组件，目前社区已经提供了 FIFO Scheduler（先进先出调度器）、Capacity Scheduler（能力调度器）、Fair Scheduler（公平调度器），用户也可以继承 ResourceScheduler 的接口实现自定义的调度器，就像 app on yarn 流程一样，不同的应用可以自己去实现，这里只是简单讲述上述三种调度器的基本原理。</p>
<h2 id="FIFO-Scheduler"><a href="#FIFO-Scheduler" class="headerlink" title="FIFO Scheduler"></a>FIFO Scheduler</h2><p>FIFO 是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应的位置，在资源调度时，<strong>按照队列的先后顺序、先进先出地进行调度和资源分配</strong>。</p>
<p>很明显这种调度器过于简单，在实际的生产中，应用不是很多，毕竟需要调度的作业是有不同的优先级的。</p>
<h2 id="公平调度器（Fair-Scheduler）"><a href="#公平调度器（Fair-Scheduler）" class="headerlink" title="公平调度器（Fair Scheduler）"></a>公平调度器（Fair Scheduler）</h2><p>公平调度器先将用户的任务分配到多个资源池（Pool）中，每个资源池设定资源分配最低保障和最高上限，管理员也可以指定资源池的优先级，优先级高的资源池将会被分配更多的资源，当一个资源池有剩余时，可以临时将剩余资源共享给其他资源池。公平调度器的调度过程如下：</p>
<ol>
<li>根据每个资源池的最小资源保障，将系统中的部分资源分配给各个资源池；</li>
<li>根据资源池的指定优先级讲剩余资源按照比例分配给各个资源池；</li>
<li>在各个资源池中，按照作业的优先级或者根据公平策略将资源分配给各个作业；</li>
</ol>
<p>公平调度器有以下几个特点：</p>
<ol>
<li><strong>支持抢占式调度</strong>，即如果某个资源池长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的资源池的任务，以空出资源供这个资源池使用；</li>
<li><strong>强调作业之间的公平性</strong>：在每个资源池中，公平调度器默认使用公平策略来实现资源分配，这种公平策略是最大最小公平算法的一种具体实现，可以尽可能保证作业间的资源分配公平性；</li>
<li><strong>负载均衡</strong>：公平调度器提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到给各个节点上；</li>
<li><strong>调度策略配置灵活</strong>：允许管理员为每个队列单独设置调度策略；</li>
<li><strong>提高小应用程序响应时间</strong>：由于采用了最大最小公平算法，小作业可以快速获得资源并运行完成。</li>
</ol>
<h2 id="能力调度器（Capacity-Scheduler）"><a href="#能力调度器（Capacity-Scheduler）" class="headerlink" title="能力调度器（Capacity Scheduler）"></a>能力调度器（Capacity Scheduler）</h2><p>能力调度器是 Yahool 为 Hadoop 开发的多用户调度器，应用于用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。</p>
<p>它将用户和任务组织成多个队列，每个队列可以设定资源最低保障和使用上限，当一个队列的资源有剩余时，可以将剩余资源暂时分享给其他队列。调度器在调度时，优先将资源分配给资源使用率最低的队列（即队列已使用资源量占分配给队列的资源量比例最小的队列）；在队列内部，则按照作业优先级的先后顺序遵循 FIFO 策略进行调度。</p>
<p>能力调度器有以下几点特点：</p>
<ol>
<li><strong>容量保证</strong>：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源；</li>
<li><strong>灵活性</strong>：如果一个队列资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列；</li>
<li><strong>多重租赁</strong>：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增多多重约束；</li>
<li><strong>安全保证</strong>：每个队列有严格的 ACL 列表规定它访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序；</li>
<li><strong>动态更新配置文件</strong>：管理可以根据需要动态修改各种配置参数。</li>
</ol>
<h1 id="Yarn-容错"><a href="#Yarn-容错" class="headerlink" title="Yarn 容错"></a>Yarn 容错</h1><p>对于分布式系统，不论是调度系统还是其他系统，容错机制都是非常必要的，这里我们简单看下 YARN 的容错机制，YARN 需要做容错的地方，有以下四个地方：</p>
<ol>
<li>ApplicationMaster 容错：ResourceManager 会和 ApplicationMaster 保持通信，一旦发现 ApplicationMaster 失败或者超时，会为其重新分配资源并重启。重启后 ApplicationMaster 的运行状态需要自己恢复，比如 MRAppMaster 会把相关的状态记录到 HDFS 上，重启后从 HDFS 读取运行状态恢复；</li>
<li>NodeManager 容错：NodeManager 如果超时，则 ResourceManager 会认为它失败，将其上的所有 container 标记为失败并通知相应的 ApplicationMaster，由 AM 决定如何处理（可以重新分配任务，可以整个作业失败，重新拉起）；</li>
<li>container 容错：如果 ApplicationMaster 在一定时间内未启动分配的 container，RM 会将其收回，如果 Container 运行失败，RM 会告诉对应的 AM 由其处理；</li>
<li>RM 容错：RM 采用 HA 机制，这里详细讲述一下。</li>
</ol>
<h2 id="ResourceManager-HA"><a href="#ResourceManager-HA" class="headerlink" title="ResourceManager HA"></a>ResourceManager HA</h2><p>因为 RM 是 YARN 架构中的一个单点，所以他的容错很难做，一般是采用 HA 的方式，有一个 active master 和一个 standby master（可参考：<a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="external">ResourceManager High Availability</a>），HA 的架构方案如下图所示：</p>
<p><img src="/images/hadoop/rm-ha-overview.png" alt="YARN RM HA 机制"></p>
<p>关于 YARN 的 RM 的 HA 机制，其实现与 HDFS 的很像，可以参考前面关于 HDFS 文章的讲述 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-2-0-%E7%9A%84-HA-%E5%AE%9E%E7%8E%B0">HDFS NN HA 实现</a>。</p>
<h1 id="分布式调度器总结"><a href="#分布式调度器总结" class="headerlink" title="分布式调度器总结"></a>分布式调度器总结</h1><p>上面基本已经把 YARN 的相关内容总结完了，这个小节主要讲述一下分布式调度系统的一些内容（调度框架只是具体的一种实现方案），主要讲述分布式调度系统要解决的一些问题和分布式调度系统的调度模型。</p>
<h2 id="调度系统设计遇到的基本问题"><a href="#调度系统设计遇到的基本问题" class="headerlink" title="调度系统设计遇到的基本问题"></a>调度系统设计遇到的基本问题</h2><p>对于分布式调度系统，在实际的生产环境中，遇到的问题很相似，这个小节就是看下调度系统主要面对的问题。</p>
<h3 id="资源异构性与工作负载异构性"><a href="#资源异构性与工作负载异构性" class="headerlink" title="资源异构性与工作负载异构性"></a>资源异构性与工作负载异构性</h3><p>在资源管理与调度场景下，有两类异质性需要考虑：</p>
<ol>
<li>资源异质性：这个是从系统拥有资源的角度来看，对于数据中心来说非常常见，数据中心的机器很难保证完全一样的配置，有的配置会高一些，有的会低一些；</li>
<li>工作负载异质性：在大型互联网公司中很常见，因为各种服务和功能特性各异，对资源的需求千差万别。</li>
</ol>
<h3 id="数据局部性（Data-Locality）"><a href="#数据局部性（Data-Locality）" class="headerlink" title="数据局部性（Data Locality）"></a>数据局部性（Data Locality）</h3><p>在大数据场景下，还有一个基本的共识：将计算任务推送到数据所在地进行而不是反过来。因为数据的移动会产生大量低效的数据网络传输开销，而计算代码相比而言数据小得多，所以将计算任务推动到数据所在地是非常常见的，这就是<strong>数据局部性</strong>，在资源调度中，有三种类型的数据局部性，分别是：</p>
<ol>
<li>节点局部性（Node Locality）：计算任务分配到数据所在机器节点，无需任务网络传输；</li>
<li>机架局部性（Rack Locality）：虽然计算任务与数据分布在不同的节点，但这两个节点在同一个机架中，这也是效率较高的一种数据性；</li>
<li>全局局部性（Global Locality）：需要跨机架的传输，会产生较大的网络传输开销。</li>
</ol>
<h3 id="抢占式调度与非抢占式调度"><a href="#抢占式调度与非抢占式调度" class="headerlink" title="抢占式调度与非抢占式调度"></a>抢占式调度与非抢占式调度</h3><p>在多用户场景下，面对已经分配的资源，资源管理调度系统可以有两种不同类型的调度方式：</p>
<ol>
<li>抢占式调度：对于某个计算任务来说，如果空闲资源不足或者出现不同任务共同竞争同一资源，调度系统可以从比当前计算任务优先级低的其他任务中获取已经分配资源，而被抢占资源的计算任务则需要出让资源停止计算；</li>
<li>非抢占式调度：只允许从空闲资源中进行分配，如果当前空闲资源不足，则须等待其他任务释放资源后才能进行。</li>
</ol>
<h3 id="资源分配粒度（Allocation-Granularity）"><a href="#资源分配粒度（Allocation-Granularity）" class="headerlink" title="资源分配粒度（Allocation Granularity）"></a>资源分配粒度（Allocation Granularity）</h3><p>大数据场景下的计算任务往往由两层结构构成：作业级（Job）和任务级（Task），一个作业由多个并发任务构成，任务之间的依赖关系往往形成有向无环图（DAG），比如：MR 作业，关于作业资源分配的粒度，常见的有两种模式：</p>
<ol>
<li>群体分配（全分或不分）：需要将作业的所有所需资源一次性分配完成；</li>
<li>增量满足式分配策略：对于某个作业，只要分配部分资源就能启动一些任务开始运行，随着空闲资源的不断出现，可以逐步增量式分配给作业的其他任务以维护作业不断向后进行。</li>
</ol>
<p>还有一种策略是 <strong>资源储备策略</strong>，它指的是只有分配到一定量的资源资源才能启动，但是在未获得足够资源的时候，作业可以先持有目前已经分配的资源，并等待其他作业释放资源，这样从调度系统不断获取新资源并进行储备和累积，直到分配到的资源量达到最低标准后开始运行。</p>
<h3 id="饿死（Starvation）与死锁（Dead-Lock）问题"><a href="#饿死（Starvation）与死锁（Dead-Lock）问题" class="headerlink" title="饿死（Starvation）与死锁（Dead Lock）问题"></a>饿死（Starvation）与死锁（Dead Lock）问题</h3><p>饿死和死锁是一个合理的资源调度系统需要避免的两个问题：</p>
<ol>
<li>饿死：指的是这个计算任务持续上时间无法获得开始执行所需的最少资源量，导致一直处于等待执行的状态，比如在资源紧张的情形下，有些低优先级的任务始终无法获得资源分配机会，如果不断出现新提交的高优先级任务，则这些低优先级任务就会出现饿死现象；</li>
<li>死锁：指的是由于资源调度不当导致整个调度无法继续正常执行，比如前面提高的资源储备策略就有可能导致调度系统进入死锁状态，多个作业占有一定作业的情况下，都在等待新的资源释放。</li>
</ol>
<h3 id="资源隔离方法"><a href="#资源隔离方法" class="headerlink" title="资源隔离方法"></a>资源隔离方法</h3><p>目前对于资源隔离最常用的手段是 Linux 容器（Linux Container，LXC，可以参考<a href="https://www.redhat.com/zh/topics/containers/whats-a-linux-container" target="_blank" rel="external">什么是 Linux 容器？</a>），YARN 和 Mesos 都是采用了这种方式来实现资源隔离。LXC 是一种轻量级的内核虚拟化技术，可以用来进行资源和进程运行的隔离，通过 LXC 可以在一台物理机上隔离出多个互相隔离的容器。LXC 在资源管理方面依赖于 Linux 内核的 cgroups 子系统，cgroups 子系统是 Linux 内核提供的一个基于进程组的资源管理的框架，可以为特定的进程组限定可以使用的资源。</p>
<h2 id="调度器模型"><a href="#调度器模型" class="headerlink" title="调度器模型"></a>调度器模型</h2><p>关于资源管理与调度功能的实际功能，分布式调度器根据运行机制的不同进行分类，可以归纳为三种资源管理与调度系统泛型：</p>
<ol>
<li>集中式调度器；</li>
<li>两级调度器；</li>
<li>状态共享调度器。</li>
</ol>
<p>它们的区别与联系如下图所示：</p>
<p><img src="/images/hadoop/scheduler.png" alt="三种调度模型"></p>
<h3 id="集中式调度器"><a href="#集中式调度器" class="headerlink" title="集中式调度器"></a>集中式调度器</h3><p>集中式调度器在整个系统中只运行一个全局的中央调度器实例，所有之上的框架或者计算任务的资源请求全部经由中央调度器来满足（也就是说：资源的使用及任务的执行状态都由中央调度器管理），因此，整个调度系统缺乏并发性且所有调度逻辑全部由中央调度器来完成。集中式调度器有以下这些特点：</p>
<ol>
<li>适合批处理任务和吞吐量较大、运行时间较长的任务；</li>
<li>调度逻辑全部融入到了中央调度器，实现逻辑复杂，灵活性和策略的可扩展性不高；</li>
<li>并发性能较差，比较适合小规模的集群系统；</li>
<li>状态同步比较容易且稳定，这是因为资源使用和任务执行的状态被统一管理，降低了状态同步和并发控制的难度。</li>
</ol>
<h3 id="两级调度器"><a href="#两级调度器" class="headerlink" title="两级调度器"></a>两级调度器</h3><p>对于集中式调度器的不足之处，两级调度器是一个很好的解决方案，它可以看做一种策略下放的机制，它将整个系统的调度工作分为两个级别：</p>
<ol>
<li>中央调度器：中央调度器可以看到集群中所有机器的可用资源并管理其状态，它可以按照一定策略将集群中的所有资源更配各个计算框架，中央调度器级别的资源调度是一种粗粒度的资源调度方式；</li>
<li>框架调度器：各个计算框架在接收到所需资源后，可以根据自身计算任务的特性，使用自身的调度策略来进一步细粒度地分配从中央调度器获得的各种资源。</li>
</ol>
<p>在这种两级调度器架构中，只有中央调度器能够观察到所有集群资源的状态，而每个框架并无全局资源概念（不知道整个集群资源使用情况），只能看到由中央调度器分配给自己的资源，Mesos、YARN 和 Hadoop on Demand 系统是3个典型的两级调度器。两级调度的缺点也非常明显：</p>
<ol>
<li><strong>各个框架无法知道整个集群的实时资源使用情况</strong>：很多框架不需要知道整个集群的实时资源使用情况就可以运行得很顺畅，但是对于其他一些应用，为之提供实时资源使用情况可以挖掘潜在的优化空间；</li>
<li><strong>采用悲观锁，并发粒度小</strong>：悲观锁通常采用锁机制控制并发，这会大大降低性能。</li>
</ol>
<h3 id="状态共享调度器"><a href="#状态共享调度器" class="headerlink" title="状态共享调度器"></a>状态共享调度器</h3><p>通过前面两种模型的介绍，可以发现集群中需要管理的状态主要包括以下两种：</p>
<ol>
<li>系统中资源分配和使用的状态；</li>
<li>系统中任务调度和执行的状态</li>
</ol>
<p>在集中式调度器中，这两个状态都由中心调度器管理，并且一并集成了调度等功能；而在双层调度器中，这两个状态分别由中央调度器和框架调度器管理。集中式调度器可以容易地保证全局状态的一致性，但是可扩展性不够；双层调度器对共享状态的管理较难达到好的一致性保证，也不容易检测资源竞争和死锁。</p>
<p>这也就催生出了另一种调度器 —— 状态共享调度器（Shared-State Scheduler），它是 Google 的 Omega 调度系统提出的一种调度器模型。在这种调度器中，每个计算框架可以看到整个集群中的所有资源，并采用互相竞争的方式去获取自己所需的资源，根据自身特性采取不同的具体资源调整策略，同时系统采用了乐观并发控制手段解决不同框架在资源竞争过程中出现的需求冲突。这样，状态共享调度器在一下两个方面对两级调度器做了相应的优化：</p>
<ol>
<li><strong>乐观并发控制增加了系统的并发性能</strong>；</li>
<li>每个计算框架都可以获得全局的资源使用状况；</li>
</ol>
<p>与两级调度器对比，两者的根本区别在于 <strong>中央调度器功能的强弱不同</strong>，两级调度器依赖中央调度器来进行第一次资源分配，而 Omega 则严重弱化中央调度器的功能，只是维护一份可恢复的集群资源状态信息的主副本，这份数据被称为 <strong>单元状态（Cell State）</strong></p>
<ol>
<li>每个框架在自身内部会维护 单元状态 的一份私有并不断更新的副本信息，而框架对资源的需求则直接在这份副本信息上进行；</li>
<li>只要框架具有特定的优先级，就可以在这份副本信息上申请相应的闲置资源，也可以抢夺已经分配给其他比自身优先级低的计算任务的资源；</li>
<li>一旦框架做出资源决策，则可以改变私有 单元状态 信息并将其同步到全局的 单元状态 信息中区，这样就完成了资源申请并使得这种变化让其他框架可见；</li>
<li>上述资源竞争过程通过 <strong>事务操作</strong> 来进行，保证了操作的原子性。</li>
</ol>
<p>如果两个框架竞争同一份资源，因其决策过程都是在各自私有数据上做出的，并通过原子事务进行提交，系统保证此种情形下只有一个竞争胜出者，而失败者可以后续继续重新申请资源，这是一种乐观并发控制手段，可以增加系统的整体并发性能。</p>
<p>从上面的过程，可以看出，这种架构是一种 <strong>以效率优先，不太考虑资源分配公平性</strong> 的策略，很明显高优先级的任务总是能够在资源竞争过程中获胜，而低优先级的任务存在由于长时间无法竞争到所需资源而被【饿死】的风险。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a>;</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">Hadoop Yarn</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/" target="_blank" rel="external">Hadoop 新 MapReduce 框架 Yarn 详解</a>；</li>
<li><a href="http://shiyanjun.cn/archives/1119.html" target="_blank" rel="external">Hadoop YARN架构设计要点</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/data/library/bd-yarn-intro/index.html" target="_blank" rel="external">YARN 简介</a>；</li>
<li><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="external">理解Hadoop YARN架构</a>；</li>
<li><a href="https://dbaplus.cn/news-141-2004-1.html" target="_blank" rel="external">这里有7种主流案例，告诉你调度器架构设计通用法则</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于 Hadoop 的介绍，这里就不再多说，可以简答来说 Hadoop 的出现真正让更多的互联网公司开始有能力解决大数据场景下的问题，其中的 HDFS 和 YARN 已经成为大数据场景下存储和资源调度的统一解决方案（MR 现在正在被 Spark 所取代，Spark 在计算这
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hadoop" scheme="http://matt33.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>如何学习开源项目</title>
    <link href="http://matt33.com/2018/08/01/system-learn-summary/"/>
    <id>http://matt33.com/2018/08/01/system-learn-summary/</id>
    <published>2018-08-01T06:55:11.000Z</published>
    <updated>2019-02-24T02:29:01.554Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章的方法论内容基本来自订阅的极客时间-李运华老师的《从0开始学架构》中的一篇文章，会结合自己的学习经验、加上以 Flink 为例来做一个总结，也为了让自己再学习其他开源项目时能够按照这样的一个方法论高效的深入学习。先简单说一下开源项目，开源项目最早从上个世纪开始，我知道最早的是 linux 项目（其他的不是很了解），再到近几年大数据领域，发展非常迅速，开源给本公司带来的好处，首先是提高这家在技术界的影响力，然后如果这个项目比较受大家认可，那么这家公司的这个技术可能会成为业界的统一解决方案，就像 Hadoop、Kafka 等。对其他公司的好处是，节省成本、可以快速应用来解决业务中的问题。</p>
<p>但是对于公司技术员工不好的地方是，作为这些项目的维护者，以后真的可能变成运维工程师，社区这些项目发展一般都很快，小厂很难有足够的人力、能力在这上面做太多的研发，一般都是跟着社区升级，可能发展到最后的结果是: 项目的研发由社区（或者背后主导的公司）来负责，其他公司融入这一生态，做好运维工作或产品化的东西就可以了，稳定性、可靠性、新功能交给社区，随着项目逐渐庞大，到最后可能其他公司很少有人能对这些项目有较强的掌控力，研发完全依赖于社区。这些开源项目的接口变得越来越简单、内部越来越复杂时，虽然降低了开发者、维护者的门槛，但也降低对开发者、维护者的要求，这是一把双刃剑，对于在技术上对自己有一定要求的工程师，不应该仅限于使用、原理等。</p>
<p>上面是一些浅薄的想法，下面开始结合李运华老师的文章总结学习开源项目的方法论。首先在学习开源项目时，有几点需要明确的是：</p>
<ol>
<li>先树立正确观念，不管你是什么身份，都可以从开源项目中学到很多东西（比如：要学习 Redis 的网络模型，不需要我们成为 Redis 的开发者，也不需要一定要用到 Redis，只需要具备一定的网络编程基础，再通过阅读 Redis 源码，就可以学习 Redis 这种单进程的 Reactor 模型）；</li>
<li>不要只盯着数据结构和算法，这些在学习开源项目时并没有那么重要（Nginx 使用红黑树来管理定时器，对于大多数人只需要这一点就足够了，并不需要研究 Nginx 实现红黑树的源码是如何写的，除非需要修改这部分的逻辑代码）；</li>
<li>采取<strong>自顶向下</strong>的学习方法，源码不是第一步，而是最后一步（基本掌握了功能、原理、关键设计之后再去看源码，看源码的主要目的是为了学习其代码的写作方式以及关键技术的实现）。</li>
</ol>
<blockquote>
<p>例如，Redis 的 RDB 持久化模式「会将当前内存中的数据库快照保存到磁盘文件中」，那这里所谓的 “数据库快照” 到底是怎么做的呢？在 Linux 平台上其实就是 fork 一个子进程保存就可以了；那为何 fork 子进程就生成了数据库快照了呢？这又和 Linux 的父子进程机制以及 copy-on-write 技术相关了。通过这种学习方式，既能够快速掌握系统设计的关键点（Redis 和 RDB 模式），又能够掌握具体的变成技巧（内存快照）。</p>
</blockquote>
<p>下面来看下李运华老师的『自顶向下』的学习方法和步骤。</p>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>这里的安装并不是对着手册执行一下命令，而是要通过安装过程，获取到如下一些关键的信息：</p>
<ul>
<li>这个系统的依赖组件，而依赖的组件又是系统设计和实现的基础;</li>
<li>安装目录也能够提供一些使用和运行的基本信息；</li>
<li>系统提供了哪些工具方便我们使用（<strong>带着问题去学习效率是最高的</strong>）。</li>
</ul>
<p>以 Nginx 为例，源码安装时依赖的库有 pcre、pcre-devel、openssl、openssl-devel、zlib，光从名字上看能够了解一些信息，例如 openssl 可能和 https 有关，zlib 可能与压缩有关。再以 Memcache 为例，它最大的依赖库就是 libevent，而根据 libevent 是一个高性能的网络库，大概能够推测 Memcache 的网络实现应该是 Reactor 模型。</p>
<p>例如，flink1.5.0安装完成后，目录如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[XXX@matt@pro flink-1.5.0]$ ll</span><br><span class="line">total 52</span><br><span class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 bin</span><br><span class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:57 conf</span><br><span class="line">drwxr-xr-x 6 XXX XXX  4096 Jul  9 23:39 examples</span><br><span class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 lib</span><br><span class="line">-rw-r--r-- 1 XXX XXX 18197 Jul  9 23:39 LICENSE</span><br><span class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:57 <span class="built_in">log</span></span><br><span class="line">-rw-r--r-- 1 XXX XXX   779 Jul  9 23:39 NOTICE</span><br><span class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 opt</span><br><span class="line">-rw-r--r-- 1 XXX XXX  1308 Jul  9 23:39 README.txt</span><br></pre></td></tr></table></figure>
<p>上面 bin 是运行程序，conf 是配置文件的目录，lib 和 opt 是依赖的相关 jar 包，但为什么分为两个目录去放，这个我还不是很明白。下面是目录的详细内容:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">[XXX@matt@pro flink-1.5.0]$ ll bin/</span><br><span class="line">total 116</span><br><span class="line">-rwxr-xr-x 1 XXX XXX 23957 Jul  9 23:39 config.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  2224 Jul  9 23:39 flink</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1271 Jul  9 23:39 flink.bat</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  2823 Jul  9 23:39 flink-console.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  6407 Jul  9 23:39 flink-daemon.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1482 Jul  9 23:39 historyserver.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  2652 Jul  9 23:39 jobmanager.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1802 Jul  9 23:39 mesos-appmaster-job.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1971 Jul  9 23:39 mesos-appmaster.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  2013 Jul  9 23:39 mesos-taskmanager.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1164 Jul  9 23:39 pyflink.bat</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1107 Jul  9 23:39 pyflink.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1182 Jul  9 23:39 pyflink-stream.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  3434 Jul  9 23:39 sql-client.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  3364 Jul  9 23:39 start-cluster.bat</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1836 Jul  9 23:39 start-cluster.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  2960 Jul  9 23:39 start-scala-shell.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1854 Jul  9 23:39 start-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1616 Jul  9 23:39 stop-cluster.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1845 Jul  9 23:39 stop-zookeeper-quorum.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  3543 Jul  9 23:39 taskmanager.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  1674 Jul  9 23:39 yarn-session.sh</span><br><span class="line">-rwxr-xr-x 1 XXX XXX  2281 Jul  9 23:39 zookeeper.sh</span><br><span class="line">[XXX@matt@pro flink-1.5.0]$ ll lib/</span><br><span class="line">total 88972</span><br><span class="line">-rw-r--r-- 1 XXX XXX 90458504 Jul  9 23:39 flink-dist_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   142041 Jul  9 23:39 flink-python_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   489884 Jul  9 23:39 log4j-1.2.17.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX     8870 Jul  9 23:39 slf4j-log4j12-1.7.7.jar</span><br><span class="line">[XXX@matt@pro flink-1.5.0]$ ll opt/</span><br><span class="line">total 193956</span><br><span class="line">-rw-r--r-- 1 XXX XXX    48215 Jul  9 23:39 flink-avro-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   124115 Jul  9 23:39 flink-cep_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX    49235 Jul  9 23:39 flink-cep-scala_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   630006 Jul  9 23:39 flink-gelly_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   759288 Jul  9 23:39 flink-gelly-scala_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX    21140 Jul  9 23:39 flink-json-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX    16835 Jul  9 23:39 flink-metrics-datadog-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   136599 Jul  9 23:39 flink-metrics-dropwizard-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   278137 Jul  9 23:39 flink-metrics-ganglia-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX   161637 Jul  9 23:39 flink-metrics-graphite-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX    89072 Jul  9 23:39 flink-metrics-prometheus-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX     6029 Jul  9 23:39 flink-metrics-slf4j-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX     7712 Jul  9 23:39 flink-metrics-statsd-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 27197071 Jul  9 23:39 flink-ml_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX    17916 Jul  9 23:39 flink-queryable-state-runtime_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 30676687 Jul  9 23:39 flink-s3-fs-hadoop-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 38244766 Jul  9 23:39 flink-s3-fs-presto-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 18517471 Jul  9 23:39 flink-sql-client-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 37325999 Jul  9 23:39 flink-streaming-python_2.11-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 26088550 Jul  9 23:39 flink-swift-fs-hadoop-1.5.0.jar</span><br><span class="line">-rw-r--r-- 1 XXX XXX 18172108 Jul  9 23:39 flink-table_2.11-1.5.0.jar</span><br><span class="line">[XXX@matt@pro flink-1.5.0]$ ll conf/</span><br><span class="line">total 56</span><br><span class="line">-rw-r--r-- 1 XXX XXX 9866 Jul  9 23:57 flink-conf.yaml</span><br><span class="line">-rw-r--r-- 1 XXX XXX 2138 Jul  9 23:39 log4j-cli.properties</span><br><span class="line">-rw-r--r-- 1 XXX XXX 1884 Jul  9 23:39 log4j-console.properties</span><br><span class="line">-rw-r--r-- 1 XXX XXX 1939 Jul  9 23:39 log4j.properties</span><br><span class="line">-rw-r--r-- 1 XXX XXX 1709 Jul  9 23:39 log4j-yarn-session.properties</span><br><span class="line">-rw-r--r-- 1 XXX XXX 2294 Jul  9 23:39 logback-console.xml</span><br><span class="line">-rw-r--r-- 1 XXX XXX 2331 Jul  9 23:39 logback.xml</span><br><span class="line">-rw-r--r-- 1 XXX XXX 1550 Jul  9 23:39 logback-yarn.xml</span><br><span class="line">-rw-r--r-- 1 XXX XXX   15 Jul  9 23:39 masters</span><br><span class="line">-rw-r--r-- 1 XXX XXX  120 Jul  9 23:39 slaves</span><br><span class="line">-rw-r--r-- 1 XXX XXX 2755 Jul  9 23:39 sql-client-defaults.yaml</span><br><span class="line">-rw-r--r-- 1 XXX XXX 1434 Jul  9 23:39 zoo.cfg</span><br></pre></td></tr></table></figure>
<p>比如这里我们想查一下 <code>sql-client.sh</code> 是做什么的？应该怎么使用？不同参数是什么意思，可以通过 help 信息查看。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">./bin/sql-client.sh</span><br><span class="line">./sql-client [MODE] [OPTIONS]</span><br><span class="line"></span><br><span class="line">The following options are available:</span><br><span class="line"></span><br><span class="line">Mode <span class="string">"embedded"</span> submits Flink <span class="built_in">jobs</span> from the <span class="built_in">local</span> machine.</span><br><span class="line"></span><br><span class="line">  Syntax: embedded [OPTIONS]</span><br><span class="line">  <span class="string">"embedded"</span> mode options:</span><br><span class="line">     -d,--defaults &lt;environment file&gt;      The environment properties with <span class="built_in">which</span></span><br><span class="line">                                           every new session is initialized.</span><br><span class="line">                                           Properties might be overwritten by</span><br><span class="line">                                           session properties.</span><br><span class="line">     -e,--environment &lt;environment file&gt;   The environment properties to be</span><br><span class="line">                                           imported into the session. It might</span><br><span class="line">                                           overwrite default environment</span><br><span class="line">                                           properties.</span><br><span class="line">     -h,--<span class="built_in">help</span>                             Show the <span class="built_in">help</span> message with</span><br><span class="line">                                           descriptions of all options.</span><br><span class="line">     -j,--jar &lt;JAR file&gt;                   A JAR file to be imported into the</span><br><span class="line">                                           session. The file might contain</span><br><span class="line">                                           user-defined classes needed <span class="keyword">for</span> the</span><br><span class="line">                                           execution of statements such as</span><br><span class="line">                                           <span class="built_in">functions</span>, table sources, or sinks.</span><br><span class="line">                                           Can be used multiple <span class="built_in">times</span>.</span><br><span class="line">     -l,--library &lt;JAR directory&gt;          A JAR file directory with <span class="built_in">which</span> every</span><br><span class="line">                                           new session is initialized. The files</span><br><span class="line">                                           might contain user-defined classes</span><br><span class="line">                                           needed <span class="keyword">for</span> the execution of</span><br><span class="line">                                           statements such as <span class="built_in">functions</span>, table</span><br><span class="line">                                           sources, or sinks. Can be used</span><br><span class="line">                                           multiple <span class="built_in">times</span>.</span><br><span class="line">     -s,--session &lt;session identifier&gt;     The identifier <span class="keyword">for</span> a session.</span><br><span class="line">                                           <span class="string">'default'</span> is the default identifier.</span><br></pre></td></tr></table></figure>
<h3 id="2-运行"><a href="#2-运行" class="headerlink" title="2. 运行"></a>2. 运行</h3><p>安装完成后，我们需要真正将系统运行起来，运行系统的时候有两个地方要特别关注：<strong>命令行和配置文件</strong>，它们主要提供了两个非常关键的信息：</p>
<ol>
<li>系统具备哪些能力（提供哪些可配置化的参数，这些参数是做什么的以及不同的配置带来的影响是什么）；</li>
<li>系统将会如何运行。</li>
</ol>
<p>这些信息是我们窥视系统内部运行机制和原理的一扇窗口。</p>
<p>例如，下面 Flink 配置中一些配置参数（Flink 集群模式的安装和运行可以参考 <a href="http://wuchong.me/blog/2016/02/26/flink-docs-setup-cluster/" target="_blank" rel="external">Flink官方文档翻译：安装部署（集群模式）</a>），通过这几个启动时的配置参数，我们可以获取下面这些信息：</p>
<ul>
<li><code>jobmanager.rpc.address</code>：The external address of the JobManager, which is the master/coordinator of the distributed system (DEFAULT: localhost)；</li>
<li><code>jobmanager.rpc.port</code>：The port number of the JobManager (DEFAULT: 6123)；</li>
<li><code>jobmanager.heap.mb</code>：JVM heap size (in megabytes) for the JobManager. You may have to increase the heap size for the JobManager if you are running very large applications (with many operators), or if you are keeping a long history of them.</li>
<li><code>taskmanager.numberOfTaskSlots</code>： JVM heap size (in megabytes) for the TaskManagers, which are the parallel workers of the system；</li>
<li><code>taskmanager.numberOfTaskSlots</code>: The number of parallel operator or user function instances that a single TaskManager can run (DEFAULT: 1).</li>
<li><code>parallelism.default</code>：The default parallelism to use for programs that have no parallelism specified. (DEFAULT: 1).；</li>
</ul>
<p>通过上面这些配置参数，我们基本上可以看到 Flink 的 Master/Salve 模型，是分为 JobManager 和 TaskManager，而 TaskManager 中又有对应的 TaskSlot，系统也提供了相应配置参数进行设置，Flink 1.5.0 的配置信息可以参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.5/ops/config.html" target="_blank" rel="external">Flink 1.5.0 配置</a>，社区的文档对这些参数描述得非常清楚，如果之前有大数据系统的基础，比如了解 HDFS、YARN、Spark、Storm、Kafka 的架构，那么在看到这些参数时，其实并不会感觉到太陌生，分布式系统很多东西都是相通的。</p>
<p>在通常情况下，如果我们将每个命令行参数和配置项的作用和原理都全部掌握清楚了的话，基本上对系统已经很熟悉了。这里李运华老师介绍了一个他的经验，那么就是：<strong>不管三七二十一，先把所有的配置项全部研究一遍，包括配置项的原理、作用、影响，并且尝试去修改配置项然后看看系统会有什么变化</strong>。</p>
<h3 id="3-原理研究"><a href="#3-原理研究" class="headerlink" title="3. 原理研究"></a>3. 原理研究</h3><p>在完成前两个步骤后，我们对系统已经有了初步的感觉和理解，此时可以更进一步去研究其原理。其实在研究命令行和配置项的时候已经涉及一部分原理了，但是并不是很系统，因此我们要专门针对原理进行系统性的研究。这里的关键就是<strong>系统性</strong>三个字，怎么才算系统性呢？主要体现在如下几个方面：</p>
<h4 id="3-1-关键特性的基本实现原理"><a href="#3-1-关键特性的基本实现原理" class="headerlink" title="3.1. 关键特性的基本实现原理"></a>3.1. 关键特性的基本实现原理</h4><p>每个应用广泛的开源项目之所以能够受到大众的欢迎，肯定是有一些卖点的，有一些它们的应用场景，常见的有高性能、高可用、可扩展等特性，那到底这些项目是如何做到其所宣称的那么牛的呢？这些牛的地方就是我们需要深入学习的地方：</p>
<ol>
<li>Memcache 的高性能具体是怎么做到的呢？首先是基于 libevent 实现了高性能的网络模型，其次是内存管理 Slab Allocator 机制。为了彻底理解 Memcache 的高性能网络模型，我们需要掌握很多知识：多路复用、Linux epoll、Reactor 模型、多线程等，通过研究 Memcache 的高性能网络模型，我们能够学习一个具体的项目中如何将这些东西全部串起来实现了高性能。</li>
<li>再以 React 为例，Virtual DOM 的实现原理是什么、为何要实现 Virtual DOM、React 是如何构建 Virtual DOM 树、Virtual DOM 与 DOM 什么关系等，通过研究学习 Virtual DOM，即使不使用 React，我们也能够学习如何写出高性能的前端的代码。</li>
<li>这里再以 Kafka 为例，Kafka 作为在大数据领域应用非常广泛的消息队列，它是如何实现它宣称的高性能的、高可靠？以及在 0.11.0 之后的版本它是如何实现幂等性、事务性的？在 2.0 之后是如何实现可以支撑千台机器、百万 partition 规模的？通过深入学习一些，能够让我们学学习到大数据存储系统的可靠性、高性能实现方案，以及分布式一致性（事务性）的实现；</li>
<li>最后以 Flink 为例，Flink 最开始的卖点是 Exactly once 和低延迟，现在的话再加上流式系统 SQL 的支持，那么它与 Storm、Spark streaming 相比，Flink 的 Exactly once 是怎么实现的？为什么 Storm 在现有机制上（不含 Trident）无法实现 Exactly once？Spark Streaming 微批处理模型延迟消耗主要在什么地方？为什么 Flink 可以做到低延迟？Flink  怎么实现窗口计算以及 Flink SQL 是怎么实现的，以及 Flink SQL 现在面对的问题是什么？</li>
</ol>
<h4 id="3-2-优缺点对比分析"><a href="#3-2-优缺点对比分析" class="headerlink" title="3.2. 优缺点对比分析"></a>3.2. 优缺点对比分析</h4><p>这是我想特别强调的一点，<strong>只有清楚掌握技术方案的优缺点后才算真正的掌握这门技术，也只有掌握了技术方案的优缺点后才能在架构设计的时候做出合理的选择。优缺点主要通过对比来分析，即：我们将两个类似的系统进行对比，看看它们的实现差异，以及不同的实现优缺点都是什么</strong>。</p>
<ol>
<li>典型的对比有 Memcache 和 Redis，例如（仅举例说明，实际上对比的点很多），Memcache 用多线程，Redis 用单进程，各有什么优缺点？Memcache 和 Redis 的集群方式，各有什么优缺点？</li>
<li>即使是 Redis 自身，我们也可以对比 RDB 和 AOF 两种模式的优缺点。</li>
</ol>
<h4 id="3-3-如何系统性学习一个开源项目"><a href="#3-3-如何系统性学习一个开源项目" class="headerlink" title="3.3. 如何系统性学习一个开源项目"></a>3.3. 如何系统性学习一个开源项目</h4><p>在你了解了什么是【系统性】后，我来介绍一下原理研究的手段，主要有三种：</p>
<ol>
<li>通读项目的设计文档：例如 Kafka 的设计文档，基本涵盖了消息队列设计的关键决策部分；Disruptor 的设计白皮书，详细的阐述了 Java 单机高性能的设计技巧（官方文档是学习一个项目的必须资料）。</li>
<li>阅读网上已有的分析文档：通常情况下比较热门的开源项目，都已经有非常多的分析文档了，我们可以站在前人的基础上，避免大量的重复投入。但需要注意的是，由于经验、水平、关注点等差异，不同的人分析的结论可能有差异，甚至有的是错误的，因此不能完全参照。一个比较好的方式就是多方对照，也就是说看很多篇分析文档，比较它们的内容共同点和差异点（网上分析文档很多，但是要知道如何区分这些分析文档，多对比一些，同一个东西，每个人的理解并不一定相同）。</li>
<li>Demo 验证：如果有些技术点难以查到资料，自己又不确定，则可以真正去写 Demo 进行验证，通过打印一些日志或者调试，能清晰的理解具体的细节。例如，写一个简单的分配内存程序，然后通过日志和命令行（jmap、jstat、jstack 等）来查看 Java 虚拟机垃圾回收时的具体表现（开源项目一般都会有一些实例供我们学习参考，这也是我们学习一个项目的重要资料，先去看如何使用，再去看不同使用方式背后的原理）。</li>
</ol>
<h3 id="4-测试"><a href="#4-测试" class="headerlink" title="4. 测试"></a>4. 测试</h3><p>通常情况下，如果你真的准备在实际项目中使用某个开源项目的话，必须进行测试。有的同学可能会说，网上的分析和测试文档很多，直接找一篇看就可以了？如果只是自己学习和研究，这样做是可以的，因为构建完整的测试用例既需要耗费较多时间，又需要较多机器资源，如果每个项目都这么做的话，投入成本有点大；但如果是要在实践项目中使用，必须自己进行测试，因为网上搜的测试结果，不一定与自己的业务场景很契合，如果简单参考别人的测试结果，很可能会得出错误的结论。例如，开源系统的版本不同，测试结果可能差异较大。同样是 K-V 存储，别人测试的 value 是 128 字节，而你的场景 value 都达到了 128k 字节，两者的测试结果也差异很大，不能简单照搬（在实际真正应用前，需要足够的性能测试，而且要能分析出测试结论背后的理论原因，如果找不到理论做为支撑，这样的测试并不是可信的，因为网络中环境有很大的随机性）。</p>
<p>测试阶段需要特别强调的一点就是：测试一定要在原理研究之后做，不能安装完成立马就测试！原因在于如果对系统不熟悉，很可能出现命令行、配置参数没用对，或者运行模式选择不对，导致没有根据业务的特点搭建正确的环境、没有设计合理的测试用例，从而使得最终的测试结果得出了错误结论，误导了设计决策。曾经有团队安装完成 MySQL 5.1 后就进行性能测试，测试结果出来让人大跌眼镜，经过定位才发现 <code>innodb_buffer_pool_size</code> 使用的是默认值 8M。</p>
<h3 id="5-源码研究"><a href="#5-源码研究" class="headerlink" title="5. 源码研究"></a>5. 源码研究</h3><p>源码研究的主要目的是<strong>学习原理背后的具体编码如何实现，通过学习这些技巧来提升我们自己的技术能力</strong>。例如 Redis 的 RDB 快照、Nginx 的多 Reactor 模型、Disruptor 如何使用 volatile 以及 CAS 来做无锁设计、Netty 的 Zero-Copy 等，这些技巧都很精巧，掌握后能够大大提升自己的编码能力。</p>
<p>通常情况下，不建议通读所有源码，因为想掌握每行代码的含义和作用还是非常耗费时间的，尤其是 MySQL、Nginx 这种规模的项目，即使是他们的开发人员，都不一定每个人都掌握了所有代码。带着明确目的去研究源码，做到有的放矢，才能事半功倍，这也是源码研究要放在最后的原因。</p>
<h3 id="时间分配"><a href="#时间分配" class="headerlink" title="时间分配"></a>时间分配</h3><p>前面介绍的【自顶向下】五个步骤，完整执行下来需要花费较长时间，而时间又是大部分技术人员比较稀缺的资源。很多人在学习技术的时候都会反馈说时间不够，版本进度很紧，很难有大量的时间进行学习，但如果不学习感觉自己又很难提升？面对这种两难问题，具体该如何做呢？</p>
<p>通常情况下，以上 5 个步骤的前 3 个步骤，不管是已经成为架构师的技术人员，还是立志成为架构师的技术人员，在研究开源项目的时候都必不可少；第四步可以在准备采用开源项目的时候才实施，第五步可以根据你的时间来进行灵活安排。这里的“灵活安排”不是说省略不去做，而是在自己有一定时间和精力的时候做，因为只有这样才能真正理解和学到具体的技术。</p>
<p>如果感觉自己时间和精力不够，与其蜻蜓点水每个开源项目都去简单了解一下，还不如集中精力将一个开源项目研究通透，就算是每个季度只学习一个开源项目，积累几年后这个数量也是很客观的；而且一旦你将一个项目研究透以后，再去研究其他类似项目，你会发现自己学习的非常快，因为共性的部分你已经都掌握了，只需要掌握新项目差异的部分即可。</p>
<p>这里个人的感想是，对于初中级工程师，最好还是要有2-3项目或者2-3个方向需要走到第五步，毕竟我们是靠这个吃饭的，对于其他的项目（目前业界统一的解决方案，比如 hdfs、hbase、spark 等），至少需要走到第四步，这样与这方面的专业人士沟通交流时，至少让自己不至于处在懵逼状态。当然对于这些项目的核心代码，也是可以深入学习，比如 spark 的 shuffle 在代码上是如何实现的等。在一个项目上深入之后，再去看同一个领域的其他项目时，当看到其他的架构时，其实我们基本上就可以清楚这架构设计的原因、要解决的问题以及这种设计带来的其他问题，每种设计都有其应用场景，比如对 Kafka 有了深入了解后，再看 RocketMQ、phxqueue 时，看到它们的架构方案基本上就明白要解决的问题以及其特定的应用场景，当然对一些独特的特性，还是需要深入到代码层面去学习的，比如 RocketMQ 的延迟队列实现。这是一些个人的感想，并不一定对，大家可以共同交流，总之，是感觉李运华老师这篇文章是值得总结分享的，希望自己在后面学习开源项目时，能够静下心、认真坚持学下去。</p>
<hr>
<p>最后，非常推荐极客时间李运华老师的《从0开始学架构》的专栏，里面涉猎的内容很多，讲得也都比较深入，如果能结合这个专栏去学习，是非常能开阔自己的技术视野，需要的同学可以通过下面的连接购买，会有一定的优惠。</p>
<p><img src="/images/share/learn-the-design-of-architecture.jpeg" alt="《从0开始学架构》专栏"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章的方法论内容基本来自订阅的极客时间-李运华老师的《从0开始学架构》中的一篇文章，会结合自己的学习经验、加上以 Flink 为例来做一个总结，也为了让自己再学习其他开源项目时能够按照这样的一个方法论高效的深入学习。先简单说一下开源项目，开源项目最早从上个世纪开始，我知
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="思考" scheme="http://matt33.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>JVM 之 ParNew 和 CMS 日志分析</title>
    <link href="http://matt33.com/2018/07/28/jvm-cms/"/>
    <id>http://matt33.com/2018/07/28/jvm-cms/</id>
    <published>2018-07-28T11:53:04.000Z</published>
    <updated>2019-02-24T02:29:01.554Z</updated>
    
    <content type="html"><![CDATA[<p>在两年前的文章 <a href="http://matt33.com/2016/09/18/jvm-basic2/">JVM 学习——垃圾收集器与内存分配策略</a> 中，已经对 GC 算法的原理以及常用的垃圾收集器做了相应的总结。今天这篇文章主要是对生产环境中（Java7）常用的两种垃圾收集器（ParNew：年轻代，CMS：老年代）从日志信息上进行分析，做一下总结，这样当我们在排查相应的问题时，看到 GC 的日志信息，不会再那么陌生，能清楚地知道这些日志是什么意思，GC 线程当前处在哪个阶段，正在做什么事情等。</p>
<h2 id="ParNew-收集器"><a href="#ParNew-收集器" class="headerlink" title="ParNew 收集器"></a>ParNew 收集器</h2><p><a href="http://matt33.com/2016/09/18/jvm-basic2/#ParNew-%E6%94%B6%E9%9B%86%E5%99%A8">ParNew 收集器</a>是年轻代常用的垃圾收集器，它采用的是复制算法，youngGC 时一个典型的日志信息如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:26.134+0800: 15578.050: [GC2018-04-12T13:48:26.135+0800: 15578.050: [ParNew: 3412467K-&gt;59681K(3774912K), 0.0971990 secs] 9702786K-&gt;6354533K(24746432K), 0.0974940 secs] [Times: user=0.95 sys=0.00, real=0.09 secs]</span><br></pre></td></tr></table></figure>
<p>依次分析一下上面日志信息的含义：</p>
<ul>
<li><code>2018-04-12T13:48:26.134+0800</code>：Mirror GC 发生的时间；</li>
<li><code>15578.050</code>：GC 开始时，相对 JVM 启动的相对时间，单位时秒，这里是4h+；</li>
<li><code>ParNew</code>：收集器名称，这里是 ParNew 收集器，它使用的是并行的 mark-copy 算法，GC 过程也会 Stop the World；</li>
<li><code>3412467K-&gt;59681K</code>：收集前后年轻代的使用情况，这里是 3.25G-&gt;58.28M；</li>
<li><code>3774912K</code>：整个年轻代的容量，这里是 3.6G；</li>
<li><code>0.0971990 secs</code>：Duration for the collection w/o final cleanup.</li>
<li><code>9702786K-&gt;6354533K</code>：收集前后整个堆的使用情况，这里是 9.25G-&gt;6.06G;</li>
<li><code>24746432K</code>：整个堆的容量，这里是 23.6G；</li>
<li><code>0.0974940 secs</code>：ParNew 收集器标记和复制年轻代活着的对象所花费的时间（包括和老年代通信的开销、对象晋升到老年代开销、垃圾收集周期结束一些最后的清理对象等的花销）；</li>
</ul>
<p>对于 <code>[Times: user=0.95 sys=0.00, real=0.09 secs]</code>，这里面涉及到三种时间类型，含义如下：</p>
<ul>
<li>user：GC 线程在垃圾收集期间所使用的 CPU 总时间；</li>
<li>sys：系统调用或者等待系统事件花费的时间；</li>
<li>real：应用被暂停的时钟时间，由于 GC 线程是多线程的，导致了 real 小于 (user+real)，如果是 gc 线程是单线程的话，real 是接近于 (user+real) 时间。</li>
</ul>
<h2 id="CMS-收集器"><a href="#CMS-收集器" class="headerlink" title="CMS 收集器"></a>CMS 收集器</h2><p><a href="http://matt33.com/2016/09/18/jvm-basic2/#CMS-%E6%94%B6%E9%9B%86%E5%99%A8">CMS 收集器</a>是老年代经常使用的收集器，它采用的是标记-清楚算法，应用程序在发生一次 Full GC 时，典型的 GC 日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:26.233+0800: 15578.148: [GC [1 CMS-initial-mark: 6294851K(20971520K)] 6354687K(24746432K), 0.0466580 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]</span><br><span class="line">2018-04-12T13:48:26.280+0800: 15578.195: [CMS-concurrent-mark-start]</span><br><span class="line">2018-04-12T13:48:26.418+0800: 15578.333: [CMS-concurrent-mark: 0.138/0.138 secs] [Times: user=1.01 sys=0.21, real=0.14 secs]</span><br><span class="line">2018-04-12T13:48:26.418+0800: 15578.334: [CMS-concurrent-preclean-start]</span><br><span class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-preclean: 0.056/0.057 secs] [Times: user=0.20 sys=0.12, real=0.06 secs]</span><br><span class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2018-04-12T13:48:29.989+0800: 15581.905: [CMS-concurrent-abortable-preclean: 3.506/3.514 secs] [Times: user=11.93 sys=6.77, real=3.51 secs]</span><br><span class="line">2018-04-12T13:48:29.991+0800: 15581.906: [GC[YG occupancy: 1805641 K (3774912 K)]2018-04-12T13:48:29.991+0800: 15581.906: [GC2018-04-12T13:48:29.991+0800: 15581.906: [ParNew: 1805641K-&gt;48395K(3774912K), 0.0826620 secs] 8100493K-&gt;6348225K(24746432K), 0.0829480 secs] [Times: user=0.81 sys=0.00, real=0.09 secs]2018-04-12T13:48:30.074+0800: 15581.989: [Rescan (parallel) , 0.0429390 secs]2018-04-12T13:48:30.117+0800: 15582.032: [weak refs processing, 0.0027800 secs]2018-04-12T13:48:30.119+0800: 15582.035: [class unloading, 0.0033120 secs]2018-04-12T13:48:30.123+0800: 15582.038: [scrub symbol table, 0.0016780 secs]2018-04-12T13:48:30.124+0800: 15582.040: [scrub string table, 0.0004780 secs] [1 CMS-remark: 6299829K(20971520K)] 6348225K(24746432K), 0.1365130 secs] [Times: user=1.24 sys=0.00, real=0.14 secs]</span><br><span class="line">2018-04-12T13:48:30.128+0800: 15582.043: [CMS-concurrent-sweep-start]</span><br><span class="line">2018-04-12T13:48:36.638+0800: 15588.553: [GC2018-04-12T13:48:36.638+0800: 15588.554: [ParNew: 3403915K-&gt;52142K(3774912K), 0.0874610 secs] 4836483K-&gt;1489601K(24746432K), 0.0877490 secs] [Times: user=0.84 sys=0.00, real=0.09 secs]</span><br><span class="line">2018-04-12T13:48:38.412+0800: 15590.327: [CMS-concurrent-sweep: 8.193/8.284 secs] [Times: user=30.34 sys=16.44, real=8.28 secs]</span><br><span class="line">2018-04-12T13:48:38.419+0800: 15590.334: [CMS-concurrent-reset-start]</span><br><span class="line">2018-04-12T13:48:38.462+0800: 15590.377: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.15 sys=0.10, real=0.04 secs]</span><br></pre></td></tr></table></figure>
<p>CMS Full GC 拆分开来，涉及的阶段比较多，下面分别来介绍各个阶段的情况。</p>
<h3 id="阶段1：Initial-Mark"><a href="#阶段1：Initial-Mark" class="headerlink" title="阶段1：Initial Mark"></a>阶段1：Initial Mark</h3><p>这个是 CMS 两次 stop-the-wolrd 事件的其中一次，这个阶段的目标是：标记那些直接被 GC root 引用或者被年轻代存活对象所引用的所有对象，标记后示例如下所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>）：</p>
<p><img src="/images/java/cms-1.png" alt="CMS 初始标记阶段"></p>
<p>上述例子对应的日志信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:26.233+0800: 15578.148: [GC [1 CMS-initial-mark: 6294851K(20971520K)] 6354687K(24746432K), 0.0466580 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]</span><br></pre></td></tr></table></figure>
<p>逐行介绍上面日志的含义：</p>
<ol>
<li><code>2018-04-12T13:48:26.233+0800: 15578.148</code>：GC 开始的时间，以及相对于 JVM 启动的相对时间（单位是秒，这里大概是4.33h），与前面 ParNew 类似，下面的分析中就直接跳过这个了；</li>
<li><code>CMS-initial-mark</code>：初始标记阶段，它会收集所有 GC Roots 以及其直接引用的对象；</li>
<li><code>6294851K</code>：当前老年代使用的容量，这里是 6G；</li>
<li><code>(20971520K)</code>：老年代可用的最大容量，这里是 20G；</li>
<li><code>6354687K</code>：整个堆目前使用的容量，这里是 6.06G；</li>
<li><code>(24746432K)</code>：堆可用的容量，这里是 23.6G；</li>
<li><code>0.0466580 secs</code>：这个阶段的持续时间；</li>
<li><code>[Times: user=0.04 sys=0.00, real=0.04 secs]</code>：与前面的类似，这里是相应 user、system and real 的时间统计。</li>
</ol>
<h3 id="阶段2：并发标记"><a href="#阶段2：并发标记" class="headerlink" title="阶段2：并发标记"></a>阶段2：并发标记</h3><p>在这个阶段 Garbage Collector 会遍历老年代，然后标记所有存活的对象，它会根据上个阶段找到的 GC Roots 遍历查找。并发标记阶段，它会与用户的应用程序并发运行。并不是老年代所有的存活对象都会被标记，因为在标记期间用户的程序可能会改变一些引用，如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>）：</p>
<p><img src="/images/java/cms-2.png" alt="CMS 并发标记阶段"></p>
<p>在上面的图中，与阶段1的图进行对比，就会发现有一个对象的引用已经发生了变化，这个阶段相应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:26.280+0800: 15578.195: [CMS-concurrent-mark-start]</span><br><span class="line">2018-04-12T13:48:26.418+0800: 15578.333: [CMS-concurrent-mark: 0.138/0.138 secs] [Times: user=1.01 sys=0.21, real=0.14 secs]</span><br></pre></td></tr></table></figure>
<p>这里详细对上面的日志解释，如下所示：</p>
<ol>
<li><code>CMS-concurrent-mark</code>：并发收集阶段，这个阶段会遍历老年代，并标记所有存活的对象；</li>
<li><code>0.138/0.138 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=1.01 sys=0.21, real=0.14 secs]</code>：如前面所示，但是这部的时间，其实意义不大，因为它是从并发标记的开始时间开始计算，这期间因为是并发进行，不仅仅包含 GC 线程的工作。</li>
</ol>
<h3 id="阶段3：Concurrent-Preclean"><a href="#阶段3：Concurrent-Preclean" class="headerlink" title="阶段3：Concurrent Preclean"></a>阶段3：Concurrent Preclean</h3><p>Concurrent Preclean：这也是一个并发阶段，与应用的线程并发运行，并不会 stop 应用的线程。在并发运行的过程中，一些对象的引用可能会发生变化，但是这种情况发生时，JVM 会将包含这个对象的区域（Card）标记为 Dirty，这也就是 Card Marking。如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：</p>
<p><img src="/images/java/cms-3.png" alt="Concurrent Preclean 1"></p>
<p>在pre-clean阶段，那些能够从 Dirty 对象到达的对象也会被标记，这个标记做完之后，dirty card 标记就会被清除了，如下（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：</p>
<p><img src="/images/java/cms-4.png" alt="Concurrent Preclean 2"></p>
<p>这个阶段相应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:26.418+0800: 15578.334: [CMS-concurrent-preclean-start]</span><br><span class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-preclean: 0.056/0.057 secs] [Times: user=0.20 sys=0.12, real=0.06 secs]</span><br></pre></td></tr></table></figure>
<p>其含义为：</p>
<ol>
<li><code>CMS-concurrent-preclean</code>：Concurrent Preclean 阶段，对在前面并发标记阶段中引用发生变化的对象进行标记；</li>
<li><code>0.056/0.057 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=0.20 sys=0.12, real=0.06 secs]</code>：同并发标记阶段中的含义。</li>
</ol>
<h3 id="阶段4：Concurrent-Abortable-Preclean"><a href="#阶段4：Concurrent-Abortable-Preclean" class="headerlink" title="阶段4：Concurrent Abortable Preclean"></a>阶段4：Concurrent Abortable Preclean</h3><p>这也是一个并发阶段，但是同样不会影响影响用户的应用线程，这个阶段是为了尽量承担 STW（stop-the-world）中最终标记阶段的工作。这个阶段持续时间依赖于很多的因素，由于这个阶段是在重复做很多相同的工作，直接满足一些条件（比如：重复迭代的次数、完成的工作量或者时钟时间等）。这个阶段的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">2018-04-12T13:48:29.989+0800: 15581.905: [CMS-concurrent-abortable-preclean: 3.506/3.514 secs] [Times: user=11.93 sys=6.77, real=3.51 secs]</span><br></pre></td></tr></table></figure>
<ol>
<li><code>CMS-concurrent-abortable-preclean</code>：Concurrent Abortable Preclean 阶段；</li>
<li><code>3.506/3.514 secs</code>：这个阶段的持续时间与时钟时间，本质上，这里的 gc 线程会在 STW 之前做更多的工作，通常会持续 5s 左右；</li>
<li><code>[Times: user=11.93 sys=6.77, real=3.51 secs]</code>：同前面。</li>
</ol>
<h3 id="阶段5：Final-Remark"><a href="#阶段5：Final-Remark" class="headerlink" title="阶段5：Final Remark"></a>阶段5：Final Remark</h3><p>这是第二个 STW 阶段，也是 CMS 中的最后一个，这个阶段的目标是标记所有老年代所有的存活对象，由于之前的阶段是并发执行的，gc 线程可能跟不上应用程序的变化，为了完成标记老年代所有存活对象的目标，STW 就非常有必要了。</p>
<p>通常 CMS 的 Final Remark 阶段会在年轻代尽可能干净的时候运行，目的是为了减少连续 STW 发生的可能性（年轻代存活对象过多的话，也会导致老年代涉及的存活对象会很多）。这个阶段会比前面的几个阶段更复杂一些，相关日志如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:29.991+0800: 15581.906: [GC[YG occupancy: 1805641 K (3774912 K)]2018-04-12T13:48:29.991+0800: 15581.906: [GC2018-04-12T13:48:29.991+0800: 15581.906: [ParNew: 1805641K-&gt;48395K(3774912K), 0.0826620 secs] 8100493K-&gt;6348225K(24746432K), 0.0829480 secs] [Times: user=0.81 sys=0.00, real=0.09 secs]2018-04-12T13:48:30.074+0800: 15581.989: [Rescan (parallel) , 0.0429390 secs]2018-04-12T13:48:30.117+0800: 15582.032: [weak refs processing, 0.0027800 secs]2018-04-12T13:48:30.119+0800: 15582.035: [class unloading, 0.0033120 secs]2018-04-12T13:48:30.123+0800: 15582.038: [scrub symbol table, 0.0016780 secs]2018-04-12T13:48:30.124+0800: 15582.040: [scrub string table, 0.0004780 secs] [1 CMS-remark: 6299829K(20971520K)] 6348225K(24746432K), 0.1365130 secs] [Times: user=1.24 sys=0.00, real=0.14 secs]</span><br></pre></td></tr></table></figure>
<p>对上面的日志进行分析：</p>
<ol>
<li><code>YG occupancy: 1805641 K (3774912 K)</code>：年轻代当前占用量及容量，这里分别是 1.71G 和 3.6G；</li>
<li><code>ParNew:...</code>：触发了一次 young GC，这里触发的原因是为了减少年轻代的存活对象，尽量使年轻代更干净一些；</li>
<li><code>[Rescan (parallel) , 0.0429390 secs]</code>：这个 Rescan 是当应用暂停的情况下完成对所有存活对象的标记，这个阶段是并行处理的，这里花费了  0.0429390s；</li>
<li><code>[weak refs processing, 0.0027800 secs]</code>：第一个子阶段，它的工作是处理弱引用；</li>
<li><code>[class unloading, 0.0033120 secs]</code>：第二个子阶段，它的工作是：unloading the unused classes；</li>
<li><code>[scrub symbol table, 0.0016780 secs] ... [scrub string table, 0.0004780 secs]</code>：最后一个子阶段，它的目的是：cleaning up symbol and string tables which hold class-level metadata and internalized string respectively，时钟的暂停也包含在这里；</li>
<li><code>6299829K(20971520K)</code>：这个阶段之后，老年代的使用量与总量，这里分别是 6G 和 20G；</li>
<li><code>6348225K(24746432K)</code>：这个阶段之后，堆的使用量与总量（包括年轻代，年轻代在前面发生过 GC），这里分别是 6.05G 和 23.6G；</li>
<li><code>0.1365130 secs</code>：这个阶段的持续时间；</li>
<li><code>[Times: user=1.24 sys=0.00, real=0.14 secs]</code>：对应的时间信息。</li>
</ol>
<p>经历过这五个阶段之后，老年代所有存活的对象都被标记过了，现在可以通过清除算法去清理那些老年代不再使用的对象。</p>
<h3 id="阶段6：Concurrent-Sweep"><a href="#阶段6：Concurrent-Sweep" class="headerlink" title="阶段6：Concurrent Sweep"></a>阶段6：Concurrent Sweep</h3><p>这里不需要 STW，它是与用户的应用程序并发运行，这个阶段是：清除那些不再使用的对象，回收它们的占用空间为将来使用。如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：<br>）：</p>
<p><img src="/images/java/cms-5.png" alt="CMS Concurrent Sweep 阶段"></p>
<p>这个阶段对应的日志信息如下（这中间又发生了一次 Young GC）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:30.128+0800: 15582.043: [CMS-concurrent-sweep-start]</span><br><span class="line">2018-04-12T13:48:36.638+0800: 15588.553: [GC2018-04-12T13:48:36.638+0800: 15588.554: [ParNew: 3403915K-&gt;52142K(3774912K), 0.0874610 secs] 4836483K-&gt;1489601K(24746432K), 0.0877490 secs] [Times: user=0.84 sys=0.00, real=0.09 secs]</span><br><span class="line">2018-04-12T13:48:38.412+0800: 15590.327: [CMS-concurrent-sweep: 8.193/8.284 secs] [Times: user=30.34 sys=16.44, real=8.28 secs]</span><br></pre></td></tr></table></figure>
<p>分别介绍一下：</p>
<ol>
<li><code>CMS-concurrent-sweep</code>：这个阶段主要是清除那些没有被标记的对象，回收它们的占用空间；</li>
<li><code>8.193/8.284 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=30.34 sys=16.44, real=8.28 secs]</code>：同前面；</li>
</ol>
<h3 id="阶段7：Concurrent-Reset"><a href="#阶段7：Concurrent-Reset" class="headerlink" title="阶段7：Concurrent Reset."></a>阶段7：Concurrent Reset.</h3><p>这个阶段也是并发执行的，它会重设 CMS 内部的数据结构，为下次的 GC 做准备，对应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2018-04-12T13:48:38.419+0800: 15590.334: [CMS-concurrent-reset-start]</span><br><span class="line">2018-04-12T13:48:38.462+0800: 15590.377: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.15 sys=0.10, real=0.04 secs]</span><br></pre></td></tr></table></figure>
<p>日志详情分别如下：</p>
<ol>
<li><code>CMS-concurrent-reset</code>：这个阶段的开始，目的如前面所述；</li>
<li><code>0.044/0.044 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=0.15 sys=0.10, real=0.04 secs]</code>：同前面。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CMS 通过将大量工作分散到并发处理阶段来在减少 STW 时间，在这块做得非常优秀，但是 CMS 也有一些其他的问题：</p>
<ol>
<li>CMS 收集器无法处理浮动垃圾（ Floating Garbage），可能出现 “Concurrnet Mode Failure” 失败而导致另一次 Full GC 的产生，可能引发串行 Full GC；</li>
<li>空间碎片，导致无法分配大对象，CMS 收集器提供了一个 <code>-XX:+UseCMSCompactAtFullCollection</code> 开关参数（默认就是开启的），用于在 CMS 收集器顶不住要进行 Full GC 时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间不得不变长；</li>
<li>对于堆比较大的应用上，GC 的时间难以预估。</li>
</ol>
<p>CMS 的一些缺陷也是 G1 收集器兴起的原因。</p>
<p>参考：</p>
<ul>
<li><a href="https://t.hao0.me/jvm/2016/03/15/jvm-gc-log.html" target="_blank" rel="external">JVM各类GC日志剖析</a>；</li>
<li><a href="http://www.cnblogs.com/zhangxiaoguang/p/5792468.html" target="_blank" rel="external">GC之详解CMS收集过程和日志分析</a>；</li>
<li><a href="http://www.oracle.com/technetwork/tutorials/tutorials-1876574.html" target="_blank" rel="external">Getting Started with the G1 Garbage Collector</a>；</li>
<li><a href="http://ifeve.com/useful-jvm-flags-part-7-cms-collector/" target="_blank" rel="external">JVM实用参数（七）CMS收集器</a>；</li>
<li><a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在两年前的文章 &lt;a href=&quot;http://matt33.com/2016/09/18/jvm-basic2/&quot;&gt;JVM 学习——垃圾收集器与内存分配策略&lt;/a&gt; 中，已经对 GC 算法的原理以及常用的垃圾收集器做了相应的总结。今天这篇文章主要是对生产环境中（Java7
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="jvm" scheme="http://matt33.com/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>HDFS 架构学习总结</title>
    <link href="http://matt33.com/2018/07/15/hdfs-architecture-learn/"/>
    <id>http://matt33.com/2018/07/15/hdfs-architecture-learn/</id>
    <published>2018-07-15T15:46:45.000Z</published>
    <updated>2019-02-24T02:29:01.554Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS（Hadoop Distributed File System）是一个分布式文件存储系统，几乎是离线存储领域的标准解决方案（有能力自研的大厂列外），业内应用非常广泛。近段抽时间，看一下 HDFS 的架构设计，虽然研究生也学习过相关内容，但是现在基本忘得差不多了，今天抽空对这块做了一个简单的总结，也算是再温习了一下这块的内容，这样后续再看 HDFS 方面的文章时，不至于处于懵逼状态。</p>
<h2 id="HDFS-1-0-架构"><a href="#HDFS-1-0-架构" class="headerlink" title="HDFS 1.0 架构"></a>HDFS 1.0 架构</h2><p>HDFS 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点，如下图所示（这个图是 HDFS1.0的架构图，经典的架构图）：</p>
<p><img src="/images/hadoop/hdfs.jpg" alt="HDFS 1.0 架构图"></p>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>NameNode 负责管理整个分布式系统的元数据，主要包括：</p>
<ul>
<li>目录树结构；</li>
<li>文件到数据库 Block 的映射关系；</li>
<li>Block 副本及其存储位置等管理数据；</li>
<li>DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。</li>
</ul>
<p>这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。</p>
<ul>
<li>fsimage：是内存命名空间元数据在外存的镜像文件；</li>
<li>editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。</li>
</ul>
<p>这两个文件相结合可以构造完整的内存数据。</p>
<h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><p>Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。</p>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。</p>
<h3 id="文件写入过程"><a href="#文件写入过程" class="headerlink" title="文件写入过程"></a>文件写入过程</h3><p>Client 向 HDFS 文件写入的过程可以参考<a href="http://shiyanjun.cn/archives/942.html" target="_blank" rel="external">HDFS写文件过程分析</a>，整体过程如下图（这个图比较经典，最开始来自<a href="https://book.douban.com/subject/3220004/" target="_blank" rel="external">《Hadoop：The Definitive Guide》</a>）所示：</p>
<p><img src="/images/hadoop/hdfs-write-flow.png" alt="HDFS 文件写入过程"></p>
<p>具体过程如下：</p>
<ol>
<li>Client 调用 DistributedFileSystem 对象的 <code>create</code> 方法，创建一个文件输出流（FSDataOutputStream）对象；</li>
<li>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</li>
<li>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</li>
<li>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</li>
<li>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</li>
<li>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 <code>close</code> 方法，完成文件写入；</li>
<li>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</li>
</ol>
<h3 id="文件读取过程"><a href="#文件读取过程" class="headerlink" title="文件读取过程"></a>文件读取过程</h3><p>相对于文件写入，文件的读取就简单一些，流程如下图所示：</p>
<p><img src="/images/hadoop/hdfs-read-flow.png" alt="HDFS 文件读取过程"></p>
<p>其具体过程总结如下（简单总结一下）：</p>
<ol>
<li>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</li>
<li>NameNode 返回存储的每个块的 DataNode 列表；</li>
<li>Client 将连接到列表中最近的 DataNode；</li>
<li>Client 开始从 DataNode 并行读取数据；</li>
<li>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</li>
</ol>
<p>在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。</p>
<h2 id="HDFS-1-0-的问题"><a href="#HDFS-1-0-的问题" class="headerlink" title="HDFS 1.0 的问题"></a>HDFS 1.0 的问题</h2><p>在前面的介绍中，关于 HDFS1.0 的架构，首先都会看到 NameNode 的单点问题，这个在生产环境中是非常要命的问题，早期的 HDFS 由于规模较小，有些问题就被隐藏了，但自从进入了移动互联网时代，很多公司都开始进入了 PB 级的大数据时代，HDFS 1.0的设计缺陷已经无法满足生产的需求，最致命的问题有以下两点：</p>
<ul>
<li>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</li>
<li>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。</li>
</ul>
<p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p>
<ol>
<li>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</li>
<li>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。</li>
</ol>
<h2 id="HDFS-2-0-的-HA-实现"><a href="#HDFS-2-0-的-HA-实现" class="headerlink" title="HDFS 2.0 的 HA 实现"></a>HDFS 2.0 的 HA 实现</h2><p>关于 HDFS 高可用方案，非常推荐这篇文章：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a>，IBM 博客的质量确实很高，这部分我这里也是主要根据这篇文章做一个总结，这里会从问题的原因、如何解决的角度去总结，并不会深入源码的实现细节，想有更深入了解还是推荐上面文章。</p>
<p>这里先看下 HDFS 高可用解决方案的架构设计，如下图（下图来自上面的文章）所示：</p>
<p><img src="/images/hadoop/hdfs-ha.png" alt="HDFS HA 架构实现"></p>
<p>这里与前面 1.0 的架构已经有很大变化，简单介绍一下上面的组件：</p>
<ol>
<li>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li>
<li>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；</li>
<li>Zookeeper 集群：为主备切换控制器提供主备选举支持；</li>
<li>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在<strong>确认元数据完全同步之后才能继续对外提供服务</strong>。</li>
<li>DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li>
</ol>
<h3 id="FailoverController"><a href="#FailoverController" class="headerlink" title="FailoverController"></a>FailoverController</h3><p>FC 最初的目的是为了实现 SNN 和 ANN 之间故障自动切换，FC 是独立与 NN 之外的故障切换控制器，ZKFC 作为 NameNode 机器上一个独立的进程启动 ，它启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，其中：</p>
<ol>
<li>HealthMonitor：主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举；</li>
<li>ActiveStandbyElector：主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。</li>
</ol>
<h3 id="自动触发主备选举"><a href="#自动触发主备选举" class="headerlink" title="自动触发主备选举"></a>自动触发主备选举</h3><p>NameNode 在选举成功后，会在 zk 上创建了一个 <code>/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock</code> 节点，而没有选举成功的备 NameNode 会监控这个节点，通过 Watcher 来监听这个节点的状态变化事件，ZKFC 的 ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件（这部分实现跟 Kafka 中 Controller 的选举一样）。</p>
<p>如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点 /hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建 /hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock  节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。</p>
<p>当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。</p>
<h3 id="HDFS-脑裂问题"><a href="#HDFS-脑裂问题" class="headerlink" title="HDFS 脑裂问题"></a>HDFS 脑裂问题</h3><p>在实际中，NameNode 可能会出现这种情况，NameNode 在垃圾回收（GC）时，可能会在长时间内整个系统无响应，因此，也就无法向 zk 写入心跳信息，这样的话可能会导致临时节点掉线，备 NameNode 会切换到 Active 状态，这种情况，可能会导致整个集群会有同时有两个 NameNode，这就是脑裂问题。</p>
<p>脑裂问题的解决方案是隔离（Fencing），主要是在以下三处采用隔离措施：</p>
<ol>
<li>第三方共享存储：任一时刻，只有一个 NN 可以写入；</li>
<li>DataNode：需要保证只有一个 NN 发出与管理数据副本有关的删除命令；</li>
<li>Client：需要保证同一时刻只有一个 NN 能够对 Client 的请求发出正确的响应。</li>
</ol>
<p>关于这个问题目前解决方案的实现如下：</p>
<ol>
<li>ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为 <strong>/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb</strong> 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息；</li>
<li>Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候，会一起删除这个持久节点；</li>
<li>但如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于 /hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来，后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing。</li>
</ol>
<p>在进行 fencing 的时候，会执行以下的操作：</p>
<ol>
<li>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 <code>transitionToStandby</code> 方法，看能不能把它转换为 Standby 状态；</li>
<li>如果 <code>transitionToStandby</code> 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施。</li>
</ol>
<p>Hadoop 目前主要提供两种隔离措施，通常会选择第一种：</p>
<ol>
<li>sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；</li>
<li>shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离。</li>
</ol>
<p>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 <code>becomeActive</code> 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。</p>
<p>NameNode 选举的实现机制与 Kafka 的 Controller 类似，那么 Kafka 是如何避免脑裂问题的呢？</p>
<ol>
<li>Controller 给 Broker 发送的请求中，都会携带 controller epoch 信息，如果 broker 发现当前请求的 epoch 小于缓存中的值，那么就证明这是来自旧 Controller 的请求，就会决绝这个请求，正常情况下是没什么问题的；</li>
<li>但是异常情况下呢？如果 Broker 先收到异常 Controller 的请求进行处理呢？现在看 Kafka 在这一部分并没有适合的方案；</li>
<li>正常情况下，Kafka 新的 Controller 选举出来之后，Controller 会向全局所有 broker 发送一个 metadata 请求，这样全局所有 Broker 都可以知道当前最新的 controller epoch，但是并不能保证可以完全避免上面这个问题，还是有出现这个问题的几率的，只不过非常小，而且即使出现了由于 Kafka 的高可靠架构，影响也非常有限，至少从目前看，这个问题并不是严重的问题。</li>
</ol>
<h3 id="第三方存储（共享存储）"><a href="#第三方存储（共享存储）" class="headerlink" title="第三方存储（共享存储）"></a>第三方存储（共享存储）</h3><p>上述 HA 方案还有一个明显缺点，那就是第三方存储节点有可能失效，之前有很多共享存储的实现方案，目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。</p>
<p>QJM（Quorum Journal Manager）本质上是利用 Paxos 协议来实现的，QJM 在 <code>2F+1</code>  个 JournalNode 上存储 NN 的 editlog，每次写入操作都通过 Paxos 保证写入的一致性，它最多可以允许有 F 个 JournalNode 节点同时故障，其实现如下（图片来自：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a> ）：</p>
<p><img src="/images/hadoop/hdfs-ha-qjm.png" alt="基于 QJM 的共享存储的数据同步机制"></p>
<p>Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog。</p>
<p>还有一点需要注意的是，在 2.0 中不再有 SNN 这个角色了，NameNode 在启动后，会先加载 FSImage 文件和共享目录上的 EditLog Segment 文件，之后 NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式，其中：</p>
<ol>
<li>EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog；</li>
<li>StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。</li>
</ol>
<h2 id="HDFS-2-0-Federation-实现"><a href="#HDFS-2-0-Federation-实现" class="headerlink" title="HDFS 2.0 Federation 实现"></a>HDFS 2.0 Federation 实现</h2><p>在 1.0 中，HDFS 的架构设计有以下缺点：</p>
<ol>
<li>namespace 扩展性差：在单一的 NN 情况下，因为所有 namespace 数据都需要加载到内存，所以物理机内存的大小限制了整个 HDFS 能够容纳文件的最大个数（namespace 指的是 HDFS 中树形目录和文件结构以及文件对应的 block 信息）；</li>
<li>性能可扩展性差：由于所有请求都需要经过 NN，单一 NN 导致所有请求都由一台机器进行处理，很容易达到单台机器的吞吐；</li>
<li>隔离性差：多租户的情况下，单一 NN 的架构无法在租户间进行隔离，会造成不可避免的相互影响。</li>
</ol>
<p>而 Federation 的设计就是为了解决这些问题，采用 Federation 的最主要原因是设计实现简单，而且还能解决问题。</p>
<h3 id="Federation-架构"><a href="#Federation-架构" class="headerlink" title="Federation 架构"></a>Federation 架构</h3><p>Federation 的架构设计如下图所示（图片来自 <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">HDFS Federation</a>）：</p>
<p><img src="/images/hadoop/hdfs-federation.gif" alt="HDFS Federation 架构实现"></p>
<h3 id="Federation-的核心设计思想"><a href="#Federation-的核心设计思想" class="headerlink" title="Federation 的核心设计思想"></a>Federation 的核心设计思想</h3><p>Federation 的核心思想是将一个大的 namespace 划分多个子 namespace，并且每个 namespace 分别由单独的 NameNode 负责，这些 NameNode 之间互相独立，不会影响，不需要做任何协调工作（其实跟拆集群有一些相似），集群的所有 DataNode 会被多个 NameNode 共享。</p>
<p>其中，每个子 namespace 和 DataNode 之间会由数据块管理层作为中介建立映射关系，数据块管理层由若干数据块池（Pool）构成，每个数据块只会唯一属于某个固定的数据块池，而一个子 namespace 可以对应多个数据块池。每个 DataNode 需要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告，并执行来自所有 NameNode 的命令。</p>
<ul>
<li>一个 block pool 由属于同一个 namespace 的数据块组成，每个 DataNode 可能会存储集群中所有 block pool 的数据块；</li>
<li>每个 block pool 内部自治，也就是说各自管理各自的 block，不会与其他 block pool 交流，如果一个 NameNode 挂掉了，不会影响其他 NameNode;</li>
<li>某个 NameNode 上的 namespace 和它对应的 block pool 一起被称为 namespace volume，它是管理的基本单位。当一个 NameNode/namespace 被删除后，其所有 DataNode 上对应的 block pool 也会被删除，当集群升级时，每个 namespace volume 可以作为一个基本单元进行升级。</li>
</ul>
<p>到这里，基本对 HDFS 这部分总结完了，虽然文章的内容基本都来自下面的参考资料，但是自己在总结的过程中，也对 HDFS 的基本架构有一定的了解，后续结合公司 HDFS 团队的 CaseStudy 深入学习这部分的内容，工作中，也慢慢感觉到分布式系统，很多的设计实现与问题解决方案都很类似，只不过因为面对业务场景的不同而采用了不同的实现。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="external">HDFS Architecture</a>;</li>
<li><a href="http://shiyanjun.cn/archives/942.html" target="_blank" rel="external">HDFS 写文件过程分析</a>;</li>
<li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSRouterFederation.html" target="_blank" rel="external">HDFS Router-based Federation</a>；</li>
<li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="external">HDFS High Availability Using the Quorum Journal Manager</a>；</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="external">HDFS Commands Guide</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">HDFS Federation</a>；</li>
<li><a href="http://dongxicheng.org/mapreduce/hdfs-federation-introduction/" target="_blank" rel="external">HDFS Federation设计动机与基本原理</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://tech.meituan.com/namenode-restart-optimization.html" target="_blank" rel="external">HDFS NameNode重启优化</a>；</li>
<li><a href="https://tech.meituan.com/hdfs-federation.html" target="_blank" rel="external">HDFS Federation在美团点评的应用与改进</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS（Hadoop Distributed File System）是一个分布式文件存储系统，几乎是离线存储领域的标准解决方案（有能力自研的大厂列外），业内应用非常广泛。近段抽时间，看一下 HDFS 的架构设计，虽然研究生也学习过相关内容，但是现在基本忘得差不多了，今天
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hadoop" scheme="http://matt33.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Controller Redesign 方案</title>
    <link href="http://matt33.com/2018/07/14/kafka-controller-redesign/"/>
    <id>http://matt33.com/2018/07/14/kafka-controller-redesign/</id>
    <published>2018-07-14T15:13:56.000Z</published>
    <updated>2019-02-24T02:29:01.553Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是一次重构，因此，社区准备在新版的系统里对 Controller 做一些相应的优化（0.11.0及以后的版本），相应的设计方案见：<a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>，本文的内容就是结合这篇文章做一个简单的总结。</p>
<h2 id="Controller-功能"><a href="#Controller-功能" class="headerlink" title="Controller 功能"></a>Controller 功能</h2><p>在一个 Kafka 中，Controller 要处理的事情总结如下表所示：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>详情</th>
</tr>
</thead>
<tbody>
<tr>
<td>cluster metadata updates</td>
<td>producer 或 consumer 可以通过 MetadataRequest 请求从集群任何一台 broker 上查询到某个 Partition 的 metadata 信息，如果一个 Partition 的 leader 或 isr 等信息变化，Controller 会广播到集群的所有 broker 上，这样每台 Broker 都会有该 Partition 的最新 Metadata 信息</td>
</tr>
<tr>
<td>topic creation</td>
<td>用户可以通过多种方式创建一个 topic，最终的结果都是在 zk 的 <code>/brokers/topics</code> 目录下新建一个 topic 节点信息，controller 通过监控这个目录来判断是否有新的 topic 需要创建</td>
</tr>
<tr>
<td>topic deletion</td>
<td>Controller 通过监控 zk 的 <code>/admin/delete_topics</code> 节点来触发 topic 删除操作</td>
</tr>
<tr>
<td>partition reassignment</td>
<td>Controller 通过监控 zk 的 <code>/admin/reassign_partitions</code> 节点来触发 Partition 的副本迁移操作</td>
</tr>
<tr>
<td>preferred replica leader election</td>
<td>Controller 通过监控 zk 的 <code>/admin/preferred_replica_election</code> 节点来触发最优 leader 选举操作，该操作的目的选举 Partition 的第一个 replica 作为 leader</td>
</tr>
<tr>
<td>topic partition expansion</td>
<td>Controller 通过监控 zk 的 <code>/brokers/topics/&lt;topic&gt;</code> 数据内容的变化，来触发 Topic 的 Partition 扩容操作</td>
</tr>
<tr>
<td>broker join</td>
<td>Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 是最新加入的，进而触发 broker 的上线操作</td>
</tr>
<tr>
<td>broker failure</td>
<td>同样，Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 掉线了，进而触发 broker 的下线操作</td>
<td></td>
</tr>
<tr>
<td>controlled shutdown</td>
<td>Controller 通过处理 ControlledShudownRequest 请求来优雅地关闭一个 broker 节点，主动关闭与直接 kill 的区别，它可以减少 Partition 的不可用时间，因为一个 broker 的 zk 临时节点消失是需要一定时间的</td>
</tr>
<tr>
<td>controller leader election</td>
<td>集群中所有 broker 会监听 zk 的 <code>/controller</code> 节点，如果该节点消失，所有的 broker 都回去抢占 controller 节点，抢占成功的，就成了最新的 controller</td>
</tr>
</tbody>
</table>
<h2 id="Controller-目前存在的问题"><a href="#Controller-目前存在的问题" class="headerlink" title="Controller 目前存在的问题"></a>Controller 目前存在的问题</h2><p>之所以要重新设计 Controller，是因为现在的 Controller 积累了一些比较难解决的问题，这些问题解决起来，代码改动量都是巨大的，甚至需要改变 controller 部门的设计，基本就跟重构差不多了，下面我们先来了看一下 controller 之前（主要是 0.11.0 之前的版本）存在的一些问题。</p>
<p>目前遇到的比较大的问题有以下几个：</p>
<ol>
<li>Partition 级别同步 zk 写；</li>
<li>sequential per-partition controller-to-broker requests；</li>
<li>Controller 复杂的并发语义；</li>
<li>代码组织混乱；</li>
<li>控制类请求与数据类请求未分离；</li>
<li>Controller 给 broker 的请求中没有 broker 的 generation信息；</li>
<li>ZkClient 阻碍 Client 的状态管理。</li>
</ol>
<h3 id="Partition-级别同步-zk-写"><a href="#Partition-级别同步-zk-写" class="headerlink" title="Partition 级别同步 zk 写"></a>Partition 级别同步 zk 写</h3><p>zookeeper 的同步写意味着在下次写之前需要等待前面整个过程的结束，而且由于它们都是 partition 粒度的（一个 Partition 一个 Partition 的去执行写操作），对于 Partition 非常多的集群来说，需要等待的时间会更长，Controller 通常会在下面这两个地方做 Partition 级别 zookeeper 同步写操作：</p>
<ol>
<li>PartitionStateMachine 在进行触发 leader 选举（partition 目的状态是 OnlinePartition），将会触发上面的操作；</li>
<li>ReplicaStateMachine 更新 LeaderAndIsr 信息到 zk（replica 状态转变为 OfflineReplica），这种情况也触发这种情况，它既阻碍了 Controller 进程，也有可能会 zk 造成压力。</li>
</ol>
<h3 id="sequential-per-partition-controller-to-broker-requests"><a href="#sequential-per-partition-controller-to-broker-requests" class="headerlink" title="sequential per-partition controller-to-broker requests"></a>sequential per-partition controller-to-broker requests</h3><p>Controller 在向 Broker 发送请求，有些情况下也是 Partition 粒度去发送的，效率非常低，比如在 Controller 处理 broker shutdown 请求时，这里是按 Partition 级别处理，每处理一个 Partition 都会执行 Partition、Replica 状态变化以及 Metadata 更新，并且调用 <code>sendRequestsToBrokers()</code> 向 broker 发送请求，这样的话，效率将变得非常低。</p>
<h3 id="Controller-复杂的并发语义"><a href="#Controller-复杂的并发语义" class="headerlink" title="Controller 复杂的并发语义"></a>Controller 复杂的并发语义</h3><p>Controller 需要在多个线程之间共享状态信息，这些线程有：</p>
<ol>
<li>IO threads handling controlled shutdown requests</li>
<li>The ZkClient org.I0Itec.zkclient.ZkEventThread processing zookeeper callbacks sequentially；</li>
<li>The TopicDeletionManager kafka.controller.DeleteTopicsThread；</li>
<li>Per-broker RequestSendThread within ControllerChannelManager.</li>
</ol>
<p>所有这些线程都需要访问或修改状态信息（ControllerContext），现在它们是通过 ControllerContext 的 controllerLock（排它锁）实现的，Controller 的并发变得虚弱无力。</p>
<h3 id="代码组织混乱"><a href="#代码组织混乱" class="headerlink" title="代码组织混乱"></a>代码组织混乱</h3><p>KafkaController 部分的代码组织（KafkaController、PartitionStateMachine 和 ReplicaStateMachine）不是很清晰，比如，下面的问题就很难回答：</p>
<ol>
<li>where and when does zookeeper get updated?</li>
<li>where and when does a controller-to-broker request get formed?</li>
<li>what impact does a failing zookeeper update or controller-to-broker request have on the cluster state?</li>
</ol>
<p>这也导致了这部分很多开发者不敢轻易去改动。</p>
<h3 id="控制类请求与数据类请求未分离"><a href="#控制类请求与数据类请求未分离" class="headerlink" title="控制类请求与数据类请求未分离"></a>控制类请求与数据类请求未分离</h3><p>现在 broker 收到的请求，有来自 client、broker 和 controller 的请求，这些请求都会被放到同一个 requestQueue 中，它们有着同样的优先级，所以来自 client 的请求很可能会影响来自 controller 请求的处理（如果是 leader 变动的请求，ack 设置的不是 all，这种情况有可能会导致数据丢失）。</p>
<h3 id="Controller-给-broker-的请求中没有-broker-的-generation信息"><a href="#Controller-给-broker-的请求中没有-broker-的-generation信息" class="headerlink" title="Controller 给 broker 的请求中没有 broker 的 generation信息"></a>Controller 给 broker 的请求中没有 broker 的 generation信息</h3><p>这里的 Broker generation 代表着一个标识，每当它重新加入集群时，这个标识都会变化。如果 Controller 的请求没有这个信息的话，可能会导致一个重启的 Broker 收到之前的请求，让 Broker 进入到一个错误的状态。</p>
<p>比如，Broker 收到之前的 StopReplica 请求，可能会导致副本同步线程退出。</p>
<h3 id="ZkClient-阻碍-Client-的状态管理"><a href="#ZkClient-阻碍-Client-的状态管理" class="headerlink" title="ZkClient 阻碍 Client 的状态管理"></a>ZkClient 阻碍 Client 的状态管理</h3><p>这里的状态管理指的是当 Client 发生重连或会话过期时，Client 可以监控这种状态变化，并做出一些处理，因为开源版的 ZKClient 在处理 notification 时，是线性处理的，一些 notification 会被先放到 ZkEventThread’s queue 中，这样会导致一些最新的 notification 不能及时被处理，特别是与 zk 连接断开重连的情况。</p>
<h2 id="Controller-改进方案"><a href="#Controller-改进方案" class="headerlink" title="Controller 改进方案"></a>Controller 改进方案</h2><p>关于上述问题，Kafka 提出了一些改进方案，有些已经在最新版的系统中实现，有的还在规划中。</p>
<h3 id="使用异步的-zk-api"><a href="#使用异步的-zk-api" class="headerlink" title="使用异步的 zk api"></a>使用异步的 zk api</h3><p>Zookeeper 的 client 提供三种执行请求的方式：</p>
<ol>
<li>同步调用，意味着下次请求需要等待当前当前请求的完成；</li>
<li>异步调用，意味着不需要等待当前请求的完成就可以开始下次请求的执行，并且我们可以通过回调机制去处理请求返回的结果；</li>
<li>单请求的 batch 调用，意味着 batch 内的所有请求都会在一次事务处理中完成，这里需要关注的是 zookeeper 的 server 对单请求的大小是有限制的（jute.maxbuffer）。</li>
</ol>
<p>文章中给出了三种请求的测试结果，Kafka 最后选取的是异步处理机制，因为对于单请求处理，异步处理更加简洁，并且相比于同步处理还可以保持一个更好的写性能。</p>
<h3 id="improve-controller-to-broker-request-batching"><a href="#improve-controller-to-broker-request-batching" class="headerlink" title="improve controller-to-broker request batching"></a>improve controller-to-broker request batching</h3><p>这个在设计文档还是 TODO 状态，具体的方案还没确定，不过基本可以猜测一下，因为目的是提高 batch 发送能力，那么只能是在调用对每个 broker 的 RequestSenderThread 线程发送请求之前，做一下检测，而不是来一个请求立马就发送，这是一个性能与时间的权衡，如果不是立马发送请求，那么可能会带来 broker 短时 metadata 信息的不一致，这个不一致时间不同的应用场景要求是不一样的。</p>
<h3 id="单线程的事件处理模型"><a href="#单线程的事件处理模型" class="headerlink" title="单线程的事件处理模型"></a>单线程的事件处理模型</h3><p>采用单线程的时间处理模型将极大简化 Controller 的并发实现，只允许这个线程访问和修改 Controller 的本地状态信息，因此在 Controller 部分也就不需要到处加锁来保证线程安全了。</p>
<p>目前 1.1.0 的实现中，Controller 使用了一个 ControllerEventThread 线程来处理所有的 event，目前可以支持13种不同类型事件：</p>
<ol>
<li>Idle：代表当前 ControllerEventThread 处理空闲状态；</li>
<li>ControllerChange：Controller 切换处理；</li>
<li>BrokerChange：Broker 变动处理，broker 可能有上线或掉线；</li>
<li>TopicChange：Topic 新增处理；</li>
<li>TopicDeletion：Topic 删除处理；</li>
<li>PartitionReassignment：Partition 副本迁移处理；</li>
<li>AutoLeaderBalance：自动 rebalance 处理；</li>
<li>ManualLeaderBalance：最优 leader 选举处理，这里叫做手动 rebalance，手动去切流量；</li>
<li>ControlledShutdown：优雅关闭 broker；</li>
<li>IsrChange：Isr 变动处理；</li>
<li>LeaderAndIsrResponseReceived；</li>
<li>LogDirChange：Broker 某个目录失败后的处理（比如磁盘坏掉等）；</li>
<li>ControllerShutdown：ControllerEventThread 处理这个事件时，会关闭当前线程。</li>
</ol>
<h3 id="重构集群状态管理"><a href="#重构集群状态管理" class="headerlink" title="重构集群状态管理"></a>重构集群状态管理</h3><p>这部分的改动，目前社区也没有一个很好的解决思路，重构这部分的目的是希望 Partition、Replica 的状态管理变得更清晰一些，让我们从代码中可以清楚地明白状态是在什么时间、什么地方、什么条件下被触发的。这个优化其实是跟上面那个有很大关联，采用单线程的事件处理模型，可以让状态管理也变得更清晰。</p>
<h4 id="prioritize-controller-requests"><a href="#prioritize-controller-requests" class="headerlink" title="prioritize controller requests"></a>prioritize controller requests</h4><p>我们想要把控制类请求与数据类请求分开，提高 controller 请求的优先级，这样的话即使 Broker 中请求有堆积，Broker 也会优先处理控制类的请求。</p>
<p>这部分的优化可以在网络层的 RequestChannel 中做，RequestChannel 可以根据请求的 id 信息把请求分为正常的和优先的，如果请求是 UpdateMetadataRequest、LeaderAndIsrRequest 或者 StopReplicaRequest，那么这个请求的优先级应该提高。实现方案有以下两种：</p>
<ol>
<li>在请求队列中增加一个优先级队列，优先级高的请求放到 the prioritized request queue 中，优先级低的放到普通请求队列中，但是无论使用一个定时拉取（poll）还是2个定时拉取，都会带来其他的问题，要么是增大普通请求的处理延迟，要么是增大了优先级高请求的延迟；</li>
<li>直接使用优先级队列代替现在的普通队列，设计上更倾向与这一种。</li>
</ol>
<p>目前这部分在1.1.0中还未实现。</p>
<h3 id="Controller-发送请求中添加-broker-的-generation-信息"><a href="#Controller-发送请求中添加-broker-的-generation-信息" class="headerlink" title="Controller 发送请求中添加 broker 的 generation 信息"></a>Controller 发送请求中添加 broker 的 generation 信息</h3><p>generation 信息是用来标识当前 broker 加入集群 epoch 信息，每当 broker 重新加入集群中，该 broker.id 对应的 generation 都应该变化（要求递增），目前有两种实现方案：</p>
<ol>
<li>为 broker 分配的一个全局唯一的 id，由 controller 广播给其他 broker；</li>
<li>直接使用 zookeeper 的 zxid 信息（broker.id 注册时的 zxid）。</li>
</ol>
<h3 id="直接使用原生的-Zookeeper-client"><a href="#直接使用原生的-Zookeeper-client" class="headerlink" title="直接使用原生的 Zookeeper client"></a>直接使用原生的 Zookeeper client</h3><p>Client 端的状态管理意味着当 Client 端发生状态变化（像连接中断或回话超时）时，我们有能力做一些操作。其中，zookeeper client 有效的状态（目前的 client 比下面又多了几种状态，这里先不深入）是:</p>
<ul>
<li>NOT_CONNECTED： the initial state of the client；</li>
<li>CONNECTING： the client is establishing a connection to zookeeper；</li>
<li>CONNECTED： the client has established a connection and session to zookeeper；</li>
<li>CLOSED： the session has closed or expired。</li>
</ul>
<p>有效的状态转移是：</p>
<ul>
<li>NOT_CONNECTED &gt; CONNECTING</li>
<li>CONNECTING &gt; CONNECTED</li>
<li>CONNECTING &gt; CLOSED</li>
<li>CONNECTED &gt; CONNECTING</li>
<li>CONNECTED &gt; CLOSED</li>
</ul>
<p>最开始的设想是直接使用原生 Client 的异步调用方式，这样的话依然可以通过回调方法监控到状态的变化（像连接中断或回话超时），同样，在每次事件处理时，可以通过检查状态信息来监控到 Client 状态的变化，及时做一些处理。</p>
<p>当一个 Client 接收到连接中断的 notification（Client 状态变成了 CONNECTING 状态），它意味着 Client 不能再从 zookeeper 接收到任何 notification 了。如果断开连接，对于 Controller 而言，无论它现在正在做什么它都应该先暂停，因为可能集群的 Controller 已经切换到其他机器上了，只是它还没接收到通知，它如果还在工作，可能会导致集群状态不一致。当连接断开后，Client 可以重新建立连接（re-establish，状态变为 CONNECTED）或者会话过期（状态变为 CLOSED，会话过期是由 zookeeper Server 来决定的）。如果变成了 CONNECTED 状态，Controller 应该重新开始这些暂停的操作，而如果状态变成了 CLOSED 状态，旧的 Controller 就会知道它不再是 controller，应该丢弃掉这些任务。</p>
<p>参考：</p>
<ul>
<li><a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>；</li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6980045.html" target="_blank" rel="external">Kafka controller重设计</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统的一致性协议之 2PC 和 3PC</title>
    <link href="http://matt33.com/2018/07/08/distribute-system-consistency-protocol/"/>
    <id>http://matt33.com/2018/07/08/distribute-system-consistency-protocol/</id>
    <published>2018-07-08T15:21:34.000Z</published>
    <updated>2019-02-24T02:29:01.553Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，最多只能满足其中两个。但在实际中，由于网络环境是不可信的，所以分区容忍性几乎是必不可选的，设计者基本就是在一致性和可用性之间做选择，当然大部分情况下，大家都会选择牺牲一部分的一致性来保证可用性（可用性较差的系统非常影响用户体验的，但是对另一些场景，比如支付场景，强一致性是必须要满足）。但是分布式系统又无法彻底放弃一致性（Consistency），如果真的放弃一致性，那么就说明这个系统中的数据根本不可信，数据也就没有意义，那么这个系统也就没有任何价值可言。</p>
<h2 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a>CAP 理论</h2><p>CAP 理论三个特性的详细含义如下：</p>
<ol>
<li>一致性（Consistency）：每次读取要么是最新的数据，要么是一个错误；</li>
<li>可用性（Availability）：client 在任何时刻的读写操作都能在限定的延迟内完成的，即每次请求都能获得一个响应（非错误），但不保证是最新的数据；</li>
<li>分区容忍性（Partition tolerance）：在大规模分布式系统中，网络分区现象，即分区间的机器无法进行网络通信的情况是必然会发生的，系统应该能保证在这种情况下可以正常工作。</li>
</ol>
<h3 id="分区容忍性"><a href="#分区容忍性" class="headerlink" title="分区容忍性"></a>分区容忍性</h3><p>很多人可能对分区容忍性不太理解，知乎有一个回答对这个解释的比较清楚（<a href="https://www.zhihu.com/question/54105974" target="_blank" rel="external">CAP理论中的P到底是个什么意思？</a>），这里引用一下：</p>
<ul>
<li>一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。</li>
<li>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</li>
<li>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里，容忍性就提高了。</li>
<li>然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。</li>
<li>要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</li>
<li>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</li>
</ul>
<h3 id="CAP-如何选择"><a href="#CAP-如何选择" class="headerlink" title="CAP 如何选择"></a>CAP 如何选择</h3><p>CAP 理论一个经典原理如下所示：</p>
<p><img src="/images/distribute/CAP.png" alt="CAP 理论原理"></p>
<p>CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。但是，对于大多数互联网应用来说，因为规模比较大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。但是对于一些金融相关行业，它有很多场景需要确保一致性，这种情况通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p>
<p>在一个分布式系统中，对于这三个特性，我们只能三选二，无法同时满足这三个特性，三选二的组合以及这样系统的特点总结如下（来自<a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>）：</p>
<ul>
<li>CA (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li>
<li>CP (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p>对于分布式系统分区容忍性是天然具备的要求，否则一旦出现网络分区，系统就拒绝所有写入只允许可读，这对大部分的场景是不可接收的，因此，在设计分布式系统时，更多的情况下是选举 CP 还是 AP，要么选择强一致性弱可用性，要么选择高可用性容忍弱一致性。</p>
<h3 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h3><p>关于分布式系统的一致性模型有以下几种：</p>
<h4 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h4><p>当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值，直到这个数据被其他数据更新为止。</p>
<p>但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。</p>
<h4 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a>弱一致性</h4><p>系统并不保证进程或者线程的访问都会返回最新更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。</p>
<h4 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h4><p>最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一个时间，在这个时间之后可以保证这一点，而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为「不一致窗口」。不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。</p>
<p>最终一致性在实际应用中又有多种变种：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>因果一致性</td>
<td>如果 A 进程在更新之后向 B 进程通知更新的完成，那么 B 的访问操作将会返回更新的值。而没有因果关系的 C 进程将会遵循最终一致性的规则（C 在不一致窗口内还是看到是旧值）。</td>
</tr>
<tr>
<td>读你所写一致性</td>
<td>因果一致性的特定形式。一个进程进行数据更新后，会给自己发送一条通知，该进程后续的操作都会以最新值作为基础，而其他的进程还是只能在不一致窗口之后才能看到最新值。</td>
</tr>
<tr>
<td>会话一致性</td>
<td>读你所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程可以读取到最新之，但如果会话终止，重新连接后，如果此时还在不一致窗口内，还是可嫩读取到旧值。</td>
</tr>
<tr>
<td>单调读一致性</td>
<td>如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。</td>
</tr>
<tr>
<td>单调写一致性</td>
<td>系统保证对同一个进程的写操作串行化。</td>
</tr>
</tbody>
</table>
<p>它们的关系又如下图所示（图来自 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>）：</p>
<p><img src="/images/distribute/consistency.png" alt="一致性模型之间关系"></p>
<h2 id="分布式一致性协议"><a href="#分布式一致性协议" class="headerlink" title="分布式一致性协议"></a>分布式一致性协议</h2><p>为了解决分布式系统的一致性问题，在长期的研究探索过程中，业内涌现出了一大批经典的一致性协议和算法，其中比较著名的有二阶段提交协议（2PC），三阶段提交协议（3PC）和 Paxos 算法（本文暂时先不介绍）。</p>
<p>Google 2009年 在<a href="https://snarfed.org/transactions_across_datacenters_io.html" target="_blank" rel="external">Transaction Across DataCenter</a> 的分享中，对一致性协议在业内的实践做了一简单的总结，如下图所示，这是 CAP 理论在工业界应用的实践经验。</p>
<p><img src="/images/distribute/cap-sumarry.png" alt="CAP 理论在工业界的实践"></p>
<p>其中，第一行表头代表了分布式系统中通用的一致性方案，包括冷备、Master/Slave、Master/Master、两阶段提交以及基于 Paxos 算法的解决方案，第一列表头代表了分布式系统大家所关心的各项指标，包括一致性、事务支持程度、数据延迟、系统吞吐量、数据丢失可能性、故障自动恢复方式。</p>
<h2 id="两阶段提交协议（2PC）"><a href="#两阶段提交协议（2PC）" class="headerlink" title="两阶段提交协议（2PC）"></a>两阶段提交协议（2PC）</h2><p>二阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现 ACID 的原子性（A）。在数据一致性中，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来保证数据的强一致性。</p>
<p>2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为<strong>协调者</strong>的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</p>
<h3 id="2PC-过程"><a href="#2PC-过程" class="headerlink" title="2PC 过程"></a>2PC 过程</h3><p>关于两阶段提交的过程如下图所示：</p>
<p><img src="/images/distribute/2pc_process.png" alt="两阶段提交过程"></p>
<p>顾名思义，2PC 分为两个过程：</p>
<ol>
<li>表决阶段：此时 Coordinator （协调者）向所有的参与者发送一个 vote request，参与者在收到这请求后，如果准备好了就会向 Coordinator 发送一个 <code>VOTE_COMMIT</code> 消息作为回应，告知 Coordinator 自己已经做好了准备，否则会返回一个 <code>VOTE_ABORT</code> 消息；</li>
<li>提交阶段：Coordinator 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么 Coordinator 就会发送 <code>GLOBAL_COMMIT</code> 消息，否则发送 <code>GLOBAL_ABORT</code> 消息；对于参与者而言，如果收到 <code>GLOBAL_COMMIT</code> 消息，就会提交本地事务，否则就会取消本地事务。</li>
</ol>
<h3 id="2PC-一致性问题"><a href="#2PC-一致性问题" class="headerlink" title="2PC 一致性问题"></a>2PC 一致性问题</h3><p>这里先讨论一下，2PC 是否可以在任何情况下都可以解决一致性问题，在实际的网络生产中，各种情况都有可能发生，这里，我们先从理论上分析各种意外情况。</p>
<p>2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 挂了，参与者没挂</td>
<td>这种情况其实比较好解决，只要找一个 Coordinator 的替代者。当他成为新的 Coordinator 的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。</td>
</tr>
<tr>
<td>参与者挂了（无法恢复），Coordinator 没挂</td>
<td>如果挂了之后没有恢复，那么是不会导致数据一致性问题。</td>
</tr>
<tr>
<td>参与者挂了（后来恢复），Coordinator 没挂</td>
<td>恢复后参与者如果发现有未执行完的事务操作，直接取消，然后再询问 Coordinator 目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。</td>
</tr>
</tbody>
</table>
<p>还有一种情况是：参与者挂了，Coordinator 也挂了，需要再细分为几种类型来讨论：</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 和参与者在第一阶段挂了</td>
<td>由于这时还没有执行 commit 操作，新选出来的 Coordinator 可以询问各个参与者的情况，再决定是进行 commit 还是 roolback。因为还没有 commit，所以不会导致数据一致性问题。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，但是挂的这个参与者在挂之前还没有做相关操作</td>
<td>这种情况下，当新的 Coordinator 被选出来之后，他同样是询问所有参与者的情况。只要有机器执行了 abort（roolback）操作或者第一阶段返回的信息是 No 的话，那就直接执行 roolback 操作。如果没有人执行 abort 操作，但是有机器执行了 commit 操作，那么就直接执行 commit 操作。这样，当挂掉的参与者恢复之后，只要按照 Coordinator 的指示进行事务的 commit 还是 roolback 操作就可以了。因为挂掉的机器并没有做 commit 或者 roolback 操作，而没有挂掉的机器们和新的 Coordinator 又执行了同样的操作，那么这种情况不会导致数据不一致现象。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，挂的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。</td>
<td>这种情况下，新的 Coordinator 被选出来之后，如果他想负起 Coordinator 的责任的话他就只能按照之前那种情况来执行 commit 或者 roolback 操作。这样新的 Coordinator 和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了 commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他已经执行完了之前的事务，如果他执行的是 commit 那还好，和其他的机器保持一致了，万一他执行的是 roolback 操作呢？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和 Coordinator 通信，再想办法把数据搞成一致的，但是，这段时间内他的数据状态已经是不一致的了！</td>
</tr>
</tbody>
</table>
<p>所以，2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。为了解决这个问题，衍生除了3PC。</p>
<h3 id="2PC-优缺点"><a href="#2PC-优缺点" class="headerlink" title="2PC 优缺点"></a>2PC 优缺点</h3><p>简单总结一下 2PC 的优缺点：</p>
<ul>
<li>优点：原理简洁清晰、实现方便；</li>
<li>缺点：同步阻塞、单点问题、某些情况可能导致数据不一致。</li>
</ul>
<p>关于这几个缺点，在实际应用中，都是对2PC 做了相应的改造：</p>
<ol>
<li>同步阻塞：2PC 有几个过程（比如 Coordinator 等待所有参与者表决的过程中）都是同步阻塞的，在实际的应用中，这可能会导致长阻塞问题，这个问题是通过超时判断机制来解决的，但并不能完全解决同步阻塞问题；</li>
<li>Coordinator 单点问题：实际生产应用中，Coordinator 都会有相应的备选节点；</li>
<li>数据不一致：这个在前面已经讲述过了，如果在第二阶段，Coordinator 和参与者都出现挂掉的情况下，是有可能导致数据不一致的。</li>
</ol>
<h2 id="三阶段提交协议（3PC）"><a href="#三阶段提交协议（3PC）" class="headerlink" title="三阶段提交协议（3PC）"></a>三阶段提交协议（3PC）</h2><p>三阶段提交协议（Three-Phase Commit， 3PC）最关键要解决的就是 Coordinator 和参与者同时挂掉导致数据不一致的问题，所以 3PC 把在 2PC 中又添加一个阶段，这样三阶段提交就有：CanCommit、PreCommit 和 DoCommit 三个阶段。</p>
<h3 id="3PC-过程"><a href="#3PC-过程" class="headerlink" title="3PC 过程"></a>3PC 过程</h3><p>三阶段提交协议的过程如下图（图来自 <a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>）所示：</p>
<p><img src="/images/distribute/Three-phase_commit_diagram.png" alt="三节点提交过程"></p>
<p>3PC 的详细过程如下（这个过程步骤内容来自 <a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>）：</p>
<h4 id="阶段一-CanCommit"><a href="#阶段一-CanCommit" class="headerlink" title="阶段一 CanCommit"></a>阶段一 CanCommit</h4><ol>
<li>事务询问：Coordinator 向各参与者发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；</li>
<li>参与者向 Coordinator 反馈询问的响应：参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No。</li>
</ol>
<h4 id="阶段二-PreCommit"><a href="#阶段二-PreCommit" class="headerlink" title="阶段二 PreCommit"></a>阶段二 PreCommit</h4><p><strong>执行事务预提交</strong>：如果 Coordinator 接收到各参与者反馈都是Yes，那么执行事务预提交：</p>
<ol>
<li>发送预提交请求：Coordinator 向各参与者发送 preCommit 请求，并进入 prepared 阶段；</li>
<li>事务预提交：参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；</li>
<li>各参与者向 Coordinator 反馈事务执行的响应：如果各参与者都成功执行了事务操作，那么反馈给协调者 ACK 响应，同时等待最终指令，提交 commit 或者终止 abort，结束流程；</li>
</ol>
<p><strong>中断事务</strong>：如果任何一个参与者向 Coordinator 反馈了 No 响应，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务。</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者发送 abort 请求；</li>
<li>中断事务：无论是收到来自 Coordinator 的 abort 请求，还是等待超时，参与者都中断事务。</li>
</ol>
<h4 id="阶段三-doCommit"><a href="#阶段三-doCommit" class="headerlink" title="阶段三 doCommit"></a>阶段三 doCommit</h4><p><strong>执行提交</strong></p>
<ol>
<li>发送提交请求：假设 Coordinator 正常工作，接收到了所有参与者的 ack 响应，那么它将从预提交阶段进入提交状态，并向所有参与者发送 doCommit 请求；</li>
<li>事务提交：参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；</li>
<li>反馈事务提交结果：参与者完成事务提交后，向 Coordinator 发送 ACK 信息；</li>
<li>完成事务：Coordinator 接收到所有参与者 ack 信息，完成事务。</li>
</ol>
<p><strong>中断事务</strong>：假设 Coordinator 正常工作，并且有任一参与者反馈 No，或者在等待超时后无法接收所有参与者的反馈，都会中断事务</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者节点发送 abort 请求；</li>
<li>事务回滚：参与者接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资源；</li>
<li>反馈事务回滚结果：参与者在完成事务回滚之后，向 Coordinator 发送 ack 信息；</li>
<li>中断事务：Coordinator 接收到所有参与者反馈的 ack 信息后，中断事务。</li>
</ol>
<h3 id="3PC-分析"><a href="#3PC-分析" class="headerlink" title="3PC 分析"></a>3PC 分析</h3><p>3PC 虽然解决了 Coordinator 与参与者都异常情况下导致数据不一致的问题，3PC 依然带来其他问题：比如，网络分区问题，在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit。</p>
<p>而且由于3PC 的设计过于复杂，在解决2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。</p>
<p>参考：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external">维基百科：二阶段提交</a>；</li>
<li><a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>；</li>
<li><a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>；</li>
<li><a href="http://www.hollischuang.com/archives/663" target="_blank" rel="external">关于分布式一致性的探究</a>；</li>
<li><a href="http://www.hollischuang.com/archives/681" target="_blank" rel="external">关于分布式事务、两阶段提交协议、三阶提交协议</a>；</li>
<li><a href="http://www.hollischuang.com/archives/1580" target="_blank" rel="external">深入理解分布式系统的2PC和3PC</a>；</li>
<li><a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://coolshell.cn/articles/10910.html" target="_blank" rel="external">分布式系统的事务处理</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="分布式系统" scheme="http://matt33.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Java 守护线程</title>
    <link href="http://matt33.com/2018/07/07/java-daemon-thread/"/>
    <id>http://matt33.com/2018/07/07/java-daemon-thread/</id>
    <published>2018-07-07T13:43:21.000Z</published>
    <updated>2019-02-24T02:29:01.553Z</updated>
    
    <content type="html"><![CDATA[<p>在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 <code>setDaemon()</code> 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了<strong>守护线程(daemon thread)</strong>，那么什么是守护线程呢？或者说守护线程与非守护线程（普通线程）的区别在什么地方呢？这个就是本文主要讲述的内容。</p>
<h2 id="守护线程"><a href="#守护线程" class="headerlink" title="守护线程"></a>守护线程</h2><p>一般来说，Java 中的线程可以分为两种：守护线程和普通线程。在 JVM 刚启动时，它创建的所有线程，除了主线程（main thread）外，其他的线程都是守护线程（比如：垃圾收集器、以及其他执行辅助操作的线程）。</p>
<p>当创建一个新线程时，新线程将会继承它线程的守护状态，默认情况下，主线程创建的所有线程都是普通线程。</p>
<p>什么情况下会需要守护线程呢？一般情况下是，当我们希望创建一个线程来执行一些辅助的工作，但是又不希望这个线程阻碍 JVM 的关闭，在这种情况下，我们就需要使用守护线程了。</p>
<h2 id="守护线程的作用"><a href="#守护线程的作用" class="headerlink" title="守护线程的作用"></a>守护线程的作用</h2><p>守护线程与普通线程唯一的区别是：当线程退出时，JVM 会检查其他正在运行的线程，如果这些线程都是守护线程，那么 JVM 会正常退出操作，但是如果有普通线程还在运行，JVM 是不会执行退出操作的。当 JVM 退出时，所有仍然存在的守护线程都将被抛弃，既不会执行 finally 部分的代码，也不会执行 stack unwound 操作，JVM 会直接退出。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">When the JVM halts any remaining daemon threads are abandoned:</span><br><span class="line"></span><br><span class="line"> 1. finally blocks are not executed,</span><br><span class="line"> 2. stacks are not unwound - the JVM just exits.</span><br></pre></td></tr></table></figure>
<p>下面有个小示例，来自 <a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DaemonTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> WorkerThread().start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">7500</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            <span class="comment">// handle here exception</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Main Thread ending"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WorkerThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// When false, (i.e. when it's a user thread), the Worker thread continues to run.</span></span><br><span class="line">        <span class="comment">// When true, (i.e. when it's a daemon thread), the Worker thread terminates when the main thread terminates.</span></span><br><span class="line">        setDaemon(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"Hello from Worker "</span> + count++);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                sleep(<span class="number">5000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                <span class="comment">// handle exception here</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当为普通线程时，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hello from Worker 0</span><br><span class="line">Hello from Worker 1</span><br><span class="line">Main Thread ending</span><br><span class="line">Hello from Worker 2</span><br><span class="line">Hello from Worker 3</span><br><span class="line">Hello from Worker 4</span><br><span class="line">Hello from Worker 5</span><br><span class="line">....</span><br></pre></td></tr></table></figure>
<p>也就是说，此时即使主线程执行完了，JVM 也会等待 WorkerThread 执行完毕才会退出，而如果将该线程设置守护线程的话，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Hello from Worker 0</span><br><span class="line">Hello from Worker 1</span><br><span class="line">Main Thread ending</span><br></pre></td></tr></table></figure>
<p>在 main 线程执行完毕后，JVM 进程就退出了，不会 care WorkerThread 线程是否执行完毕。</p>
<p>参考：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>;</li>
<li><a href="http://www.javaconcurrencyinpractice.com/" target="_blank" rel="external">《Java 并发编程实战》</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 &lt;code&gt;setDaemon()&lt;/code&gt; 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了&lt;stron
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="java" scheme="http://matt33.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 1+N+M 网络处理模型（二十三）</title>
    <link href="http://matt33.com/2018/06/27/kafka-server-process-model/"/>
    <id>http://matt33.com/2018/06/27/kafka-server-process-model/</id>
    <published>2018-06-27T15:18:01.000Z</published>
    <updated>2019-02-24T02:29:01.553Z</updated>
    
    <content type="html"><![CDATA[<p>前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实现，如果还有印象，就会知道它们都是通过 KafkaApis 对象处理的，但是前面并没有详细讲述 Server 端是如何监听到相应的请求、请求是如何交给 KafkaApis 对象进行处理，以及处理后是如何返回给请求者（请求者可以是 client 也可以是 server），这些都属于 Server 的网络处理模型，也是本文讲述的主要内容。</p>
<h2 id="Server-网络模型整体流程"><a href="#Server-网络模型整体流程" class="headerlink" title="Server 网络模型整体流程"></a>Server 网络模型整体流程</h2><p>Kafka Server 启动后，会通过 KafkaServer 的 <code>startup()</code> 方法初始化涉及到网络模型的相关对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>()&#123;</span><br><span class="line">  <span class="comment">//note: socketServer</span></span><br><span class="line">  socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</span><br><span class="line">  socketServer.startup()</span><br><span class="line">  <span class="comment">//<span class="doctag">NOTE:</span> 初始化 KafkaApis 实例,每个 Server 只会启动一个线程</span></span><br><span class="line">  apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</span><br><span class="line">    kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</span><br><span class="line">    clusterId, time)</span><br><span class="line"></span><br><span class="line">  requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</span><br><span class="line">    config.numIoThreads)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Kafka Server 在启动时会初始化 SocketServer、KafkaApis 和 KafkaRequestHandlerPool 对象，这也是 Server 网络处理模型的主要组成部分。Kafka Server 的网络处理模型也是基于 Java NIO 机制实现的，实现模式与 Reactor 模式类似，其完整的处理流程图如下所示：</p>
<p><img src="/images/kafka/server-nio.png" alt="Kafka Server 1+N+M 网络处理模型"></p>
<p>上图如果现在不理解，并不要紧，这里先简单介绍一些，讲述一下整体的流程，本节下面会结合 Kafka 的代码详细来讲述图中的过程。上图的网络模型可以简要总结为以下三个重要组成部分：</p>
<ol>
<li>1 个 Acceptor 线程，负责监听 Socket 新的连接请求，注册了 <code>OP_ACCEPT</code> 事件，将新的连接按照 round robin 方式交给对应的 Processor 线程处理；</li>
<li>N 个 Processor 线程，其中每个 Processor 都有自己的 selector，它会向 Acceptor 分配的 SocketChannel 注册相应的 <code>OP_READ</code> 事件，N 的大小由 <code>num.networker.threads</code> 决定；</li>
<li>M 个 KafkaRequestHandler  线程处理请求，并将处理的结果返回给 Processor 线程对应的 response queue 中，由 Processor 将处理的结果返回给相应的请求发送者，M 的大小由 <code>num.io.threads</code> 来决定。</li>
</ol>
<p>上图展示的整体的处理流程如下所示：</p>
<ol>
<li>Acceptor 监听到来自请求者（请求者可以是来自 client，也可以来自 server）的新的连接，Acceptor 将这个请求者按照 round robin 的方式交给对对应的 Processor 进行处理；</li>
<li>Processor 注册这个 SocketChannel 的 <code>OP_READ</code> 的事件，如果有请求发送过来就可以被 Processor 的 Selector 选中；</li>
<li>Processor 将请求者发送的请求放入到一个 Request Queue 中，这是所有 Processor 共有的一个队列；</li>
<li>KafkaRequestHandler 从 Request Queue 中取出请求；</li>
<li>调用 KafkaApis 进行相应的处理；</li>
<li>处理的结果放入到该 Processor 对应的 Response Queue 中（每个 request 都标识它们来自哪个 Processor），Request Queue 的数量与 Processor 的数量保持一致；</li>
<li>Processor 从对应的 Response Queue 中取出 response；</li>
<li>Processor 将处理的结果返回给对应的请求者。</li>
</ol>
<p>上面是 Server 端网络处理的整体流程，下面我们开始详细讲述上面内容在 Kafka 中实现。</p>
<h2 id="SocketServer"><a href="#SocketServer" class="headerlink" title="SocketServer"></a>SocketServer</h2><p>SocketServer 是接收 Socket 连接、处理请求并返回处理结果的地方，Acceptor 及 Processor 的初始化、处理逻辑都是在这里实现的。在SocketServer 内有几个比较重要的变量，这里先来看下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SocketServer</span>(<span class="params">val config: <span class="type">KafkaConfig</span>, val metrics: <span class="type">Metrics</span>, val time: <span class="type">Time</span>, val credentialProvider: <span class="type">CredentialProvider</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> endpoints = config.listeners.map(l =&gt; l.listenerName -&gt; l).toMap <span class="comment">//note: broker 开放的端口数</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> numProcessorThreads = config.numNetworkThreads <span class="comment">//note: num.network.threads 默认为 3个，即 processor</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxQueuedRequests = config.queuedMaxRequests <span class="comment">//note:  queued.max.requests，request 队列中允许的最多请求数，默认是500</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> totalProcessorThreads = numProcessorThreads * endpoints.size <span class="comment">//note: 每个端口会对应 N 个 processor</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIp = config.maxConnectionsPerIp <span class="comment">//note: 默认 2147483647</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIpOverrides = config.maxConnectionsPerIpOverrides</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Socket Server on Broker "</span> + config.brokerId + <span class="string">"], "</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 请求队列</span></span><br><span class="line">  <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(totalProcessorThreads, maxQueuedRequests)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> processors = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Processor</span>](totalProcessorThreads)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[network] <span class="keyword">val</span> acceptors = mutable.<span class="type">Map</span>[<span class="type">EndPoint</span>, <span class="type">Acceptor</span>]()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val numProcessors: <span class="type">Int</span>, val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> responseListeners: <span class="type">List</span>[(<span class="type">Int</span>) =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="comment">//note: 一个 requestQueue 队列,N 个 responseQueues 队列</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Request</span>](queueSize)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> responseQueues = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]](numProcessors)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中</p>
<ol>
<li><code>numProcessorThreads</code>：决定了 Processor 的个数，默认是3个，也就是 1+N+M 的 N 的数值；</li>
<li><code>maxQueuedRequests</code>：决定了 request queue 中最多允许放入多少个请求（等待处理的请求），默认是 500；</li>
<li>在 <code>RequestChannel</code> 中初始化了一个 requestQueue 和 N 个 responseQueue。</li>
</ol>
<h3 id="SocketServer-初始化"><a href="#SocketServer-初始化" class="headerlink" title="SocketServer 初始化"></a>SocketServer 初始化</h3><p>在 SocketServer 初始化方法 <code>startup()</code> 中，会初始化 1 个 Acceptor 和 N 个 Processor 线程（每个 EndPoint 都会初始化这么多，一般来说一个 Server 只会设置一个端口），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">    <span class="comment">//note: 一台 broker 一般只设置一个端口，当然这里也可以设置两个</span></span><br><span class="line">    config.listeners.foreach &#123; endpoint =&gt;</span><br><span class="line">      <span class="keyword">val</span> listenerName = endpoint.listenerName</span><br><span class="line">      <span class="keyword">val</span> securityProtocol = endpoint.securityProtocol</span><br><span class="line">      <span class="keyword">val</span> processorEndIndex = processorBeginIndex + numProcessorThreads</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: N 个 processor</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- processorBeginIndex until processorEndIndex)</span><br><span class="line">        processors(i) = newProcessor(i, connectionQuotas, listenerName, securityProtocol)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 1个 Acceptor</span></span><br><span class="line">      <span class="keyword">val</span> acceptor = <span class="keyword">new</span> <span class="type">Acceptor</span>(endpoint, sendBufferSize, recvBufferSize, brokerId,</span><br><span class="line">        processors.slice(processorBeginIndex, processorEndIndex), connectionQuotas)</span><br><span class="line">      acceptors.put(endpoint, acceptor)</span><br><span class="line">      <span class="type">Utils</span>.newThread(<span class="string">s"kafka-socket-acceptor-<span class="subst">$listenerName</span>-<span class="subst">$securityProtocol</span>-<span class="subst">$&#123;endpoint.port&#125;</span>"</span>, acceptor, <span class="literal">false</span>).start()</span><br><span class="line">      acceptor.awaitStartup()</span><br><span class="line"></span><br><span class="line">      processorBeginIndex = processorEndIndex</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Acceptor-处理"><a href="#Acceptor-处理" class="headerlink" title="Acceptor 处理"></a>Acceptor 处理</h3><p>SocketServer 在初始化后 Acceptor 线程后，Acceptor 启动，会首先注册 <code>OP_ACCEPT</code> 事件，监听是否有新的连接，如果来了新的连接就将该 SocketChannel 交给对应的 Processor 进行处理，Processor 是通过 round robin 方法选择的，这样可以保证 Processor 的负载相差无几（至少可以保证监听的 SocketChannel 差不多），实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">  serverChannel.register(nioSelector, <span class="type">SelectionKey</span>.<span class="type">OP_ACCEPT</span>)<span class="comment">//note: 注册 accept 事件</span></span><br><span class="line">  startupComplete()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">var</span> currentProcessor = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> ready = nioSelector.select(<span class="number">500</span>)</span><br><span class="line">        <span class="keyword">if</span> (ready &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> keys = nioSelector.selectedKeys()</span><br><span class="line">          <span class="keyword">val</span> iter = keys.iterator()</span><br><span class="line">          <span class="keyword">while</span> (iter.hasNext &amp;&amp; isRunning) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="keyword">val</span> key = iter.next</span><br><span class="line">              iter.remove()</span><br><span class="line">              <span class="keyword">if</span> (key.isAcceptable)</span><br><span class="line">                accept(key, processors(currentProcessor))<span class="comment">//note: 拿到一个socket 连接，轮询选择一个processor进行处理</span></span><br><span class="line">              <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Unrecognized key state for acceptor thread."</span>)</span><br><span class="line"></span><br><span class="line">              <span class="comment">//note: 轮询算法,使用 round robin</span></span><br><span class="line">              <span class="comment">// round robin to the next processor thread</span></span><br><span class="line">              currentProcessor = (currentProcessor + <span class="number">1</span>) % processors.length</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while accepting connection"</span>, e)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="comment">// We catch all the throwables to prevent the acceptor thread from exiting on exceptions due</span></span><br><span class="line">        <span class="comment">// to a select operation on a specific channel or a bad request. We don't want</span></span><br><span class="line">        <span class="comment">// the broker to stop responding to requests from other clients in these scenarios.</span></span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error occurred"</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    debug(<span class="string">"Closing server socket and selector."</span>)</span><br><span class="line">    swallowError(serverChannel.close())</span><br><span class="line">    swallowError(nioSelector.close())</span><br><span class="line">    shutdownComplete()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Acceptor 通过 <code>accept()</code> 将该新连接交给对应的 Processor，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 处理一个新的连接</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</span><br><span class="line">  <span class="comment">//note: accept 事件发生时，获取注册到 selector 上的 ServerSocketChannel</span></span><br><span class="line">  <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]</span><br><span class="line">  <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    connectionQuotas.inc(socketChannel.socket().getInetAddress)</span><br><span class="line">    socketChannel.configureBlocking(<span class="literal">false</span>)</span><br><span class="line">    socketChannel.socket().setTcpNoDelay(<span class="literal">true</span>)</span><br><span class="line">    socketChannel.socket().setKeepAlive(<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">if</span> (sendBufferSize != <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>)</span><br><span class="line">      socketChannel.socket().setSendBufferSize(sendBufferSize)</span><br><span class="line"></span><br><span class="line">    debug(<span class="string">"Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]"</span></span><br><span class="line">          .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id,</span><br><span class="line">                socketChannel.socket.getSendBufferSize, sendBufferSize,</span><br><span class="line">                socketChannel.socket.getReceiveBufferSize, recvBufferSize))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 轮询选择不同的 processor 进行处理</span></span><br><span class="line">    processor.accept(socketChannel)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">TooManyConnectionsException</span> =&gt;</span><br><span class="line">      info(<span class="string">"Rejected connection from %s, address already has the configured maximum of %d connections."</span>.format(e.ip, e.count))</span><br><span class="line">      close(socketChannel)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Processor-处理"><a href="#Processor-处理" class="headerlink" title="Processor 处理"></a>Processor 处理</h3><p>在前面，Acceptor 通过 <code>accept()</code> 将新的连接交给 Processor，Processor 实际上是将该 SocketChannel 添加到该 Processor 的 <code>newConnections</code> 队列中，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</span><br><span class="line">  newConnections.add(socketChannel)<span class="comment">//note: 添加到队列中</span></span><br><span class="line">  wakeup()<span class="comment">//note: 唤醒 Processor 的 selector（如果此时在阻塞的话）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里详细看下 Processor 线程做了什么事情，其 <code>run()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">  startupComplete()</span><br><span class="line">  <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// setup any new connections that have been queued up</span></span><br><span class="line">      configureNewConnections()<span class="comment">//note: 对新的 socket 连接,并注册 READ 事件</span></span><br><span class="line">      <span class="comment">// register any new responses for writing</span></span><br><span class="line">      processNewResponses()<span class="comment">//note: 处理 response 队列中 response</span></span><br><span class="line">      poll() <span class="comment">//note: 监听所有的 socket channel，是否有新的请求发送过来</span></span><br><span class="line">      processCompletedReceives() <span class="comment">//note: 处理接收到请求，将其放入到 request queue 中</span></span><br><span class="line">      processCompletedSends() <span class="comment">//note: 处理已经完成的发送</span></span><br><span class="line">      processDisconnected() <span class="comment">//note: 处理断开的连接</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// We catch all the throwables here to prevent the processor thread from exiting. We do this because</span></span><br><span class="line">      <span class="comment">// letting a processor exit might cause a bigger impact on the broker. Usually the exceptions thrown would</span></span><br><span class="line">      <span class="comment">// be either associated with a specific socket channel or a bad request. We just ignore the bad socket channel</span></span><br><span class="line">      <span class="comment">// or request. This behavior might need to be reviewed if we see an exception that need the entire broker to stop.</span></span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        error(<span class="string">"Processor got uncaught exception."</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  debug(<span class="string">"Closing selector - processor "</span> + id)</span><br><span class="line">  swallowError(closeAll())</span><br><span class="line">  shutdownComplete()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Processor 在一次循环中，主要做的事情如下：</p>
<ol>
<li><code>configureNewConnections()</code>：对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，这里主要是 Processor 的 selector 注册该连接的 <code>OP_READ</code> 事件；</li>
<li><code>processNewResponses()</code>：从该 Processor 对应的 response queue 中取出一个 response，进行发送；</li>
<li><code>poll()</code>：调用 selector 的 <code>poll()</code> 方法，遍历注册的 SocketChannel，查看是否有事件准备就绪；</li>
<li><code>processCompletedReceives()</code>：将接收到请求添加到的 request queue 中；</li>
<li><code>processCompletedSends()</code>：处理已经完成的响应发送；</li>
<li><code>processDisconnected()</code>：处理断开的 SocketChannel。</li>
</ol>
<p>上面就是 Processor 线程处理的主要逻辑，先是向新的 SocketChannel 注册相应的事件，监控是否有请求发送过来，接着从 response queue 中取出处理完成的请求发送给对应的请求者，然后调用一下 selector 的 <code>poll()</code>，遍历一下注册的所有 SocketChannel，判断是否有事件就绪，然后做相应的处理。这里需要注意的是，request queue 是所有 Processor 公用的一个队列，而 response queue 则是与 Processor 一一对应的，因为每个 Processor 监听的 SocketChannel 并不是同一批的，如果公有一个 response queue，那么这个 N 个 Processor 的 selector 要去监听所有的 SocketChannel，而不是现在这种，只需要去关注分配给自己的 SocketChannel。</p>
<p>下面分别看下上面的这些方法的具体实现。</p>
<h4 id="configureNewConnections"><a href="#configureNewConnections" class="headerlink" title="configureNewConnections"></a>configureNewConnections</h4><p><code>configureNewConnections()</code> 对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，主要是 selector 注册相应的 <code>OP_READ</code> 事件，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如果有新的连接过来，将该 Channel 的 OP_READ 事件注册到 selector 上</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">configureNewConnections</span></span>() &#123;</span><br><span class="line">  <span class="keyword">while</span> (!newConnections.isEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> channel = newConnections.poll()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      debug(<span class="string">s"Processor <span class="subst">$id</span> listening to new connection from <span class="subst">$&#123;channel.socket.getRemoteSocketAddress&#125;</span>"</span>)</span><br><span class="line">      <span class="keyword">val</span> localHost = channel.socket().getLocalAddress.getHostAddress</span><br><span class="line">      <span class="keyword">val</span> localPort = channel.socket().getLocalPort</span><br><span class="line">      <span class="keyword">val</span> remoteHost = channel.socket().getInetAddress.getHostAddress</span><br><span class="line">      <span class="keyword">val</span> remotePort = channel.socket().getPort</span><br><span class="line">      <span class="keyword">val</span> connectionId = <span class="type">ConnectionId</span>(localHost, localPort, remoteHost, remotePort).toString</span><br><span class="line">      selector.register(connectionId, channel)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// We explicitly catch all non fatal exceptions and close the socket to avoid a socket leak. The other</span></span><br><span class="line">      <span class="comment">// throwables will be caught in processor and logged as uncaught exceptions.</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        <span class="keyword">val</span> remoteAddress = channel.getRemoteAddress</span><br><span class="line">        <span class="comment">// need to close the channel here to avoid a socket leak.</span></span><br><span class="line">        close(channel)</span><br><span class="line">        error(<span class="string">s"Processor <span class="subst">$id</span> closed connection from <span class="subst">$remoteAddress</span>"</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="processNewResponses"><a href="#processNewResponses" class="headerlink" title="processNewResponses"></a>processNewResponses</h4><p><code>processNewResponses()</code> 方法是从该 Processor 对应的 response queue 中取出一个 response，Processor 是通过 RequestChannel 的 <code>receiveResponse()</code> 从该 Processor 对应的 response queue 中取出 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 获取 response</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">receiveResponse</span></span>(processor: <span class="type">Int</span>): <span class="type">RequestChannel</span>.<span class="type">Response</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> response = responseQueues(processor).poll()</span><br><span class="line">  <span class="keyword">if</span> (response != <span class="literal">null</span>)</span><br><span class="line">    response.request.responseDequeueTimeMs = <span class="type">Time</span>.<span class="type">SYSTEM</span>.milliseconds</span><br><span class="line">  response</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>取到相应的 response 之后，会判断该 response 的类型，进行相应的操作，如果需要返回，那么会调用 <code>sendResponse()</code> 发送该 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 处理一个新的 response 响应</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processNewResponses</span></span>() &#123;</span><br><span class="line">  <span class="keyword">var</span> curr = requestChannel.receiveResponse(id)</span><br><span class="line">  <span class="keyword">while</span> (curr != <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      curr.responseAction <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">NoOpAction</span> =&gt; <span class="comment">//note: 如果这个请求不需要返回 response，再次注册该监听事件</span></span><br><span class="line">          <span class="comment">// There is no response to send to the client, we need to read more pipelined requests</span></span><br><span class="line">          <span class="comment">// that are sitting in the server's socket buffer</span></span><br><span class="line">          curr.request.updateRequestMetrics</span><br><span class="line">          trace(<span class="string">"Socket server received empty response to send, registering for read: "</span> + curr)</span><br><span class="line">          <span class="keyword">val</span> channelId = curr.request.connectionId</span><br><span class="line">          <span class="keyword">if</span> (selector.channel(channelId) != <span class="literal">null</span> || selector.closingChannel(channelId) != <span class="literal">null</span>)</span><br><span class="line">              selector.unmute(channelId)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">SendAction</span> =&gt; <span class="comment">//note: 需要发送的 response，那么进行发送</span></span><br><span class="line">          sendResponse(curr)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">CloseConnectionAction</span> =&gt; <span class="comment">//note: 要关闭的 response</span></span><br><span class="line">          curr.request.updateRequestMetrics</span><br><span class="line">          trace(<span class="string">"Closing socket connection actively according to the response code."</span>)</span><br><span class="line">          close(selector, curr.request.connectionId)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      curr = requestChannel.receiveResponse(id)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* `protected` for test usage */</span></span><br><span class="line"><span class="comment">//note: 发送的对应的 response</span></span><br><span class="line"><span class="keyword">protected</span>[network] <span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</span><br><span class="line">  trace(<span class="string">s"Socket server received response to send, registering for write and sending data: <span class="subst">$response</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> channel = selector.channel(response.responseSend.destination)</span><br><span class="line">  <span class="comment">// `channel` can be null if the selector closed the connection because it was idle for too long</span></span><br><span class="line">  <span class="keyword">if</span> (channel == <span class="literal">null</span>) &#123;</span><br><span class="line">    warn(<span class="string">s"Attempting to send response via channel for which there is no open connection, connection id <span class="subst">$id</span>"</span>)</span><br><span class="line">    response.request.updateRequestMetrics()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    selector.send(response.responseSend) <span class="comment">//note: 发送该 response</span></span><br><span class="line">    inflightResponses += (response.request.connectionId -&gt; response) <span class="comment">//note: 添加到 inflinght 中</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="processCompletedReceives"><a href="#processCompletedReceives" class="headerlink" title="processCompletedReceives"></a>processCompletedReceives</h4><p><code>processCompletedReceives()</code> 方法的主要作用是处理接收到请求，并将其放入到 request queue 中，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 处理接收到的所有请求</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedReceives</span></span>() &#123;</span><br><span class="line">  selector.completedReceives.asScala.foreach &#123; receive =&gt;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> openChannel = selector.channel(receive.source)</span><br><span class="line">      <span class="keyword">val</span> session = &#123;</span><br><span class="line">        <span class="comment">// Only methods that are safe to call on a disconnected channel should be invoked on 'channel'.</span></span><br><span class="line">        <span class="keyword">val</span> channel = <span class="keyword">if</span> (openChannel != <span class="literal">null</span>) openChannel <span class="keyword">else</span> selector.closingChannel(receive.source)</span><br><span class="line">        <span class="type">RequestChannel</span>.<span class="type">Session</span>(<span class="keyword">new</span> <span class="type">KafkaPrincipal</span>(<span class="type">KafkaPrincipal</span>.<span class="type">USER_TYPE</span>, channel.principal.getName), channel.socketAddress)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> req = <span class="type">RequestChannel</span>.<span class="type">Request</span>(processor = id, connectionId = receive.source, session = session,</span><br><span class="line">        buffer = receive.payload, startTimeMs = time.milliseconds, listenerName = listenerName,</span><br><span class="line">        securityProtocol = securityProtocol)</span><br><span class="line">      requestChannel.sendRequest(req) <span class="comment">//note: 添加到请求队列，如果队列满了，将会阻塞</span></span><br><span class="line">      selector.mute(receive.source) <span class="comment">//note: 移除该连接的 OP_READ 监听</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e @ (_: <span class="type">InvalidRequestException</span> | _: <span class="type">SchemaException</span>) =&gt;</span><br><span class="line">        <span class="comment">// note that even though we got an exception, we can assume that receive.source is valid. Issues with constructing a valid receive object were handled earlier</span></span><br><span class="line">        error(<span class="string">s"Closing socket for <span class="subst">$&#123;receive.source&#125;</span> because of error"</span>, e)</span><br><span class="line">        close(selector, receive.source)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="processCompletedSends"><a href="#processCompletedSends" class="headerlink" title="processCompletedSends"></a>processCompletedSends</h4><p><code>processCompletedSends()</code> 方法是处理已经完成的发送，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedSends</span></span>() &#123;</span><br><span class="line">  selector.completedSends.asScala.foreach &#123; send =&gt;</span><br><span class="line">    <span class="comment">//note: response 发送完成，从正在发送的集合中移除</span></span><br><span class="line">    <span class="keyword">val</span> resp = inflightResponses.remove(send.destination).getOrElse &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Send for <span class="subst">$&#123;send.destination&#125;</span> completed, but not in `inflightResponses`"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    resp.request.updateRequestMetrics()</span><br><span class="line">    selector.unmute(send.destination) <span class="comment">//note: 完成这个请求之后再次监听 OP_READ 事件</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="KafkaRequestHandlerPool"><a href="#KafkaRequestHandlerPool" class="headerlink" title="KafkaRequestHandlerPool"></a>KafkaRequestHandlerPool</h2><p>上面主要是讲述 SocketServer 中 Acceptor 与 Processor 的处理内容，也就是 1+N+M 模型中 1+N 部分，下面开始讲述 M 部分，也就是 KafkaRequestHandler 的内容，其初始化实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              val requestChannel: <span class="type">RequestChannel</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              val apis: <span class="type">KafkaApis</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              time: <span class="type">Time</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              numThreads: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">"RequestHandlerAvgIdlePercent"</span>, <span class="string">"percent"</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Request Handler on Broker "</span> + brokerId + <span class="string">"], "</span></span><br><span class="line">  <span class="keyword">val</span> threads = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Thread</span>](numThreads)</span><br><span class="line">  <span class="keyword">val</span> runnables = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</span><br><span class="line">  <span class="comment">//note: 建立 M 个（numThreads）KafkaRequestHandler</span></span><br><span class="line">  <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">    <span class="comment">//note: requestChannel 是 Processor 存放 request 请求的地方,也是 Handler 处理完请求存放 response 的地方</span></span><br><span class="line">    runnables(i) = <span class="keyword">new</span> <span class="type">KafkaRequestHandler</span>(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis, time)</span><br><span class="line">    threads(i) = <span class="type">Utils</span>.daemonThread(<span class="string">"kafka-request-handler-"</span> + i, runnables(i))</span><br><span class="line">    threads(i).start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shutdown</span></span>() &#123;</span><br><span class="line">    info(<span class="string">"shutting down"</span>)</span><br><span class="line">    <span class="keyword">for</span>(handler &lt;- runnables)</span><br><span class="line">      handler.shutdown</span><br><span class="line">    <span class="keyword">for</span>(thread &lt;- threads)</span><br><span class="line">      thread.join</span><br><span class="line">    info(<span class="string">"shut down completely"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如上面实现所示：</p>
<ol>
<li>KafkaRequestHandlerPool 会初始化 M 个 KafkaRequestHandler 线程，并启动该线程；</li>
<li>在初始化 KafkaRequestHandler 时，传入一个 requestChannel 变量，这个是 Processor 存放 request 的地方，KafkaRequestHandler 在处理请求时，会从这个 queue 中取出相应的 request。</li>
</ol>
<h3 id="KafkaRequestHandler"><a href="#KafkaRequestHandler" class="headerlink" title="KafkaRequestHandler"></a>KafkaRequestHandler</h3><p>KafkaRequestHandler 线程的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">var</span> req : <span class="type">RequestChannel</span>.<span class="type">Request</span> = <span class="literal">null</span></span><br><span class="line">      <span class="keyword">while</span> (req == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// We use a single meter for aggregate idle percentage for the thread pool.</span></span><br><span class="line">        <span class="comment">// Since meter is calculated as total_recorded_value / time_window and</span></span><br><span class="line">        <span class="comment">// time_window is independent of the number of threads, each recorded idle</span></span><br><span class="line">        <span class="comment">// time should be discounted by # threads.</span></span><br><span class="line">        <span class="keyword">val</span> startSelectTime = time.nanoseconds</span><br><span class="line">        req = requestChannel.receiveRequest(<span class="number">300</span>) <span class="comment">//note: 从 request queue 中拿去 request</span></span><br><span class="line">        <span class="keyword">val</span> idleTime = time.nanoseconds - startSelectTime</span><br><span class="line">        aggregateIdleMeter.mark(idleTime / totalHandlerThreads)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(req eq <span class="type">RequestChannel</span>.<span class="type">AllDone</span>) &#123;</span><br><span class="line">        debug(<span class="string">"Kafka request handler %d on broker %d received shut down command"</span>.format(</span><br><span class="line">          id, brokerId))</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">      req.requestDequeueTimeMs = time.milliseconds</span><br><span class="line">      trace(<span class="string">"Kafka request handler %d on broker %d handling request %s"</span>.format(id, brokerId, req))</span><br><span class="line">      apis.handle(req) <span class="comment">//note: 处理请求,并将处理的结果通过 sendResponse 放入 response queue 中</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Exception when handling request"</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述方法的实现逻辑：</p>
<ol>
<li>从 RequestChannel 取出相应的 request；</li>
<li>KafkaApis 处理这个 request，并通过 <code>requestChannel.sendResponse()</code> 将处理的结果放入 requestChannel 的 response queue 中，如下所示：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 response 添加到对应的队列中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</span><br><span class="line">  responseQueues(response.processor).put(response)</span><br><span class="line">  <span class="keyword">for</span>(onResponse &lt;- responseListeners)</span><br><span class="line">    onResponse(response.processor) <span class="comment">//note: 调用对应 processor 的 wakeup 方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里为止，一个请求从 Processor 接收，到 KafkaRequestHandler 通过 KafkaApis 处理并放回该 Processor 对应的 response queue 这整个过程就完成了（建议阅读本文的时候结合最前面的流程图一起看）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 LeaderAndIsr 请求的处理（二十二）</title>
    <link href="http://matt33.com/2018/06/25/leaderAndIsr-process/"/>
    <id>http://matt33.com/2018/06/25/leaderAndIsr-process/</id>
    <published>2018-06-25T01:01:12.000Z</published>
    <updated>2019-02-24T02:29:01.552Z</updated>
    
    <content type="html"><![CDATA[<p>本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka Server 是如何处理 LeaderAndIsr 请求的。</p>
<h2 id="LeaderAndIsr-请求"><a href="#LeaderAndIsr-请求" class="headerlink" title="LeaderAndIsr 请求"></a>LeaderAndIsr 请求</h2><p>LeaderAndIsr 请求是在一个 Topic Partition 的 leader、isr、assignment replicas 变动时，Controller 向 Broker 发送的一种请求，有时候是向这个 Topic Partition 的所有副本发送，有时候是其中的某个副本，跟具体的触发情况有关系。在一个 LeaderAndIsr 请求中，会封装多个 Topic Partition 的信息，每个 Topic Partition 会对应一个 PartitionState 对象，这个对象主要成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionState</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> controllerEpoch;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leader;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leaderEpoch;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> List&lt;Integer&gt; isr;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> zkVersion;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> Set&lt;Integer&gt; replicas;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由此可见，在 LeaderAndIsr 请求中，会包含一个 Partition 的以下信息：</p>
<ol>
<li>当前 Controller 的 epoch（Broker 收到这个请求后，如果发现是过期的 Controller 请求，就会拒绝这个请求）；</li>
<li>leader，Partition 的 leader 信息；</li>
<li>leader epoch，Partition leader epoch 信息（leader、isr、AR 变动时，这个 epoch 都会加1）；</li>
<li>isr 列表；</li>
<li>zkVersion，；</li>
<li>AR，所有的 replica 列表。</li>
</ol>
<h3 id="LeaderAndIsr-请求处理"><a href="#LeaderAndIsr-请求处理" class="headerlink" title="LeaderAndIsr 请求处理"></a>LeaderAndIsr 请求处理</h3><h3 id="处理整体流程"><a href="#处理整体流程" class="headerlink" title="处理整体流程"></a>处理整体流程</h3><p>LeaderAndIsr 请求可谓是包含了一个 Partition 的所有 metadata 信息，Server 在接收到 Controller 发送的这个请求后，其处理的逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//KafkaApis</span></span><br><span class="line"><span class="comment">//note: LeaderAndIsr 请求的处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleLeaderAndIsrRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></span><br><span class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></span><br><span class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></span><br><span class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</span><br><span class="line">  <span class="keyword">val</span> leaderAndIsrRequest = request.body.asInstanceOf[<span class="type">LeaderAndIsrRequest</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</span><br><span class="line">      <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></span><br><span class="line">      <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></span><br><span class="line">      <span class="comment">// leadership changes</span></span><br><span class="line">      <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></span><br><span class="line">      updatedLeaders.foreach &#123; partition =&gt;</span><br><span class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</span><br><span class="line">          coordinator.handleGroupImmigration(partition.partitionId)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></span><br><span class="line">      updatedFollowers.foreach &#123; partition =&gt;</span><br><span class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</span><br><span class="line">          coordinator.handleGroupEmigration(partition.partitionId)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> leaderAndIsrResponse =</span><br><span class="line">      <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;<span class="comment">//note: 有权限的情况下</span></span><br><span class="line">        <span class="comment">//note: replicaManager 进行相应的处理</span></span><br><span class="line">        <span class="keyword">val</span> result = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest, metadataCache, onLeadershipChange)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(result.errorCode, result.responseMap.mapValues(<span class="keyword">new</span> <span class="type">JShort</span>(_)).asJava)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> result = leaderAndIsrRequest.partitionStates.asScala.keys.map((_, <span class="keyword">new</span> <span class="type">JShort</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code))).toMap</span><br><span class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code, result.asJava)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, leaderAndIsrResponse))</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</span><br><span class="line">      fatal(<span class="string">"Disk error during leadership change."</span>, e)</span><br><span class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述处理逻辑分为以下两步：</p>
<ol>
<li>ReplicaManager 调用 <code>becomeLeaderOrFollower()</code> 方法对这个请求进行相应的处理；</li>
<li>如果请求中包含 <code>__consumer_offset</code> 的 Partition（对应两种情况：之前是 fllower 现在变成了 leader、之前是 leader 现在变成了 follower），那么还需要调用这个方法中定义的 <code>onLeadershipChange()</code> 方法进行相应的处理。</li>
</ol>
<p><code>becomeLeaderOrFollower()</code>  的整体处理流程如下：</p>
<p><img src="/images/kafka/leader-and-isr.png" alt="LeaderAndIsr 请求的处理"></p>
<h3 id="becomeLeaderOrFollower"><a href="#becomeLeaderOrFollower" class="headerlink" title="becomeLeaderOrFollower"></a>becomeLeaderOrFollower</h3><p>这里先看下 ReplicaManager 的 <code>becomeLeaderOrFollower()</code> 方法，它是 LeaderAndIsr 请求处理的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 处理 LeaderAndIsr 请求</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">becomeLeaderOrFollower</span></span>(correlationId: <span class="type">Int</span>,leaderAndISRRequest: <span class="type">LeaderAndIsrRequest</span>,</span><br><span class="line">                           metadataCache: <span class="type">MetadataCache</span>,</span><br><span class="line">                           onLeadershipChange: (<span class="type">Iterable</span>[<span class="type">Partition</span>], <span class="type">Iterable</span>[<span class="type">Partition</span>]) =&gt; <span class="type">Unit</span>): <span class="type">BecomeLeaderOrFollowerResult</span> = &#123;</span><br><span class="line">  leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</span><br><span class="line">    stateChangeLogger.trace(<span class="string">"Broker %d received LeaderAndIsr request %s correlation id %d from controller %d epoch %d for partition [%s,%d]"</span></span><br><span class="line">                              .format(localBrokerId, stateInfo, correlationId,</span><br><span class="line">                                      leaderAndISRRequest.controllerId, leaderAndISRRequest.controllerEpoch, topicPartition.topic, topicPartition.partition))</span><br><span class="line">  &#125;</span><br><span class="line">  replicaStateChangeLock synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> responseMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]</span><br><span class="line">    <span class="comment">//note: 1. 验证 controller 的 epoch，如果是来自旧的 controller，就拒绝这个请求</span></span><br><span class="line">    <span class="keyword">if</span> (leaderAndISRRequest.controllerEpoch &lt; controllerEpoch) &#123;</span><br><span class="line">      stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d since "</span> +</span><br><span class="line">        <span class="string">"its controller epoch %d is old. Latest known controller epoch is %d"</span>).format(localBrokerId, leaderAndISRRequest.controllerId,</span><br><span class="line">        correlationId, leaderAndISRRequest.controllerEpoch, controllerEpoch))</span><br><span class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 controller 的请求</span></span><br><span class="line">      <span class="keyword">val</span> controllerId = leaderAndISRRequest.controllerId</span><br><span class="line">      controllerEpoch = leaderAndISRRequest.controllerEpoch</span><br><span class="line"></span><br><span class="line">      <span class="comment">// First check partition's leader epoch</span></span><br><span class="line">      <span class="comment">//note: 2. 检查 leader epoch，得到一个 partitionState map，epoch 满足条件并且有副本在本地的集合</span></span><br><span class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>]()</span><br><span class="line">      leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</span><br><span class="line">        <span class="keyword">val</span> partition = getOrCreatePartition(topicPartition) <span class="comment">//note: 对应的 tp 如果没有 Partition 实例的话,就新建一个</span></span><br><span class="line">        <span class="keyword">val</span> partitionLeaderEpoch = partition.getLeaderEpoch <span class="comment">//note: 更新 leader epoch</span></span><br><span class="line">        <span class="comment">// If the leader epoch is valid record the epoch of the controller that made the leadership decision.</span></span><br><span class="line">        <span class="comment">// This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path</span></span><br><span class="line">        <span class="keyword">if</span> (partitionLeaderEpoch &lt; stateInfo.leaderEpoch) &#123;</span><br><span class="line">          <span class="keyword">if</span>(stateInfo.replicas.contains(localBrokerId))</span><br><span class="line">            partitionState.put(partition, stateInfo)  <span class="comment">//note: 更新 replica 的 stateInfo</span></span><br><span class="line">          <span class="keyword">else</span> &#123;</span><br><span class="line">            stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</span><br><span class="line">              <span class="string">"epoch %d for partition [%s,%d] as itself is not in assigned replica list %s"</span>)</span><br><span class="line">              .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</span><br><span class="line">                topicPartition.topic, topicPartition.partition, stateInfo.replicas.asScala.mkString(<span class="string">","</span>)))</span><br><span class="line">            responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;  <span class="comment">//note: 忽略这个请求，因为请求的 leader epoch 小于缓存的 epoch</span></span><br><span class="line">          <span class="comment">// Otherwise record the error code in response</span></span><br><span class="line">          stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</span><br><span class="line">            <span class="string">"epoch %d for partition [%s,%d] since its associated leader epoch %d is not higher than the current leader epoch %d"</span>)</span><br><span class="line">            .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</span><br><span class="line">              topicPartition.topic, topicPartition.partition, stateInfo.leaderEpoch, partitionLeaderEpoch))</span><br><span class="line">          responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 3. 过滤出本地副本设置为 leader 的 Partition 列表</span></span><br><span class="line">      <span class="keyword">val</span> partitionsTobeLeader = partitionState.filter &#123; <span class="keyword">case</span> (_, stateInfo) =&gt;</span><br><span class="line">        stateInfo.leader == localBrokerId</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//note: 4. 过滤出本地副本设置为 follower 的 Partition 列表</span></span><br><span class="line">      <span class="keyword">val</span> partitionsToBeFollower = partitionState -- partitionsTobeLeader.keys <span class="comment">//note: 这些 tp 设置为了 follower</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 5. 将为 leader 的副本设置为 leader</span></span><br><span class="line">      <span class="keyword">val</span> partitionsBecomeLeader = <span class="keyword">if</span> (partitionsTobeLeader.nonEmpty)</span><br><span class="line">        makeLeaders(controllerId, controllerEpoch, partitionsTobeLeader, correlationId, responseMap)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 6. 将为 follower 的副本设置为 follower</span></span><br><span class="line">      <span class="keyword">val</span> partitionsBecomeFollower = <span class="keyword">if</span> (partitionsToBeFollower.nonEmpty)</span><br><span class="line">        makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap, metadataCache)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 7. 如果 hw checkpoint 的线程没有初始化，这里需要进行一次初始化</span></span><br><span class="line">      <span class="comment">// we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions</span></span><br><span class="line">      <span class="comment">// have been completely populated before starting the checkpointing there by avoiding weird race conditions</span></span><br><span class="line">      <span class="keyword">if</span> (!hwThreadInitialized) &#123;</span><br><span class="line">        startHighWaterMarksCheckPointThread()</span><br><span class="line">        hwThreadInitialized = <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//note: 8. 检查 replica fetcher 是否需要关闭（有些副本需要关闭因为可能从 follower 变为 leader）</span></span><br><span class="line">      replicaFetcherManager.shutdownIdleFetcherThreads()</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 9. 检查是否 __consumer_offset 的 Partition 的 leaderAndIsr 信息，有的话进行相应的操作</span></span><br><span class="line">      onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)</span><br><span class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述实现，其处理逻辑总结如下：</p>
<ol>
<li>检查 Controller 的 epoch，如果是来自旧的 Controller，那么就拒绝这个请求；</li>
<li>获取请求的 Partition 列表的 PartitionState 信息，在遍历的过程中，会进行一个检查，如果 leader epoch 小于缓存中的 epoch 值，那么就过滤掉这个 Partition 信息，如果这个 Partition 在本地不存在，那么会初始化这个 Partition 的对象（这时候并不会初始化本地副本）；</li>
<li>获取出本地副本为 leader 的 Partition 列表（partitionsTobeLeader）；</li>
<li>获取出本地副本为 follower 的 Partition 列表（partitionsToBeFollower）；</li>
<li>调用 <code>makeLeaders()</code> 方法将 leader 的副本设置为 leader；</li>
<li>调用 <code>makeFollowers()</code> 方法将 leader 的副本设置为 follower；</li>
<li>检查 HW checkpoint 的线程是否初始化，如果没有，这里需要进行一次初始化；</li>
<li>检查 ReplicaFetcherManager 是否有线程需要关闭（如果这个线程上没有分配要拉取的 Topic Partition，那么在这里这个线程就会被关闭，下次需要时会再次启动）；</li>
<li>检查是否有 <code>__consumer_offset</code> Partition 的 leaderAndIsr 信息，有的话进行相应的操作。</li>
</ol>
<p>这其中，比较复杂的部分是第 5、6、9步，也前面图中标出的 1、2、4步，文章下面接着分析这三部分。</p>
<h3 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h3><p>ReplicaManager 的 <code>makeLeaders()</code> 的作用是将指定的这批 Partition 列表设置为 Leader，并返回是新 leader 对应的 Partition 列表（之前不是 leader，现在选举为了 leader），其实实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></span><br><span class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></span><br><span class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></span><br><span class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</span><br><span class="line">                        epoch: <span class="type">Int</span>,</span><br><span class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</span><br><span class="line">                        correlationId: <span class="type">Int</span>,</span><br><span class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">  partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</span><br><span class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</span><br><span class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</span><br><span class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// First stop fetchers for all the partitions</span></span><br><span class="line">    <span class="comment">//note: 1. 停止这些副本同步请求</span></span><br><span class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</span><br><span class="line">    <span class="comment">// Update the partition information to be the leader</span></span><br><span class="line">    <span class="comment">//note: 2. 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></span><br><span class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</span><br><span class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></span><br><span class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</span><br><span class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></span><br><span class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</span><br><span class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</span><br><span class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</span><br><span class="line">    &#125;</span><br><span class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</span><br><span class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</span><br><span class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</span><br><span class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</span><br><span class="line">        stateChangeLogger.error(errorMsg, e)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></span><br><span class="line">      <span class="keyword">throw</span> e</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></span><br><span class="line">  partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</span><br><span class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</span><br><span class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  partitionsToMakeLeaders</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实现逻辑如下：</p>
<ol>
<li>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 方法移除这些 Partition 的副本同步线程；</li>
<li>遍历这些 Partition，通过 Partition 的 <code>makeLeader()</code> 方法将这个 Partition 设置为 Leader，如果设置成功（如果 leader 没有变化，证明这个 Partition 之前就是 leader，这个方法返回的是 false，这种情况下不会更新到缓存中），那么将 leader 信息更新到缓存中。</li>
</ol>
<p>下面来看下在 Partition 中是如何真正初始化一个 Partition 的 leader？其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将本地副本设置为 leader, 如果 leader 不变,向 ReplicaManager 返回 false</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeLeader</span></span>(controllerId: <span class="type">Int</span>, partitionStateInfo: <span class="type">PartitionState</span>, correlationId: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> (leaderHWIncremented, isNewLeader) = inWriteLock(leaderIsrUpdateLock) &#123;</span><br><span class="line">    <span class="keyword">val</span> allReplicas = partitionStateInfo.replicas.asScala.map(_.toInt)</span><br><span class="line">    <span class="comment">// record the epoch of the controller that made the leadership decision. This is useful while updating the isr</span></span><br><span class="line">    <span class="comment">// to maintain the decision maker controller's epoch in the zookeeper path</span></span><br><span class="line">    controllerEpoch = partitionStateInfo.controllerEpoch</span><br><span class="line">    <span class="comment">// add replicas that are new</span></span><br><span class="line">    <span class="comment">//note: 为了新的 replica 创建副本实例</span></span><br><span class="line">    allReplicas.foreach(replica =&gt; getOrCreateReplica(replica))</span><br><span class="line">    <span class="comment">//note: 获取新的 isr 列表</span></span><br><span class="line">    <span class="keyword">val</span> newInSyncReplicas = partitionStateInfo.isr.asScala.map(r =&gt; getOrCreateReplica(r)).toSet</span><br><span class="line">    <span class="comment">// remove assigned replicas that have been removed by the controller</span></span><br><span class="line">    <span class="comment">//note: 将已经在不在 AR 中的副本移除</span></span><br><span class="line">    (assignedReplicas.map(_.brokerId) -- allReplicas).foreach(removeReplica)</span><br><span class="line">    inSyncReplicas = newInSyncReplicas</span><br><span class="line">    leaderEpoch = partitionStateInfo.leaderEpoch</span><br><span class="line">    zkVersion = partitionStateInfo.zkVersion</span><br><span class="line">    <span class="comment">//note: 判断是否是新的 leader</span></span><br><span class="line">    <span class="keyword">val</span> isNewLeader =</span><br><span class="line">      <span class="keyword">if</span> (leaderReplicaIdOpt.isDefined &amp;&amp; leaderReplicaIdOpt.get == localBrokerId) &#123;<span class="comment">//note: leader 没有更新</span></span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        leaderReplicaIdOpt = <span class="type">Some</span>(localBrokerId)</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> leaderReplica = getReplica().get <span class="comment">//note: 获取在当前上的副本,也就是 leader replica</span></span><br><span class="line">    <span class="keyword">val</span> curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset <span class="comment">//note: 获取 leader replica 的 the end offset</span></span><br><span class="line">    <span class="keyword">val</span> curTimeMs = time.milliseconds</span><br><span class="line">    <span class="comment">// initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.</span></span><br><span class="line">    (assignedReplicas - leaderReplica).foreach &#123; replica =&gt; <span class="comment">//note: 对于 isr 中的 replica,更新 LastCaughtUpTime</span></span><br><span class="line">      <span class="keyword">val</span> lastCaughtUpTimeMs = <span class="keyword">if</span> (inSyncReplicas.contains(replica)) curTimeMs <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">      replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></span><br><span class="line">    <span class="keyword">if</span> (isNewLeader) &#123;  <span class="comment">//note: 如果是新的 leader,那么需要</span></span><br><span class="line">      <span class="comment">// construct the high watermark metadata for the new leader replica</span></span><br><span class="line">      <span class="comment">//note: 为新的 leader 构造 replica 的 HW metadata</span></span><br><span class="line">      leaderReplica.convertHWToLocalOffsetMetadata()</span><br><span class="line">      <span class="comment">// reset log end offset for remote replicas</span></span><br><span class="line">      <span class="comment">//note: 更新远程副本的副本同步信息（设置为 unKnown）</span></span><br><span class="line">      assignedReplicas.filter(_.brokerId != localBrokerId).foreach(_.updateLogReadResult(<span class="type">LogReadResult</span>.<span class="type">UnknownLogReadResult</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 如果满足更新 isr 的条件,就更新 HW 信息</span></span><br><span class="line">    (maybeIncrementLeaderHW(leaderReplica), isNewLeader)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></span><br><span class="line">  <span class="keyword">if</span> (leaderHWIncremented) <span class="comment">//note: HW 更新的情况下</span></span><br><span class="line">    tryCompleteDelayedRequests()</span><br><span class="line">  isNewLeader</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>简单总结一下上述的实现：</p>
<ol>
<li>首先更新这个 Partition 的相应信息，包括：isr、AR、leader epoch、zkVersion 等，并为每个副本创建一个 Replica 对象（如果不存在该对象的情况下才会创建，只有本地副本才会初始化相应的日志对象）；</li>
<li>如果这个 Partition 的 leader 本来就是本地副本，那么返回的结果设置为 false，证明这个 leader 并不是新的 leader；</li>
<li>对于 isr 中的所有 Replica，更新 LastCaughtUpTime 值，即最近一次赶得上 leader 的时间；</li>
<li>如果是新的 leader，那么为 leader 初始化相应的 HighWatermarkMetadata 对象，并将所有副本的副本同步信息更新为 UnknownLogReadResult；</li>
<li>检查一下是否需要更新 HW 值。</li>
</ol>
<p>如果这个本地副本是新选举的 leader，那么它所做的事情就是初始化 Leader 应该记录的相关信息。</p>
<h3 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h3><p>ReplicaManager 的 <code>makeFollowers()</code> 方法，是将哪些 Partition 设置为 Follower，返回的结果是那些新的 follower 对应的 Partition 列表（之前是 leader，现在变成了 follower），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</span><br><span class="line">                          epoch: <span class="type">Int</span>,</span><br><span class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</span><br><span class="line">                          correlationId: <span class="type">Int</span>,</span><br><span class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</span><br><span class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">  partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</span><br><span class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</span><br><span class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</span><br><span class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 1. 统计 follower 的集合</span></span><br><span class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></span><br><span class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</span><br><span class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</span><br><span class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的 Partition</span></span><br><span class="line">        <span class="comment">// Only change partition state when the leader is available</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: 2. 将 Partition 的本地副本设置为 follower</span></span><br><span class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</span><br><span class="line">            partitionsToMakeFollower += partition</span><br><span class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></span><br><span class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</span><br><span class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</span><br><span class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</span><br><span class="line">              partition.topicPartition, newLeaderBrokerId))</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></span><br><span class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></span><br><span class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</span><br><span class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</span><br><span class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</span><br><span class="line">            partition.topicPartition, newLeaderBrokerId))</span><br><span class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></span><br><span class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></span><br><span class="line">          partition.getOrCreateReplica()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 3. 移除这些 Partition 的副本同步线程,这样在 MakeFollower 期间,这些 Partition 就不会进行副本同步了</span></span><br><span class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</span><br><span class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</span><br><span class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</span><br><span class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 4. Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></span><br><span class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</span><br><span class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</span><br><span class="line">    &#125;.toMap)</span><br><span class="line">    <span class="comment">//note: 5. 完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）</span></span><br><span class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</span><br><span class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</span><br><span class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</span><br><span class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</span><br><span class="line">        partition.topicPartition, correlationId, controllerId, epoch))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</span><br><span class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</span><br><span class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</span><br><span class="line">          controllerId, epoch, partition.topicPartition))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></span><br><span class="line">      <span class="comment">//note: 6. 启动副本同步线程</span></span><br><span class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</span><br><span class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</span><br><span class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</span><br><span class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></span><br><span class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</span><br><span class="line"></span><br><span class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</span><br><span class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</span><br><span class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</span><br><span class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</span><br><span class="line">      stateChangeLogger.error(errorMsg, e)</span><br><span class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></span><br><span class="line">      <span class="keyword">throw</span> e</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</span><br><span class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</span><br><span class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  partitionsToMakeFollower</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</span><br><span class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</span><br><span class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</span><br><span class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</span><br><span class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</span><br><span class="line">        <span class="comment">//note: 更新副本的相关信息</span></span><br><span class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></span><br><span class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></span><br><span class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>简单总结一下上述的逻辑过程：</p>
<ol>
<li>首先遍历所有的 Partition，获到那些 leader 可用、并且 Partition 可以成功设置为 Follower 的 Partition 列表（partitionsToMakeFollower）；</li>
<li>在上面遍历的过程中，会调用 Partition 的 <code>makeFollower()</code> 方法将 Partition 设置为 Follower（在这里，如果该 Partition 的本地副本不存在，会初始化相应的日志对象，如果该 Partition 的 leader 已经存在，并且没有变化，那么就返回 false，只有 leader 变化的 Partition，才会返回 true，才会加入到 partitionsToMakeFollower 集合中，这是因为 leader 没有变化的 Partition 是不需要变更副本同步线程的）；</li>
<li>移除这些 Partition 的副本同步线程，这样在 MakeFollower 期间，这些 Partition 就不会进行副本同步了；</li>
<li>Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset，因为前面已经移除了这个 Partition 的副本同步线程，所以这里在 checkpoint 后可以保证所有缓存的数据都可以刷新到磁盘；</li>
<li>完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）；</li>
<li>启动相应的副本同步线程。</li>
</ol>
<p>到这里 LeaderAndIsr 请求的大部分处理已经完成，但是有一个比较特殊的 topic（<code>__consumer_offset</code>），如果这 Partition 的 leader 发生变化，是需要一些额外的处理。</p>
<h2 id="consumer-offset-leader-切换处理"><a href="#consumer-offset-leader-切换处理" class="headerlink" title="__consumer_offset leader 切换处理"></a><code>__consumer_offset</code> leader 切换处理</h2><p><code>__consumer_offset</code> 这个 Topic 如果发生了 leader 切换，GroupCoordinator 需要进行相应的处理，其处理过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</span><br><span class="line">  <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></span><br><span class="line">  <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></span><br><span class="line">  <span class="comment">// leadership changes</span></span><br><span class="line">  <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></span><br><span class="line">  updatedLeaders.foreach &#123; partition =&gt;</span><br><span class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</span><br><span class="line">      coordinator.handleGroupImmigration(partition.partitionId)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></span><br><span class="line">  updatedFollowers.foreach &#123; partition =&gt;</span><br><span class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</span><br><span class="line">      coordinator.handleGroupEmigration(partition.partitionId)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="成为-leader"><a href="#成为-leader" class="headerlink" title="成为 leader"></a>成为 leader</h3><p>如果当前节点这个 <code>__consumer_offset</code> 有 Partition 成为 leader，GroupCoordinator 通过 <code>handleGroupImmigration()</code> 方法进行相应的处理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 加载这个 Partition 对应的 group offset 信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupImmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</span><br><span class="line">  groupManager.loadGroupsForPartition(offsetTopicPartitionId, onGroupLoaded)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 异步地加载这个 offset Partition 的信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doLoadGroupsAndOffsets</span></span>() &#123;</span><br><span class="line">    info(<span class="string">s"Loading offsets and group metadata from <span class="subst">$topicPartition</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 添加到  loadingPartitions 集合中</span></span><br><span class="line">    inLock(partitionLock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (loadingPartitions.contains(offsetsPartition)) &#123;</span><br><span class="line">        info(<span class="string">s"Offset load from <span class="subst">$topicPartition</span> already in progress."</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        loadingPartitions.add(offsetsPartition)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 开始加载，加载成功的话，将该 Partition 从 loadingPartitions 集合中移除，添加到 ownedPartition 集合中</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      loadGroupsAndOffsets(topicPartition, onGroupLoaded)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt; error(<span class="string">s"Error loading offsets from <span class="subst">$topicPartition</span>"</span>, t)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      inLock(partitionLock) &#123;</span><br><span class="line">        ownedPartitions.add(offsetsPartition)</span><br><span class="line">        loadingPartitions.remove(offsetsPartition)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  scheduler.schedule(topicPartition.toString, doLoadGroupsAndOffsets)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法的做的事情是：</p>
<ol>
<li>将正在处理的 Partition 添加到 loadingPartitions 集合中，这个集合内都是当前正在加载的 Partition（特指 <code>__consumer_offset</code> Topic）；</li>
<li>通过 <code>loadGroupsAndOffsets()</code> 加载这个 Partition 的数据，处理完成后，该 Partition 从 loadingPartitions 中清除，并添加到 ownedPartitions 集合中。</li>
</ol>
<p><code>loadGroupsAndOffsets()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 读取该 group offset Partition 数据</span></span><br><span class="line"><span class="keyword">private</span>[coordinator] <span class="function"><span class="keyword">def</span> <span class="title">loadGroupsAndOffsets</span></span>(topicPartition: <span class="type">TopicPartition</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</span><br><span class="line">  <span class="comment">//note: 这个必然有本地副本，现获取 hw（如果本地是 leader 的情况，否则返回-1）</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">highWaterMark</span> </span>= replicaManager.getHighWatermark(topicPartition).getOrElse(<span class="number">-1</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> startMs = time.milliseconds()</span><br><span class="line">  replicaManager.getLog(topicPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      warn(<span class="string">s"Attempted to load offsets and group metadata from <span class="subst">$topicPartition</span>, but found no log"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</span><br><span class="line">      <span class="keyword">var</span> currOffset = log.logStartOffset <span class="comment">//note: 这副本最起始的 offset</span></span><br><span class="line">      <span class="keyword">val</span> buffer = <span class="type">ByteBuffer</span>.allocate(config.loadBufferSize) <span class="comment">//note: 默认5MB</span></span><br><span class="line">      <span class="comment">// loop breaks if leader changes at any time during the load, since getHighWatermark is -1</span></span><br><span class="line">      <span class="comment">//note: group 与 offset 的对应关系</span></span><br><span class="line">      <span class="keyword">val</span> loadedOffsets = mutable.<span class="type">Map</span>[<span class="type">GroupTopicPartition</span>, <span class="type">OffsetAndMetadata</span>]()</span><br><span class="line">      <span class="keyword">val</span> removedOffsets = mutable.<span class="type">Set</span>[<span class="type">GroupTopicPartition</span>]()</span><br><span class="line">      <span class="comment">//note: Group 对应的 meta 信息</span></span><br><span class="line">      <span class="keyword">val</span> loadedGroups = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">GroupMetadata</span>]()</span><br><span class="line">      <span class="keyword">val</span> removedGroups = mutable.<span class="type">Set</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (currOffset &lt; highWaterMark &amp;&amp; !shuttingDown.get()) &#123; <span class="comment">//note: 直到读取到 hw 位置，或服务关闭</span></span><br><span class="line">        buffer.clear()</span><br><span class="line">        <span class="keyword">val</span> fileRecords = log.read(currOffset, config.loadBufferSize, maxOffset = <span class="type">None</span>, minOneMessage = <span class="literal">true</span>)</span><br><span class="line">          .records.asInstanceOf[<span class="type">FileRecords</span>]</span><br><span class="line">        <span class="keyword">val</span> bufferRead = fileRecords.readInto(buffer, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="type">MemoryRecords</span>.readableRecords(bufferRead).deepEntries.asScala.foreach &#123; entry =&gt;</span><br><span class="line">          <span class="keyword">val</span> record = entry.record</span><br><span class="line">          require(record.hasKey, <span class="string">"Group metadata/offset entry key should not be null"</span>)</span><br><span class="line"></span><br><span class="line">          <span class="type">GroupMetadataManager</span>.readMessageKey(record.key) <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> offsetKey: <span class="type">OffsetKey</span> =&gt; <span class="comment">//note: GroupTopicPartition，有 group 和 topic-partition</span></span><br><span class="line">              <span class="comment">// load offset</span></span><br><span class="line">              <span class="comment">//note: 加载 offset 信息</span></span><br><span class="line">              <span class="keyword">val</span> key = offsetKey.key</span><br><span class="line">              <span class="keyword">if</span> (record.hasNullValue) &#123; <span class="comment">//note: value 为空</span></span><br><span class="line">                loadedOffsets.remove(key)</span><br><span class="line">                removedOffsets.add(key)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 有 commit offset 信息</span></span><br><span class="line">                <span class="keyword">val</span> value = <span class="type">GroupMetadataManager</span>.readOffsetMessageValue(record.value)</span><br><span class="line">                loadedOffsets.put(key, value)</span><br><span class="line">                removedOffsets.remove(key)</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">case</span> groupMetadataKey: <span class="type">GroupMetadataKey</span> =&gt;</span><br><span class="line">              <span class="comment">// load group metadata</span></span><br><span class="line">              <span class="comment">//note: 加载 group metadata 信息</span></span><br><span class="line">              <span class="keyword">val</span> groupId = groupMetadataKey.key</span><br><span class="line">              <span class="keyword">val</span> groupMetadata = <span class="type">GroupMetadataManager</span>.readGroupMessageValue(groupId, record.value)</span><br><span class="line">              <span class="keyword">if</span> (groupMetadata != <span class="literal">null</span>) &#123;</span><br><span class="line">                trace(<span class="string">s"Loaded group metadata for group <span class="subst">$groupId</span> with generation <span class="subst">$&#123;groupMetadata.generationId&#125;</span>"</span>)</span><br><span class="line">                removedGroups.remove(groupId)</span><br><span class="line">                loadedGroups.put(groupId, groupMetadata)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 更新最新的信息</span></span><br><span class="line">                loadedGroups.remove(groupId)</span><br><span class="line">                removedGroups.add(groupId)</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">case</span> unknownKey =&gt;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected message key <span class="subst">$unknownKey</span> while loading offsets and group metadata"</span>)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          currOffset = entry.nextOffset</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> (groupOffsets, emptyGroupOffsets) = loadedOffsets</span><br><span class="line">        .groupBy(_._1.group)</span><br><span class="line">        .mapValues(_.map &#123; <span class="keyword">case</span> (groupTopicPartition, offset) =&gt; (groupTopicPartition.topicPartition, offset)&#125; )</span><br><span class="line">        .partition &#123; <span class="keyword">case</span> (group, _) =&gt; loadedGroups.contains(group) &#125; <span class="comment">//note: 把集合根据条件分两个部分</span></span><br><span class="line"></span><br><span class="line">      loadedGroups.values.foreach &#123; group =&gt;</span><br><span class="line">        <span class="keyword">val</span> offsets = groupOffsets.getOrElse(group.groupId, <span class="type">Map</span>.empty[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>])</span><br><span class="line">        loadGroup(group, offsets) <span class="comment">//note: 在缓存中添加 group 和初始化 offset 信息</span></span><br><span class="line">        onGroupLoaded(group) <span class="comment">//note: 设置 group 下一次心跳超时时间</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// load groups which store offsets in kafka, but which have no active members and thus no group</span></span><br><span class="line">      <span class="comment">// metadata stored in the log</span></span><br><span class="line">      <span class="comment">//note: 加载哪些有 offset 信息但是当前没有活跃的 member 信息的 group</span></span><br><span class="line">      emptyGroupOffsets.foreach &#123; <span class="keyword">case</span> (groupId, offsets) =&gt;</span><br><span class="line">        <span class="keyword">val</span> group = <span class="keyword">new</span> <span class="type">GroupMetadata</span>(groupId)</span><br><span class="line">        loadGroup(group, offsets)</span><br><span class="line">        onGroupLoaded(group)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      removedGroups.foreach &#123; groupId =&gt;</span><br><span class="line">        <span class="comment">// if the cache already contains a group which should be removed, raise an error. Note that it</span></span><br><span class="line">        <span class="comment">// is possible (however unlikely) for a consumer group to be removed, and then to be used only for</span></span><br><span class="line">        <span class="comment">// offset storage (i.e. by "simple" consumers)</span></span><br><span class="line">        <span class="keyword">if</span> (groupMetadataCache.contains(groupId) &amp;&amp; !emptyGroupOffsets.contains(groupId))</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected unload of active group <span class="subst">$groupId</span> while "</span> +</span><br><span class="line">            <span class="string">s"loading partition <span class="subst">$topicPartition</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (!shuttingDown.get())</span><br><span class="line">        info(<span class="string">"Finished loading offsets from %s in %d milliseconds."</span></span><br><span class="line">          .format(topicPartition, time.milliseconds() - startMs))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面方法的实现虽然比较长，但是处理逻辑还是比较简单的，实现结果如下：</p>
<ol>
<li>获取这个 Partition 的 HW 值（如果 leader 不在本地，那么返回-1）；</li>
<li>初始化 loadedOffsets 和 removedOffsets、loadedGroups 和 removedGroups 集合，它们就是 group offset 信息以及 consumer member 信息；</li>
<li>从这个 Partition 第一条数据开始读取，直到读取到 HW 位置，加载相应的 commit offset、consumer member 信息，因为是顺序读取的，所以会新的值会覆盖前面的值；</li>
<li>通过 <code>loadGroup()</code> 加载到 GroupCoordinator 的缓存中。</li>
</ol>
<p>经过上面这些步骤，这个 Partition 的数据就被完整加载缓存中了。</p>
<h3 id="变成-follower"><a href="#变成-follower" class="headerlink" title="变成 follower"></a>变成 follower</h3><p>如果 <code>__consumer_offset</code> 有 Partition 变成了 follower（之前是 leader，如果之前不是 leader，不会走到这一步的），GroupCoordinator 通过 <code>handleGroupEmigration()</code> 移除这个 Partition 相应的缓存信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 移除这个 Partition 对应的 group offset 信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupEmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</span><br><span class="line">  groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>removeGroupsForPartition()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 当一个 broker 变成一个 follower 时，清空这个 partition 的相关缓存信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>,</span><br><span class="line">                             onGroupUnloaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</span><br><span class="line">  scheduler.schedule(topicPartition.toString, removeGroupsAndOffsets)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">removeGroupsAndOffsets</span></span>() &#123;</span><br><span class="line">    <span class="keyword">var</span> numOffsetsRemoved = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> numGroupsRemoved = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    inLock(partitionLock) &#123;</span><br><span class="line">      <span class="comment">// we need to guard the group removal in cache in the loading partition lock</span></span><br><span class="line">      <span class="comment">// to prevent coordinator's check-and-get-group race condition</span></span><br><span class="line">      ownedPartitions.remove(offsetsPartition)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (group &lt;- groupMetadataCache.values) &#123;</span><br><span class="line">        <span class="keyword">if</span> (partitionFor(group.groupId) == offsetsPartition) &#123;</span><br><span class="line">          onGroupUnloaded(group) <span class="comment">//note: 将 group 状态转移成 dead</span></span><br><span class="line">          groupMetadataCache.remove(group.groupId, group) <span class="comment">//note: 清空 group 的信息</span></span><br><span class="line">          numGroupsRemoved += <span class="number">1</span></span><br><span class="line">          numOffsetsRemoved += group.numOffsets</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (numOffsetsRemoved &gt; <span class="number">0</span>)</span><br><span class="line">      info(<span class="string">s"Removed <span class="subst">$numOffsetsRemoved</span> cached offsets for <span class="subst">$topicPartition</span> on follower transition."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (numGroupsRemoved &gt; <span class="number">0</span>)</span><br><span class="line">      info(<span class="string">s"Removed <span class="subst">$numGroupsRemoved</span> cached groups for <span class="subst">$topicPartition</span> on follower transition."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onGroupUnloaded</span></span>(group: <span class="type">GroupMetadata</span>) &#123;</span><br><span class="line">  group synchronized &#123;</span><br><span class="line">    info(<span class="string">s"Unloading group metadata for <span class="subst">$&#123;group.groupId&#125;</span> with generation <span class="subst">$&#123;group.generationId&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">val</span> previousState = group.currentState</span><br><span class="line">    group.transitionTo(<span class="type">Dead</span>) <span class="comment">//note: 状态转移成 dead</span></span><br><span class="line"></span><br><span class="line">    previousState <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Empty</span> | <span class="type">Dead</span> =&gt;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt;</span><br><span class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息返回异常</span></span><br><span class="line">          <span class="keyword">if</span> (member.awaitingJoinCallback != <span class="literal">null</span>) &#123;</span><br><span class="line">            member.awaitingJoinCallback(joinError(member.memberId, <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code))</span><br><span class="line">            member.awaitingJoinCallback = <span class="literal">null</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt;</span><br><span class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息，返回异常</span></span><br><span class="line">          <span class="keyword">if</span> (member.awaitingSyncCallback != <span class="literal">null</span>) &#123;</span><br><span class="line">            member.awaitingSyncCallback(<span class="type">Array</span>.empty[<span class="type">Byte</span>], <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</span><br><span class="line">            member.awaitingSyncCallback = <span class="literal">null</span></span><br><span class="line">          &#125;</span><br><span class="line">          heartbeatPurgatory.checkAndComplete(<span class="type">MemberKey</span>(member.groupId, member.memberId))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于在这个 Partition 上的所有 Group，会按下面的步骤执行：</p>
<ol>
<li>通过 <code>onGroupUnloaded()</code> 方法先将这个 Group 的状态转换为 dead，如果 Group 处在 PreparingRebalance/Stable/AwaitingSync 状态，并且设置了相应的回调函数，那么就在回调函数中返回带有 NOT_COORDINATOR_FOR_GROUP 异常信息的响应，consumer 在收到这个异常信息会重新加入 group；</li>
<li>从缓存中移除这个 Group 的信息。</li>
</ol>
<p>这个遍历执行完成之后，这个 Topic Partition 就从 Leader 变成了 follower 状态。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 发送模型（二十一）</title>
    <link href="http://matt33.com/2018/06/23/controller-request-model/"/>
    <id>http://matt33.com/2018/06/23/controller-request-model/</id>
    <published>2018-06-23T05:26:38.000Z</published>
    <updated>2019-02-24T02:29:01.552Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 部分的一个补充，在这篇文章中，将会看到 Controller 在处理 leader 切换、ShutDown 请求时如何向 Broker 发送相应的请求。</p>
<p>Kafka Controller 向 Broker 发送的请求类型主要分为三种：LeaderAndIsr、UpdateMetadata、StopReplica 请求，正如  <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 这里介绍的，Controller 会为每台 Broker 初始化为一个 ControllerBrokerStateInfo 对象，该对象主要包含以下四个内容：</p>
<ol>
<li>NetworkClient：与 Broker 的网络连接对象；</li>
<li>Node：Broker 的节点信息；</li>
<li>MessageQueue：每个 Broker 对应的请求队列，Controller 向 Broker 发送的请求会想放在这个队列里；</li>
<li>RequestSendThread：每台 Broker 对应的请求发送线程。</li>
</ol>
<h2 id="Controller-的请求发送模型"><a href="#Controller-的请求发送模型" class="headerlink" title="Controller 的请求发送模型"></a>Controller 的请求发送模型</h2><p>在讲述 Controller 发送模型之前，先看下 Controller 是如何向 Broker 发送请求的，这里以发送 metadata 更新请求为例，简略的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 创建新的批量请求</span></span><br><span class="line">brokerRequestBatch.newBatch()</span><br><span class="line"><span class="comment">//note: 为目标 Broker 添加相应的请求</span></span><br><span class="line">brokerRequestBatch.addUpdateMetadataRequestForBrokers(brokers, partitions)</span><br><span class="line"><span class="comment">//note: 发送请求，实际上只是把请求添加发送线程的 request queue 中</span></span><br><span class="line">brokerRequestBatch.sendRequestsToBrokers(epoch)</span><br></pre></td></tr></table></figure>
<p>这里有一个比较重要的对象，就是 ControllerBrokerRequestBatch 对象，可以认为它是一个专门用于批量请求发送的对象，在这个对象中有几个重要成员变量：</p>
<ol>
<li>leaderAndIsrRequestMap：记录每个 broker 与要发送的 LeaderAndIsr 请求集合的 map；</li>
<li>stopReplicaRequestMap：记录每个 broker 与要发送的 StopReplica 集合的 map；</li>
<li>updateMetadataRequestBrokerSet：记录要发送的 update-metadata 请求的 broker 集合；</li>
<li>updateMetadataRequestPartitionInfoMap：记录 update-metadata 请求要更新的 Topic Partition 集合。</li>
</ol>
<p>Controller 可以通过下面这三方法向这些集合添加相应的请求：</p>
<ol>
<li><code>addLeaderAndIsrRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 LeaderAndIsr 请求；</li>
<li><code>addStopReplicaRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 StopReplica 请求；</li>
<li><code>addUpdateMetadataRequestForBrokers()</code>：向给定的 Broker 发送某一批 Partitions 的 UpdateMetadata 请求。</li>
</ol>
<p>Controller 整体的请求模型概况如下图所示：</p>
<p><img src="/images/kafka/controller-request-model.png" alt="Controller 的请求发送模型"></p>
<p>上述三个方法将相应的请求添加到对应的集合中后，然后通过 <code>sendRequestsToBrokers()</code> 方法将该请求添加到该 Broker 对应的请求队列中，接着再由该 Broker 对应的 RequestSendThread 去发送相应的请求。</p>
<h2 id="ControllerBrokerRequestBatch"><a href="#ControllerBrokerRequestBatch" class="headerlink" title="ControllerBrokerRequestBatch"></a>ControllerBrokerRequestBatch</h2><p>这节详细讲述一下关于 ControllerBrokerRequestBatch 的一些方法实现。</p>
<h3 id="newBatch-方法"><a href="#newBatch-方法" class="headerlink" title="newBatch 方法"></a>newBatch 方法</h3><p>Controller 在添加请求前，都会先调用 <code>newBatch()</code> 方法，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 创建新的请求前,确保前一批请求全部发送完毕</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newBatch</span></span>() &#123;</span><br><span class="line">  <span class="comment">// raise error if the previous batch is not empty</span></span><br><span class="line">  <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating "</span> +</span><br><span class="line">      <span class="string">"a new one. Some LeaderAndIsr state changes %s might be lost "</span>.format(leaderAndIsrRequestMap.toString()))</span><br><span class="line">  <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</span><br><span class="line">      <span class="string">"new one. Some StopReplica state changes %s might be lost "</span>.format(stopReplicaRequestMap.toString()))</span><br><span class="line">  <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</span><br><span class="line">      <span class="string">"new one. Some UpdateMetadata state changes to brokers %s with partition info %s might be lost "</span>.format(</span><br><span class="line">        updateMetadataRequestBrokerSet.toString(), updateMetadataRequestPartitionInfoMap.toString()))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法的主要作用是检查上一波的 LeaderAndIsr、UpdateMetadata、StopReplica 请求是否已经发送，正常情况下，Controller 在调用 <code>sendRequestsToBrokers()</code> 方法之后，这些集合中的请求都会被发送，发送之后，会将相应的请求集合清空，当然在异常情况可能会导致部分集合没有被清空，导致无法 <code>newBatch()</code>，这种情况下，通常策略是重启 controller，因为现在 Controller 的设计还是有些复杂，在某些情况下还是可能会导致异常发生，并且有些异常还是无法恢复的。</p>
<h3 id="添加-LeaderAndIsr-请求"><a href="#添加-LeaderAndIsr-请求" class="headerlink" title="添加 LeaderAndIsr 请求"></a>添加 LeaderAndIsr 请求</h3><p>Controller 可以通过 <code>addLeaderAndIsrRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 LeaderAndIsr 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 LeaderAndIsr 添加到对应的 broker 中,还未开始发送数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addLeaderAndIsrRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</span><br><span class="line">                                     leaderIsrAndControllerEpoch: <span class="type">LeaderIsrAndControllerEpoch</span>,</span><br><span class="line">                                     replicas: <span class="type">Seq</span>[<span class="type">Int</span>], callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 将请求添加到对应的 broker 上</span></span><br><span class="line">  brokerIds.filter(_ &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</span><br><span class="line">    <span class="keyword">val</span> result = leaderAndIsrRequestMap.getOrElseUpdate(brokerId, mutable.<span class="type">Map</span>.empty)</span><br><span class="line">    result.put(topicPartition, <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas.toSet))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 在更新 LeaderAndIsr 信息时,主题的 metadata 相当于也进行了更新,需要发送这个 topic 的 metadata 给所有存活的 broker</span></span><br><span class="line">  addUpdateMetadataRequestForBrokers(controllerContext.liveOrShuttingDownBrokerIds.toSeq,</span><br><span class="line">                                     <span class="type">Set</span>(<span class="type">TopicAndPartition</span>(topic, partition)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法的处理流程如下：</p>
<ol>
<li>向对应的 Broker 添加 LeaderAndIsr 请求，请求会被添加到 leaderAndIsrRequestMap 集合中；</li>
<li>并通过 <code>addUpdateMetadataRequestForBrokers()</code> 方法向所有的 Broker 添加这个 Topic-Partition 的 UpdateMatedata 请求，leader 或 isr 变动时，会向所有 broker 同步这个 Partition 的 metadata 信息，这样可以保证每台 Broker 上都有最新的 metadata 信息。</li>
</ol>
<h3 id="添加-UpdateMetadata-请求"><a href="#添加-UpdateMetadata-请求" class="headerlink" title="添加 UpdateMetadata 请求"></a>添加 UpdateMetadata 请求</h3><p>Controller 可以通过 <code>addUpdateMetadataRequestForBrokers()</code> 向指定 Broker 列表添加某批 Partitions 的 UpdateMetadata 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 向给行的 Broker 发送 UpdateMetadataRequest 请求</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addUpdateMetadataRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">                                       partitions: collection.<span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = <span class="type">Set</span>.empty[<span class="type">TopicAndPartition</span>],</span><br><span class="line">                                       callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</span><br><span class="line">  <span class="comment">//note: 将 Topic-Partition 添加到对应的 map 中</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateMetadataRequestPartitionInfo</span></span>(partition: <span class="type">TopicAndPartition</span>, beingDeleted: <span class="type">Boolean</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> leaderIsrAndControllerEpochOpt = controllerContext.partitionLeadershipInfo.get(partition)</span><br><span class="line">    leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">        <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(partition).toSet</span><br><span class="line">        <span class="keyword">val</span> partitionStateInfo = <span class="keyword">if</span> (beingDeleted) &#123; <span class="comment">//note: 正在删除的 Partition,设置 leader 为-2</span></span><br><span class="line">          <span class="keyword">val</span> leaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(<span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>, leaderIsrAndControllerEpoch.leaderAndIsr.isr)</span><br><span class="line">          <span class="type">PartitionStateInfo</span>(<span class="type">LeaderIsrAndControllerEpoch</span>(leaderAndIsr, leaderIsrAndControllerEpoch.controllerEpoch), replicas)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//note: 添加到对应的 request map 中</span></span><br><span class="line">        updateMetadataRequestPartitionInfoMap.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(partition.topic, partition.partition), partitionStateInfo)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        info(<span class="string">"Leader not yet assigned for partition %s. Skip sending UpdateMetadataRequest."</span>.format(partition))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note:过滤出要发送的 partition</span></span><br><span class="line">  <span class="keyword">val</span> filteredPartitions = &#123;</span><br><span class="line">    <span class="keyword">val</span> givenPartitions = <span class="keyword">if</span> (partitions.isEmpty)</span><br><span class="line">      controllerContext.partitionLeadershipInfo.keySet <span class="comment">//note: Partitions 为空时，就过滤出所有的 topic</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      partitions</span><br><span class="line">    <span class="keyword">if</span> (controller.deleteTopicManager.partitionsToBeDeleted.isEmpty)</span><br><span class="line">      givenPartitions</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      givenPartitions -- controller.deleteTopicManager.partitionsToBeDeleted <span class="comment">//note: 将要删除的 topic 过滤掉</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  updateMetadataRequestBrokerSet ++= brokerIds.filter(_ &gt;= <span class="number">0</span>) <span class="comment">//note: 将 broker 列表更新到要发送的集合中</span></span><br><span class="line">  <span class="comment">//note: 对于要更新 metadata 的 Partition,设置 beingDeleted 为 False</span></span><br><span class="line">  filteredPartitions.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">false</span>))</span><br><span class="line">  <span class="comment">//note: 要删除的 Partition 设置 BeingDeleted 为 True</span></span><br><span class="line">  controller.deleteTopicManager.partitionsToBeDeleted.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">true</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法的实现逻辑如下：</p>
<ol>
<li>首先过滤出要发送的 Partition 列表，如果没有指定要发送 partitions 列表，那么默认就是发送全局的 metadata 信息；</li>
<li>接着将已经标记为删除的 Partition 从上面的列表中移除；</li>
<li>将要发送的 Broker 列表添加到 updateMetadataRequestBrokerSet 集合中；</li>
<li>将前面过滤的 Partition 列表对应的 metadata 信息添加到对应的 updateMetadataRequestPartitionInfoMap 集合中;</li>
<li>将当前设置为删除的所有 Partition 的 metadata 信息也添加到 updateMetadataRequestPartitionInfoMap 集合中，添加前会把其 leader 设置为-2，这样 Broker 收到这个 Partition 的 metadata 信息之后就会知道这个 Partition 是设置删除标志。</li>
</ol>
<h3 id="添加-StopReplica-请求"><a href="#添加-StopReplica-请求" class="headerlink" title="添加 StopReplica 请求"></a>添加 StopReplica 请求</h3><p>Controller 可以通过 <code>addStopReplicaRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 StopReplica 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 StopReplica 添加到对应的 Broker 中,还未开始发送数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addStopReplicaRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>, deletePartition: <span class="type">Boolean</span>,</span><br><span class="line">                                    callback: (<span class="type">AbstractResponse</span>, <span class="type">Int</span>) =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</span><br><span class="line">  brokerIds.filter(b =&gt; b &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</span><br><span class="line">    stopReplicaRequestMap.getOrElseUpdate(brokerId, <span class="type">Seq</span>.empty[<span class="type">StopReplicaRequestInfo</span>])</span><br><span class="line">    <span class="keyword">val</span> v = stopReplicaRequestMap(brokerId)</span><br><span class="line">    <span class="keyword">if</span>(callback != <span class="literal">null</span>)</span><br><span class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</span><br><span class="line">        deletePartition, (r: <span class="type">AbstractResponse</span>) =&gt; callback(r, brokerId))</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</span><br><span class="line">        deletePartition)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法的实现逻辑比较简单，直接将 StopReplica 添加到 stopReplicaRequestMap 中。</p>
<h3 id="向-Broker-发送请求"><a href="#向-Broker-发送请求" class="headerlink" title="向 Broker 发送请求"></a>向 Broker 发送请求</h3><p>Controller 在添加完相应的请求后，最后一步都会去调用 <code>sendRequestsToBrokers()</code> 方法构造相应的请求，并把请求添加到 Broker 对应的 RequestQueue 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 发送请求给 broker（只是将对应处理后放入到对应的 queue 中）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequestsToBrokers</span></span>(controllerEpoch: <span class="type">Int</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//note: LeaderAndIsr 请求</span></span><br><span class="line">    leaderAndIsrRequestMap.foreach &#123; <span class="keyword">case</span> (broker, partitionStateInfos) =&gt;</span><br><span class="line">      partitionStateInfos.foreach &#123; <span class="keyword">case</span> (topicPartition, state) =&gt;</span><br><span class="line">        <span class="keyword">val</span> typeOfRequest = <span class="keyword">if</span> (broker == state.leaderIsrAndControllerEpoch.leaderAndIsr.leader) <span class="string">"become-leader"</span> <span class="keyword">else</span> <span class="string">"become-follower"</span></span><br><span class="line">        stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending %s LeaderAndIsr request %s to broker %d "</span> +</span><br><span class="line">                                 <span class="string">"for partition [%s,%d]"</span>).format(controllerId, controllerEpoch, typeOfRequest,</span><br><span class="line">                                                                 state.leaderIsrAndControllerEpoch, broker,</span><br><span class="line">                                                                 topicPartition.topic, topicPartition.partition))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//note: leader id 集合</span></span><br><span class="line">      <span class="keyword">val</span> leaderIds = partitionStateInfos.map(_._2.leaderIsrAndControllerEpoch.leaderAndIsr.leader).toSet</span><br><span class="line">      <span class="keyword">val</span> leaders = controllerContext.liveOrShuttingDownBrokers.filter(b =&gt; leaderIds.contains(b.id)).map &#123;</span><br><span class="line">        _.getNode(controller.config.interBrokerListenerName)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//note: requests.PartitionState</span></span><br><span class="line">      <span class="keyword">val</span> partitionStates = partitionStateInfos.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</span><br><span class="line">        <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</span><br><span class="line">        <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</span><br><span class="line">          leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</span><br><span class="line">          partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</span><br><span class="line">        topicPartition -&gt; partitionState</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">//note: 构造 LeaderAndIsr 请求,并添加到对应的 queue 中</span></span><br><span class="line">      <span class="keyword">val</span> leaderAndIsrRequest = <span class="keyword">new</span> <span class="type">LeaderAndIsrRequest</span>.</span><br><span class="line">          <span class="type">Builder</span>(controllerId, controllerEpoch, partitionStates.asJava, leaders.asJava)</span><br><span class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">LEADER_AND_ISR</span>, leaderAndIsrRequest, <span class="literal">null</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    leaderAndIsrRequestMap.clear() <span class="comment">//note: 清空 leaderAndIsr 集合</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: update-metadata 请求</span></span><br><span class="line">    updateMetadataRequestPartitionInfoMap.foreach(p =&gt; stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending UpdateMetadata request %s "</span> +</span><br><span class="line">      <span class="string">"to brokers %s for partition %s"</span>).format(controllerId, controllerEpoch, p._2.leaderIsrAndControllerEpoch,</span><br><span class="line">      updateMetadataRequestBrokerSet.toString(), p._1)))</span><br><span class="line">    <span class="keyword">val</span> partitionStates = updateMetadataRequestPartitionInfoMap.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</span><br><span class="line">      <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</span><br><span class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</span><br><span class="line">        leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</span><br><span class="line">        partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</span><br><span class="line">      topicPartition -&gt; partitionState</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> version: <span class="type">Short</span> =</span><br><span class="line">      <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_2_IV0</span>) <span class="number">3</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_0_IV1</span>) <span class="number">2</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_9_0</span>) <span class="number">1</span></span><br><span class="line">      <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 构造 update-metadata 请求</span></span><br><span class="line">    <span class="keyword">val</span> updateMetadataRequest = &#123;</span><br><span class="line">      <span class="keyword">val</span> liveBrokers = <span class="keyword">if</span> (version == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// Version 0 of UpdateMetadataRequest only supports PLAINTEXT.</span></span><br><span class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</span><br><span class="line">          <span class="keyword">val</span> securityProtocol = <span class="type">SecurityProtocol</span>.<span class="type">PLAINTEXT</span></span><br><span class="line">          <span class="keyword">val</span> listenerName = <span class="type">ListenerName</span>.forSecurityProtocol(securityProtocol)</span><br><span class="line">          <span class="keyword">val</span> node = broker.getNode(listenerName)</span><br><span class="line">          <span class="keyword">val</span> endPoints = <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">EndPoint</span>(node.host, node.port, securityProtocol, listenerName))</span><br><span class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</span><br><span class="line">          <span class="keyword">val</span> endPoints = broker.endPoints.map &#123; endPoint =&gt;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">EndPoint</span>(endPoint.host, endPoint.port, endPoint.securityProtocol, endPoint.listenerName)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Builder</span>(</span><br><span class="line">        controllerId, controllerEpoch, partitionStates.asJava, liveBrokers.asJava).</span><br><span class="line">        setVersion(version)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 将请求添加到对应的 queue</span></span><br><span class="line">    updateMetadataRequestBrokerSet.foreach &#123; broker =&gt;</span><br><span class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">UPDATE_METADATA_KEY</span>, updateMetadataRequest, <span class="literal">null</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    updateMetadataRequestBrokerSet.clear() <span class="comment">//note: 清空对应的请求记录</span></span><br><span class="line">    updateMetadataRequestPartitionInfoMap.clear()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: StopReplica 请求的处理</span></span><br><span class="line">    stopReplicaRequestMap.foreach &#123; <span class="keyword">case</span> (broker, replicaInfoList) =&gt;</span><br><span class="line">      <span class="keyword">val</span> stopReplicaWithDelete = replicaInfoList.filter(_.deletePartition).map(_.replica).toSet</span><br><span class="line">      <span class="keyword">val</span> stopReplicaWithoutDelete = replicaInfoList.filterNot(_.deletePartition).map(_.replica).toSet</span><br><span class="line">      debug(<span class="string">"The stop replica request (delete = true) sent to broker %d is %s"</span></span><br><span class="line">        .format(broker, stopReplicaWithDelete.mkString(<span class="string">","</span>)))</span><br><span class="line">      debug(<span class="string">"The stop replica request (delete = false) sent to broker %d is %s"</span></span><br><span class="line">        .format(broker, stopReplicaWithoutDelete.mkString(<span class="string">","</span>)))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> (replicasToGroup, replicasToNotGroup) = replicaInfoList.partition(r =&gt; !r.deletePartition &amp;&amp; r.callback == <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Send one StopReplicaRequest for all partitions that require neither delete nor callback. This potentially</span></span><br><span class="line">      <span class="comment">// changes the order in which the requests are sent for the same partitions, but that's OK.</span></span><br><span class="line">      <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(controllerId, controllerEpoch, <span class="literal">false</span>,</span><br><span class="line">        replicasToGroup.map(r =&gt; <span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).toSet.asJava)</span><br><span class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest)</span><br><span class="line"></span><br><span class="line">      replicasToNotGroup.foreach &#123; r =&gt;</span><br><span class="line">        <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(</span><br><span class="line">            controllerId, controllerEpoch, r.deletePartition,</span><br><span class="line">            <span class="type">Set</span>(<span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).asJava)</span><br><span class="line">        controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest, r.callback)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stopReplicaRequestMap.clear()</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty) &#123;</span><br><span class="line">        error(<span class="string">"Haven't been able to send leader and isr requests, current state of "</span> +</span><br><span class="line">            <span class="string">s"the map is <span class="subst">$leaderAndIsrRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty) &#123;</span><br><span class="line">        error(<span class="string">s"Haven't been able to send metadata update requests to brokers <span class="subst">$updateMetadataRequestBrokerSet</span>, "</span> +</span><br><span class="line">              <span class="string">s"current state of the partition info is <span class="subst">$updateMetadataRequestPartitionInfoMap</span>. Exception message: <span class="subst">$e</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty) &#123;</span><br><span class="line">        error(<span class="string">"Haven't been able to send stop replica requests, current state of "</span> +</span><br><span class="line">            <span class="string">s"the map is <span class="subst">$stopReplicaRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(e)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面这个方法看着很复杂，其实做的事情很明确，就是将三个集合中的请求发送对应 Broker 的请求队列中，这里简单作一个总结：</p>
<ol>
<li>从 leaderAndIsrRequestMap 集合中构造相应的 LeaderAndIsr 请求，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 leaderAndIsrRequestMap 集合；</li>
<li>从 updateMetadataRequestPartitionInfoMap 集合中构造相应的 UpdateMetadata 请求，，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 updateMetadataRequestBrokerSet 和 updateMetadataRequestPartitionInfoMap 集合；</li>
<li>从 stopReplicaRequestMap 集合中构造相应的 StopReplica 请求，在构造时会根据是否设置删除标志将要涉及的 Partition 分成两类，构造对应的请求，对于要删除数据的 StopReplica 会设置相应的回调函数，然后通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 stopReplicaRequestMap 集合。</li>
</ol>
<p>走到这一步，Controller 要发送的请求算是都添加到对应 Broker 的 MessageQueue 中，后台的 RequestSendThread 线程会从这个请求队列中遍历相应的请求，发送给对应的 Broker。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 &lt;a href=&quot;http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager&quot;&gt;Contro
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Topic 的新建/扩容/删除（二十）</title>
    <link href="http://matt33.com/2018/06/18/topic-create-alter-delete/"/>
    <id>http://matt33.com/2018/06/18/topic-create-alter-delete/</id>
    <published>2018-06-18T09:24:21.000Z</published>
    <updated>2019-02-24T02:29:01.552Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 <a href="http://matt33.com/2017/07/21/kafka-topic-create/">Kafka 源码解析之 topic 创建过程（三）</a> 中已经讲述过了，本篇与前面不同的是，本篇主要是从 Controller 角度来讲述，而且是把新建、扩容、删除这三个 Topic 级别的操作放在一起做一个总结。</p>
<h2 id="Topic-新建与扩容"><a href="#Topic-新建与扩容" class="headerlink" title="Topic 新建与扩容"></a>Topic 新建与扩容</h2><p>这里把 Topic 新建与扩容放在一起讲解，主要是因为无论 Topic 是新建还是扩容，在 Kafka 内部其实都是 Partition 的新建，底层的实现机制是一样的，Topic 的新建与扩容的整体流程如下图所示：</p>
<p><img src="/images/kafka/topic-create-alter.png" alt="Topic 新建与扩容流程"></p>
<p>Topic 新建与扩容触发条件的不同如下所示：</p>
<ol>
<li>对于 Topic 扩容，监控的节点是 <code>/brokers/topics/TOPIC_NAME</code>，监控的是具体的 Topic 节点，通过 PartitionStateMachine 的 <code>registerPartitionChangeListener(topic)</code> 方法注册的相应 listener；</li>
<li>对于 Topic 新建，监控的节点是 <code>/brokers/topics</code>，监控的是 Topic 列表，通过 PartitionStateMachine 的 <code>registerTopicChangeListener()</code> 方法注册的相应 listener。</li>
</ol>
<p>下面开始详细讲述这两种情况。</p>
<h3 id="Topic-扩容"><a href="#Topic-扩容" class="headerlink" title="Topic 扩容"></a>Topic 扩容</h3><p>Kafka 提供了 Topic 扩容工具，假设一个 Topic（topic_test）只有一个 partition，这时候我们想把它扩容到两个 Partition，可以通过下面两个命令来实现：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --partitions 2</span><br><span class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --replica-assignment 1:2,2:1 --partitions 2</span><br></pre></td></tr></table></figure>
<p>这两种方法的区别是：第二种方法直接指定了要扩容的 Partition 2 的副本需要分配到哪台机器上，这样的话我们可以精确控制到哪些 Topic 放下哪些机器上。</p>
<p>无论是使用哪种方案，上面两条命令产生的结果只有一个，将 Topic 各个 Partition 的副本写入到 ZK 对应的节点上，这样的话 <code>/brokers/topics/topic_test</code> 节点的内容就会发生变化，PartitionModificationsListener 监听器就会被触发，该监听器的处理流程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: Partition change 监听器,主要是用于 Partition 扩容的监听</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionModificationsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"AddPartitionsListener"</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</span><br><span class="line">    inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        info(<span class="string">s"Partition modification triggered <span class="subst">$data</span> for path <span class="subst">$dataPath</span>"</span>)</span><br><span class="line">        <span class="keyword">val</span> partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(<span class="type">List</span>(topic))</span><br><span class="line">        <span class="comment">//note: 获取新增的 partition 列表及其对应的分配副本列表</span></span><br><span class="line">        <span class="keyword">val</span> partitionsToBeAdded = partitionReplicaAssignment.filter(p =&gt;</span><br><span class="line">          !controllerContext.partitionReplicaAssignment.contains(p._1))</span><br><span class="line">        <span class="comment">//note: 如果该 topic 被标记为删除,那么直接跳过,不再处理,否则创建该 Partition</span></span><br><span class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(topic))</span><br><span class="line">          error(<span class="string">"Skipping adding partitions %s for topic %s since it is currently being deleted"</span></span><br><span class="line">                .format(partitionsToBeAdded.map(_._1.partition).mkString(<span class="string">","</span>), topic))</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (partitionsToBeAdded.nonEmpty) &#123;</span><br><span class="line">            info(<span class="string">"New partitions to be added %s"</span>.format(partitionsToBeAdded))</span><br><span class="line">            controllerContext.partitionReplicaAssignment.++=(partitionsToBeAdded)</span><br><span class="line">            controller.onNewPartitionCreation(partitionsToBeAdded.keySet)<span class="comment">//note: 创建新的 partition</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling add partitions for data path "</span> + dataPath, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// this is not implemented for partition change</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(parentPath: <span class="type">String</span>): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其 <code>doHandleDataChange()</code> 方法的处理流程如下：</p>
<ol>
<li>首先获取该 Topic 在 ZK 的 Partition 副本列表，跟本地的缓存做对比，获取新增的 Partition 列表；</li>
<li>检查这个 Topic 是否被标记为删除，如果被标记了，那么直接跳过，不再处理这个 Partition 扩容的请求；</li>
<li>调用 KafkaController 的 <code>onNewPartitionCreation()</code> 新建该 Partition。</li>
</ol>
<p>下面我们看下 <code>onNewPartitionCreation()</code> 方法，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 用于 Topic Partition 的新建</span></span><br><span class="line"><span class="comment">//note: 1. 将新创建的 partition 状态置为 NewPartition 状态;</span></span><br><span class="line"><span class="comment">//note: 2. 将新创建的 Replica 状态置为 NewReplica 状态;</span></span><br><span class="line"><span class="comment">//note: 3. 将该 Partition 从 NewPartition 改为 OnlinePartition 状态,这期间会 为该 Partition 选举 leader 和 isr，更新到 zk 和 controller的缓存中</span></span><br><span class="line"><span class="comment">//note: 4. 将副本状态从 NewReplica 改为 OnlineReplica 状态。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewPartitionCreation</span></span>(newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</span><br><span class="line">  info(<span class="string">"New partition creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</span><br><span class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">NewPartition</span>)</span><br><span class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">NewReplica</span>)</span><br><span class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">OnlinePartition</span>, offlinePartitionSelector)</span><br><span class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">OnlineReplica</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关于 Partition 的新建，总共分了以下四步：</p>
<ol>
<li>将新创建的 Partition 状态置为 NewPartition 状态，此时 Partition 刚刚创建，只是分配了相应的 Replica 但是还没有 leader 和 isr，不能正常工作;</li>
<li>将该 Partition 对应的 Replica 列表状态设置为 NewReplica 状态，这部分只是将 Replica 的状态设置为了 NewReplica，并没有做其他的处理;</li>
<li>将该 Partition 的状态从 NewPartition 改为 OnlinePartition 状态，这期间会为该 Partition 选举 leader 和 isr，并将结果更新到 ZK 和 Controller 的缓存中，并向该 Partition 的所有副本发送对应的 LeaderAndIsr 信息（发送 LeaderAndIsr 请求的同时也会向所有 Broker 发送该 Topic 的 leader、isr metadata 信息）；</li>
<li>将副本状态从 NewReplica 转移为 OnlineReplica 状态。</li>
</ol>
<p>经过上面几个阶段，一个 Partition 算是真正创建出来，可以正常进行读写工作了，当然上面只是讲述了 Controller 端做的内容，Partition 副本所在节点对 LeaderAndIsr 请求会做更多的工作，这部分会在后面关于 LeaderAndIsr 请求的处理中只能够详细讲述。</p>
<h3 id="Topic-新建"><a href="#Topic-新建" class="headerlink" title="Topic 新建"></a>Topic 新建</h3><p>Kafka 也提供了 Topic 创建的工具，假设我们要创建一个名叫 topic_test，Partition 数为2的 Topic，创建的命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --partitions 2 --replication-factor 2</span><br><span class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --replica-assignment 1:2,2:1 --partitions 2</span><br></pre></td></tr></table></figure>
<p>跟前面的类似，方法二是可以精确控制新建 Topic 每个 Partition 副本所在位置，Topic 创建的本质上是在 <code>/brokers/topics</code> 下新建一个节点信息，并将 Topic 的分区详情写入进去，当 <code>/brokers/topics</code> 有了新增的 Topic 节点后，会触发 TopicChangeListener 监听器，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 监控 zk 上 Topic 子节点的变化 ,KafkaController 会进行相应的处理</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"TopicChangeListener"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 当 zk 上 topic 节点上有变更时,这个方法就会调用</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> currentChildren = &#123;</span><br><span class="line">            debug(<span class="string">"Topic change listener fired for path %s with children %s"</span>.format(parentPath, children.mkString(<span class="string">","</span>)))</span><br><span class="line">            children.toSet</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">//note: 新创建的 topic 列表</span></span><br><span class="line">          <span class="keyword">val</span> newTopics = currentChildren -- controllerContext.allTopics</span><br><span class="line">          <span class="comment">//note: 已经删除的 topic 列表</span></span><br><span class="line">          <span class="keyword">val</span> deletedTopics = controllerContext.allTopics -- currentChildren</span><br><span class="line">          controllerContext.allTopics = currentChildren</span><br><span class="line"></span><br><span class="line">          <span class="comment">//note: 新创建 topic 对应的 partition 列表及副本列表添加到 Controller 的缓存中</span></span><br><span class="line">          <span class="keyword">val</span> addedPartitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(newTopics.toSeq)</span><br><span class="line">          <span class="comment">//note: Controller 从缓存中把已经删除 partition 过滤掉</span></span><br><span class="line">          controllerContext.partitionReplicaAssignment = controllerContext.partitionReplicaAssignment.filter(p =&gt;</span><br><span class="line">            !deletedTopics.contains(p._1.topic))</span><br><span class="line">          controllerContext.partitionReplicaAssignment.++=(addedPartitionReplicaAssignment)<span class="comment">//note: 将新增的 tp-replicas 更新到缓存中</span></span><br><span class="line">          info(<span class="string">"New topics: [%s], deleted topics: [%s], new partition replica assignment [%s]"</span>.format(newTopics,</span><br><span class="line">            deletedTopics, addedPartitionReplicaAssignment))</span><br><span class="line">          <span class="keyword">if</span> (newTopics.nonEmpty)<span class="comment">//note: 处理新建的 topic</span></span><br><span class="line">            controller.onNewTopicCreation(newTopics, addedPartitionReplicaAssignment.keySet)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling new topic"</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只要 <code>/brokers/topics</code> 下子节点信息有变化（topic 新增或者删除），TopicChangeListener 都会被触发，其 <code>doHandleChildChange()</code> 方法的处理流程如下：</p>
<ol>
<li>获取 ZK 当前的所有 Topic 列表，根据本地缓存的 Topic 列表记录，可以得到新增的 Topic 记录与已经删除的 Topic 列表；</li>
<li>将新增 Topic 的相信信息更新到 Controller 的缓存中，将已经删除的 Topic 从 Controller 的副本缓存中移除；</li>
<li>调用 KafkaController 的 <code>onNewTopicCreation()</code> 方法创建该 topic。</li>
</ol>
<p>接着看下 <code>onNewTopicCreation()</code> 方法实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 当 partition state machine 监控到有新 topic 或 partition 时,这个方法将会被调用</span></span><br><span class="line"><span class="comment">//note: 1. 注册 partition change listener, 监听 Parition 变化;</span></span><br><span class="line"><span class="comment">//note: 2. 触发 the new partition, 也即是 onNewPartitionCreation()</span></span><br><span class="line"><span class="comment">//note: 3. 发送 metadata 请求给所有的 Broker</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewTopicCreation</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>], newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</span><br><span class="line">  info(<span class="string">"New topic creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</span><br><span class="line">  <span class="comment">// subscribe to partition changes</span></span><br><span class="line">  topics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</span><br><span class="line">  onNewPartitionCreation(newPartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述方法主要做了两件事：</p>
<ol>
<li>注册这个 topic 的 PartitionModificationsListener 监听器；</li>
<li>通过 <code>onNewPartitionCreation()</code> 创建该 Topic 的所有 Partition。</li>
</ol>
<p><code>onNewPartitionCreation()</code> 的实现在前面 Topic 扩容部分已经讲述过，这里不再重复，最好参考前面流程图来梳理 Topic 扩容和新建的整个过程。</p>
<h2 id="Topic-删除"><a href="#Topic-删除" class="headerlink" title="Topic 删除"></a>Topic 删除</h2><p>Kafka Topic 删除这部分的逻辑是一个单独线程去做的，这个线程是在 Controller 启动时初始化和启动的。</p>
<h3 id="TopicDeletionManager-初始化"><a href="#TopicDeletionManager-初始化" class="headerlink" title="TopicDeletionManager 初始化"></a>TopicDeletionManager 初始化</h3><p>TopicDeletionManager 启动实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Invoked at the end of new controller initiation</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: Controller 初始化完成,触发这个操作,删除 topic 线程启动</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</span><br><span class="line">  <span class="keyword">if</span> (isDeleteTopicEnabled) &#123;</span><br><span class="line">    deleteTopicsThread = <span class="keyword">new</span> <span class="type">DeleteTopicsThread</span>()</span><br><span class="line">    <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty)</span><br><span class="line">      deleteTopicStateChanged.set(<span class="literal">true</span>)</span><br><span class="line">    deleteTopicsThread.start() <span class="comment">//note: 启动 DeleteTopicsThread</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TopicDeletionManager 启动时只是初始化了一个 DeleteTopicsThread 线程，并启动该线程。TopicDeletionManager 这个类从名字上去看，它是 Topic 删除的管理器，它是如何实现 Topic 删除管理呢，这里先看下该类的几个重要的成员变量：</p>
<ol>
<li>topicsToBeDeleted：需要删除的 Topic 列表，每当有新的 topic 需要删除时，Controller 就通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到这个列表中，而 DeleteTopicsThread 线程则会从列表拿到需要进行删除的 Topic 信息；</li>
<li>partitionsToBeDeleted：需要删除的 Partition 列表，跟上面的 Topic 列表保持一致，只不过纬度不同；</li>
<li>topicsIneligibleForDeletion：非法删除的 Topic 列表，当一个 Topic 正在进行副本迁移、leader 选举或者有副本 dead 的情况下，该 Topic 都会设置被非法删除状态，只有恢复正常后，这个状态才会解除，处在这个状态的 Topic 是无法删除的。</li>
</ol>
<h3 id="Topic-删除整体流程"><a href="#Topic-删除整体流程" class="headerlink" title="Topic 删除整体流程"></a>Topic 删除整体流程</h3><p>前面一小节，简单介绍了 TopicDeletionManager、DeleteTopicsThread 的启动以及它们之间的关系，这里我们看下一个 Topic 被设置删除后，其处理的整理流程，简单做了一个小图，如下所示：</p>
<p><img src="/images/kafka/topic-delete.png" alt="Topic 删除整理流程"></p>
<p>这里先简单讲述上面的流程，当一个 Topic 设置为删除后：</p>
<ol>
<li>首先 DeleteTopicsListener 会被触发，然后通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表中；</li>
<li>DeleteTopicsThread 这个线程会不断调用 <code>doWork()</code> 方法，这个方法被调用时，它会遍历 <code>topicsToBeDeleted</code> 中的所有 Topic 列表；</li>
<li>对于之前没有处理过的 Topic（之前还没有开始删除），会通过 TopicDeletionManager 的 <code>onTopicDeletion()</code> 方法执行删除操作；</li>
<li>如果 Topic 删除完成（所有 Replica 的状态都变为 ReplicaDeletionSuccessful 状态），那么就执行 TopicDeletionManager 的 <code>completeDeleteTopic()</code> 完成删除流程，即更新状态信息，并将 Topic 的 meta 信息从缓存和 ZK 中清除。</li>
</ol>
<h3 id="Topic-删除详细实现"><a href="#Topic-删除详细实现" class="headerlink" title="Topic 删除详细实现"></a>Topic 删除详细实现</h3><p>先看下 DeleteTopicsListener 的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 删除 Topic 包括以下操作:</span></span><br><span class="line"><span class="comment">//note: 1. 如果要删除的 topic 存在,将 Topic 添加到 Topic 将要删除的缓存中;</span></span><br><span class="line"><span class="comment">//note: 2. 如果有 Topic 将要被删除,那么将触发 Topic 删除线程</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"DeleteTopicsListener"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when a topic is being deleted</span></span><br><span class="line"><span class="comment">   * @throws Exception On any error.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="comment">//note: 当 topic 需要被删除时,才会触发</span></span><br><span class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      <span class="keyword">var</span> topicsToBeDeleted = children.toSet</span><br><span class="line">      debug(<span class="string">"Delete topics listener fired for topics %s to be deleted"</span>.format(topicsToBeDeleted.mkString(<span class="string">","</span>)))</span><br><span class="line">      <span class="comment">//note: 不存在的、需要删除的 topic, 直接清除 zk 上的记录</span></span><br><span class="line">      <span class="keyword">val</span> nonExistentTopics = topicsToBeDeleted -- controllerContext.allTopics</span><br><span class="line">      <span class="keyword">if</span> (nonExistentTopics.nonEmpty) &#123;</span><br><span class="line">        warn(<span class="string">"Ignoring request to delete non-existing topics "</span> + nonExistentTopics.mkString(<span class="string">","</span>))</span><br><span class="line">        nonExistentTopics.foreach(topic =&gt; zkUtils.deletePathRecursive(getDeleteTopicPath(topic)))</span><br><span class="line">      &#125;</span><br><span class="line">      topicsToBeDeleted --= nonExistentTopics</span><br><span class="line">      <span class="keyword">if</span> (controller.config.deleteTopicEnable) &#123; <span class="comment">//note: 如果允许 topic 删除</span></span><br><span class="line">        <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 有 Topic 需要删除</span></span><br><span class="line">          info(<span class="string">"Starting topic deletion for topics "</span> + topicsToBeDeleted.mkString(<span class="string">","</span>))</span><br><span class="line">          <span class="comment">// mark topic ineligible for deletion if other state changes are in progress</span></span><br><span class="line">          topicsToBeDeleted.foreach &#123; topic =&gt; <span class="comment">//note: 如果 topic 正在最优 leader 选举或正在迁移,那么将 topic 标记为非法删除状态</span></span><br><span class="line">            <span class="keyword">val</span> preferredReplicaElectionInProgress =</span><br><span class="line">              controllerContext.partitionsUndergoingPreferredReplicaElection.map(_.topic).contains(topic)</span><br><span class="line">            <span class="keyword">val</span> partitionReassignmentInProgress =</span><br><span class="line">              controllerContext.partitionsBeingReassigned.keySet.map(_.topic).contains(topic)</span><br><span class="line">            <span class="keyword">if</span> (preferredReplicaElectionInProgress || partitionReassignmentInProgress)</span><br><span class="line">              controller.deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// add topic to deletion list</span></span><br><span class="line">          <span class="comment">//note: 将要删除的 topic 添加到待删除的 topic</span></span><br><span class="line">          controller.deleteTopicManager.enqueueTopicsForDeletion(topicsToBeDeleted)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// If delete topic is disabled remove entries under zookeeper path : /admin/delete_topics</span></span><br><span class="line">        <span class="keyword">for</span> (topic &lt;- topicsToBeDeleted) &#123;</span><br><span class="line">          info(<span class="string">"Removing "</span> + getDeleteTopicPath(topic) + <span class="string">" since delete topic is disabled"</span>)</span><br><span class="line">          zkUtils.zkClient.delete(getDeleteTopicPath(topic))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其 <code>doHandleChildChange()</code> 的实现逻辑如下：</p>
<ol>
<li>根据要删除的 Topic 列表，过滤出那些不存在的 Topic 列表，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）；</li>
<li>如果集群不允许 Topic 删除，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）这些 Topic 列表，结束流程；</li>
<li>如果这个列表中有正在进行副本迁移或 leader 选举的 Topic，那么先将这些 Topic 加入到 <code>topicsIneligibleForDeletion</code> 中，即标记为非法删除；</li>
<li>通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表（<code>topicsToBeDeleted</code>）、将 Partition 添加到要删除的 Partition 列表中（<code>partitionsToBeDeleted</code>）。</li>
</ol>
<p>接下来，看下 Topic 删除线程 DeleteTopicsThread 的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">/note: topic 删除线程</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsThread</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">ShutdownableThread</span>(<span class="params">name = "delete-topics-thread-" + controller.config.brokerId, isInterruptible = false</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</span><br><span class="line">    awaitTopicDeletionNotification()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!isRunning.get)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      <span class="comment">//note: 要删除的 topic 列表</span></span><br><span class="line">      <span class="keyword">val</span> topicsQueuedForDeletion = <span class="type">Set</span>.empty[<span class="type">String</span>] ++ topicsToBeDeleted</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span>(topicsQueuedForDeletion.nonEmpty)</span><br><span class="line">        info(<span class="string">"Handling deletion for topics "</span> + topicsQueuedForDeletion.mkString(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">      topicsQueuedForDeletion.foreach &#123; topic =&gt;</span><br><span class="line">      <span class="comment">// if all replicas are marked as deleted successfully, then topic deletion is done</span></span><br><span class="line">        <span class="keyword">if</span>(controller.replicaStateMachine.areAllReplicasForTopicDeleted(topic)) &#123;<span class="comment">//note: 如果 Topic 所有副本都删除成功的情况下</span></span><br><span class="line">          <span class="comment">// clear up all state for this topic from controller cache and zookeeper</span></span><br><span class="line">          <span class="comment">//note: 从 controller 的缓存和 zk 中清除这个 topic 的所有记录,这个 topic 彻底删除成功了</span></span><br><span class="line">          completeDeleteTopic(topic)</span><br><span class="line">          info(<span class="string">"Deletion of topic %s successfully completed"</span>.format(topic))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">if</span>(controller.replicaStateMachine.isAtLeastOneReplicaInDeletionStartedState(topic)) &#123;</span><br><span class="line">            <span class="comment">//note: Topic 的副本至少有一个状态为 ReplicaDeletionStarted 时</span></span><br><span class="line">            <span class="comment">// ignore since topic deletion is in progress</span></span><br><span class="line">            <span class="comment">//note: 过滤出 Topic 中副本状态为 ReplicaDeletionStarted 的 Partition 列表</span></span><br><span class="line">            <span class="keyword">val</span> replicasInDeletionStartedState = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionStarted</span>)</span><br><span class="line">            <span class="comment">//note: 表明了上面这些副本正在删除中</span></span><br><span class="line">            <span class="keyword">val</span> replicaIds = replicasInDeletionStartedState.map(_.replica)</span><br><span class="line">            <span class="keyword">val</span> partitions = replicasInDeletionStartedState.map(r =&gt; <span class="type">TopicAndPartition</span>(r.topic, r.partition))</span><br><span class="line">            info(<span class="string">"Deletion for replicas %s for partition %s of topic %s in progress"</span>.format(replicaIds.mkString(<span class="string">","</span>),</span><br><span class="line">              partitions.mkString(<span class="string">","</span>), topic))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note:副本既没有全部删除完成、也没有一个副本是在删除过程中，证明这个 topic 还没有开始删除或者删除完成但是至少一个副本删除失败</span></span><br><span class="line">            <span class="comment">// if you come here, then no replica is in TopicDeletionStarted and all replicas are not in</span></span><br><span class="line">            <span class="comment">// TopicDeletionSuccessful. That means, that either given topic haven't initiated deletion</span></span><br><span class="line">            <span class="comment">// or there is at least one failed replica (which means topic deletion should be retried).</span></span><br><span class="line">            <span class="keyword">if</span>(controller.replicaStateMachine.isAnyReplicaInState(topic, <span class="type">ReplicaDeletionIneligible</span>)) &#123;</span><br><span class="line">              <span class="comment">//note: 如果有副本删除失败,那么进行重试操作</span></span><br><span class="line">              <span class="comment">// mark topic for deletion retry</span></span><br><span class="line">              markTopicForDeletionRetry(topic)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Try delete topic if it is eligible for deletion.</span></span><br><span class="line">        <span class="keyword">if</span>(isTopicEligibleForDeletion(topic)) &#123; <span class="comment">//note: 如果 topic 可以被删除</span></span><br><span class="line">          info(<span class="string">"Deletion of topic %s (re)started"</span>.format(topic))</span><br><span class="line">          <span class="comment">// topic deletion will be kicked off</span></span><br><span class="line">          <span class="comment">//note: 开始删除 topic</span></span><br><span class="line">          onTopicDeletion(<span class="type">Set</span>(topic))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(isTopicIneligibleForDeletion(topic)) &#123;</span><br><span class="line">          info(<span class="string">"Not retrying deletion of topic %s at this time since it is marked ineligible for deletion"</span>.format(topic))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>doWork()</code> 方法处理逻辑如下：</p>
<ol>
<li>遍历所有要删除的 Topic，进行如下处理；</li>
<li>如果该 Topic 的所有副本都下线成功（状态为 ReplicaDeletionSuccessful）时，那么执行 <code>completeDeleteTopic()</code> 方法完成 Topic 的删除；</li>
<li>否则，如果 Topic 在删除过程有失败的副本（状态为 ReplicaDeletionIneligible），那么执行 <code>markTopicForDeletionRetry()</code> 将失败的 Replica 状态设置为 OfflineReplica；</li>
<li>判断 Topic 是否允许删除（不在非法删除的集合中就代表运允许），调用 <code>onTopicDeletion()</code> 执行 Topic 删除。</li>
</ol>
<p>先看下 <code>onTopicDeletion()</code> 方法，这是 Topic 最开始删除时的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: Topic 删除</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onTopicDeletion</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  info(<span class="string">"Topic deletion callback for %s"</span>.format(topics.mkString(<span class="string">","</span>)))</span><br><span class="line">  <span class="comment">// send update metadata so that brokers stop serving data for topics to be deleted</span></span><br><span class="line">  <span class="keyword">val</span> partitions = topics.flatMap(controllerContext.partitionsForTopic) <span class="comment">//note: topic 的所有 Partition</span></span><br><span class="line">  controller.sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, partitions) <span class="comment">//note: 更新meta</span></span><br><span class="line">  <span class="keyword">val</span> partitionReplicaAssignmentByTopic = controllerContext.partitionReplicaAssignment.groupBy(p =&gt; p._1.topic)</span><br><span class="line">  topics.foreach &#123; topic =&gt; <span class="comment">//note:  删除 topic 的每一个 Partition</span></span><br><span class="line">    onPartitionDeletion(partitionReplicaAssignmentByTopic(topic).keySet)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 这个方法是用于 delete-topic, 用于删除 topic 的所有 partition</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onPartitionDeletion</span></span>(partitionsToBeDeleted: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</span><br><span class="line">  info(<span class="string">"Partition deletion callback for %s"</span>.format(partitionsToBeDeleted.mkString(<span class="string">","</span>)))</span><br><span class="line">  <span class="keyword">val</span> replicasPerPartition = controllerContext.replicasForPartition(partitionsToBeDeleted)</span><br><span class="line">  startReplicaDeletion(replicasPerPartition)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Topic 的删除的真正实现方法还是在 <code>startReplicaDeletion()</code> 方法中，Topic 删除时，会先调用 <code>onPartitionDeletion()</code> 方法删除所有的 Partition，然后在 Partition 删除时，执行 <code>startReplicaDeletion()</code> 方法删除该 Partition 的副本，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 被 onPartitionDeletion 方法触发,删除副本具体的实现的地方</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startReplicaDeletion</span></span>(replicasForTopicsToBeDeleted: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>]) &#123;</span><br><span class="line">  replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach &#123; topic =&gt;</span><br><span class="line">    <span class="comment">//note: topic 所有存活的 replica</span></span><br><span class="line">    <span class="keyword">val</span> aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p =&gt; p.topic == topic)</span><br><span class="line">    <span class="comment">//note: topic 的 dead replica</span></span><br><span class="line">    <span class="keyword">val</span> deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic</span><br><span class="line">    <span class="comment">//note: topic 中已经处于 ReplicaDeletionSuccessful 状态的副本</span></span><br><span class="line">    <span class="keyword">val</span> successfullyDeletedReplicas = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</span><br><span class="line">    <span class="comment">//note: 还没有成功删除的、存活的副本</span></span><br><span class="line">    <span class="keyword">val</span> replicasForDeletionRetry = aliveReplicasForTopic -- successfullyDeletedReplicas</span><br><span class="line">    <span class="comment">// move dead replicas directly to failed state</span></span><br><span class="line">    <span class="comment">//note: 将 dead replica 设置为 ReplicaDeletionIneligible（删除无效的状态）</span></span><br><span class="line">    replicaStateMachine.handleStateChanges(deadReplicasForTopic, <span class="type">ReplicaDeletionIneligible</span>)</span><br><span class="line">    <span class="comment">// send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader</span></span><br><span class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 OfflineReplica（发送 StopReplica 请求）</span></span><br><span class="line">    replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">OfflineReplica</span>)</span><br><span class="line">    debug(<span class="string">"Deletion started for replicas %s"</span>.format(replicasForDeletionRetry.mkString(<span class="string">","</span>)))</span><br><span class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 ReplicaDeletionStarted 状态</span></span><br><span class="line">    controller.replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">ReplicaDeletionStarted</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Callbacks</span>.<span class="type">CallbackBuilder</span>().stopReplicaCallback(deleteTopicStopReplicaCallback).build)</span><br><span class="line">    <span class="keyword">if</span>(deadReplicasForTopic.nonEmpty) &#123; <span class="comment">//note: 将 topic 标记为不能删除</span></span><br><span class="line">      debug(<span class="string">"Dead Replicas (%s) found for topic %s"</span>.format(deadReplicasForTopic.mkString(<span class="string">","</span>), topic))</span><br><span class="line">      markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法的执行逻辑如下：</p>
<ol>
<li>首先获取当前集群所有存活的 broker 信息，根据这个信息可以知道 Topic 哪些副本所在节点是处于 dead 状态；</li>
<li>找到那些已经成功删除的 Replica 列表（状态为 ReplicaDeletionSuccessful），进而可以得到那些还没有成功删除、并且存活的 Replica 列表（<code>replicasForDeletionRetry</code>）；</li>
<li>将处于 dead 节点上的 Replica 的状态设置为 ReplicaDeletionIneligible 状态；</li>
<li>然后重新删除 replicasForDeletionRetry 列表中的副本，先将其状态转移为 OfflineReplica，再转移为 ReplicaDeletionStarted 状态（真正从发送 StopReplica +从物理上删除数据）；</li>
<li>如果有 Replica 所在的机器处于 dead 状态，那么将 Topic 设置为非法删除状态。</li>
</ol>
<p>在将副本状态从 OfflineReplica 转移成 ReplicaDeletionStarted 时，会设置一个回调方法 <code>deleteTopicStopReplicaCallback()</code>，该方法会将删除成功的 Replica 设置为 ReplicaDeletionSuccessful 状态，删除失败的 Replica 设置为 ReplicaDeletionIneligible 状态（需要根据 StopReplica 请求处理的过程，看下哪些情况下 Replica 会删除失败，这个会在后面讲解）。</p>
<p>下面看下这个方法 <code>completeDeleteTopic()</code>，当一个 Topic 的所有 Replica 都删除成功时，即其状态都在 ReplicaDeletionSuccessful 时，会调用这个方法，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: topic 删除后,从 controller 缓存、状态机以及 zk 移除这个 topic 相关记录</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">completeDeleteTopic</span></span>(topic: <span class="type">String</span>) &#123;</span><br><span class="line">  <span class="comment">// deregister partition change listener on the deleted topic. This is to prevent the partition change listener</span></span><br><span class="line">  <span class="comment">// firing before the new topic listener when a deleted topic gets auto created</span></span><br><span class="line">  <span class="comment">//note: 1. 取消 zk 对这个 topic 的 partition-modify-listener</span></span><br><span class="line">  partitionStateMachine.deregisterPartitionChangeListener(topic)</span><br><span class="line">  <span class="comment">//note: 2. 过滤出副本状态为 ReplicaDeletionSuccessful 的副本列表</span></span><br><span class="line">  <span class="keyword">val</span> replicasForDeletedTopic = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</span><br><span class="line">  <span class="comment">// controller will remove this replica from the state machine as well as its partition assignment cache</span></span><br><span class="line">  <span class="comment">//note: controller 将会从副本状态机移除这些副本</span></span><br><span class="line">  replicaStateMachine.handleStateChanges(replicasForDeletedTopic, <span class="type">NonExistentReplica</span>)</span><br><span class="line">  <span class="keyword">val</span> partitionsForDeletedTopic = controllerContext.partitionsForTopic(topic)</span><br><span class="line">  <span class="comment">// move respective partition to OfflinePartition and NonExistentPartition state</span></span><br><span class="line">  <span class="comment">//note: 3. 从分区状态机中下线并移除这个 topic 的分区</span></span><br><span class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">OfflinePartition</span>)</span><br><span class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">NonExistentPartition</span>)</span><br><span class="line">  topicsToBeDeleted -= topic <span class="comment">//note: 删除成功,从删除 topic 列表中移除</span></span><br><span class="line">  partitionsToBeDeleted.retain(_.topic != topic) <span class="comment">//note: 从 partitionsToBeDeleted 移除这个 topic</span></span><br><span class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</span><br><span class="line">  <span class="comment">//note: 4. 删除 zk 上关于这个 topic 的相关记录</span></span><br><span class="line">  zkUtils.zkClient.deleteRecursive(getTopicPath(topic))</span><br><span class="line">  zkUtils.zkClient.deleteRecursive(getEntityConfigPath(<span class="type">ConfigType</span>.<span class="type">Topic</span>, topic))</span><br><span class="line">  zkUtils.zkClient.delete(getDeleteTopicPath(topic))</span><br><span class="line">  <span class="comment">//note: 5. 从 controller 的所有缓存中再次移除关于这个 topic 的信息</span></span><br><span class="line">  controllerContext.removeTopic(topic)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当一个 Topic 所有副本都删除后，会进行如下处理：</p>
<ol>
<li>取消对该 Topic 的 partition-modify-listener 监听器；</li>
<li>将状态为 ReplicaDeletionSuccessful 的副本状态都转移成 NonExistentReplica；</li>
<li>将该 Topic Partition 状态先后转移成 OfflinePartition、NonExistentPartition 状态，正式下线了该 Partition；</li>
<li>从分区状态机和副本状态机中移除这个 Topic 记录；</li>
<li>从 Controller 缓存和 ZK 中清除这个 Topic 的相关记录。</li>
</ol>
<p>至此，一个 Topic 算是真正删除完成。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 &lt;a href=&quot;ht
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Broker 上线下线（十九）</title>
    <link href="http://matt33.com/2018/06/17/broker-online-offline/"/>
    <id>http://matt33.com/2018/06/17/broker-online-offline/</id>
    <published>2018-06-17T07:04:21.000Z</published>
    <updated>2019-02-24T02:29:01.552Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 <code>/brokers/ids</code> 下注册一个节点，节点名字就是 broker id，这个节点是临时节点，该节点内部会有这个 Broker 的详细节点信息。Controller 会监听 <code>/brokers/ids</code> 这个路径下的所有子节点，如果有新的节点出现，那么就代表有新的 Broker 上线，如果有节点消失，就代表有 broker 下线，Controller 会进行相应的处理，Kafka 就是利用 ZK 的这种 watch 机制及临时节点的特性来完成集群 Broker 的上下线，本文将会深入讲解这一过程。</p>
<h2 id="BrokerChangeListener"><a href="#BrokerChangeListener" class="headerlink" title="BrokerChangeListener"></a>BrokerChangeListener</h2><p>KafkaController 在启动时，会通过副本状态机注册一个监控 broker 上下线的监听器，通过 ReplicaStateMachine 的 <code>registerListeners()</code> 方法实现的，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// register ZK listeners of the replica state machine</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">registerListeners</span></span>() &#123;</span><br><span class="line">   <span class="comment">// register broker change listener</span></span><br><span class="line">   registerBrokerChangeListener() <span class="comment">//note: 监听【/brokers/ids】，broker 的上线下线</span></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerBrokerChangeListener</span></span>() = &#123;</span><br><span class="line">  zkUtils.zkClient.subscribeChildChanges(<span class="type">ZkUtils</span>.<span class="type">BrokerIdsPath</span>, brokerChangeListener)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>BrokerChangeListener 是监听 <code>/brokers/ids</code> 节点的监听器，当该节点有变化时会触发 <code>doHandleChildChange()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如果 【/brokers/ids】 目录下子节点有变化将会触发这个操作</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrokerChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"BrokerChangeListener"</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, currentBrokerList: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    info(<span class="string">"Broker change listener fired for path %s with children %s"</span>.format(parentPath, currentBrokerList.sorted.mkString(<span class="string">","</span>)))</span><br><span class="line">    inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</span><br><span class="line">        <span class="type">ControllerStats</span>.leaderElectionTimer.time &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//note: 当前 zk 的 broker 列表</span></span><br><span class="line">            <span class="keyword">val</span> curBrokers = currentBrokerList.map(_.toInt).toSet.flatMap(zkUtils.getBrokerInfo)</span><br><span class="line">            <span class="comment">//note: ZK 中的 broker id 列表</span></span><br><span class="line">            <span class="keyword">val</span> curBrokerIds = curBrokers.map(_.id)</span><br><span class="line">            <span class="comment">//note: Controller 缓存中的 broker 列表</span></span><br><span class="line">            <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</span><br><span class="line">            <span class="comment">//note: 新上线的 broker id 列表</span></span><br><span class="line">            <span class="keyword">val</span> newBrokerIds = curBrokerIds -- liveOrShuttingDownBrokerIds</span><br><span class="line">            <span class="comment">//note: 掉线的 broker id 列表</span></span><br><span class="line">            <span class="keyword">val</span> deadBrokerIds = liveOrShuttingDownBrokerIds -- curBrokerIds</span><br><span class="line">            <span class="comment">//note: 新上线的 Broker 列表</span></span><br><span class="line">            <span class="keyword">val</span> newBrokers = curBrokers.filter(broker =&gt; newBrokerIds(broker.id))</span><br><span class="line">            controllerContext.liveBrokers = curBrokers <span class="comment">//note: 更新缓存中当前 broker 列表</span></span><br><span class="line">            <span class="keyword">val</span> newBrokerIdsSorted = newBrokerIds.toSeq.sorted</span><br><span class="line">            <span class="keyword">val</span> deadBrokerIdsSorted = deadBrokerIds.toSeq.sorted</span><br><span class="line">            <span class="keyword">val</span> liveBrokerIdsSorted = curBrokerIds.toSeq.sorted</span><br><span class="line">            info(<span class="string">"Newly added brokers: %s, deleted brokers: %s, all live brokers: %s"</span></span><br><span class="line">              .format(newBrokerIdsSorted.mkString(<span class="string">","</span>), deadBrokerIdsSorted.mkString(<span class="string">","</span>), liveBrokerIdsSorted.mkString(<span class="string">","</span>)))</span><br><span class="line">            <span class="comment">//note: Broker 上线, 在 Controller Channel Manager 中添加该 broker</span></span><br><span class="line">            newBrokers.foreach(controllerContext.controllerChannelManager.addBroker)</span><br><span class="line">            <span class="comment">//note: Broker 下线处理, 在 Controller Channel Manager 移除该 broker</span></span><br><span class="line">            deadBrokerIds.foreach(controllerContext.controllerChannelManager.removeBroker)</span><br><span class="line">            <span class="keyword">if</span>(newBrokerIds.nonEmpty) <span class="comment">//note: 启动该 Broker</span></span><br><span class="line">              controller.onBrokerStartup(newBrokerIdsSorted)</span><br><span class="line">            <span class="keyword">if</span>(deadBrokerIds.nonEmpty) <span class="comment">//note: broker 掉线后开始 leader 选举</span></span><br><span class="line">              controller.onBrokerFailure(deadBrokerIdsSorted)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling broker changes"</span>, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里需要重点关注 <code>doHandleChildChange()</code> 方法的实现，该方法处理逻辑如下：</p>
<ol>
<li>从 ZK 获取当前的 Broker 列表（<code>curBrokers</code>）及 broker id 的列表（<code>curBrokerIds</code>）；</li>
<li>获取当前 Controller 中缓存的 broker id 列表（<code>liveOrShuttingDownBrokerIds</code>）；</li>
<li>获取新上线 broker id 列表：<code>newBrokerIds</code> = <code>curBrokerIds</code> – <code>liveOrShuttingDownBrokerIds</code>；</li>
<li>获取掉线的 broker id 列表：<code>deadBrokerIds</code> = <code>liveOrShuttingDownBrokerIds</code> – <code>curBrokerIds</code>；</li>
<li>对于新上线的 broker，先在 ControllerChannelManager 中添加该 broker（即建立与该 Broker 的连接、初始化相应的发送线程和请求队列），最后 Controller 调用 <code>onBrokerStartup()</code> 上线该 Broker；</li>
<li>对于掉线的 broker，先在 ControllerChannelManager 中移除该 broker（即关闭与 Broker 的连接、关闭相应的发送线程和清空请求队列），最后 Controller 调用 <code>onBrokerFailure()</code> 下线该 Broker。</li>
</ol>
<p>整体的处理流程如下图所示：</p>
<p><img src="/images/kafka/broker_online_offline.png" alt="Broker 上线下线处理过程"></p>
<h2 id="Broker-上线"><a href="#Broker-上线" class="headerlink" title="Broker 上线"></a>Broker 上线</h2><p>本节主要讲述一台 Broker 上线的过程，如前面图中所示，一台 Broker 上线主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">controllerContext.controllerChannelManager.addBroker</span><br><span class="line">controller.onBrokerStartup(newBrokerIdsSorted)</span><br></pre></td></tr></table></figure>
<ol>
<li>在 Controller Channel Manager 中添加该 Broker 节点，主要的内容是：Controller 建立与该 Broker 的连接、初始化相应的请求发送线程与请求队列；</li>
<li>调用 Controller 的 <code>onBrokerStartup()</code> 方法上线该节点。</li>
</ol>
<p>Controller Channel Manager 添加 Broker 的实现如下，这里就不重复讲述了，前面讲述 Controller 服务初始化的文章（ <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager </a>）已经讲述过这部分的内容。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</span><br><span class="line">  <span class="comment">// be careful here. Maybe the startup() API has already started the request send thread</span></span><br><span class="line">  brokerLock synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span>(!brokerStateInfo.contains(broker.id)) &#123;</span><br><span class="line">      addNewBroker(broker)</span><br><span class="line">      startRequestSendThread(broker.id)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面再看下 Controller 如何在 <code>onBrokerStartup()</code> 方法中实现 Broker 上线操作的，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 这个是被 副本状态机触发的</span></span><br><span class="line"><span class="comment">//note: 1. 发送 update-metadata 请求给所有存活的 broker;</span></span><br><span class="line"><span class="comment">//note: 2. 对于所有 new/offline partition 触发选主操作, 选举成功的, Partition 状态设置为 Online</span></span><br><span class="line"><span class="comment">//note: 3. 检查是否有分区的重新副本分配分配到了这个台机器上, 如果有, 就进行相应的操作</span></span><br><span class="line"><span class="comment">//note: 4. 检查这台机器上是否有 Topic 被设置为了删除标志, 如果是, 那么机器启动完成后, 重新尝试删除操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerStartup</span></span>(newBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</span><br><span class="line">  info(<span class="string">"New broker startup callback for %s"</span>.format(newBrokers.mkString(<span class="string">","</span>)))</span><br><span class="line">  <span class="keyword">val</span> newBrokersSet = newBrokers.toSet <span class="comment">//note: 新启动的 broker</span></span><br><span class="line">  <span class="comment">// send update metadata request to all live and shutting down brokers. Old brokers will get to know of the new</span></span><br><span class="line">  <span class="comment">// broker via this update.</span></span><br><span class="line">  <span class="comment">// In cases of controlled shutdown leaders will not be elected when a new broker comes up. So at least in the</span></span><br><span class="line">  <span class="comment">// common controlled shutdown case, the metadata will reach the new brokers faster</span></span><br><span class="line">  <span class="comment">//note: 发送 metadata 更新给所有的 broker, 这样的话旧的 broker 将会知道有机器新上线了</span></span><br><span class="line">  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</span><br><span class="line">  <span class="comment">// the very first thing to do when a new broker comes up is send it the entire list of partitions that it is</span></span><br><span class="line">  <span class="comment">// supposed to host. Based on that the broker starts the high watermark threads for the input list of partitions</span></span><br><span class="line">  <span class="comment">//note:  获取这个机器上的所有 replica 请求</span></span><br><span class="line">  <span class="keyword">val</span> allReplicasOnNewBrokers = controllerContext.replicasOnBrokers(newBrokersSet)</span><br><span class="line">  <span class="comment">//note: 将这些副本的状态设置为 OnlineReplica</span></span><br><span class="line">  replicaStateMachine.handleStateChanges(allReplicasOnNewBrokers, <span class="type">OnlineReplica</span>)</span><br><span class="line">  <span class="comment">// when a new broker comes up, the controller needs to trigger leader election for all new and offline partitions</span></span><br><span class="line">  <span class="comment">// to see if these brokers can become leaders for some/all of those</span></span><br><span class="line">  <span class="comment">//note: 新的 broker 上线也会触发所有处于 new/offline 的 partition 进行 leader 选举</span></span><br><span class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</span><br><span class="line">  <span class="comment">// check if reassignment of some partitions need to be restarted</span></span><br><span class="line">  <span class="comment">//note: 检查是否副本的重新分配分配到了这台机器上</span></span><br><span class="line">  <span class="keyword">val</span> partitionsWithReplicasOnNewBrokers = controllerContext.partitionsBeingReassigned.filter &#123;</span><br><span class="line">    <span class="keyword">case</span> (_, reassignmentContext) =&gt; reassignmentContext.newReplicas.exists(newBrokersSet.contains(_))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 如果需要副本进行迁移的话,就执行副本迁移操作</span></span><br><span class="line">  partitionsWithReplicasOnNewBrokers.foreach(p =&gt; onPartitionReassignment(p._1, p._2))</span><br><span class="line">  <span class="comment">// check if topic deletion needs to be resumed. If at least one replica that belongs to the topic being deleted exists</span></span><br><span class="line">  <span class="comment">// on the newly restarted brokers, there is a chance that topic deletion can resume</span></span><br><span class="line">  <span class="comment">//note: 检查 topic 删除操作是否需要重新启动</span></span><br><span class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnNewBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</span><br><span class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123;</span><br><span class="line">    info((<span class="string">"Some replicas %s for topics scheduled for deletion %s are on the newly restarted brokers %s. "</span> +</span><br><span class="line">      <span class="string">"Signaling restart of topic deletion for these topics"</span>).format(replicasForTopicsToBeDeleted.mkString(<span class="string">","</span>),</span><br><span class="line">      deleteTopicManager.topicsToBeDeleted.mkString(<span class="string">","</span>), newBrokers.mkString(<span class="string">","</span>)))</span><br><span class="line">    deleteTopicManager.resumeDeletionForTopics(replicasForTopicsToBeDeleted.map(_.topic))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>onBrokerStartup()</code> 方法在实现的逻辑上分为以下几步：</p>
<ol>
<li>调用 <code>sendUpdateMetadataRequest()</code> 方法向当前集群所有存活的 Broker 发送 Update Metadata 请求，这样的话其他的节点就会知道当前的 Broker 已经上线了；</li>
<li>获取当前节点分配的所有的 Replica 列表，并将其状态转移为 OnlineReplica 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 leader 选举，如果 leader 选举成功，那么该 Partition 的状态就会转移到 OnlinePartition 状态，否则状态转移失败；</li>
<li>如果副本迁移中有新的 Replica 落在这台新上线的节点上，那么开始执行副本迁移操作（见<a href="http://matt33.com/2018/06/16/partition-reassignment/">Kafka 源码解析之 Partition 副本迁移实现</a>）;</li>
<li>如果之前由于这个 Topic 设置为删除标志，但是由于其中有 Replica 掉线而导致无法删除，这里在节点启动后，尝试重新执行删除操作。</li>
</ol>
<p>到此为止，一台 Broker 算是真正加入到了 Kafka 的集群中，在上述过程中，涉及到 leader 选举的操作，都会触发 LeaderAndIsr 请求及 Metadata 请求的发送。</p>
<h2 id="Broker-掉线"><a href="#Broker-掉线" class="headerlink" title="Broker 掉线"></a>Broker 掉线</h2><p>本节主要讲述一台 Broker 掉线后的处理过程，正如前面图中所示，一台 Broker 掉线后主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">controllerContext.controllerChannelManager.removeBroker</span><br><span class="line">controller.onBrokerFailure(deadBrokerIdsSorted)</span><br></pre></td></tr></table></figure>
<ol>
<li>首先在 Controller Channel Manager 中移除该 Broker 节点，主要的内容是：关闭 Controller  与 Broker 的连接和相应的请求发送线程，并清空请求队列；</li>
<li>调用 Controller 的 <code>onBrokerFailure()</code> 方法下线该节点。</li>
</ol>
<p>Controller Channel Manager 下线 Broker 的处理如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeBroker</span></span>(brokerId: <span class="type">Int</span>) &#123;</span><br><span class="line">  brokerLock synchronized &#123;</span><br><span class="line">    removeExistingBroker(brokerStateInfo(brokerId))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 移除旧的 broker（关闭网络连接、关闭请求发送线程）</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">removeExistingBroker</span></span>(brokerState: <span class="type">ControllerBrokerStateInfo</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    brokerState.networkClient.close()</span><br><span class="line">    brokerState.messageQueue.clear()</span><br><span class="line">    brokerState.requestSendThread.shutdown()</span><br><span class="line">    brokerStateInfo.remove(brokerState.brokerNode.id)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while removing broker by the controller"</span>, e)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 Controller Channel Manager 处理完掉线的 Broker 节点后，下面 KafkaController 将会调用 <code>onBrokerFailure()</code> 进行相应的处理，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 这个方法会被副本状态机调用（进行 broker 节点下线操作）</span></span><br><span class="line"><span class="comment">//note: 1. 将 leader 在这台机器上的分区设置为 Offline</span></span><br><span class="line"><span class="comment">//note: 2. 通过 OfflinePartitionLeaderSelector 为 new/offline partition 选举新的 leader</span></span><br><span class="line"><span class="comment">//note: 3. leader 选举后,发送 LeaderAndIsr 请求给该分区所有存活的副本;</span></span><br><span class="line"><span class="comment">//note: 4. 分区选举 leader 后,状态更新为 Online</span></span><br><span class="line"><span class="comment">//note: 5. 要下线的 broker 上的所有 replica 改为 Offline 状态</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerFailure</span></span>(deadBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</span><br><span class="line">  info(<span class="string">"Broker failure callback for %s"</span>.format(deadBrokers.mkString(<span class="string">","</span>)))</span><br><span class="line">  <span class="comment">//note: 从正在下线的 broker 集合中移除已经下线的机器</span></span><br><span class="line">  <span class="keyword">val</span> deadBrokersThatWereShuttingDown =</span><br><span class="line">    deadBrokers.filter(id =&gt; controllerContext.shuttingDownBrokerIds.remove(id))</span><br><span class="line">  info(<span class="string">"Removed %s from list of shutting down brokers."</span>.format(deadBrokersThatWereShuttingDown))</span><br><span class="line">  <span class="keyword">val</span> deadBrokersSet = deadBrokers.toSet</span><br><span class="line">  <span class="comment">// trigger OfflinePartition state for all partitions whose current leader is one amongst the dead brokers</span></span><br><span class="line">  <span class="comment">//note: 1. 将 leader 在这台机器上的、并且未设置删除的分区状态设置为 Offline</span></span><br><span class="line">  <span class="keyword">val</span> partitionsWithoutLeader = controllerContext.partitionLeadershipInfo.filter(partitionAndLeader =&gt;</span><br><span class="line">    deadBrokersSet.contains(partitionAndLeader._2.leaderAndIsr.leader) &amp;&amp;</span><br><span class="line">      !deleteTopicManager.isTopicQueuedUpForDeletion(partitionAndLeader._1.topic)).keySet</span><br><span class="line">  partitionStateMachine.handleStateChanges(partitionsWithoutLeader, <span class="type">OfflinePartition</span>)</span><br><span class="line">  <span class="comment">// trigger OnlinePartition state changes for offline or new partitions</span></span><br><span class="line">  <span class="comment">//note: 2. 选举 leader, 选举成功后设置为 Online 状态</span></span><br><span class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</span><br><span class="line">  <span class="comment">// filter out the replicas that belong to topics that are being deleted</span></span><br><span class="line">  <span class="comment">//note: 过滤出 replica 在这个机器上、并且没有被设置为删除的 topic 列表</span></span><br><span class="line">  <span class="keyword">var</span> allReplicasOnDeadBrokers = controllerContext.replicasOnBrokers(deadBrokersSet)</span><br><span class="line">  <span class="keyword">val</span> activeReplicasOnDeadBrokers = allReplicasOnDeadBrokers.filterNot(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</span><br><span class="line">  <span class="comment">// handle dead replicas</span></span><br><span class="line">  <span class="comment">//note: 将这些 replica 状态转为 Offline</span></span><br><span class="line">  replicaStateMachine.handleStateChanges(activeReplicasOnDeadBrokers, <span class="type">OfflineReplica</span>)</span><br><span class="line">  <span class="comment">// check if topic deletion state for the dead replicas needs to be updated</span></span><br><span class="line">  <span class="comment">//note: 过滤设置为删除的 replica</span></span><br><span class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnDeadBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</span><br><span class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 将上面这个 topic 列表的 topic 标记为删除失败</span></span><br><span class="line">    <span class="comment">// it is required to mark the respective replicas in TopicDeletionFailed state since the replica cannot be</span></span><br><span class="line">    <span class="comment">// deleted when the broker is down. This will prevent the replica from being in TopicDeletionStarted state indefinitely</span></span><br><span class="line">    <span class="comment">// since topic deletion cannot be retried until at least one replica is in TopicDeletionStarted state</span></span><br><span class="line">    deleteTopicManager.failReplicaDeletion(replicasForTopicsToBeDeleted)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If broker failure did not require leader re-election, inform brokers of failed broker</span></span><br><span class="line">  <span class="comment">// Note that during leader re-election, brokers update their metadata</span></span><br><span class="line">  <span class="keyword">if</span> (partitionsWithoutLeader.isEmpty) &#123;</span><br><span class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Controller 对于掉线 Broker 的处理过程主要有以下几步：</p>
<ol>
<li>首先找到 Leader 在该 Broker 上所有 Partition 列表，然后将这些 Partition 的状态全部转移为 OfflinePartition 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 Leader 选举，如果 Leader 选举成功，那么该 Partition 的状态就会迁移到 OnlinePartition 状态，否则状态转移失败（Broker 上线/掉线、Controller 初始化时都会触发这个方法）；</li>
<li>获取在该 Broker 上的所有 Replica 列表，将其状态转移成 OfflineReplica 状态；</li>
<li>过滤出设置为删除、并且有副本在该节点上的 Topic 列表，先将该 Replica 的转移成 ReplicaDeletionIneligible 状态，然后再将该 Topic 标记为非法删除，即因为有 Replica 掉线导致该 Topic 无法删除；</li>
<li>如果 leader 在该 Broker 上所有 Partition 列表不为空，证明有 Partition 的 leader 需要选举，在最后一步会触发全局 metadata 信息的更新。</li>
</ol>
<p>到这里，一台掉线的 Broker 算是真正下线完成了。</p>
<h2 id="Broker-优雅下线"><a href="#Broker-优雅下线" class="headerlink" title="Broker 优雅下线"></a>Broker 优雅下线</h2><p>前面部分是关于通过监听节点变化来实现对 Broker 的上下线，这也是 Kafka 上下线 Broker 的主要流程，但是还有一种情况是：主动关闭 Kafka 服务，这种情况又被称为 Broker 的优雅关闭。</p>
<p>优雅关闭的节点会向 Controller 发送 ControlledShutdownRequest 请求，Controller 在收到这个情况会进行相应的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleControlledShutdownRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></span><br><span class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></span><br><span class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></span><br><span class="line">  <span class="keyword">val</span> controlledShutdownRequest = request.requestObj.asInstanceOf[<span class="type">ControlledShutdownRequest</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 判断该连接是否经过认证</span></span><br><span class="line">  authorizeClusterAction(request)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 处理该请求</span></span><br><span class="line">  <span class="keyword">val</span> partitionsRemaining = controller.shutdownBroker(controlledShutdownRequest.brokerId)</span><br><span class="line">  <span class="comment">//note: 返回的 response</span></span><br><span class="line">  <span class="keyword">val</span> controlledShutdownResponse = <span class="keyword">new</span> <span class="type">ControlledShutdownResponse</span>(controlledShutdownRequest.correlationId,</span><br><span class="line">    <span class="type">Errors</span>.<span class="type">NONE</span>.code, partitionsRemaining)</span><br><span class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, <span class="keyword">new</span> <span class="type">RequestOrResponseSend</span>(request.connectionId, controlledShutdownResponse)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Controller 在接收这个关闭服务的请求，通过 <code>shutdownBroker()</code> 方法进行处理，实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 优雅地关闭 Broker</span></span><br><span class="line"><span class="comment">//note: controller 首先决定将这个 broker 上的 leader 迁移到其他可用的机器上</span></span><br><span class="line"><span class="comment">//note: 返回还没有 leader 的迁移的 TopicPartition 集合</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownBroker</span></span>(id: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!isActive) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(<span class="string">"Controller moved to another broker. Aborting controlled shutdown"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  controllerContext.brokerShutdownLock synchronized &#123; <span class="comment">//note: 拿到 broker shutdown 的唯一锁</span></span><br><span class="line">    info(<span class="string">"Shutting down broker "</span> + id)</span><br><span class="line"></span><br><span class="line">    inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 拿到 controllerLock 的排它锁</span></span><br><span class="line">      <span class="keyword">if</span> (!controllerContext.liveOrShuttingDownBrokerIds.contains(id))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BrokerNotAvailableException</span>(<span class="string">"Broker id %d does not exist."</span>.format(id))</span><br><span class="line"></span><br><span class="line">      controllerContext.shuttingDownBrokerIds.add(id) <span class="comment">//note: 将 broker id 添加到正在关闭的 broker 列表中</span></span><br><span class="line">      debug(<span class="string">"All shutting down brokers: "</span> + controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>))</span><br><span class="line">      debug(<span class="string">"Live brokers: "</span> + controllerContext.liveBrokerIds.mkString(<span class="string">","</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 获取这个 broker 上所有 Partition 与副本数的 map</span></span><br><span class="line">    <span class="keyword">val</span> allPartitionsAndReplicationFactorOnBroker: <span class="type">Set</span>[(<span class="type">TopicAndPartition</span>, <span class="type">Int</span>)] =</span><br><span class="line">      inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">        controllerContext.partitionsOnBroker(id)</span><br><span class="line">          .map(topicAndPartition =&gt; (topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition).size))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 处理这些 TopicPartition，更新 Partition 或 Replica 的状态，必要时进行 leader 选举</span></span><br><span class="line">    allPartitionsAndReplicationFactorOnBroker.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span>(topicAndPartition, replicationFactor) =&gt;</span><br><span class="line">        <span class="comment">// Move leadership serially to relinquish lock.</span></span><br><span class="line">        inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">          controllerContext.partitionLeadershipInfo.get(topicAndPartition).foreach &#123; currLeaderIsrAndControllerEpoch =&gt;</span><br><span class="line">            <span class="keyword">if</span> (replicationFactor &gt; <span class="number">1</span>) &#123; <span class="comment">//note: 副本数大于1</span></span><br><span class="line">              <span class="keyword">if</span> (currLeaderIsrAndControllerEpoch.leaderAndIsr.leader == id) &#123; <span class="comment">//note: leader 正好是下线的节点</span></span><br><span class="line">                <span class="comment">// If the broker leads the topic partition, transition the leader and update isr. Updates zk and</span></span><br><span class="line">                <span class="comment">// notifies all affected brokers</span></span><br><span class="line">                <span class="comment">//todo: 这种情况下 Replica 的状态不需要修改么？（Replica 的处理还是通过监听器还实现的,这里只是在服务关闭前进行 leader 切换和停止副本同步）</span></span><br><span class="line">                <span class="comment">//note: 状态变化（变为 OnlinePartition，并且进行 leader 选举，使用 controlledShutdownPartitionLeaderSelector 算法）</span></span><br><span class="line">                partitionStateMachine.handleStateChanges(<span class="type">Set</span>(topicAndPartition), <span class="type">OnlinePartition</span>,</span><br><span class="line">                  controlledShutdownPartitionLeaderSelector)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Stop the replica first. The state change below initiates ZK changes which should take some time</span></span><br><span class="line">                <span class="comment">// before which the stop replica request should be completed (in most cases)</span></span><br><span class="line">                <span class="keyword">try</span> &#123; <span class="comment">//note: 要下线的机器停止副本迁移，发送 StopReplica 请求</span></span><br><span class="line">                  brokerRequestBatch.newBatch()</span><br><span class="line">                  brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">Seq</span>(id), topicAndPartition.topic,</span><br><span class="line">                    topicAndPartition.partition, deletePartition = <span class="literal">false</span>)</span><br><span class="line">                  brokerRequestBatch.sendRequestsToBrokers(epoch)</span><br><span class="line">                &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                  <span class="keyword">case</span> e : <span class="type">IllegalStateException</span> =&gt; &#123;</span><br><span class="line">                    <span class="comment">// Resign if the controller is in an illegal state</span></span><br><span class="line">                    error(<span class="string">"Forcing the controller to resign"</span>)</span><br><span class="line">                    brokerRequestBatch.clear()</span><br><span class="line">                    controllerElector.resign()</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">throw</span> e</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// If the broker is a follower, updates the isr in ZK and notifies the current leader</span></span><br><span class="line">                <span class="comment">//note: 更新这个副本的状态，变为 OfflineReplica</span></span><br><span class="line">                replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="type">PartitionAndReplica</span>(topicAndPartition.topic,</span><br><span class="line">                  topicAndPartition.partition, id)), <span class="type">OfflineReplica</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 返回 leader 在这个要下线节点上并且副本数大于 1 的 TopicPartition 集合</span></span><br><span class="line">    <span class="comment">//note: 在已经进行前面 leader 迁移后</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replicatedPartitionsBrokerLeads</span></span>() = inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      trace(<span class="string">"All leaders = "</span> + controllerContext.partitionLeadershipInfo.mkString(<span class="string">","</span>))</span><br><span class="line">      controllerContext.partitionLeadershipInfo.filter &#123;</span><br><span class="line">        <span class="keyword">case</span> (topicAndPartition, leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">          leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &amp;&amp; controllerContext.partitionReplicaAssignment(topicAndPartition).size &gt; <span class="number">1</span></span><br><span class="line">      &#125;.keys</span><br><span class="line">    &#125;</span><br><span class="line">    replicatedPartitionsBrokerLeads().toSet</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述方法的处理逻辑如下：</p>
<ol>
<li>先将要下线的 Broker 添加到 shuttingDownBrokerIds 集合中，该集合记录了当前正在进行关闭的 broker 列表；</li>
<li>获取副本在该节点上的所有 Partition 的列表集合；</li>
<li>遍历上述 Partition 列表进行处理：如果该 Partition 的 leader 是要下线的节点，那么通过 PartitionStateMachine 进行状态转移（OnlinePartition –&gt; OnlinePartition）触发 leader 选举，使用的 leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#ControlledShutdownLeaderSelector">ControlledShutdownLeaderSelector</a>，它会选举 isr 中第一个没有正在关闭的 Replica 作为 leader，否则抛出 StateChangeFailedException 异常；</li>
<li>否则的话，即要下线的节点不是 leader，那么就向要下线的节点发送 StopReplica 请求停止副本同步，并将该副本设置为 OfflineReplica 状态，这里对 Replica 进行处理的原因是为了让要下线的机器关闭副本同步流程，这样 Kafka 服务才能正常关闭。</li>
</ol>
<p>我在看这部分的代码是有一个疑问的，那就是如果要下线的节点是 Partition leader 的情况下，并没有对 Replica 进行相应的处理，这里的原因是，这部分 Replica 的处理可以放在 <code>onBrokerFailure()</code> 方法中处理，即使通过优雅下线的方法下线了 Broker，但是监听 ZK 的 BrokerChangeListener 监听器还是会被触发的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 &lt;code&gt;/brokers/ids&lt;/code&gt; 下注册一个节点，节点名字就是 brok
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Partition 副本迁移实现（十八）</title>
    <link href="http://matt33.com/2018/06/16/partition-reassignment/"/>
    <id>http://matt33.com/2018/06/16/partition-reassignment/</id>
    <published>2018-06-16T15:40:00.000Z</published>
    <updated>2019-02-24T02:29:01.551Z</updated>
    
    <content type="html"><![CDATA[<p>前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Controller 在初始化时，会利用 ZK 的 watch 机制注册很多不同类型的监听器，当监听的事件被触发时，Controller 就会触发相应的操作。</p>
<p>Controller 在初始化时，会注册多种类型的监听器，主要有以下6种：</p>
<ol>
<li>监听 <code>/admin/reassign_partitions</code> 节点，用于分区副本迁移的监听；</li>
<li>监听 <code>/isr_change_notification</code> 节点，用于 Partition Isr 变动的监听，；</li>
<li>监听 <code>/admin/preferred_replica_election</code> 节点，用于需要进行 Partition 最优 leader 选举的监听；</li>
<li>监听 <code>/brokers/topics</code> 节点，用于 Topic 新建的监听；</li>
<li>监听 <code>/brokers/topics/TOPIC_NAME</code> 节点，用于 Topic Partition 扩容的监听；</li>
<li>监听 <code>/admin/delete_topics</code> 节点，用于 Topic 删除的监听；</li>
<li>监听 <code>/brokers/ids</code> 节点，用于 Broker 上下线的监听。</li>
</ol>
<p>本文主要讲解第一部分，也就是 Controller 对 Partition 副本迁移的处理，后续会单独一篇文章讲述 Topic 的新建、扩容和删除，再单独一篇文章讲述 Broker 的上下线，另外两部分将会在对 LeaderAndIsr 请求处理的文章中讲述。</p>
<h2 id="Partition-副本迁移整体流程"><a href="#Partition-副本迁移整体流程" class="headerlink" title="Partition 副本迁移整体流程"></a>Partition 副本迁移整体流程</h2><p>Partition 的副本迁移实际上就是将分区的副本重新分配到不同的代理节点上，如果 zk 中新副本的集合与 Partition 原来的副本集合相同，那么这个副本就不需要重新分配了。</p>
<p>Partition 的副本迁移是通过监听 zk 的 <code>/admin/reassign_partitions</code> 节点触发的，Kafka 也向用户提供相应的脚本工具进行副本迁移，副本迁移的脚本使用方法如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-reassign-partitions.sh --zookeeper XXX --reassignment-json-file XXX.json --execute</span><br></pre></td></tr></table></figure>
<p>其中 XXX.json 为要进行 Partition 副本迁移的 json 文件，json 文件的格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"version"</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="attr">"partitions"</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</span><br><span class="line">            <span class="attr">"partition"</span>:<span class="number">19</span>,</span><br><span class="line">            <span class="attr">"replicas"</span>:[</span><br><span class="line">                <span class="number">3</span>,</span><br><span class="line">                <span class="number">9</span>,</span><br><span class="line">                <span class="number">2</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</span><br><span class="line">            <span class="attr">"partition"</span>:<span class="number">26</span>,</span><br><span class="line">            <span class="attr">"replicas"</span>:[</span><br><span class="line">                <span class="number">2</span>,</span><br><span class="line">                <span class="number">6</span>,</span><br><span class="line">                <span class="number">4</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</span><br><span class="line">            <span class="attr">"partition"</span>:<span class="number">27</span>,</span><br><span class="line">            <span class="attr">"replicas"</span>:[</span><br><span class="line">                <span class="number">5</span>,</span><br><span class="line">                <span class="number">3</span>,</span><br><span class="line">                <span class="number">8</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个 json 文件的意思是将 Topic <code>__consumer_offsets</code> Partition 19 的副本迁移到 {3, 2, 9} 上，Partition 26 的副本迁移到 {6, 2, 4} 上，Partition 27 的副本迁移到 {5, 3, 8} 上。</p>
<p>在调用脚本向 zk 提交 Partition 的迁移计划时，迁移计划更新到 zk 前需要进行一步判断，如果该节点（写入迁移计划的节点）已经存在，即副本迁移还在进行，那么本次副本迁移计划是无法提交的，实现的逻辑如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">executeAssignment</span></span>(zkUtils: <span class="type">ZkUtils</span>, reassignmentJsonString: <span class="type">String</span>, throttle: <span class="type">Long</span> = <span class="number">-1</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> partitionsToBeReassigned = parseAndValidate(zkUtils, reassignmentJsonString)</span><br><span class="line">  <span class="keyword">val</span> reassignPartitionsCommand = <span class="keyword">new</span> <span class="type">ReassignPartitionsCommand</span>(zkUtils, partitionsToBeReassigned.toMap)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If there is an existing rebalance running, attempt to change its throttle</span></span><br><span class="line">  <span class="comment">//note: 如果副本迁移正在进行,那么这次的副本迁移计划是无法提交的</span></span><br><span class="line">  <span class="keyword">if</span> (zkUtils.pathExists(<span class="type">ZkUtils</span>.<span class="type">ReassignPartitionsPath</span>)) &#123;</span><br><span class="line">    println(<span class="string">"There is an existing assignment running."</span>)</span><br><span class="line">    reassignPartitionsCommand.maybeLimit(throttle)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    printCurrentAssignment(zkUtils, partitionsToBeReassigned)</span><br><span class="line">    <span class="keyword">if</span> (throttle &gt;= <span class="number">0</span>)</span><br><span class="line">      println(<span class="type">String</span>.format(<span class="string">"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value."</span>))</span><br><span class="line">    <span class="comment">//note: 将迁移计划更新到 zk 上</span></span><br><span class="line">    <span class="keyword">if</span> (reassignPartitionsCommand.reassignPartitions(throttle)) &#123;</span><br><span class="line">      println(<span class="string">"Successfully started reassignment of partitions."</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">      println(<span class="string">"Failed to reassign partitions %s"</span>.format(partitionsToBeReassigned))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在迁移计划提交到 zk 之后，Controller 的 PartitionsReassignedListener 就会被触发，Controller 开始 Partition 的副本迁移，触发之后 Controller 的处理流程大体如下图所示：</p>
<p><img src="/images/kafka/partition_reassignment.png" alt="Partition 迁移过程"></p>
<h2 id="PartitionsReassignedListener-副本迁移处理"><a href="#PartitionsReassignedListener-副本迁移处理" class="headerlink" title="PartitionsReassignedListener 副本迁移处理"></a>PartitionsReassignedListener 副本迁移处理</h2><p>在 zk 的 <code>/admin/reassign_partitions</code> 节点数据有变化时，就会触发 PartitionsReassignedListener 的 <code>doHandleDataChange()</code> 方法，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 开始进行 partition reassignment 除非这三种情况发生:</span></span><br><span class="line"><span class="comment">//note: 1. 这个 partition 的 reassignment 之前已经存在, 即正在迁移中;</span></span><br><span class="line"><span class="comment">//note: 2. new replica 与已经存在的 replicas 相同;</span></span><br><span class="line"><span class="comment">//note: 3. Partition 所有新分配 replica 都已经 dead;</span></span><br><span class="line"><span class="comment">//note: 这种情况发生时,会输出一条日志,并从 zk 移除该 Partition 的迁移计划。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionsReassignedListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"PartitionsReassignedListener"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when some partitions are reassigned by the admin command</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @throws Exception On any error.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="comment">//note: 当一些分区需要进行迁移时</span></span><br><span class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</span><br><span class="line">    debug(<span class="string">"Partitions reassigned listener fired for path %s. Record partitions to be reassigned %s"</span></span><br><span class="line">      .format(dataPath, data))</span><br><span class="line">    <span class="keyword">val</span> partitionsReassignmentData = <span class="type">ZkUtils</span>.parsePartitionReassignmentData(data.toString)</span><br><span class="line">    <span class="keyword">val</span> partitionsToBeReassigned = inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 需要迁移的新副本</span></span><br><span class="line">      <span class="comment">//note: 过滤掉正在迁移的副本,如果 Partition 正在迁移,这一波迁移完之前不允许再次迁移</span></span><br><span class="line">      partitionsReassignmentData.filterNot(p =&gt; controllerContext.partitionsBeingReassigned.contains(p._1))</span><br><span class="line">    &#125;</span><br><span class="line">    partitionsToBeReassigned.foreach &#123; partitionToBeReassigned =&gt;</span><br><span class="line">      inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(partitionToBeReassigned._1.topic)) &#123;</span><br><span class="line">          <span class="comment">//note: 如果这个 topic 已经设置了删除，那么就不会进行迁移了（从需要副本迁移的集合中移除）</span></span><br><span class="line">          error(<span class="string">"Skipping reassignment of partition %s for topic %s since it is currently being deleted"</span></span><br><span class="line">            .format(partitionToBeReassigned._1, partitionToBeReassigned._1.topic))</span><br><span class="line">          controller.removePartitionFromReassignedPartitions(partitionToBeReassigned._1)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 添加到需要迁移的副本集合中</span></span><br><span class="line">          <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">ReassignedPartitionsContext</span>(partitionToBeReassigned._2)</span><br><span class="line">          controller.initiateReassignReplicasForTopicPartition(partitionToBeReassigned._1, context)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果 Partition 出现下面的情况，将不会进行副本迁移，直接将 Partition 的迁移计划从 ZK 移除：</p>
<ol>
<li>这个 Partition 的 reassignment 之前已经存在, 即正在迁移中;</li>
<li>这个 Partition 新分配的 replica 与之前的 replicas 相同;</li>
<li>这个 Partition 所有新分配 replica 都已经 dead;</li>
<li>这个 Partition 已经被设置了删除标志。</li>
</ol>
<p>对于可以进行副本迁移的 Partition 集合，这里将会调用 Kafka Controller 的 <code>initiateReassignReplicasForTopicPartition()</code> 方法对每个 Partition 进行处理。</p>
<h2 id="副本迁移初始化"><a href="#副本迁移初始化" class="headerlink" title="副本迁移初始化"></a>副本迁移初始化</h2><p>进行了前面的判断后，这个 Partition 满足了可以迁移的条件，Controller 会首先初始化副本迁移的流程，实现如下所示：</p>
<blockquote>
<p>如果 Partition 新分配的 replica 与之前的 replicas 相同，那么不会进行副本迁移，这部分的判断实际上是在这里实现的，前面只是为了更好地讲述。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 初始化 Topic-Partition 的副本迁移</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initiateReassignReplicasForTopicPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>,</span><br><span class="line">                                      reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</span><br><span class="line">  <span class="comment">//note: 要迁移的 topic-partition，及新的副本</span></span><br><span class="line">  <span class="keyword">val</span> newReplicas = reassignedPartitionContext.newReplicas</span><br><span class="line">  <span class="keyword">val</span> topic = topicAndPartition.topic</span><br><span class="line">  <span class="keyword">val</span> partition = topicAndPartition.partition</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> assignedReplicasOpt = controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="comment">//note: partition 的 AR</span></span><br><span class="line">    assignedReplicasOpt <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (assignedReplicas == newReplicas) &#123; <span class="comment">//note: 不需要迁移</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Partition %s to be reassigned is already assigned to replicas"</span>.format(topicAndPartition) +</span><br><span class="line">            <span class="string">" %s. Ignoring request for partition reassignment"</span>.format(newReplicas.mkString(<span class="string">","</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          info(<span class="string">"Handling reassignment of partition %s to new replicas %s"</span>.format(topicAndPartition, newReplicas.mkString(<span class="string">","</span>)))</span><br><span class="line">          <span class="comment">// first register ISR change listener</span></span><br><span class="line">          <span class="comment">//note: 首先注册 ISR 监听的变化</span></span><br><span class="line">          watchIsrChangesForReassignedPartition(topic, partition, reassignedPartitionContext)</span><br><span class="line">          <span class="comment">//note: 正在迁移 Partition 添加到缓存中</span></span><br><span class="line">          controllerContext.partitionsBeingReassigned.put(topicAndPartition, reassignedPartitionContext)</span><br><span class="line">          <span class="comment">// mark topic ineligible for deletion for the partitions being reassigned</span></span><br><span class="line">          <span class="comment">//note: 设置正在迁移的副本为不能删除</span></span><br><span class="line">          deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</span><br><span class="line">          <span class="comment">//note: 进行副本迁移</span></span><br><span class="line">          onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Attempt to reassign partition %s that doesn't exist"</span></span><br><span class="line">        .format(topicAndPartition))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error completing reassignment of partition %s"</span>.format(topicAndPartition), e)</span><br><span class="line">    <span class="comment">// remove the partition from the admin path to unblock the admin client</span></span><br><span class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于副本迁移流程初始化如下：</p>
<ol>
<li>通过 <code>watchIsrChangesForReassignedPartition()</code> 方法监控这个 Partition 的 LeaderAndIsr 变化，如果有新的副本数据同步完成，那么 leader 会将其加到 isr 中更新到 zk 中，这时候 Controller 是可以接收到相关的信息通知的；</li>
<li>将正在迁移的 Partition 添加到 partitionsBeingReassigned 中，它会记录当前正在迁移的 Partition 列表；</li>
<li>将要迁移的 Topic 设置为非法删除删除状态，在这个状态的 Topic 是无法进行删除的；</li>
<li>调用 <code>onPartitionReassignment()</code>，进行副本迁移。</li>
</ol>
<p>在第一步中，会向这个 Partition 注册一个额外的监听器，监听其 LeaderAndIsr 信息变化，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: ISR 变动的监听器（这个不是由 leader 主动触发的，而是 controller 自己触发的，主要用于 partition 迁移时，isr 变动的监听处理）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionsIsrChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                            reassignedReplicas: <span class="type">Set</span>[<span class="type">Int</span>]</span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controller.controllerContext.zkUtils</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"ReassignedPartitionsIsrChangeListener"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked when some partitions need to move leader to preferred replica</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</span><br><span class="line">    inLock(controllerContext.controllerLock) &#123;</span><br><span class="line">      debug(<span class="string">"Reassigned partitions isr change listener fired for path %s with children %s"</span>.format(dataPath, data))</span><br><span class="line">      <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// check if this partition is still being reassigned or not</span></span><br><span class="line">        <span class="comment">//note: 检查这个副本是不是还在迁移中（这个方法只用于副本迁移中）</span></span><br><span class="line">        controllerContext.partitionsBeingReassigned.get(topicAndPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(reassignedPartitionContext) =&gt;</span><br><span class="line">            <span class="comment">// need to re-read leader and isr from zookeeper since the zkclient callback doesn't return the Stat object</span></span><br><span class="line">            <span class="comment">//note: 从 zk 获取最新的 leader 和 isr 信息</span></span><br><span class="line">            <span class="keyword">val</span> newLeaderAndIsrOpt = zkUtils.getLeaderAndIsrForPartition(topic, partition)</span><br><span class="line">            newLeaderAndIsrOpt <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Some</span>(leaderAndIsr) =&gt; <span class="comment">// check if new replicas have joined ISR</span></span><br><span class="line">                <span class="keyword">val</span> caughtUpReplicas = reassignedReplicas &amp; leaderAndIsr.isr.toSet</span><br><span class="line">                <span class="keyword">if</span>(caughtUpReplicas == reassignedReplicas) &#123; <span class="comment">//note: 新分配的副本已经全部在 isr 中了</span></span><br><span class="line">                  <span class="comment">// resume the partition reassignment process</span></span><br><span class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></span><br><span class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</span><br><span class="line">                    <span class="string">"Resuming partition reassignment"</span>)</span><br><span class="line">                  <span class="comment">//note: 再次触发 onPartitionReassignment 方法,副本已经迁移完成</span></span><br><span class="line">                  controller.onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> &#123;  <span class="comment">//note: 否则不进行任何处理</span></span><br><span class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></span><br><span class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</span><br><span class="line">                    <span class="string">"Replica(s) %s still need to catch up"</span>.format((reassignedReplicas -- leaderAndIsr.isr.toSet).mkString(<span class="string">","</span>)))</span><br><span class="line">                &#125;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">None</span> =&gt; error(<span class="string">"Error handling reassignment of partition %s to replicas %s as it was never created"</span></span><br><span class="line">                .format(topicAndPartition, reassignedReplicas.mkString(<span class="string">","</span>)))</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling partition reassignment"</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果该 Partition 的 LeaderAndIsr 信息有变动，那么就会触发这个 listener 的 <code>doHandleDataChange()</code> 方法：</p>
<ol>
<li>首先检查这个 Partition 是否在还在迁移中，不在的话直接结束流程，因为这个监听器本来就是为了 Partition 副本迁移而服务的；</li>
<li>从 zk 获取最新的 leader 和 isr 信息，如果新分配的副本全部都在 isr 中，那么就再次触发 controller 的 <code>onPartitionReassignment()</code> 方法，再次调用时实际上已经证明了这个 Partition 的副本迁移已经完成，否则的话就会不进行任何处理，等待新分配的所有副本迁移完成。</li>
</ol>
<h2 id="副本迁移"><a href="#副本迁移" class="headerlink" title="副本迁移"></a>副本迁移</h2><p>Partition 副本迁移真正实际处理是在 Controller 的 <code>onPartitionReassignment()</code> 方法完成的，在看这个方法之前，先介绍几个基本的概念（假设一个 Partition 原来的 replica 是 {1、2、3}，新分配的副本列表是：{2、3、4}）：</p>
<ul>
<li>RAR = Reassigned replicas，即新分配的副本列表，也就是 {2、3、4}；</li>
<li>OAR = Original list of replicas for partition，即这个 Partition 原来的副本列表，也就是 {1、2、3}；</li>
<li>AR = current assigned replicas，该 Partition 当前的副本列表，这个会随着阶段的不同而变化；</li>
<li>RAR-OAR：需要创建、数据同步的新副本，也就是 {4}；</li>
<li>OAR-RAR：不需要创建、数据同步的副本，也就是{2、3}</li>
</ul>
<p>这个方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 这个回调方法被 reassigned partitions listener 触发,当需要进行分区副本迁移时,会在【/admin/reassign_partitions】下创建一个节点来触发操作</span></span><br><span class="line"><span class="comment">//note: RAR: 重新分配的副本, OAR: 这个分区原来的副本列表, AR: 当前的分配的副本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPartitionReassignment</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> reassignedReplicas = reassignedPartitionContext.newReplicas</span><br><span class="line">  <span class="keyword">if</span> (!areReplicasInIsr(topicAndPartition.topic, topicAndPartition.partition, reassignedReplicas)) &#123;</span><br><span class="line">    <span class="comment">//note: 新分配的并没有权限在 isr 中</span></span><br><span class="line">    info(<span class="string">"New replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</span><br><span class="line">      <span class="string">"reassigned not yet caught up with the leader"</span>)</span><br><span class="line">    <span class="comment">//note: RAR-OAR</span></span><br><span class="line">    <span class="keyword">val</span> newReplicasNotInOldReplicaList = reassignedReplicas.toSet -- controllerContext.partitionReplicaAssignment(topicAndPartition).toSet</span><br><span class="line">    <span class="comment">//note: RAR+OAR</span></span><br><span class="line">    <span class="keyword">val</span> newAndOldReplicas = (reassignedPartitionContext.newReplicas ++ controllerContext.partitionReplicaAssignment(topicAndPartition)).toSet</span><br><span class="line">    <span class="comment">//1. Update AR in ZK with OAR + RAR.</span></span><br><span class="line">    updateAssignedReplicasForPartition(topicAndPartition, newAndOldReplicas.toSeq)</span><br><span class="line">    <span class="comment">//2. Send LeaderAndIsr request to every replica in OAR + RAR (with AR as OAR + RAR).</span></span><br><span class="line">    updateLeaderEpochAndSendRequest(topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition),</span><br><span class="line">      newAndOldReplicas.toSeq)</span><br><span class="line">    <span class="comment">//3. replicas in RAR - OAR -&gt; NewReplica</span></span><br><span class="line">    <span class="comment">//note: 新分配的副本状态更新为 NewReplica（在第二步中发送 LeaderAndIsr 请求时,新的副本会开始创建并且同步数据）</span></span><br><span class="line">    startNewReplicasForReassignedPartition(topicAndPartition, reassignedPartitionContext, newReplicasNotInOldReplicaList)</span><br><span class="line">    info(<span class="string">"Waiting for new replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</span><br><span class="line">      <span class="string">"reassigned to catch up with the leader"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 新副本全在 isr 中了</span></span><br><span class="line">    <span class="comment">//4. Wait until all replicas in RAR are in sync with the leader.</span></span><br><span class="line">   <span class="comment">//note: 【OAR-RAR】</span></span><br><span class="line">    <span class="keyword">val</span> oldReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).toSet -- reassignedReplicas.toSet</span><br><span class="line">    <span class="comment">//5. replicas in RAR -&gt; OnlineReplica</span></span><br><span class="line">    <span class="comment">//note: RAR 中的副本都在 isr 中了,将副本状态设置为 OnlineReplica</span></span><br><span class="line">    reassignedReplicas.foreach &#123; replica =&gt;</span><br><span class="line">      replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="keyword">new</span> <span class="type">PartitionAndReplica</span>(topicAndPartition.topic, topicAndPartition.partition,</span><br><span class="line">        replica)), <span class="type">OnlineReplica</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//6. Set AR to RAR in memory.</span></span><br><span class="line">    <span class="comment">//7. Send LeaderAndIsr request with a potential new leader (if current leader not in RAR) and</span></span><br><span class="line">    <span class="comment">//   a new AR (using RAR) and same isr to every broker in RAR</span></span><br><span class="line">    <span class="comment">//note: 到这一步,新加入的 replica 已经同步完成,leader和isr都更新到最新的结果</span></span><br><span class="line">    moveReassignedPartitionLeaderIfRequired(topicAndPartition, reassignedPartitionContext)</span><br><span class="line">    <span class="comment">//8. replicas in OAR - RAR -&gt; Offline (force those replicas out of isr)</span></span><br><span class="line">    <span class="comment">//9. replicas in OAR - RAR -&gt; NonExistentReplica (force those replicas to be deleted)</span></span><br><span class="line">    <span class="comment">//note: 下线旧的副本</span></span><br><span class="line">    stopOldReplicasOfReassignedPartition(topicAndPartition, reassignedPartitionContext, oldReplicas)</span><br><span class="line">    <span class="comment">//10. Update AR in ZK with RAR.</span></span><br><span class="line">    updateAssignedReplicasForPartition(topicAndPartition, reassignedReplicas)</span><br><span class="line">    <span class="comment">//11. Update the /admin/reassign_partitions path in ZK to remove this partition.</span></span><br><span class="line">    <span class="comment">//note: partition 迁移完成,从待迁移的集合中移除该 Partition</span></span><br><span class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</span><br><span class="line">    info(<span class="string">"Removed partition %s from the list of reassigned partitions in zookeeper"</span>.format(topicAndPartition))</span><br><span class="line">    controllerContext.partitionsBeingReassigned.remove(topicAndPartition)</span><br><span class="line">    <span class="comment">//12. After electing leader, the replicas and isr information changes, so resend the update metadata request to every broker</span></span><br><span class="line">    <span class="comment">//note: 发送 metadata 更新请求给所有存活的 broker</span></span><br><span class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, <span class="type">Set</span>(topicAndPartition))</span><br><span class="line">    <span class="comment">// signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed</span></span><br><span class="line">    <span class="comment">//note: topic 删除恢复（如果当前 topic 设置了删除,之前由于无法删除）</span></span><br><span class="line">    deleteTopicManager.resumeDeletionForTopics(<span class="type">Set</span>(topicAndPartition.topic))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个方法整体分为以下12个步骤：</p>
<ol>
<li>把 AR = OAR+RAR （{1、2、3、4}）更新到 zk 及本地 Controller 缓存中;</li>
<li>发送 LeaderAndIsr 给 AR 中每一个副本,并且会强制更新 zk 中 leader 的 epoch;</li>
<li>创建需要新建的副本（【RAR-OAR】，即 {4}）,将其状态设置为 NewReplica；</li>
<li>等待直到 RAR（{2、3、4}） 中的所有副本都在 ISR 中;</li>
<li>把 RAR（{2、3、4}） 中的所有副本设置为 OnReplica 状态;</li>
<li>将缓存中 AR 更新为 RAR（重新分配的副本列表，即 {2、3、4}）;</li>
<li>如果 leader 不在 RAR 中, 就从 RAR 选择对应的 leader, 然后发送 LeaderAndIsr 请求；如果不需要，那么只会更新 leader epoch，然后发送 LeaderAndIsr 请求; 在发送 LeaderAndIsr 请求前设置了 AR=RAR, 这将确保了 leader 在 isr 中不会添加任何 【RAR-OAR】中的副本（old replica，即 {1}）；</li>
<li>将【OAR-RAR】（{1}）中的副本设置为 OfflineReplica 状态，OfflineReplica 状态的变化，将会从 ISR 中删除【OAR-RAR】的副本，更新到 zk 中并发送 LeaderAndIsr 请求给 leader，通知 leader isr 变动。之后再发送 StopReplica 请求（delete=false）给【OAR-RAR】中的副本；</li>
<li>将【OAR-RAR】中的副本设置为 NonExistentReplica 状态。这将发送 StopReplica 请求（delete=true）给【OAR-RAR】中的副本，这些副本将会从本地上删除数据；</li>
<li>在 zk 中更新 AR 为 RAR；</li>
<li>更新 zk 中路径 【/admin/reassign_partitions】信息，移除已经成功迁移的 Partition；</li>
<li>leader 选举之后，这个 replica 和 isr 信息将会变动，发送 metadata 更新给所有的 broker。</li>
</ol>
<p>上面的流程简单来说，就是先创建新的 replica，开始同步数据，等待所有新的分配都加入到了 isr 中后，开始进行 leader 选举（需要的情况下），下线不需要的副本（OAR-RAR），下线完成后将 Partition 的最新 AR （即 RAR）信息更新到 zk 中，最后发送相应的请求给 broker，到这里一个 Partition 的副本迁移算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Co
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本状态机与分区状态机（十七）</title>
    <link href="http://matt33.com/2018/06/16/controller-state-machine/"/>
    <id>http://matt33.com/2018/06/16/controller-state-machine/</id>
    <published>2018-06-16T03:04:14.000Z</published>
    <updated>2019-02-24T02:29:01.551Z</updated>
    
    <content type="html"><![CDATA[<p>上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副本状态机则是记录着当前集群所有 Replica 的状态信息以及如何对 Replica 状态转变进行相应的处理。</p>
<h2 id="ReplicaStateMachine"><a href="#ReplicaStateMachine" class="headerlink" title="ReplicaStateMachine"></a>ReplicaStateMachine</h2><p>ReplicaStateMachine 记录着集群所有 Replica 的状态信息，它决定着一个 replica 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中副本的状态总共有以下七种类型：</p>
<ol>
<li>NewReplica：这种状态下 Controller 可以创建这个 Replica，这种状态下该 Replica 只能作为 follower，它可以是 Replica 删除后的一个临时状态，它有效的前置状态是 NonExistentReplica；</li>
<li>OnlineReplica：一旦这个 Replica 被分配到指定的 Partition 上，并且 Replica 创建完成，那么它将会被置为这个状态，在这个状态下，这个 Replica 既可以作为 leader 也可以作为 follower，它有效的前置状态是  NewReplica、OnlineReplica 或 OfflineReplica；</li>
<li>OfflineReplica：如果一个 Replica 挂掉（所在的节点宕机或者其他情况），该 Replica 将会被转换到这个状态，它有的效前置状态是 NewReplica、OfflineReplica 或者 OnlineReplica；</li>
<li>ReplicaDeletionStarted：Replica 开始删除时被置为的状态，它有效的前置状态是 OfflineReplica；</li>
<li>ReplicaDeletionSuccessful：如果 Replica 在删除时没有遇到任何错误信息，它将被置为这个状态，这个状态代表该 Replica 的数据已经从节点上清除了，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>ReplicaDeletionIneligible：如果 Replica 删除失败，它将会转移到这个状态，这个状态意思是非法删除，也就是删除是无法成功的，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>NonExistentReplica：如果 Replica 删除成功，它将被转移到这个状态，它有效的前置状态是：ReplicaDeletionSuccessful。</li>
</ol>
<p>上面的状态中其中后面4是专门为 Replica 删除而服务的，副本状态机转移图如下所示：</p>
<p><img src="/images/kafka/replica_state.png" alt="副本状态机"></p>
<p>这张图是副本状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 ReplicaStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="ReplicaStateMachine-初始化"><a href="#ReplicaStateMachine-初始化" class="headerlink" title="ReplicaStateMachine 初始化"></a>ReplicaStateMachine 初始化</h3><p>副本状态机初始化的过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: Controller 重新选举后触发的操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  <span class="comment">// initialize replica state</span></span><br><span class="line">  <span class="comment">//note: 初始化 zk 上所有的 Replica 状态信息（replica 存活的话设置为 Online,不存活的设置为 ReplicaDeletionIneligible）</span></span><br><span class="line">  initializeReplicaState()</span><br><span class="line">  <span class="comment">// set started flag</span></span><br><span class="line">  hasStarted.set(<span class="literal">true</span>)</span><br><span class="line">  <span class="comment">// move all Online replicas to Online</span></span><br><span class="line">  <span class="comment">//note: 将存活的副本状态转变为 OnlineReplica</span></span><br><span class="line">  handleStateChanges(controllerContext.allLiveReplicas(), <span class="type">OnlineReplica</span>)</span><br><span class="line"></span><br><span class="line">  info(<span class="string">"Started replica state machine with initial state -&gt; "</span> + replicaState.toString())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法中，ReplicaStateMachine 先调用 <code>initializeReplicaState()</code> 方法初始化集群中所有 Replica 的状态信息，如果 Replica 所在机器是 alive 的，那么将其状态设置为 OnlineReplica，否则设置为 ReplicaDeletionIneligible 状态，这里只是将 Replica 的状态信息更新副本状态机的缓存 <code>replicaState</code> 中，并没有真正进行状态转移的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 初始化所有副本的状态信息</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeReplicaState</span></span>() &#123;</span><br><span class="line">  <span class="keyword">for</span>((topicPartition, assignedReplicas) &lt;- controllerContext.partitionReplicaAssignment) &#123;</span><br><span class="line">    <span class="keyword">val</span> topic = topicPartition.topic</span><br><span class="line">    <span class="keyword">val</span> partition = topicPartition.partition</span><br><span class="line">    assignedReplicas.foreach &#123; replicaId =&gt;</span><br><span class="line">      <span class="keyword">val</span> partitionAndReplica = <span class="type">PartitionAndReplica</span>(topic, partition, replicaId)</span><br><span class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(replicaId)) <span class="comment">//note: 如果副本是存活,那么将状态都设置为 OnlineReplica</span></span><br><span class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="comment">// mark replicas on dead brokers as failed for topic deletion, if they belong to a topic to be deleted.</span></span><br><span class="line">        <span class="comment">// This is required during controller failover since during controller failover a broker can go down,</span></span><br><span class="line">        <span class="comment">// so the replicas on that broker should be moved to ReplicaDeletionIneligible to be on the safer side.</span></span><br><span class="line">        <span class="comment">//note: 将不存活的副本状态设置为 ReplicaDeletionIneligible</span></span><br><span class="line">        replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接着第二步调用 <code>handleStateChanges()</code> 将所有存活的副本状态转移为 OnlineReplica 状态，这里才是真正进行状态转移的地方，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 用于处理 Replica 状态的变化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</span><br><span class="line">                       callbacks: <span class="type">Callbacks</span> = (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build) &#123;</span><br><span class="line">  <span class="keyword">if</span>(replicas.nonEmpty) &#123;</span><br><span class="line">    info(<span class="string">"Invoking state change to %s for replicas %s"</span>.format(targetState, replicas.mkString(<span class="string">","</span>)))</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      brokerRequestBatch.newBatch()</span><br><span class="line">      <span class="comment">//note: 状态转变</span></span><br><span class="line">      replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks))</span><br><span class="line">      <span class="comment">//note: 向 broker 发送相应请求</span></span><br><span class="line">      brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</span><br><span class="line">    &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some replicas to %s state"</span>.format(targetState), e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里是副本状态机 <code>startup()</code> 方法的最后一步，它的目的是将所有 alive 的 Replica 状态转移到 OnlineReplica 状态，由于前面已经这些 alive replica 的状态设置成了 OnlineReplica，所以这里 Replica 的状态转移情况是：<strong>OnlineReplica –&gt; OnlineReplica</strong>，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="副本的状态转移"><a href="#副本的状态转移" class="headerlink" title="副本的状态转移"></a>副本的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewReplica、ReplicaDeletionStarted、ReplicaDeletionIneligible、ReplicaDeletionSuccessful、NonExistentReplica、OnlineReplica 或者 OfflineReplica 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewReplica"><a href="#TargetState-NewReplica" class="headerlink" title="TargetState: NewReplica"></a>TargetState: NewReplica</h4><p>NewReplica 这个状态是 Replica 准备开始创建是的一个状态，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> currState = replicaState.getOrElseUpdate(partitionAndReplica, <span class="type">NonExistentReplica</span>)<span class="comment">//note: Replica 不存在的话,状态初始化为 NonExistentReplica</span></span><br><span class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">NonExistentReplica</span>), targetState)<span class="comment">//note: 验证</span></span><br><span class="line"><span class="comment">// start replica as a follower to the current leader for its partition</span></span><br><span class="line"><span class="comment">//note: 从 zk 获取 Partition 的 leaderAndIsr 信息</span></span><br><span class="line"><span class="keyword">val</span> leaderIsrAndControllerEpochOpt = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topic, partition)</span><br><span class="line">leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">    <span class="keyword">if</span>(leaderIsrAndControllerEpoch.leaderAndIsr.leader == replicaId)<span class="comment">//note: 这个状态的 Replica 不能作为 leader</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Replica %d for partition %s cannot be moved to NewReplica"</span></span><br><span class="line">        .format(replicaId, topicAndPartition) + <span class="string">"state as it is being requested to become leader"</span>)</span><br><span class="line">    <span class="comment">//note: 向该 replicaId 发送 LeaderAndIsr 请求,这个方法同时也会向所有的 broker 发送 updateMeta 请求</span></span><br><span class="line">    brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId),</span><br><span class="line">                                                        topic, partition, leaderIsrAndControllerEpoch,</span><br><span class="line">                                                        replicaAssignment)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// new leader request will be sent to this replica when one gets elected</span></span><br><span class="line">&#125;</span><br><span class="line">replicaState.put(partitionAndReplica, <span class="type">NewReplica</span>)<span class="comment">//note: 缓存这个 replica 对象的状态</span></span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">                          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</span><br><span class="line">                                  targetState))</span><br></pre></td></tr></table></figure>
<p>当想要把 Replica 的状态转移为 NewReplica 时，副本状态机的处理逻辑如下：</p>
<ol>
<li>校验 Replica 的前置状态，只有处于 NonExistentReplica 状态的副本才能转移到 NewReplica 状态；</li>
<li>从 zk 中获取该 Topic-Partition 的 LeaderIsrAndControllerEpoch 信息；</li>
<li>如果获取不到上述信息，直接将该 Replica 的状态转移成 NewReplica，然后结束流程（对与新建的 Partition，处于这个状态时，该 Partition 是没有相应的 LeaderAndIsr 信息的）；</li>
<li>获取到 Partition 的 LeaderIsrAndControllerEpoch 信息，如果发现该 Partition 的 leader 是当前副本，那么就抛出 StateChangeFailedException 异常，因为处在这个状态的 Replica 是不能被选举为 leader 的；</li>
<li>获取到了 Partition 的 LeaderIsrAndControllerEpoch 信息，并且该 Partition 的 leader 不是当前 replica，那么向该 Partition 的所有 Replica 添加一个 LeaderAndIsr 请求（添加 LeaderAndIsr 请求时，实际上也会向所有的 Broker 都添加一个 Update-Metadata 请求）；</li>
<li>最后将该 Replica 的状态转移成 NewReplica，然后结束流程。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionStarted"><a href="#TargetState-ReplicaDeletionStarted" class="headerlink" title="TargetState: ReplicaDeletionStarted"></a>TargetState: ReplicaDeletionStarted</h4><p>这是 Replica 开始删除时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">OfflineReplica</span>), targetState)</span><br><span class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionStarted</span>)</span><br><span class="line"><span class="comment">// send stop replica command</span></span><br><span class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,并设置 deletePartition=true</span></span><br><span class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">true</span>,</span><br><span class="line">  callbacks.stopReplicaResponseCallback)</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</span><br></pre></td></tr></table></figure>
<p>这部分的实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 OfflineReplica 的情况下才能转移到这种状态；</li>
<li>更新向该 Replica 的状态为 ReplicaDeletionStarted；</li>
<li>向该 replica 发送 StopReplica 请求（deletePartition = true），收到这请求后，broker 会从物理存储上删除这个 Replica 的数据内容；</li>
<li>如果请求返回的话会触发其回调函数（这部分会在 topic 删除部分讲解）。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionIneligible"><a href="#TargetState-ReplicaDeletionIneligible" class="headerlink" title="TargetState: ReplicaDeletionIneligible"></a>TargetState: ReplicaDeletionIneligible</h4><p>ReplicaDeletionIneligible 是副本删除失败时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</span><br><span class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionIneligible。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionSuccessful"><a href="#TargetState-ReplicaDeletionSuccessful" class="headerlink" title="TargetState: ReplicaDeletionSuccessful"></a>TargetState: ReplicaDeletionSuccessful</h4><p>ReplicaDeletionSuccessful 是副本删除成功时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</span><br><span class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionSuccessful</span>)</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionSuccessful。</li>
</ol>
<h4 id="TargetState-NonExistentReplica"><a href="#TargetState-NonExistentReplica" class="headerlink" title="TargetState: NonExistentReplica"></a>TargetState: NonExistentReplica</h4><p>NonExistentReplica 是副本完全删除、不存在这个副本的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionSuccessful</span>), targetState)</span><br><span class="line"><span class="comment">// remove this replica from the assigned replicas list for its partition</span></span><br><span class="line"><span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</span><br><span class="line"><span class="comment">//note: 从 controller 和副本状态机的缓存中清除这个 Replica 的记录西溪</span></span><br><span class="line">controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas.filterNot(_ == replicaId))</span><br><span class="line">replicaState.remove(partitionAndReplica)</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionSuccessful 下才能转移这种状态；</li>
<li>在 controller 的 partitionReplicaAssignment 删除这个 Partition 对应的 replica 信息；</li>
<li>从 Controller 和副本状态机中将这个 Topic 从缓存中删除。</li>
</ol>
<h4 id="TargetState-OnlineReplica"><a href="#TargetState-OnlineReplica" class="headerlink" title="TargetState: OnlineReplica"></a>TargetState: OnlineReplica</h4><p>OnlineReplica 是副本正常工作时的状态，此时的 Replica 既可以作为 leader 也可以作为 follower，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">assertValidPreviousStates(partitionAndReplica,</span><br><span class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</span><br><span class="line">replicaState(partitionAndReplica) <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">NewReplica</span> =&gt; <span class="comment">//note: NewReplica --&gt; OnlineReplica</span></span><br><span class="line">    <span class="comment">// add this replica to the assigned replicas list for its partition</span></span><br><span class="line">    <span class="comment">//note: 向 the assigned replicas list 添加这个 replica（正常情况下这些 replicas 已经更新到 list 中了）</span></span><br><span class="line">    <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</span><br><span class="line">    <span class="keyword">if</span>(!currentAssignedReplicas.contains(replicaId))</span><br><span class="line">      controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas :+ replicaId)</span><br><span class="line">    stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">                              .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</span><br><span class="line">                                      targetState))</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">//note: OnlineReplica/OfflineReplica/ReplicaDeletionIneligible --&gt; OnlineReplica</span></span><br><span class="line">    <span class="comment">// check if the leader for this partition ever existed</span></span><br><span class="line">    <span class="comment">//note: 如果该 Partition 的 LeaderIsrAndControllerEpoch 信息存在,那么就更新副本的状态,并发送相应的请求</span></span><br><span class="line">    controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">        brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, leaderIsrAndControllerEpoch,</span><br><span class="line">          replicaAssignment)</span><br><span class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</span><br><span class="line">        stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// that means the partition was never in OnlinePartition state, this means the broker never</span></span><br><span class="line">        <span class="comment">// started a log for that partition and does not have a high watermark value for this partition</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</span><br></pre></td></tr></table></figure>
<p>从前面的状态转移图中可以看出，当 Replica 处在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，Replica 是可以转移到 OnlineReplica 状态的，下面分两种情况讲述：</p>
<p><strong>NewReplica –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionReplicaAssignment 中获取这个 Partition 的 AR；</li>
<li>如果 Replica 不在 AR 中的话，那么就将其添加到 Partition 的 AR 中；</li>
<li>最后将 Replica 的状态设置为 OnlineReplica 状态。</li>
</ol>
<p><strong>OnlineReplica/OfflineReplica/ReplicaDeletionIneligible –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionLeadershipInfo 中获取 Partition 的 LeaderAndIsr 信息；</li>
<li>如果该信息存在，那么就向这个 Replica 所在 broker 添加这个 Partition 的 LeaderAndIsr 请求，并将 Replica 的状态设置为 OnlineReplica 状态；</li>
<li>否则不做任务处理；</li>
<li>最后更新R Replica 的状态为 OnlineReplica。</li>
</ol>
<h4 id="TargetState-OfflineReplica"><a href="#TargetState-OfflineReplica" class="headerlink" title="TargetState: OfflineReplica"></a>TargetState: OfflineReplica</h4><p>OfflineReplica 是 Replica 所在 Broker 掉线时 Replica 的状态，转移到这种状态的处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">assertValidPreviousStates(partitionAndReplica,</span><br><span class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</span><br><span class="line"><span class="comment">// send stop replica command to the replica so that it stops fetching from the leader</span></span><br><span class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,先停止副本同步</span></span><br><span class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// As an optimization, the controller removes dead replicas from the ISR</span></span><br><span class="line"><span class="keyword">val</span> leaderAndIsrIsEmpty: <span class="type">Boolean</span> =</span><br><span class="line">  controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</span><br><span class="line">      controller.removeReplicaFromIsr(topic, partition, replicaId) <span class="keyword">match</span> &#123; <span class="comment">//note: 从 isr 中移除这个副本（前提是 ISR 有其他有效副本）</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(updatedLeaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">          <span class="comment">// send the shrunk ISR state change request to all the remaining alive replicas of the partition.</span></span><br><span class="line">          <span class="comment">//note: 发送 LeaderAndIsr 请求给剩余的其他副本,因为 ISR 变动了</span></span><br><span class="line">          <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</span><br><span class="line">          <span class="keyword">if</span> (!controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition)) &#123;</span><br><span class="line">            brokerRequestBatch.addLeaderAndIsrRequestForBrokers(currentAssignedReplicas.filterNot(_ == replicaId),</span><br><span class="line">              topic, partition, updatedLeaderIsrAndControllerEpoch, replicaAssignment)</span><br><span class="line">          &#125;</span><br><span class="line">          replicaState.put(partitionAndReplica, <span class="type">OfflineReplica</span>)</span><br><span class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></span><br><span class="line">            .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</span><br><span class="line">          <span class="literal">false</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">if</span> (leaderAndIsrIsEmpty &amp;&amp; !controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition))</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(</span><br><span class="line">    <span class="string">"Failed to change state of replica %d for partition %s since the leader and isr path in zookeeper is empty"</span></span><br><span class="line">    .format(replicaId, topicAndPartition))</span><br></pre></td></tr></table></figure>
<p>处理逻辑如下：</p>
<ol>
<li>校验其前置状态，只有 Replica 在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，才能转移到这种状态；</li>
<li>向该 Replica 所在节点发送 StopReplica 请求（deletePartition = false）；</li>
<li>调用 Controller 的 <code>removeReplicaFromIsr()</code> 方法将该 replica 从 Partition 的 isr 移除这个 replica（前提 isr 中还有其他有效副本），然后向该 Partition 的其他副本发送 LeaderAndIsr 请求；</li>
<li>更新这个 Replica 的状态为 OfflineReplica。</li>
</ol>
<h3 id="状态转移触发的条件"><a href="#状态转移触发的条件" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Replica 各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onBrokerStartup()</td>
<td>Broker 启动时，目的是将在该节点的 Replica 状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>新建 Partition 时，Replica 初始化及 Partition 状态变成 OnlinePartition 后，新创建的 Replica 状态也变为 OnlineReplica；</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onPartitionReassignment()</td>
<td>副本迁移完成后，RAR 中的副本设置为 OnlineReplica 状态</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>ReplicaStateMachine 的 startup()</td>
<td>副本状态机刚初始化启动时，将存活的副本状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的  markTopicForDeletionRetry()</td>
<td>将删除失败的 Replica 设置为 OfflineReplica，重新进行删除</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的 startReplicaDeletion()</td>
<td>开始副本删除时，先将副本设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 shutdownBroker() 方法</td>
<td>优雅关闭 broker 时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 onBrokerFailure()</td>
<td>broker 掉线时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>Partition 新建时，当 Partition 状态变为 NewPartition 后，副本的状态变为 NewReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 startNewReplicasForReassignedPartition()</td>
<td>Partition 副本迁移时，将新分配的副本状态设置为 NewReplica；</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>下线副本时，将成功设置为 OfflineReplica 的 Replica 设置为 ReplicaDeletionStarted 状态，开始物理上删除副本数据（也是发送 StopReplica）</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，目的是下线那些 old replica，新的 replica 已经迁移到新分配的副本上了</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>物理将数据成功删除的 Replica 状态会变为这个</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，在下线那些旧 Replica 时的一个状态，删除成功</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>开始副本删除时，删除失败的副本会设置成这个状态</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 副本迁移时，在下线那些旧的 Replica 时的一个状态，删除失败</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>副本删除成功后（状态为 ReplicaDeletionSuccessful），从状态机和 Controller 的缓存中清除该副本的记录；</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本成功迁移、旧副本成功删除后，从状态机和 Controller 的缓存中清除旧副本的记录</td>
</tr>
</tbody>
</table>
<h2 id="PartitionStateMachine"><a href="#PartitionStateMachine" class="headerlink" title="PartitionStateMachine"></a>PartitionStateMachine</h2><p>PartitionStateMachine 记录着集群所有 Partition 的状态信息，它决定着一个 Partition 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中 Partition 的状态总共有以下四种类型：</p>
<ol>
<li>NonExistentPartition：这个代表着这个 Partition 之前没有被创建过或者之前创建了现在又被删除了，它有效的前置状态是 OfflinePartition；</li>
<li>NewPartition：Partition 创建后，它将处于这个状态，这个状态的 Partition 还没有 leader 和 isr，它有效的前置状态是 NonExistentPartition；</li>
<li>OnlinePartition：一旦这个 Partition 的 leader 被选举出来了，它将处于这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition；</li>
<li>OfflinePartition：如果这个 Partition 的 leader 掉线，这个 Partition 将被转移到这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition。</li>
</ol>
<p>分区状态机转移图如下所示：</p>
<p><img src="/images/kafka/partition_state.png" alt="分区状态机"></p>
<p>这张图是分区状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 PartitionStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="PartitionStateMachine-初始化"><a href="#PartitionStateMachine-初始化" class="headerlink" title="PartitionStateMachine 初始化"></a>PartitionStateMachine 初始化</h3><p>PartitionStateMachine 的初始化方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: Controller 启动时触发</span></span><br><span class="line"><span class="comment">//note: 初始化所有 Partition 的状态（从 zk 获取）, 然后对于 new/offline Partition 触发选主（选主成功的话,变为 OnlinePartition）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  <span class="comment">// initialize partition state</span></span><br><span class="line">  <span class="comment">//note: 初始化 partition 的状态,如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></span><br><span class="line">  initializePartitionState()</span><br><span class="line">  <span class="comment">// set started flag</span></span><br><span class="line">  hasStarted.set(<span class="literal">true</span>)</span><br><span class="line">  <span class="comment">// try to move partitions to online state</span></span><br><span class="line">  <span class="comment">//note: 为所有处理 NewPartition 或 OnlinePartition 状态 Partition 选举 leader</span></span><br><span class="line">  triggerOnlinePartitionStateChange()</span><br><span class="line"></span><br><span class="line">  info(<span class="string">"Started partition state machine with initial state -&gt; "</span> + partitionState.toString())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法中，PartitionStateMachine 先调用 <code>initializePartitionState()</code> 方法初始化集群中所有 Partition 的状态信息：</p>
<ol>
<li>如果该 Partition 有 LeaderAndIsr 信息，那么如果 Partition leader 所在的机器是 alive 的，那么将其状态设置为 OnlinePartition，否则设置为 OfflinePartition 状态；</li>
<li>如果该 Partition 没有 LeaderAndIsr 信息，那么将其状态设置为 NewPartition。</li>
</ol>
<p>这里只是将 Partition 的状态信息更新分区状态机的缓存 <code>partitionState</code> 中，并没有真正进行状态的转移。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 根据从 zk 获取的所有 Partition,进行状态初始化</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializePartitionState</span></span>() &#123;</span><br><span class="line">  <span class="keyword">for</span> (topicPartition &lt;- controllerContext.partitionReplicaAssignment.keys) &#123;</span><br><span class="line">    <span class="comment">// check if leader and isr path exists for partition. If not, then it is in NEW state</span></span><br><span class="line">    controllerContext.partitionLeadershipInfo.get(topicPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(currentLeaderIsrAndEpoch) =&gt;</span><br><span class="line">        <span class="comment">// else, check if the leader for partition is alive. If yes, it is in Online state, else it is in Offline state</span></span><br><span class="line">        <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(currentLeaderIsrAndEpoch.leaderAndIsr.leader))</span><br><span class="line">          <span class="comment">// leader is alive</span></span><br><span class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,并且 leader 存活,设置为 OnlinePartition 状态</span></span><br><span class="line">          partitionState.put(topicPartition, <span class="type">OnlinePartition</span>)</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,但是 leader 不存活,设置为 OfflinePartition 状态</span></span><br><span class="line">          partitionState.put(topicPartition, <span class="type">OfflinePartition</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">//note: 没有 LeaderAndIsr 信息,设置为 NewPartition 状态（这个 Partition 还没有）</span></span><br><span class="line">        partitionState.put(topicPartition, <span class="type">NewPartition</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在初始化的第二步，将会调用 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有的状态为 NewPartition/OnlinePartition 的 Partition 进行 leader 选举，选举成功后的话，其状态将会设置为 OnlinePartition，调用的 Leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#OfflinePartitionLeaderSelector">OfflinePartitionLeaderSelector</a>（具体实现参考链接）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 这个方法是在 controller 选举后或 broker 上线或下线时时触发的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triggerOnlinePartitionStateChange</span></span>() &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    brokerRequestBatch.newBatch()</span><br><span class="line">    <span class="comment">// try to move all partitions in NewPartition or OfflinePartition state to OnlinePartition state except partitions</span></span><br><span class="line">    <span class="comment">// that belong to topics to be deleted</span></span><br><span class="line">    <span class="comment">//note: 开始为所有状态在 NewPartition or OfflinePartition 状态的 partition 更新状态（除去将要被删除的 topic）</span></span><br><span class="line">    <span class="keyword">for</span>((topicAndPartition, partitionState) &lt;- partitionState</span><br><span class="line">        <span class="keyword">if</span> !controller.deleteTopicManager.isTopicQueuedUpForDeletion(topicAndPartition.topic)) &#123;</span><br><span class="line">      <span class="keyword">if</span>(partitionState.equals(<span class="type">OfflinePartition</span>) || partitionState.equals(<span class="type">NewPartition</span>))</span><br><span class="line">        <span class="comment">//note: 尝试为处在 OfflinePartition 或 NewPartition 状态的 Partition 选主,成功后转换为 OnlinePartition</span></span><br><span class="line">        handleStateChange(topicAndPartition.topic, topicAndPartition.partition, <span class="type">OnlinePartition</span>, controller.offlinePartitionSelector,</span><br><span class="line">                          (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 发送请求给所有的 broker,包括 LeaderAndIsr 请求和 UpdateMetadata 请求（这里只是添加到 Broker 对应的 RequestQueue 中,后台有线程去发送）</span></span><br><span class="line">    brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some partitions to the online state"</span>, e)</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> It is not enough to bail out and log an error, it is important to trigger leader election for those partitions</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面方法的目的是为尝试将所有的状态为 NewPartition/OnlinePartition 的 Partition 状态转移到 OnlinePartition，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="分区的状态转移"><a href="#分区的状态转移" class="headerlink" title="分区的状态转移"></a>分区的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewPartition、OfflinePartition、NonExistentPartition 或者 OnlinePartition 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewPartition"><a href="#TargetState-NewPartition" class="headerlink" title="TargetState: NewPartition"></a>TargetState: NewPartition</h4><p>NewPartition 是 Partition 刚创建时的一个状态，其处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如果该 Partition 的状态不存在,默认为 NonExistentPartition</span></span><br><span class="line"><span class="keyword">val</span> currState = partitionState.getOrElseUpdate(topicAndPartition, <span class="type">NonExistentPartition</span>)</span><br><span class="line"><span class="comment">// pre: partition did not exist before this</span></span><br><span class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</span><br><span class="line">partitionState.put(topicAndPartition, <span class="type">NewPartition</span>) <span class="comment">//note: 缓存 partition 的状态</span></span><br><span class="line"><span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></span><br><span class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</span><br><span class="line">                                  assignedReplicas))</span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NonExistentPartition；</li>
<li>将该 Partition 的状态转移为 NewPartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-OnlinePartition"><a href="#TargetState-OnlinePartition" class="headerlink" title="TargetState: OnlinePartition"></a>TargetState: OnlinePartition</h4><p>OnlinePartition 是一个 Partition 正常工作时的状态，这个状态下的 Partition 已经成功选举出了 leader 和 isr 信息，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 判断 Partition 之前的状态是否可以转换为目的状态</span></span><br><span class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</span><br><span class="line">partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt; <span class="comment">//note: 新建的 Partition</span></span><br><span class="line">    <span class="comment">//note: 选举 leader 和 isr,更新到 zk 和 controller 中,如果没有存活的 replica,抛出异常</span></span><br><span class="line">    <span class="comment">// initialize leader and isr path for new partition</span></span><br><span class="line">    initializeLeaderAndIsrForPartition(topicAndPartition)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt; <span class="comment">//note: leader 挂掉的 Partition</span></span><br><span class="line">    <span class="comment">//note: 进行 leader 选举,更新到 zk 及 controller 缓存中,失败的抛出异常</span></span><br><span class="line">    electLeaderForPartition(topic, partition, leaderSelector)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></span><br><span class="line">    <span class="comment">//note:这种只有在 leader 需要重新选举时才会触发</span></span><br><span class="line">    electLeaderForPartition(topic, partition, leaderSelector)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></span><br><span class="line">&#125;</span><br><span class="line">partitionState.put(topicAndPartition, <span class="type">OnlinePartition</span>)</span><br><span class="line"><span class="keyword">val</span> leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s from %s to %s with leader %d"</span></span><br><span class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader))</span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验这个 Partition 的前置状态，有效的前置状态是：NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>如果前置状态是 NewPartition，那么为该 Partition 选举 leader 和 isr，更新到 zk 和 controller 的缓存中，如果副本没有处于 alive 状态的话，就抛出异常；</li>
<li>如果前置状态是 OnlinePartition，那么只是触发 leader 选举，在 OnlinePartition –&gt; OnlinePartition 这种状态转移时，需要传入 leader 选举的方法，触发该 Partition 的 leader 选举；</li>
<li>如果前置状态是 OfflinePartition，同上，也是触发 leader 选举。</li>
<li>更新 Partition 的状态为 OnlinePartition。</li>
</ol>
<p>对于以上这几种情况，无论前置状态是什么，最后都会触发这个 Partition 的 leader 选举，leader 成功后，都会触发向这个 Partition 的所有 replica 发送 LeaderAndIsr 请求。</p>
<h4 id="TargetState-OfflinePartition"><a href="#TargetState-OfflinePartition" class="headerlink" title="TargetState: OfflinePartition"></a>TargetState: OfflinePartition</h4><p>OfflinePartition 是这个 Partition 的 leader 挂掉时转移的一个状态，如果 Partition 转移到这个状态，那么就意味着这个 Partition 没有了可用 leader。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pre: partition should be in New or Online state</span></span><br><span class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OfflinePartition</span>)</span><br><span class="line"><span class="comment">// should be called when the leader for a partition is no longer alive</span></span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></span><br><span class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</span><br><span class="line">partitionState.put(topicAndPartition, <span class="type">OfflinePartition</span>)</span><br><span class="line"><span class="comment">// post: partition has no alive leader</span></span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 OfflinePartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-NonExistentPartition"><a href="#TargetState-NonExistentPartition" class="headerlink" title="TargetState: NonExistentPartition"></a>TargetState: NonExistentPartition</h4><p>NonExistentPartition 代表了已经处于 OfflinePartition 状态的 Partition 已经从 metadata 和 zk 中删除后进入的状态。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// pre: partition should be in Offline state</span></span><br><span class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">OfflinePartition</span>), <span class="type">NonExistentPartition</span>)</span><br><span class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></span><br><span class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</span><br><span class="line">partitionState.put(topicAndPartition, <span class="type">NonExistentPartition</span>)</span><br><span class="line"><span class="comment">// post: partition state is deleted from all brokers and zookeeper</span></span><br></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 NonExistentPartition 状态，并且更新到缓存中。</li>
</ol>
<h3 id="状态转移触发的条件-1"><a href="#状态转移触发的条件-1" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Partition   各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 shutdownBroker()</td>
<td>优雅关闭 Broker 时调用，因为要下线的节点是 leader，所以需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 新建时，这个是在 Replica 已经变为 NewPartition 状态后进行的，为新建的 Partition 初始化 leader 和 isr</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 onPreferredReplicaElection()</td>
<td>对 Partition 进行最优 leader 选举，目的是触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 moveReassignedPartitionLeaderIfRequired()</td>
<td>分区副本迁移完成后，1. 当前的 leader 不在 RAR 中，需要触发 leader 选举；2. 当前 leader 在 RAR 但是掉线了，也需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 triggerOnlinePartitionStateChange()</td>
<td>当 Controller 重新选举出来或 broker 有变化时，目的为了那些状态为 NewPartition/OfflinePartition 的 Partition 重新选举 leader，选举成功后状态变为 OnlinePartition</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 initializePartitionState()</td>
<td>Controller 初始化时，遍历 zk 的所有的分区，如果有 LeaderAndIsr 信息并且 leader 在 alive broker 上，那么就将状态转为 OnlinePartition。</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>controller 的 onBrokerFailure()</td>
<td>当有 broker 掉线时，将 leader 在这个机器上的 Partition 设置为 OfflinePartition</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，中间会将该 Partition 的状态先转变为 OfflinePartition</td>
</tr>
<tr>
<td>NonExistentPartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，最后会将该 Partition 的状态转移为 NonExistentPartition</td>
</tr>
<tr>
<td>NewPartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 刚创建时的一个中间状态 ，此时还没选举 leader 和设置 isr 信息</td>
</tr>
</tbody>
</table>
<p>上面就是副本状态机与分区状态机的所有内容，这里只是单纯地讲述了一下这两种状态机，后续文章会开始介绍 Controller 一些其他内容，包括 Partition 迁移、Topic 新建、Topic 下线等，这些内容都会用到这篇文章讲述的内容。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
</feed>
