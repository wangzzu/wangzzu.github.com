<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matt&#39;s Blog</title>
  <subtitle>王蒙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matt33.com/"/>
  <updated>2018-07-15T04:17:12.000Z</updated>
  <id>http://matt33.com/</id>
  
  <author>
    <name>Matt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka Controller Redesign 方案</title>
    <link href="http://matt33.com/2018/07/14/kafka-controller-redesign/"/>
    <id>http://matt33.com/2018/07/14/kafka-controller-redesign/</id>
    <published>2018-07-14T15:13:56.000Z</published>
    <updated>2018-07-15T04:17:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是一次重构，因此，社区准备在新版的系统里对 Controller 做一些相应的优化（0.11.0及以后的版本），相应的设计方案见：<a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>，本文的内容就是结合这篇文章做一个简单的总结。</p>
<h2 id="Controller-功能"><a href="#Controller-功能" class="headerlink" title="Controller 功能"></a>Controller 功能</h2><p>在一个 Kafka 中，Controller 要处理的事情总结如下表所示：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>详情</th>
</tr>
</thead>
<tbody>
<tr>
<td>cluster metadata updates</td>
<td>producer 或 consumer 可以通过 MetadataRequest 请求从集群任何一台 broker 上查询到某个 Partition 的 metadata 信息，如果一个 Partition 的 leader 或 isr 等信息变化，Controller 会广播到集群的所有 broker 上，这样每台 Broker 都会有该 Partition 的最新 Metadata 信息</td>
</tr>
<tr>
<td>topic creation</td>
<td>用户可以通过多种方式创建一个 topic，最终的结果都是在 zk 的 <code>/brokers/topics</code> 目录下新建一个 topic 节点信息，controller 通过监控这个目录来判断是否有新的 topic 需要创建</td>
</tr>
<tr>
<td>topic deletion</td>
<td>Controller 通过监控 zk 的 <code>/admin/delete_topics</code> 节点来触发 topic 删除操作</td>
</tr>
<tr>
<td>partition reassignment</td>
<td>Controller 通过监控 zk 的 <code>/admin/reassign_partitions</code> 节点来触发 Partition 的副本迁移操作</td>
</tr>
<tr>
<td>preferred replica leader election</td>
<td>Controller 通过监控 zk 的 <code>/admin/preferred_replica_election</code> 节点来触发最优 leader 选举操作，该操作的目的选举 Partition 的第一个 replica 作为 leader</td>
</tr>
<tr>
<td>topic partition expansion</td>
<td>Controller 通过监控 zk 的 <code>/brokers/topics/&lt;topic&gt;</code> 数据内容的变化，来触发 Topic 的 Partition 扩容操作</td>
</tr>
<tr>
<td>broker join</td>
<td>Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 是最新加入的，进而触发 broker 的上线操作</td>
</tr>
<tr>
<td>broker failure</td>
<td>同样，Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 掉线了，进而触发 broker 的下线操作</td>
<td></td>
</tr>
<tr>
<td>controlled shutdown</td>
<td>Controller 通过处理 ControlledShudownRequest 请求来优雅地关闭一个 broker 节点，主动关闭与直接 kill 的区别，它可以减少 Partition 的不可用时间，因为一个 broker 的 zk 临时节点消失是需要一定时间的</td>
</tr>
<tr>
<td>controller leader election</td>
<td>集群中所有 broker 会监听 zk 的 <code>/controller</code> 节点，如果该节点消失，所有的 broker 都回去抢占 controller 节点，抢占成功的，就成了最新的 controller</td>
</tr>
</tbody>
</table>
<h2 id="Controller-目前存在的问题"><a href="#Controller-目前存在的问题" class="headerlink" title="Controller 目前存在的问题"></a>Controller 目前存在的问题</h2><p>之所以要重新设计 Controller，是因为现在的 Controller 积累了一些比较难解决的问题，这些问题解决起来，代码改动量都是巨大的，甚至需要改变 controller 部门的设计，基本就跟重构差不多了，下面我们先来了看一下 controller 之前（主要是 0.11.0 之前的版本）存在的一些问题。</p>
<p>目前遇到的比较大的问题有以下几个：</p>
<ol>
<li>Partition 级别同步 zk 写；</li>
<li>sequential per-partition controller-to-broker requests；</li>
<li>Controller 复杂的并发语义；</li>
<li>代码组织混乱；</li>
<li>控制类请求与数据类请求未分离；</li>
<li>Controller 给 broker 的请求中没有 broker 的 generation信息；</li>
<li>ZkClient 阻碍 Client 的状态管理。</li>
</ol>
<h3 id="Partition-级别同步-zk-写"><a href="#Partition-级别同步-zk-写" class="headerlink" title="Partition 级别同步 zk 写"></a>Partition 级别同步 zk 写</h3><p>zookeeper 的同步写意味着在下次写之前需要等待前面整个过程的结束，而且由于它们都是 partition 粒度的（一个 Partition 一个 Partition 的去执行写操作），对于 Partition 非常多的集群来说，需要等待的时间会更长，Controller 通常会在下面这两个地方做 Partition 级别 zookeeper 同步写操作：</p>
<ol>
<li>PartitionStateMachine 在进行触发 leader 选举（partition 目的状态是 OnlinePartition），将会触发上面的操作；</li>
<li>ReplicaStateMachine 更新 LeaderAndIsr 信息到 zk（replica 状态转变为 OfflineReplica），这种情况也触发这种情况，它既阻碍了 Controller 进程，也有可能会 zk 造成压力。</li>
</ol>
<h3 id="sequential-per-partition-controller-to-broker-requests"><a href="#sequential-per-partition-controller-to-broker-requests" class="headerlink" title="sequential per-partition controller-to-broker requests"></a>sequential per-partition controller-to-broker requests</h3><p>Controller 在向 Broker 发送请求，有些情况下也是 Partition 粒度去发送的，效率非常低，比如在 Controller 处理 broker shutdown 请求时，这里是按 Partition 级别处理，每处理一个 Partition 都会执行 Partition、Replica 状态变化以及 Metadata 更新，并且调用 <code>sendRequestsToBrokers()</code> 向 broker 发送请求，这样的话，效率将变得非常低。</p>
<h3 id="Controller-复杂的并发语义"><a href="#Controller-复杂的并发语义" class="headerlink" title="Controller 复杂的并发语义"></a>Controller 复杂的并发语义</h3><p>Controller 需要在多个线程之间共享状态信息，这些线程有：</p>
<ol>
<li>IO threads handling controlled shutdown requests</li>
<li>The ZkClient org.I0Itec.zkclient.ZkEventThread processing zookeeper callbacks sequentially；</li>
<li>The TopicDeletionManager kafka.controller.DeleteTopicsThread；</li>
<li>Per-broker RequestSendThread within ControllerChannelManager.</li>
</ol>
<p>所有这些线程都需要访问或修改状态信息（ControllerContext），现在它们是通过 ControllerContext 的 controllerLock（排它锁）实现的，Controller 的并发变得虚弱无力。</p>
<h3 id="代码组织混乱"><a href="#代码组织混乱" class="headerlink" title="代码组织混乱"></a>代码组织混乱</h3><p>KafkaController 部分的代码组织（KafkaController、PartitionStateMachine 和 ReplicaStateMachine）不是很清晰，比如，下面的问题就很难回答：</p>
<ol>
<li>where and when does zookeeper get updated?</li>
<li>where and when does a controller-to-broker request get formed?</li>
<li>what impact does a failing zookeeper update or controller-to-broker request have on the cluster state?</li>
</ol>
<p>这也导致了这部分很多开发者不敢轻易去改动。</p>
<h3 id="控制类请求与数据类请求未分离"><a href="#控制类请求与数据类请求未分离" class="headerlink" title="控制类请求与数据类请求未分离"></a>控制类请求与数据类请求未分离</h3><p>现在 broker 收到的请求，有来自 client、broker 和 controller 的请求，这些请求都会被放到同一个 requestQueue 中，它们有着同样的优先级，所以来自 client 的请求很可能会影响来自 controller 请求的处理（如果是 leader 变动的请求，ack 设置的不是 all，这种情况有可能会导致数据丢失）。</p>
<h3 id="Controller-给-broker-的请求中没有-broker-的-generation信息"><a href="#Controller-给-broker-的请求中没有-broker-的-generation信息" class="headerlink" title="Controller 给 broker 的请求中没有 broker 的 generation信息"></a>Controller 给 broker 的请求中没有 broker 的 generation信息</h3><p>这里的 Broker generation 代表着一个标识，每当它重新加入集群时，这个标识都会变化。如果 Controller 的请求没有这个信息的话，可能会导致一个重启的 Broker 收到之前的请求，让 Broker 进入到一个错误的状态。</p>
<p>比如，Broker 收到之前的 StopReplica 请求，可能会导致副本同步线程退出。</p>
<h3 id="ZkClient-阻碍-Client-的状态管理"><a href="#ZkClient-阻碍-Client-的状态管理" class="headerlink" title="ZkClient 阻碍 Client 的状态管理"></a>ZkClient 阻碍 Client 的状态管理</h3><p>这里的状态管理指的是当 Client 发生重连或会话过期时，Client 可以监控这种状态变化，并做出一些处理，因为开源版的 ZKClient 在处理 notification 时，是线性处理的，一些 notification 会被先放到 ZkEventThread’s queue 中，这样会导致一些最新的 notification 不能及时被处理，特别是与 zk 连接断开重连的情况。</p>
<h2 id="Controller-改进方案"><a href="#Controller-改进方案" class="headerlink" title="Controller 改进方案"></a>Controller 改进方案</h2><p>关于上述问题，Kafka 提出了一些改进方案，有些已经在最新版的系统中实现，有的还在规划中。</p>
<h3 id="使用异步的-zk-api"><a href="#使用异步的-zk-api" class="headerlink" title="使用异步的 zk api"></a>使用异步的 zk api</h3><p>Zookeeper 的 client 提供三种执行请求的方式：</p>
<ol>
<li>同步调用，意味着下次请求需要等待当前当前请求的完成；</li>
<li>异步调用，意味着不需要等待当前请求的完成就可以开始下次请求的执行，并且我们可以通过回调机制去处理请求返回的结果；</li>
<li>单请求的 batch 调用，意味着 batch 内的所有请求都会在一次事务处理中完成，这里需要关注的是 zookeeper 的 server 对单请求的大小是有限制的（jute.maxbuffer）。</li>
</ol>
<p>文章中给出了三种请求的测试结果，Kafka 最后选取的是异步处理机制，因为对于单请求处理，异步处理更加简洁，并且相比于同步处理还可以保持一个更好的写性能。</p>
<h3 id="improve-controller-to-broker-request-batching"><a href="#improve-controller-to-broker-request-batching" class="headerlink" title="improve controller-to-broker request batching"></a>improve controller-to-broker request batching</h3><p>这个在设计文档还是 TODO 状态，具体的方案还没确定，不过基本可以猜测一下，因为目的是提高 batch 发送能力，那么只能是在调用对每个 broker 的 RequestSenderThread 线程发送请求之前，做一下检测，而不是来一个请求立马就发送，这是一个性能与时间的权衡，如果不是立马发送请求，那么可能会带来 broker 短时 metadata 信息的不一致，这个不一致时间不同的应用场景要求是不一样的。</p>
<h3 id="单线程的事件处理模型"><a href="#单线程的事件处理模型" class="headerlink" title="单线程的事件处理模型"></a>单线程的事件处理模型</h3><p>采用单线程的时间处理模型将极大简化 Controller 的并发实现，只允许这个线程访问和修改 Controller 的本地状态信息，因此在 Controller 部分也就不需要到处加锁来保证线程安全了。</p>
<p>目前 1.1.0 的实现中，Controller 使用了一个 ControllerEventThread 线程来处理所有的 event，目前可以支持13种不同类型事件：</p>
<ol>
<li>Idle：代表当前 ControllerEventThread 处理空闲状态；</li>
<li>ControllerChange：Controller 切换处理；</li>
<li>BrokerChange：Broker 变动处理，broker 可能有上线或掉线；</li>
<li>TopicChange：Topic 新增处理；</li>
<li>TopicDeletion：Topic 删除处理；</li>
<li>PartitionReassignment：Partition 副本迁移处理；</li>
<li>AutoLeaderBalance：自动 rebalance 处理；</li>
<li>ManualLeaderBalance：最优 leader 选举处理，这里叫做手动 rebalance，手动去切流量；</li>
<li>ControlledShutdown：优雅关闭 broker；</li>
<li>IsrChange：Isr 变动处理；</li>
<li>LeaderAndIsrResponseReceived；</li>
<li>LogDirChange：Broker 某个目录失败后的处理（比如磁盘坏掉等）；</li>
<li>ControllerShutdown：ControllerEventThread 处理这个事件时，会关闭当前线程。</li>
</ol>
<h3 id="重构集群状态管理"><a href="#重构集群状态管理" class="headerlink" title="重构集群状态管理"></a>重构集群状态管理</h3><p>这部分的改动，目前社区也没有一个很好的解决思路，重构这部分的目的是希望 Partition、Replica 的状态管理变得更清晰一些，让我们从代码中可以清楚地明白状态是在什么时间、什么地方、什么条件下被触发的。这个优化其实是跟上面那个有很大关联，采用单线程的事件处理模型，可以让状态管理也变得更清晰。</p>
<h4 id="prioritize-controller-requests"><a href="#prioritize-controller-requests" class="headerlink" title="prioritize controller requests"></a>prioritize controller requests</h4><p>我们想要把控制类请求与数据类请求分开，提高 controller 请求的优先级，这样的话即使 Broker 中请求有堆积，Broker 也会优先处理控制类的请求。</p>
<p>这部分的优化可以在网络层的 RequestChannel 中做，RequestChannel 可以根据请求的 id 信息把请求分为正常的和优先的，如果请求是 UpdateMetadataRequest、LeaderAndIsrRequest 或者 StopReplicaRequest，那么这个请求的优先级应该提高。实现方案有以下两种：</p>
<ol>
<li>在请求队列中增加一个优先级队列，优先级高的请求放到 the prioritized request queue 中，优先级低的放到普通请求队列中，但是无论使用一个定时拉取（poll）还是2个定时拉取，都会带来其他的问题，要么是增大普通请求的处理延迟，要么是增大了优先级高请求的延迟；</li>
<li>直接使用优先级队列代替现在的普通队列，设计上更倾向与这一种。</li>
</ol>
<p>目前这部分在1.1.0中还未实现。</p>
<h3 id="Controller-发送请求中添加-broker-的-generation-信息"><a href="#Controller-发送请求中添加-broker-的-generation-信息" class="headerlink" title="Controller 发送请求中添加 broker 的 generation 信息"></a>Controller 发送请求中添加 broker 的 generation 信息</h3><p>generation 信息是用来标识当前 broker 加入集群 epoch 信息，每当 broker 重新加入集群中，该 broker.id 对应的 generation 都应该变化（要求递增），目前有两种实现方案：</p>
<ol>
<li>为 broker 分配的一个全局唯一的 id，由 controller 广播给其他 broker；</li>
<li>直接使用 zookeeper 的 zxid 信息（broker.id 注册时的 zxid）。</li>
</ol>
<h3 id="直接使用原生的-Zookeeper-client"><a href="#直接使用原生的-Zookeeper-client" class="headerlink" title="直接使用原生的 Zookeeper client"></a>直接使用原生的 Zookeeper client</h3><p>Client 端的状态管理意味着当 Client 端发生状态变化（像连接中断或回话超时）时，我们有能力做一些操作。其中，zookeeper client 有效的状态（目前的 client 比下面又多了几种状态，这里先不深入）是:</p>
<ul>
<li>NOT_CONNECTED： the initial state of the client；</li>
<li>CONNECTING： the client is establishing a connection to zookeeper；</li>
<li>CONNECTED： the client has established a connection and session to zookeeper；</li>
<li>CLOSED： the session has closed or expired。</li>
</ul>
<p>有效的状态转移是：</p>
<ul>
<li>NOT_CONNECTED &gt; CONNECTING</li>
<li>CONNECTING &gt; CONNECTED</li>
<li>CONNECTING &gt; CLOSED</li>
<li>CONNECTED &gt; CONNECTING</li>
<li>CONNECTED &gt; CLOSED</li>
</ul>
<p>最开始的设想是直接使用原生 Client 的异步调用方式，这样的话依然可以通过回调方法监控到状态的变化（像连接中断或回话超时），同样，在每次事件处理时，可以通过检查状态信息来监控到 Client 状态的变化，及时做一些处理。</p>
<p>当一个 Client 接收到连接中断的 notification（Client 状态变成了 CONNECTING 状态），它意味着 Client 不能再从 zookeeper 接收到任何 notification 了。如果断开连接，对于 Controller 而言，无论它现在正在做什么它都应该先暂停，因为可能集群的 Controller 已经切换到其他机器上了，只是它还没接收到通知，它如果还在工作，可能会导致集群状态不一致。当连接断开后，Client 可以重新建立连接（re-establish，状态变为 CONNECTED）或者会话过期（状态变为 CLOSED，会话过期是由 zookeeper Server 来决定的）。如果变成了 CONNECTED 状态，Controller 应该重新开始这些暂停的操作，而如果状态变成了 CLOSED 状态，旧的 Controller 就会知道它不再是 controller，应该丢弃掉这些任务。</p>
<p>参考：</p>
<ul>
<li><a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>；</li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6980045.html" target="_blank" rel="external">Kafka controller重设计</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统的一致性协议之 2PC 和 3PC</title>
    <link href="http://matt33.com/2018/07/08/distribute-system-consistency-protocol/"/>
    <id>http://matt33.com/2018/07/08/distribute-system-consistency-protocol/</id>
    <published>2018-07-08T15:21:34.000Z</published>
    <updated>2018-07-08T16:02:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，最多只能满足其中两个。但在实际中，由于网络环境是不可信的，所以分区容忍性几乎是必不可选的，设计者基本就是在一致性和可用性之间做选择，当然大部分情况下，大家都会选择牺牲一部分的一致性来保证可用性（可用性较差的系统非常影响用户体验的，但是对另一些场景，比如支付场景，强一致性是必须要满足）。但是分布式系统又无法彻底放弃一致性（Consistency），如果真的放弃一致性，那么就说明这个系统中的数据根本不可信，数据也就没有意义，那么这个系统也就没有任何价值可言。</p>
<h2 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a>CAP 理论</h2><p>CAP 理论三个特性的详细含义如下：</p>
<ol>
<li>一致性（Consistency）：每次读取要么是最新的数据，要么是一个错误；</li>
<li>可用性（Availability）：client 在任何时刻的读写操作都能在限定的延迟内完成的，即每次请求都能获得一个响应（非错误），但不保证是最新的数据；</li>
<li>分区容忍性（Partition tolerance）：在大规模分布式系统中，网络分区现象，即分区间的机器无法进行网络通信的情况是必然会发生的，系统应该能保证在这种情况下可以正常工作。</li>
</ol>
<h3 id="分区容忍性"><a href="#分区容忍性" class="headerlink" title="分区容忍性"></a>分区容忍性</h3><p>很多人可能对分区容忍性不太理解，知乎有一个回答对这个解释的比较清楚（<a href="https://www.zhihu.com/question/54105974" target="_blank" rel="external">CAP理论中的P到底是个什么意思？</a>），这里引用一下：</p>
<ul>
<li>一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。</li>
<li>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</li>
<li>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里，容忍性就提高了。</li>
<li>然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。</li>
<li>要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</li>
<li>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</li>
</ul>
<h3 id="CAP-如何选择"><a href="#CAP-如何选择" class="headerlink" title="CAP 如何选择"></a>CAP 如何选择</h3><p>CAP 理论一个经典原理如下所示：</p>
<p><img src="/images/distribute/CAP.png" alt="CAP 理论原理"></p>
<p>CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。但是，对于大多数互联网应用来说，因为规模比较大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。但是对于一些金融相关行业，它有很多场景需要确保一致性，这种情况通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p>
<p>在一个分布式系统中，对于这三个特性，我们只能三选二，无法同时满足这三个特性，三选二的组合以及这样系统的特点总结如下（来自<a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>）：</p>
<ul>
<li>CA (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li>
<li>CP (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p>对于分布式系统分区容忍性是天然具备的要求，否则一旦出现网络分区，系统就拒绝所有写入只允许可读，这对大部分的场景是不可接收的，因此，在设计分布式系统时，更多的情况下是选举 CP 还是 AP，要么选择强一致性弱可用性，要么选择高可用性容忍弱一致性。</p>
<h3 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h3><p>关于分布式系统的一致性模型有以下几种：</p>
<h4 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h4><p>当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值，直到这个数据被其他数据更新为止。</p>
<p>但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。</p>
<h4 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a>弱一致性</h4><p>系统并不保证进程或者线程的访问都会返回最新更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。</p>
<h4 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h4><p>最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一个时间，在这个时间之后可以保证这一点，而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为「不一致窗口」。不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。</p>
<p>最终一致性在实际应用中又有多种变种：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>因果一致性</td>
<td>如果 A 进程在更新之后向 B 进程通知更新的完成，那么 B 的访问操作将会返回更新的值。而没有因果关系的 C 进程将会遵循最终一致性的规则（C 在不一致窗口内还是看到是旧值）。</td>
</tr>
<tr>
<td>读你所写一致性</td>
<td>因果一致性的特定形式。一个进程进行数据更新后，会给自己发送一条通知，该进程后续的操作都会以最新值作为基础，而其他的进程还是只能在不一致窗口之后才能看到最新值。</td>
</tr>
<tr>
<td>会话一致性</td>
<td>读你所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程可以读取到最新之，但如果会话终止，重新连接后，如果此时还在不一致窗口内，还是可嫩读取到旧值。</td>
</tr>
<tr>
<td>单调读一致性</td>
<td>如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。</td>
</tr>
<tr>
<td>单调写一致性</td>
<td>系统保证对同一个进程的写操作串行化。</td>
</tr>
</tbody>
</table>
<p>它们的关系又如下图所示（图来自 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>）：</p>
<p><img src="/images/distribute/consistency.png" alt="一致性模型之间关系"></p>
<h2 id="分布式一致性协议"><a href="#分布式一致性协议" class="headerlink" title="分布式一致性协议"></a>分布式一致性协议</h2><p>为了解决分布式系统的一致性问题，在长期的研究探索过程中，业内涌现出了一大批经典的一致性协议和算法，其中比较著名的有二阶段提交协议（2PC），三阶段提交协议（3PC）和 Paxos 算法（本文暂时先不介绍）。</p>
<p>Google 2009年 在<a href="https://snarfed.org/transactions_across_datacenters_io.html" target="_blank" rel="external">Transaction Across DataCenter</a> 的分享中，对一致性协议在业内的实践做了一简单的总结，如下图所示，这是 CAP 理论在工业界应用的实践经验。</p>
<p><img src="/images/distribute/cap-sumarry.png" alt="CAP 理论在工业界的实践"></p>
<p>其中，第一行表头代表了分布式系统中通用的一致性方案，包括冷备、Master/Slave、Master/Master、两阶段提交以及基于 Paxos 算法的解决方案，第一列表头代表了分布式系统大家所关心的各项指标，包括一致性、事务支持程度、数据延迟、系统吞吐量、数据丢失可能性、故障自动恢复方式。</p>
<h2 id="两阶段提交协议（2PC）"><a href="#两阶段提交协议（2PC）" class="headerlink" title="两阶段提交协议（2PC）"></a>两阶段提交协议（2PC）</h2><p>二阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现 ACID 的原子性（A）。在数据一致性中，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来保证数据的强一致性。</p>
<p>2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为<strong>协调者</strong>的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</p>
<h3 id="2PC-过程"><a href="#2PC-过程" class="headerlink" title="2PC 过程"></a>2PC 过程</h3><p>关于两阶段提交的过程如下图所示：</p>
<p><img src="/images/distribute/2pc_process.png" alt="两阶段提交过程"></p>
<p>顾名思义，2PC 分为两个过程：</p>
<ol>
<li>表决阶段：此时 Coordinator （协调者）向所有的参与者发送一个 vote request，参与者在收到这请求后，如果准备好了就会向 Coordinator 发送一个 <code>VOTE_COMMIT</code> 消息作为回应，告知 Coordinator 自己已经做好了准备，否则会返回一个 <code>VOTE_ABORT</code> 消息；</li>
<li>提交阶段：Coordinator 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么 Coordinator 就会发送 <code>GLOBAL_COMMIT</code> 消息，否则发送 <code>GLOBAL_ABORT</code> 消息；对于参与者而言，如果收到 <code>GLOBAL_COMMIT</code> 消息，就会提交本地事务，否则就会取消本地事务。</li>
</ol>
<h3 id="2PC-一致性问题"><a href="#2PC-一致性问题" class="headerlink" title="2PC 一致性问题"></a>2PC 一致性问题</h3><p>这里先讨论一下，2PC 是否可以在任何情况下都可以解决一致性问题，在实际的网络生产中，各种情况都有可能发生，这里，我们先从理论上分析各种意外情况。</p>
<p>2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 挂了，参与者没挂</td>
<td>这种情况其实比较好解决，只要找一个 Coordinator 的替代者。当他成为新的 Coordinator 的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。</td>
</tr>
<tr>
<td>参与者挂了（无法恢复），Coordinator 没挂</td>
<td>如果挂了之后没有恢复，那么是不会导致数据一致性问题。</td>
</tr>
<tr>
<td>参与者挂了（后来恢复），Coordinator 没挂</td>
<td>恢复后参与者如果发现有未执行完的事务操作，直接取消，然后再询问 Coordinator 目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。</td>
</tr>
</tbody>
</table>
<p>还有一种情况是：参与者挂了，Coordinator 也挂了，需要再细分为几种类型来讨论：</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 和参与者在第一阶段挂了</td>
<td>由于这时还没有执行 commit 操作，新选出来的 Coordinator 可以询问各个参与者的情况，再决定是进行 commit 还是 roolback。因为还没有 commit，所以不会导致数据一致性问题。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，但是挂的这个参与者在挂之前还没有做相关操作</td>
<td>这种情况下，当新的 Coordinator 被选出来之后，他同样是询问所有参与者的情况。只要有机器执行了 abort（roolback）操作或者第一阶段返回的信息是 No 的话，那就直接执行 roolback 操作。如果没有人执行 abort 操作，但是有机器执行了 commit 操作，那么就直接执行 commit 操作。这样，当挂掉的参与者恢复之后，只要按照 Coordinator 的指示进行事务的 commit 还是 roolback 操作就可以了。因为挂掉的机器并没有做 commit 或者 roolback 操作，而没有挂掉的机器们和新的 Coordinator 又执行了同样的操作，那么这种情况不会导致数据不一致现象。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，挂的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。</td>
<td>这种情况下，新的 Coordinator 被选出来之后，如果他想负起 Coordinator 的责任的话他就只能按照之前那种情况来执行 commit 或者 roolback 操作。这样新的 Coordinator 和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了 commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他已经执行完了之前的事务，如果他执行的是 commit 那还好，和其他的机器保持一致了，万一他执行的是 roolback 操作呢？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和 Coordinator 通信，再想办法把数据搞成一致的，但是，这段时间内他的数据状态已经是不一致的了！</td>
</tr>
</tbody>
</table>
<p>所以，2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。为了解决这个问题，衍生除了3PC。</p>
<h3 id="2PC-优缺点"><a href="#2PC-优缺点" class="headerlink" title="2PC 优缺点"></a>2PC 优缺点</h3><p>简单总结一下 2PC 的优缺点：</p>
<ul>
<li>优点：原理简洁清晰、实现方便；</li>
<li>缺点：同步阻塞、单点问题、某些情况可能导致数据不一致。</li>
</ul>
<p>关于这几个缺点，在实际应用中，都是对2PC 做了相应的改造：</p>
<ol>
<li>同步阻塞：2PC 有几个过程（比如 Coordinator 等待所有参与者表决的过程中）都是同步阻塞的，在实际的应用中，这可能会导致长阻塞问题，这个问题是通过超时判断机制来解决的，但并不能完全解决同步阻塞问题；</li>
<li>Coordinator 单点问题：实际生产应用中，Coordinator 都会有相应的备选节点；</li>
<li>数据不一致：这个在前面已经讲述过了，如果在第二阶段，Coordinator 和参与者都出现挂掉的情况下，是有可能导致数据不一致的。</li>
</ol>
<h2 id="三阶段提交协议（3PC）"><a href="#三阶段提交协议（3PC）" class="headerlink" title="三阶段提交协议（3PC）"></a>三阶段提交协议（3PC）</h2><p>三阶段提交协议（Three-Phase Commit， 3PC）最关键要解决的就是 Coordinator 和参与者同时挂掉导致数据不一致的问题，所以 3PC 把在 2PC 中又添加一个阶段，这样三阶段提交就有：CanCommit、PreCommit 和 DoCommit 三个阶段。</p>
<h3 id="3PC-过程"><a href="#3PC-过程" class="headerlink" title="3PC 过程"></a>3PC 过程</h3><p>三阶段提交协议的过程如下图（图来自 <a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>）所示：</p>
<p><img src="/images/distribute/Three-phase_commit_diagram.png" alt="三节点提交过程"></p>
<p>3PC 的详细过程如下（这个过程步骤内容来自 <a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>）：</p>
<h4 id="阶段一-CanCommit"><a href="#阶段一-CanCommit" class="headerlink" title="阶段一 CanCommit"></a>阶段一 CanCommit</h4><ol>
<li>事务询问：Coordinator 向各参与者发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；</li>
<li>参与者向 Coordinator 反馈询问的响应：参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No。</li>
</ol>
<h4 id="阶段二-PreCommit"><a href="#阶段二-PreCommit" class="headerlink" title="阶段二 PreCommit"></a>阶段二 PreCommit</h4><p><strong>执行事务预提交</strong>：如果 Coordinator 接收到各参与者反馈都是Yes，那么执行事务预提交：</p>
<ol>
<li>发送预提交请求：Coordinator 向各参与者发送 preCommit 请求，并进入 prepared 阶段；</li>
<li>事务预提交：参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；</li>
<li>各参与者向 Coordinator 反馈事务执行的响应：如果各参与者都成功执行了事务操作，那么反馈给协调者 ACK 响应，同时等待最终指令，提交 commit 或者终止 abort，结束流程；</li>
</ol>
<p><strong>中断事务</strong>：如果任何一个参与者向 Coordinator 反馈了 No 响应，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务。</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者发送 abort 请求；</li>
<li>中断事务：无论是收到来自 Coordinator 的 abort 请求，还是等待超时，参与者都中断事务。</li>
</ol>
<h4 id="阶段三-doCommit"><a href="#阶段三-doCommit" class="headerlink" title="阶段三 doCommit"></a>阶段三 doCommit</h4><p><strong>执行提交</strong></p>
<ol>
<li>发送提交请求：假设 Coordinator 正常工作，接收到了所有参与者的 ack 响应，那么它将从预提交阶段进入提交状态，并向所有参与者发送 doCommit 请求；</li>
<li>事务提交：参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；</li>
<li>反馈事务提交结果：参与者完成事务提交后，向 Coordinator 发送 ACK 信息；</li>
<li>完成事务：Coordinator 接收到所有参与者 ack 信息，完成事务。</li>
</ol>
<p><strong>中断事务</strong>：假设 Coordinator 正常工作，并且有任一参与者反馈 No，或者在等待超时后无法接收所有参与者的反馈，都会中断事务</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者节点发送 abort 请求；</li>
<li>事务回滚：参与者接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资源；</li>
<li>反馈事务回滚结果：参与者在完成事务回滚之后，向 Coordinator 发送 ack 信息；</li>
<li>中断事务：Coordinator 接收到所有参与者反馈的 ack 信息后，中断事务。</li>
</ol>
<h3 id="3PC-分析"><a href="#3PC-分析" class="headerlink" title="3PC 分析"></a>3PC 分析</h3><p>3PC 虽然解决了 Coordinator 与参与者都异常情况下导致数据不一致的问题，3PC 依然带来其他问题：比如，网络分区问题，在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit。</p>
<p>而且由于3PC 的设计过于复杂，在解决2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。</p>
<p>参考：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external">维基百科：二阶段提交</a>；</li>
<li><a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>；</li>
<li><a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>；</li>
<li><a href="http://www.hollischuang.com/archives/663" target="_blank" rel="external">关于分布式一致性的探究</a>；</li>
<li><a href="http://www.hollischuang.com/archives/681" target="_blank" rel="external">关于分布式事务、两阶段提交协议、三阶提交协议</a>；</li>
<li><a href="http://www.hollischuang.com/archives/1580" target="_blank" rel="external">深入理解分布式系统的2PC和3PC</a>；</li>
<li><a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://coolshell.cn/articles/10910.html" target="_blank" rel="external">分布式系统的事务处理</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="分布式系统" scheme="http://matt33.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Java 守护线程</title>
    <link href="http://matt33.com/2018/07/07/java-daemon-thread/"/>
    <id>http://matt33.com/2018/07/07/java-daemon-thread/</id>
    <published>2018-07-07T13:43:21.000Z</published>
    <updated>2018-07-07T14:40:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 <code>setDaemon()</code> 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了<strong>守护线程(daemon thread)</strong>，那么什么是守护线程呢？或者说守护线程与非守护线程（普通线程）的区别在什么地方呢？这个就是本文主要讲述的内容。</p>
<h2 id="守护线程"><a href="#守护线程" class="headerlink" title="守护线程"></a>守护线程</h2><p>一般来说，Java 中的线程可以分为两种：守护线程和普通线程。在 JVM 刚启动时，它创建的所有线程，除了主线程（main thread）外，其他的线程都是守护线程（比如：垃圾收集器、以及其他执行辅助操作的线程）。</p>
<p>当创建一个新线程时，新线程将会继承它线程的守护状态，默认情况下，主线程创建的所有线程都是普通线程。</p>
<p>什么情况下会需要守护线程呢？一般情况下是，当我们希望创建一个线程来执行一些辅助的工作，但是又不希望这个线程阻碍 JVM 的关闭，在这种情况下，我们就需要使用守护线程了。</p>
<h2 id="守护线程的作用"><a href="#守护线程的作用" class="headerlink" title="守护线程的作用"></a>守护线程的作用</h2><p>守护线程与普通线程唯一的区别是：当线程退出时，JVM 会检查其他正在运行的线程，如果这些线程都是守护线程，那么 JVM 会正常退出操作，但是如果有普通线程还在运行，JVM 是不会执行退出操作的。当 JVM 退出时，所有仍然存在的守护线程都将被抛弃，既不会执行 finally 部分的代码，也不会执行 stack unwound 操作，JVM 会直接退出。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">When the JVM halts any remaining daemon threads are abandoned:</div><div class="line"></div><div class="line"> 1. finally blocks are not executed,</div><div class="line"> 2. stacks are not unwound - the JVM just exits.</div></pre></td></tr></table></figure>
<p>下面有个小示例，来自 <a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DaemonTest</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="keyword">new</span> WorkerThread().start();</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            Thread.sleep(<span class="number">7500</span>);</div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">            <span class="comment">// handle here exception</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(<span class="string">"Main Thread ending"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WorkerThread</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="comment">// When false, (i.e. when it's a user thread), the Worker thread continues to run.</span></div><div class="line">        <span class="comment">// When true, (i.e. when it's a daemon thread), the Worker thread terminates when the main thread terminates.</span></div><div class="line">        setDaemon(<span class="keyword">false</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            System.out.println(<span class="string">"Hello from Worker "</span> + count++);</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">                sleep(<span class="number">5000</span>);</div><div class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                <span class="comment">// handle exception here</span></div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当为普通线程时，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Hello from Worker 0</div><div class="line">Hello from Worker 1</div><div class="line">Main Thread ending</div><div class="line">Hello from Worker 2</div><div class="line">Hello from Worker 3</div><div class="line">Hello from Worker 4</div><div class="line">Hello from Worker 5</div><div class="line">....</div></pre></td></tr></table></figure>
<p>也就是说，此时即使主线程执行完了，JVM 也会等待 WorkerThread 执行完毕才会退出，而如果将该线程设置守护线程的话，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Hello from Worker 0</div><div class="line">Hello from Worker 1</div><div class="line">Main Thread ending</div></pre></td></tr></table></figure>
<p>在 main 线程执行完毕后，JVM 进程就退出了，不会 care WorkerThread 线程是否执行完毕。</p>
<p>参考：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>;</li>
<li><a href="http://www.javaconcurrencyinpractice.com/" target="_blank" rel="external">《Java 并发编程实战》</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 &lt;code&gt;setDaemon()&lt;/code&gt; 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了&lt;stron
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="java" scheme="http://matt33.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 1+N+M 网络处理模型（二十三）</title>
    <link href="http://matt33.com/2018/06/27/kafka-server-process-model/"/>
    <id>http://matt33.com/2018/06/27/kafka-server-process-model/</id>
    <published>2018-06-27T15:18:01.000Z</published>
    <updated>2018-06-27T15:43:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实现，如果还有印象，就会知道它们都是通过 KafkaApis 对象处理的，但是前面并没有详细讲述 Server 端是如何监听到相应的请求、请求是如何交给 KafkaApis 对象进行处理，以及处理后是如何返回给请求者（请求者可以是 client 也可以是 server），这些都属于 Server 的网络处理模型，也是本文讲述的主要内容。</p>
<h2 id="Server-网络模型整体流程"><a href="#Server-网络模型整体流程" class="headerlink" title="Server 网络模型整体流程"></a>Server 网络模型整体流程</h2><p>Kafka Server 启动后，会通过 KafkaServer 的 <code>startup()</code> 方法初始化涉及到网络模型的相关对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>()&#123;</div><div class="line">  <span class="comment">//note: socketServer</span></div><div class="line">  socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">  socketServer.startup()</div><div class="line">  <span class="comment">//<span class="doctag">NOTE:</span> 初始化 KafkaApis 实例,每个 Server 只会启动一个线程</span></div><div class="line">  apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">    kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">    clusterId, time)</div><div class="line"></div><div class="line">  requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">    config.numIoThreads)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Kafka Server 在启动时会初始化 SocketServer、KafkaApis 和 KafkaRequestHandlerPool 对象，这也是 Server 网络处理模型的主要组成部分。Kafka Server 的网络处理模型也是基于 Java NIO 机制实现的，实现模式与 Reactor 模式类似，其完整的处理流程图如下所示：</p>
<p><img src="/images/kafka/server-nio.png" alt="Kafka Server 1+N+M 网络处理模型"></p>
<p>上图如果现在不理解，并不要紧，这里先简单介绍一些，讲述一下整体的流程，本节下面会结合 Kafka 的代码详细来讲述图中的过程。上图的网络模型可以简要总结为以下三个重要组成部分：</p>
<ol>
<li>1 个 Acceptor 线程，负责监听 Socket 新的连接请求，注册了 <code>OP_ACCEPT</code> 事件，将新的连接按照 round robin 方式交给对应的 Processor 线程处理；</li>
<li>N 个 Processor 线程，其中每个 Processor 都有自己的 selector，它会向 Acceptor 分配的 SocketChannel 注册相应的 <code>OP_READ</code> 事件，N 的大小由 <code>num.networker.threads</code> 决定；</li>
<li>M 个 KafkaRequestHandler  线程处理请求，并将处理的结果返回给 Processor 线程对应的 response queue 中，由 Processor 将处理的结果返回给相应的请求发送者，M 的大小由 <code>num.io.threads</code> 来决定。</li>
</ol>
<p>上图展示的整体的处理流程如下所示：</p>
<ol>
<li>Acceptor 监听到来自请求者（请求者可以是来自 client，也可以来自 server）的新的连接，Acceptor 将这个请求者按照 round robin 的方式交给对对应的 Processor 进行处理；</li>
<li>Processor 注册这个 SocketChannel 的 <code>OP_READ</code> 的事件，如果有请求发送过来就可以被 Processor 的 Selector 选中；</li>
<li>Processor 将请求者发送的请求放入到一个 Request Queue 中，这是所有 Processor 共有的一个队列；</li>
<li>KafkaRequestHandler 从 Request Queue 中取出请求；</li>
<li>调用 KafkaApis 进行相应的处理；</li>
<li>处理的结果放入到该 Processor 对应的 Response Queue 中（每个 request 都标识它们来自哪个 Processor），Request Queue 的数量与 Processor 的数量保持一致；</li>
<li>Processor 从对应的 Response Queue 中取出 response；</li>
<li>Processor 将处理的结果返回给对应的请求者。</li>
</ol>
<p>上面是 Server 端网络处理的整体流程，下面我们开始详细讲述上面内容在 Kafka 中实现。</p>
<h2 id="SocketServer"><a href="#SocketServer" class="headerlink" title="SocketServer"></a>SocketServer</h2><p>SocketServer 是接收 Socket 连接、处理请求并返回处理结果的地方，Acceptor 及 Processor 的初始化、处理逻辑都是在这里实现的。在SocketServer 内有几个比较重要的变量，这里先来看下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SocketServer</span>(<span class="params">val config: <span class="type">KafkaConfig</span>, val metrics: <span class="type">Metrics</span>, val time: <span class="type">Time</span>, val credentialProvider: <span class="type">CredentialProvider</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> endpoints = config.listeners.map(l =&gt; l.listenerName -&gt; l).toMap <span class="comment">//note: broker 开放的端口数</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> numProcessorThreads = config.numNetworkThreads <span class="comment">//note: num.network.threads 默认为 3个，即 processor</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxQueuedRequests = config.queuedMaxRequests <span class="comment">//note:  queued.max.requests，request 队列中允许的最多请求数，默认是500</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> totalProcessorThreads = numProcessorThreads * endpoints.size <span class="comment">//note: 每个端口会对应 N 个 processor</span></div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIp = config.maxConnectionsPerIp <span class="comment">//note: 默认 2147483647</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIpOverrides = config.maxConnectionsPerIpOverrides</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Socket Server on Broker "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">  <span class="comment">//note: 请求队列</span></div><div class="line">  <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(totalProcessorThreads, maxQueuedRequests)</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> processors = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Processor</span>](totalProcessorThreads)</div><div class="line"></div><div class="line">  <span class="keyword">private</span>[network] <span class="keyword">val</span> acceptors = mutable.<span class="type">Map</span>[<span class="type">EndPoint</span>, <span class="type">Acceptor</span>]()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val numProcessors: <span class="type">Int</span>, val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> responseListeners: <span class="type">List</span>[(<span class="type">Int</span>) =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></div><div class="line">  <span class="comment">//note: 一个 requestQueue 队列,N 个 responseQueues 队列</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Request</span>](queueSize)</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> responseQueues = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]](numProcessors)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中</p>
<ol>
<li><code>numProcessorThreads</code>：决定了 Processor 的个数，默认是3个，也就是 1+N+M 的 N 的数值；</li>
<li><code>maxQueuedRequests</code>：决定了 request queue 中最多允许放入多少个请求（等待处理的请求），默认是 500；</li>
<li>在 <code>RequestChannel</code> 中初始化了一个 requestQueue 和 N 个 responseQueue。</li>
</ol>
<h3 id="SocketServer-初始化"><a href="#SocketServer-初始化" class="headerlink" title="SocketServer 初始化"></a>SocketServer 初始化</h3><p>在 SocketServer 初始化方法 <code>startup()</code> 中，会初始化 1 个 Acceptor 和 N 个 Processor 线程（每个 EndPoint 都会初始化这么多，一般来说一个 Server 只会设置一个端口），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">this</span>.synchronized &#123;</div><div class="line">    <span class="comment">//note: 一台 broker 一般只设置一个端口，当然这里也可以设置两个</span></div><div class="line">    config.listeners.foreach &#123; endpoint =&gt;</div><div class="line">      <span class="keyword">val</span> listenerName = endpoint.listenerName</div><div class="line">      <span class="keyword">val</span> securityProtocol = endpoint.securityProtocol</div><div class="line">      <span class="keyword">val</span> processorEndIndex = processorBeginIndex + numProcessorThreads</div><div class="line"></div><div class="line">      <span class="comment">//note: N 个 processor</span></div><div class="line">      <span class="keyword">for</span> (i &lt;- processorBeginIndex until processorEndIndex)</div><div class="line">        processors(i) = newProcessor(i, connectionQuotas, listenerName, securityProtocol)</div><div class="line"></div><div class="line">      <span class="comment">//note: 1个 Acceptor</span></div><div class="line">      <span class="keyword">val</span> acceptor = <span class="keyword">new</span> <span class="type">Acceptor</span>(endpoint, sendBufferSize, recvBufferSize, brokerId,</div><div class="line">        processors.slice(processorBeginIndex, processorEndIndex), connectionQuotas)</div><div class="line">      acceptors.put(endpoint, acceptor)</div><div class="line">      <span class="type">Utils</span>.newThread(<span class="string">s"kafka-socket-acceptor-<span class="subst">$listenerName</span>-<span class="subst">$securityProtocol</span>-<span class="subst">$&#123;endpoint.port&#125;</span>"</span>, acceptor, <span class="literal">false</span>).start()</div><div class="line">      acceptor.awaitStartup()</div><div class="line"></div><div class="line">      processorBeginIndex = processorEndIndex</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Acceptor-处理"><a href="#Acceptor-处理" class="headerlink" title="Acceptor 处理"></a>Acceptor 处理</h3><p>SocketServer 在初始化后 Acceptor 线程后，Acceptor 启动，会首先注册 <code>OP_ACCEPT</code> 事件，监听是否有新的连接，如果来了新的连接就将该 SocketChannel 交给对应的 Processor 进行处理，Processor 是通过 round robin 方法选择的，这样可以保证 Processor 的负载相差无几（至少可以保证监听的 SocketChannel 差不多），实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  serverChannel.register(nioSelector, <span class="type">SelectionKey</span>.<span class="type">OP_ACCEPT</span>)<span class="comment">//note: 注册 accept 事件</span></div><div class="line">  startupComplete()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">var</span> currentProcessor = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> (isRunning) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">val</span> ready = nioSelector.select(<span class="number">500</span>)</div><div class="line">        <span class="keyword">if</span> (ready &gt; <span class="number">0</span>) &#123;</div><div class="line">          <span class="keyword">val</span> keys = nioSelector.selectedKeys()</div><div class="line">          <span class="keyword">val</span> iter = keys.iterator()</div><div class="line">          <span class="keyword">while</span> (iter.hasNext &amp;&amp; isRunning) &#123;</div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">              <span class="keyword">val</span> key = iter.next</div><div class="line">              iter.remove()</div><div class="line">              <span class="keyword">if</span> (key.isAcceptable)</div><div class="line">                accept(key, processors(currentProcessor))<span class="comment">//note: 拿到一个socket 连接，轮询选择一个processor进行处理</span></div><div class="line">              <span class="keyword">else</span></div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Unrecognized key state for acceptor thread."</span>)</div><div class="line"></div><div class="line">              <span class="comment">//note: 轮询算法,使用 round robin</span></div><div class="line">              <span class="comment">// round robin to the next processor thread</span></div><div class="line">              currentProcessor = (currentProcessor + <span class="number">1</span>) % processors.length</div><div class="line">            &#125; <span class="keyword">catch</span> &#123;</div><div class="line">              <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while accepting connection"</span>, e)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="comment">// We catch all the throwables to prevent the acceptor thread from exiting on exceptions due</span></div><div class="line">        <span class="comment">// to a select operation on a specific channel or a bad request. We don't want</span></div><div class="line">        <span class="comment">// the broker to stop responding to requests from other clients in these scenarios.</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error occurred"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> &#123;</div><div class="line">    debug(<span class="string">"Closing server socket and selector."</span>)</div><div class="line">    swallowError(serverChannel.close())</div><div class="line">    swallowError(nioSelector.close())</div><div class="line">    shutdownComplete()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Acceptor 通过 <code>accept()</code> 将该新连接交给对应的 Processor，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理一个新的连接</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</div><div class="line">  <span class="comment">//note: accept 事件发生时，获取注册到 selector 上的 ServerSocketChannel</span></div><div class="line">  <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]</div><div class="line">  <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    connectionQuotas.inc(socketChannel.socket().getInetAddress)</div><div class="line">    socketChannel.configureBlocking(<span class="literal">false</span>)</div><div class="line">    socketChannel.socket().setTcpNoDelay(<span class="literal">true</span>)</div><div class="line">    socketChannel.socket().setKeepAlive(<span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (sendBufferSize != <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>)</div><div class="line">      socketChannel.socket().setSendBufferSize(sendBufferSize)</div><div class="line"></div><div class="line">    debug(<span class="string">"Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]"</span></div><div class="line">          .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id,</div><div class="line">                socketChannel.socket.getSendBufferSize, sendBufferSize,</div><div class="line">                socketChannel.socket.getReceiveBufferSize, recvBufferSize))</div><div class="line"></div><div class="line">    <span class="comment">//note: 轮询选择不同的 processor 进行处理</span></div><div class="line">    processor.accept(socketChannel)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">TooManyConnectionsException</span> =&gt;</div><div class="line">      info(<span class="string">"Rejected connection from %s, address already has the configured maximum of %d connections."</span>.format(e.ip, e.count))</div><div class="line">      close(socketChannel)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Processor-处理"><a href="#Processor-处理" class="headerlink" title="Processor 处理"></a>Processor 处理</h3><p>在前面，Acceptor 通过 <code>accept()</code> 将新的连接交给 Processor，Processor 实际上是将该 SocketChannel 添加到该 Processor 的 <code>newConnections</code> 队列中，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</div><div class="line">  newConnections.add(socketChannel)<span class="comment">//note: 添加到队列中</span></div><div class="line">  wakeup()<span class="comment">//note: 唤醒 Processor 的 selector（如果此时在阻塞的话）</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里详细看下 Processor 线程做了什么事情，其 <code>run()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  startupComplete()</div><div class="line">  <span class="keyword">while</span> (isRunning) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// setup any new connections that have been queued up</span></div><div class="line">      configureNewConnections()<span class="comment">//note: 对新的 socket 连接,并注册 READ 事件</span></div><div class="line">      <span class="comment">// register any new responses for writing</span></div><div class="line">      processNewResponses()<span class="comment">//note: 处理 response 队列中 response</span></div><div class="line">      poll() <span class="comment">//note: 监听所有的 socket channel，是否有新的请求发送过来</span></div><div class="line">      processCompletedReceives() <span class="comment">//note: 处理接收到请求，将其放入到 request queue 中</span></div><div class="line">      processCompletedSends() <span class="comment">//note: 处理已经完成的发送</span></div><div class="line">      processDisconnected() <span class="comment">//note: 处理断开的连接</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// We catch all the throwables here to prevent the processor thread from exiting. We do this because</span></div><div class="line">      <span class="comment">// letting a processor exit might cause a bigger impact on the broker. Usually the exceptions thrown would</span></div><div class="line">      <span class="comment">// be either associated with a specific socket channel or a bad request. We just ignore the bad socket channel</span></div><div class="line">      <span class="comment">// or request. This behavior might need to be reviewed if we see an exception that need the entire broker to stop.</span></div><div class="line">      <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Processor got uncaught exception."</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  debug(<span class="string">"Closing selector - processor "</span> + id)</div><div class="line">  swallowError(closeAll())</div><div class="line">  shutdownComplete()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Processor 在一次循环中，主要做的事情如下：</p>
<ol>
<li><code>configureNewConnections()</code>：对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，这里主要是 Processor 的 selector 注册该连接的 <code>OP_READ</code> 事件；</li>
<li><code>processNewResponses()</code>：从该 Processor 对应的 response queue 中取出一个 response，进行发送；</li>
<li><code>poll()</code>：调用 selector 的 <code>poll()</code> 方法，遍历注册的 SocketChannel，查看是否有事件准备就绪；</li>
<li><code>processCompletedReceives()</code>：将接收到请求添加到的 request queue 中；</li>
<li><code>processCompletedSends()</code>：处理已经完成的响应发送；</li>
<li><code>processDisconnected()</code>：处理断开的 SocketChannel。</li>
</ol>
<p>上面就是 Processor 线程处理的主要逻辑，先是向新的 SocketChannel 注册相应的事件，监控是否有请求发送过来，接着从 response queue 中取出处理完成的请求发送给对应的请求者，然后调用一下 selector 的 <code>poll()</code>，遍历一下注册的所有 SocketChannel，判断是否有事件就绪，然后做相应的处理。这里需要注意的是，request queue 是所有 Processor 公用的一个队列，而 response queue 则是与 Processor 一一对应的，因为每个 Processor 监听的 SocketChannel 并不是同一批的，如果公有一个 response queue，那么这个 N 个 Processor 的 selector 要去监听所有的 SocketChannel，而不是现在这种，只需要去关注分配给自己的 SocketChannel。</p>
<p>下面分别看下上面的这些方法的具体实现。</p>
<h4 id="configureNewConnections"><a href="#configureNewConnections" class="headerlink" title="configureNewConnections"></a>configureNewConnections</h4><p><code>configureNewConnections()</code> 对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，主要是 selector 注册相应的 <code>OP_READ</code> 事件，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果有新的连接过来，将该 Channel 的 OP_READ 事件注册到 selector 上</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">configureNewConnections</span></span>() &#123;</div><div class="line">  <span class="keyword">while</span> (!newConnections.isEmpty) &#123;</div><div class="line">    <span class="keyword">val</span> channel = newConnections.poll()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      debug(<span class="string">s"Processor <span class="subst">$id</span> listening to new connection from <span class="subst">$&#123;channel.socket.getRemoteSocketAddress&#125;</span>"</span>)</div><div class="line">      <span class="keyword">val</span> localHost = channel.socket().getLocalAddress.getHostAddress</div><div class="line">      <span class="keyword">val</span> localPort = channel.socket().getLocalPort</div><div class="line">      <span class="keyword">val</span> remoteHost = channel.socket().getInetAddress.getHostAddress</div><div class="line">      <span class="keyword">val</span> remotePort = channel.socket().getPort</div><div class="line">      <span class="keyword">val</span> connectionId = <span class="type">ConnectionId</span>(localHost, localPort, remoteHost, remotePort).toString</div><div class="line">      selector.register(connectionId, channel)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// We explicitly catch all non fatal exceptions and close the socket to avoid a socket leak. The other</span></div><div class="line">      <span class="comment">// throwables will be caught in processor and logged as uncaught exceptions.</span></div><div class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</div><div class="line">        <span class="keyword">val</span> remoteAddress = channel.getRemoteAddress</div><div class="line">        <span class="comment">// need to close the channel here to avoid a socket leak.</span></div><div class="line">        close(channel)</div><div class="line">        error(<span class="string">s"Processor <span class="subst">$id</span> closed connection from <span class="subst">$remoteAddress</span>"</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processNewResponses"><a href="#processNewResponses" class="headerlink" title="processNewResponses"></a>processNewResponses</h4><p><code>processNewResponses()</code> 方法是从该 Processor 对应的 response queue 中取出一个 response，Processor 是通过 RequestChannel 的 <code>receiveResponse()</code> 从该 Processor 对应的 response queue 中取出 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 response</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">receiveResponse</span></span>(processor: <span class="type">Int</span>): <span class="type">RequestChannel</span>.<span class="type">Response</span> = &#123;</div><div class="line">  <span class="keyword">val</span> response = responseQueues(processor).poll()</div><div class="line">  <span class="keyword">if</span> (response != <span class="literal">null</span>)</div><div class="line">    response.request.responseDequeueTimeMs = <span class="type">Time</span>.<span class="type">SYSTEM</span>.milliseconds</div><div class="line">  response</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>取到相应的 response 之后，会判断该 response 的类型，进行相应的操作，如果需要返回，那么会调用 <code>sendResponse()</code> 发送该 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理一个新的 response 响应</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processNewResponses</span></span>() &#123;</div><div class="line">  <span class="keyword">var</span> curr = requestChannel.receiveResponse(id)</div><div class="line">  <span class="keyword">while</span> (curr != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      curr.responseAction <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">NoOpAction</span> =&gt; <span class="comment">//note: 如果这个请求不需要返回 response，再次注册该监听事件</span></div><div class="line">          <span class="comment">// There is no response to send to the client, we need to read more pipelined requests</span></div><div class="line">          <span class="comment">// that are sitting in the server's socket buffer</span></div><div class="line">          curr.request.updateRequestMetrics</div><div class="line">          trace(<span class="string">"Socket server received empty response to send, registering for read: "</span> + curr)</div><div class="line">          <span class="keyword">val</span> channelId = curr.request.connectionId</div><div class="line">          <span class="keyword">if</span> (selector.channel(channelId) != <span class="literal">null</span> || selector.closingChannel(channelId) != <span class="literal">null</span>)</div><div class="line">              selector.unmute(channelId)</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">SendAction</span> =&gt; <span class="comment">//note: 需要发送的 response，那么进行发送</span></div><div class="line">          sendResponse(curr)</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">CloseConnectionAction</span> =&gt; <span class="comment">//note: 要关闭的 response</span></div><div class="line">          curr.request.updateRequestMetrics</div><div class="line">          trace(<span class="string">"Closing socket connection actively according to the response code."</span>)</div><div class="line">          close(selector, curr.request.connectionId)</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      curr = requestChannel.receiveResponse(id)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* `protected` for test usage */</span></div><div class="line"><span class="comment">//note: 发送的对应的 response</span></div><div class="line"><span class="keyword">protected</span>[network] <span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</div><div class="line">  trace(<span class="string">s"Socket server received response to send, registering for write and sending data: <span class="subst">$response</span>"</span>)</div><div class="line">  <span class="keyword">val</span> channel = selector.channel(response.responseSend.destination)</div><div class="line">  <span class="comment">// `channel` can be null if the selector closed the connection because it was idle for too long</span></div><div class="line">  <span class="keyword">if</span> (channel == <span class="literal">null</span>) &#123;</div><div class="line">    warn(<span class="string">s"Attempting to send response via channel for which there is no open connection, connection id <span class="subst">$id</span>"</span>)</div><div class="line">    response.request.updateRequestMetrics()</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    selector.send(response.responseSend) <span class="comment">//note: 发送该 response</span></div><div class="line">    inflightResponses += (response.request.connectionId -&gt; response) <span class="comment">//note: 添加到 inflinght 中</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processCompletedReceives"><a href="#processCompletedReceives" class="headerlink" title="processCompletedReceives"></a>processCompletedReceives</h4><p><code>processCompletedReceives()</code> 方法的主要作用是处理接收到请求，并将其放入到 request queue 中，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理接收到的所有请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedReceives</span></span>() &#123;</div><div class="line">  selector.completedReceives.asScala.foreach &#123; receive =&gt;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> openChannel = selector.channel(receive.source)</div><div class="line">      <span class="keyword">val</span> session = &#123;</div><div class="line">        <span class="comment">// Only methods that are safe to call on a disconnected channel should be invoked on 'channel'.</span></div><div class="line">        <span class="keyword">val</span> channel = <span class="keyword">if</span> (openChannel != <span class="literal">null</span>) openChannel <span class="keyword">else</span> selector.closingChannel(receive.source)</div><div class="line">        <span class="type">RequestChannel</span>.<span class="type">Session</span>(<span class="keyword">new</span> <span class="type">KafkaPrincipal</span>(<span class="type">KafkaPrincipal</span>.<span class="type">USER_TYPE</span>, channel.principal.getName), channel.socketAddress)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> req = <span class="type">RequestChannel</span>.<span class="type">Request</span>(processor = id, connectionId = receive.source, session = session,</div><div class="line">        buffer = receive.payload, startTimeMs = time.milliseconds, listenerName = listenerName,</div><div class="line">        securityProtocol = securityProtocol)</div><div class="line">      requestChannel.sendRequest(req) <span class="comment">//note: 添加到请求队列，如果队列满了，将会阻塞</span></div><div class="line">      selector.mute(receive.source) <span class="comment">//note: 移除该连接的 OP_READ 监听</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e @ (_: <span class="type">InvalidRequestException</span> | _: <span class="type">SchemaException</span>) =&gt;</div><div class="line">        <span class="comment">// note that even though we got an exception, we can assume that receive.source is valid. Issues with constructing a valid receive object were handled earlier</span></div><div class="line">        error(<span class="string">s"Closing socket for <span class="subst">$&#123;receive.source&#125;</span> because of error"</span>, e)</div><div class="line">        close(selector, receive.source)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processCompletedSends"><a href="#processCompletedSends" class="headerlink" title="processCompletedSends"></a>processCompletedSends</h4><p><code>processCompletedSends()</code> 方法是处理已经完成的发送，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedSends</span></span>() &#123;</div><div class="line">  selector.completedSends.asScala.foreach &#123; send =&gt;</div><div class="line">    <span class="comment">//note: response 发送完成，从正在发送的集合中移除</span></div><div class="line">    <span class="keyword">val</span> resp = inflightResponses.remove(send.destination).getOrElse &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Send for <span class="subst">$&#123;send.destination&#125;</span> completed, but not in `inflightResponses`"</span>)</div><div class="line">    &#125;</div><div class="line">    resp.request.updateRequestMetrics()</div><div class="line">    selector.unmute(send.destination) <span class="comment">//note: 完成这个请求之后再次监听 OP_READ 事件</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="KafkaRequestHandlerPool"><a href="#KafkaRequestHandlerPool" class="headerlink" title="KafkaRequestHandlerPool"></a>KafkaRequestHandlerPool</h2><p>上面主要是讲述 SocketServer 中 Acceptor 与 Processor 的处理内容，也就是 1+N+M 模型中 1+N 部分，下面开始讲述 M 部分，也就是 KafkaRequestHandler 的内容，其初始化实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></div><div class="line">                              val requestChannel: <span class="type">RequestChannel</span>,</div><div class="line">                              val apis: <span class="type">KafkaApis</span>,</div><div class="line">                              time: <span class="type">Time</span>,</div><div class="line">                              numThreads: <span class="type">Int</span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> &#123;</div><div class="line"></div><div class="line">  <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">"RequestHandlerAvgIdlePercent"</span>, <span class="string">"percent"</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Request Handler on Broker "</span> + brokerId + <span class="string">"], "</span></div><div class="line">  <span class="keyword">val</span> threads = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Thread</span>](numThreads)</div><div class="line">  <span class="keyword">val</span> runnables = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</div><div class="line">  <span class="comment">//note: 建立 M 个（numThreads）KafkaRequestHandler</span></div><div class="line">  <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numThreads) &#123;</div><div class="line">    <span class="comment">//note: requestChannel 是 Processor 存放 request 请求的地方,也是 Handler 处理完请求存放 response 的地方</span></div><div class="line">    runnables(i) = <span class="keyword">new</span> <span class="type">KafkaRequestHandler</span>(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis, time)</div><div class="line">    threads(i) = <span class="type">Utils</span>.daemonThread(<span class="string">"kafka-request-handler-"</span> + i, runnables(i))</div><div class="line">    threads(i).start()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shutdown</span></span>() &#123;</div><div class="line">    info(<span class="string">"shutting down"</span>)</div><div class="line">    <span class="keyword">for</span>(handler &lt;- runnables)</div><div class="line">      handler.shutdown</div><div class="line">    <span class="keyword">for</span>(thread &lt;- threads)</div><div class="line">      thread.join</div><div class="line">    info(<span class="string">"shut down completely"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如上面实现所示：</p>
<ol>
<li>KafkaRequestHandlerPool 会初始化 M 个 KafkaRequestHandler 线程，并启动该线程；</li>
<li>在初始化 KafkaRequestHandler 时，传入一个 requestChannel 变量，这个是 Processor 存放 request 的地方，KafkaRequestHandler 在处理请求时，会从这个 queue 中取出相应的 request。</li>
</ol>
<h3 id="KafkaRequestHandler"><a href="#KafkaRequestHandler" class="headerlink" title="KafkaRequestHandler"></a>KafkaRequestHandler</h3><p>KafkaRequestHandler 线程的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> req : <span class="type">RequestChannel</span>.<span class="type">Request</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">while</span> (req == <span class="literal">null</span>) &#123;</div><div class="line">        <span class="comment">// We use a single meter for aggregate idle percentage for the thread pool.</span></div><div class="line">        <span class="comment">// Since meter is calculated as total_recorded_value / time_window and</span></div><div class="line">        <span class="comment">// time_window is independent of the number of threads, each recorded idle</span></div><div class="line">        <span class="comment">// time should be discounted by # threads.</span></div><div class="line">        <span class="keyword">val</span> startSelectTime = time.nanoseconds</div><div class="line">        req = requestChannel.receiveRequest(<span class="number">300</span>) <span class="comment">//note: 从 request queue 中拿去 request</span></div><div class="line">        <span class="keyword">val</span> idleTime = time.nanoseconds - startSelectTime</div><div class="line">        aggregateIdleMeter.mark(idleTime / totalHandlerThreads)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(req eq <span class="type">RequestChannel</span>.<span class="type">AllDone</span>) &#123;</div><div class="line">        debug(<span class="string">"Kafka request handler %d on broker %d received shut down command"</span>.format(</div><div class="line">          id, brokerId))</div><div class="line">        <span class="keyword">return</span></div><div class="line">      &#125;</div><div class="line">      req.requestDequeueTimeMs = time.milliseconds</div><div class="line">      trace(<span class="string">"Kafka request handler %d on broker %d handling request %s"</span>.format(id, brokerId, req))</div><div class="line">      apis.handle(req) <span class="comment">//note: 处理请求,并将处理的结果通过 sendResponse 放入 response queue 中</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Exception when handling request"</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法的实现逻辑：</p>
<ol>
<li>从 RequestChannel 取出相应的 request；</li>
<li>KafkaApis 处理这个 request，并通过 <code>requestChannel.sendResponse()</code> 将处理的结果放入 requestChannel 的 response queue 中，如下所示：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 response 添加到对应的队列中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</div><div class="line">  responseQueues(response.processor).put(response)</div><div class="line">  <span class="keyword">for</span>(onResponse &lt;- responseListeners)</div><div class="line">    onResponse(response.processor) <span class="comment">//note: 调用对应 processor 的 wakeup 方法</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里为止，一个请求从 Processor 接收，到 KafkaRequestHandler 通过 KafkaApis 处理并放回该 Processor 对应的 response queue 这整个过程就完成了（建议阅读本文的时候结合最前面的流程图一起看）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 LeaderAndIsr 请求的处理（二十二）</title>
    <link href="http://matt33.com/2018/06/25/leaderAndIsr-process/"/>
    <id>http://matt33.com/2018/06/25/leaderAndIsr-process/</id>
    <published>2018-06-25T01:01:12.000Z</published>
    <updated>2018-06-25T04:11:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka Server 是如何处理 LeaderAndIsr 请求的。</p>
<h2 id="LeaderAndIsr-请求"><a href="#LeaderAndIsr-请求" class="headerlink" title="LeaderAndIsr 请求"></a>LeaderAndIsr 请求</h2><p>LeaderAndIsr 请求是在一个 Topic Partition 的 leader、isr、assignment replicas 变动时，Controller 向 Broker 发送的一种请求，有时候是向这个 Topic Partition 的所有副本发送，有时候是其中的某个副本，跟具体的触发情况有关系。在一个 LeaderAndIsr 请求中，会封装多个 Topic Partition 的信息，每个 Topic Partition 会对应一个 PartitionState 对象，这个对象主要成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionState</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> controllerEpoch;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leader;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leaderEpoch;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> List&lt;Integer&gt; isr;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> zkVersion;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> Set&lt;Integer&gt; replicas;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>由此可见，在 LeaderAndIsr 请求中，会包含一个 Partition 的以下信息：</p>
<ol>
<li>当前 Controller 的 epoch（Broker 收到这个请求后，如果发现是过期的 Controller 请求，就会拒绝这个请求）；</li>
<li>leader，Partition 的 leader 信息；</li>
<li>leader epoch，Partition leader epoch 信息（leader、isr、AR 变动时，这个 epoch 都会加1）；</li>
<li>isr 列表；</li>
<li>zkVersion，；</li>
<li>AR，所有的 replica 列表。</li>
</ol>
<h3 id="LeaderAndIsr-请求处理"><a href="#LeaderAndIsr-请求处理" class="headerlink" title="LeaderAndIsr 请求处理"></a>LeaderAndIsr 请求处理</h3><h3 id="处理整体流程"><a href="#处理整体流程" class="headerlink" title="处理整体流程"></a>处理整体流程</h3><p>LeaderAndIsr 请求可谓是包含了一个 Partition 的所有 metadata 信息，Server 在接收到 Controller 发送的这个请求后，其处理的逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//KafkaApis</span></div><div class="line"><span class="comment">//note: LeaderAndIsr 请求的处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleLeaderAndIsrRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></div><div class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></div><div class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> leaderAndIsrRequest = request.body.asInstanceOf[<span class="type">LeaderAndIsrRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</div><div class="line">      <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></div><div class="line">      <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></div><div class="line">      <span class="comment">// leadership changes</span></div><div class="line">      <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></div><div class="line">      updatedLeaders.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">          coordinator.handleGroupImmigration(partition.partitionId)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></div><div class="line">      updatedFollowers.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">          coordinator.handleGroupEmigration(partition.partitionId)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> leaderAndIsrResponse =</div><div class="line">      <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;<span class="comment">//note: 有权限的情况下</span></div><div class="line">        <span class="comment">//note: replicaManager 进行相应的处理</span></div><div class="line">        <span class="keyword">val</span> result = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest, metadataCache, onLeadershipChange)</div><div class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(result.errorCode, result.responseMap.mapValues(<span class="keyword">new</span> <span class="type">JShort</span>(_)).asJava)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> result = leaderAndIsrRequest.partitionStates.asScala.keys.map((_, <span class="keyword">new</span> <span class="type">JShort</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code))).toMap</div><div class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code, result.asJava)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, leaderAndIsrResponse))</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">"Disk error during leadership change."</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述处理逻辑分为以下两步：</p>
<ol>
<li>ReplicaManager 调用 <code>becomeLeaderOrFollower()</code> 方法对这个请求进行相应的处理；</li>
<li>如果请求中包含 <code>__consumer_offset</code> 的 Partition（对应两种情况：之前是 fllower 现在变成了 leader、之前是 leader 现在变成了 follower），那么还需要调用这个方法中定义的 <code>onLeadershipChange()</code> 方法进行相应的处理。</li>
</ol>
<p><code>becomeLeaderOrFollower()</code>  的整体处理流程如下：</p>
<p><img src="/images/kafka/leader-and-isr.png" alt="LeaderAndIsr 请求的处理"></p>
<h3 id="becomeLeaderOrFollower"><a href="#becomeLeaderOrFollower" class="headerlink" title="becomeLeaderOrFollower"></a>becomeLeaderOrFollower</h3><p>这里先看下 ReplicaManager 的 <code>becomeLeaderOrFollower()</code> 方法，它是 LeaderAndIsr 请求处理的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 LeaderAndIsr 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">becomeLeaderOrFollower</span></span>(correlationId: <span class="type">Int</span>,leaderAndISRRequest: <span class="type">LeaderAndIsrRequest</span>,</div><div class="line">                           metadataCache: <span class="type">MetadataCache</span>,</div><div class="line">                           onLeadershipChange: (<span class="type">Iterable</span>[<span class="type">Partition</span>], <span class="type">Iterable</span>[<span class="type">Partition</span>]) =&gt; <span class="type">Unit</span>): <span class="type">BecomeLeaderOrFollowerResult</span> = &#123;</div><div class="line">  leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</div><div class="line">    stateChangeLogger.trace(<span class="string">"Broker %d received LeaderAndIsr request %s correlation id %d from controller %d epoch %d for partition [%s,%d]"</span></div><div class="line">                              .format(localBrokerId, stateInfo, correlationId,</div><div class="line">                                      leaderAndISRRequest.controllerId, leaderAndISRRequest.controllerEpoch, topicPartition.topic, topicPartition.partition))</div><div class="line">  &#125;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> responseMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]</div><div class="line">    <span class="comment">//note: 1. 验证 controller 的 epoch，如果是来自旧的 controller，就拒绝这个请求</span></div><div class="line">    <span class="keyword">if</span> (leaderAndISRRequest.controllerEpoch &lt; controllerEpoch) &#123;</div><div class="line">      stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d since "</span> +</div><div class="line">        <span class="string">"its controller epoch %d is old. Latest known controller epoch is %d"</span>).format(localBrokerId, leaderAndISRRequest.controllerId,</div><div class="line">        correlationId, leaderAndISRRequest.controllerEpoch, controllerEpoch))</div><div class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 controller 的请求</span></div><div class="line">      <span class="keyword">val</span> controllerId = leaderAndISRRequest.controllerId</div><div class="line">      controllerEpoch = leaderAndISRRequest.controllerEpoch</div><div class="line"></div><div class="line">      <span class="comment">// First check partition's leader epoch</span></div><div class="line">      <span class="comment">//note: 2. 检查 leader epoch，得到一个 partitionState map，epoch 满足条件并且有副本在本地的集合</span></div><div class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>]()</div><div class="line">      leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</div><div class="line">        <span class="keyword">val</span> partition = getOrCreatePartition(topicPartition) <span class="comment">//note: 对应的 tp 如果没有 Partition 实例的话,就新建一个</span></div><div class="line">        <span class="keyword">val</span> partitionLeaderEpoch = partition.getLeaderEpoch <span class="comment">//note: 更新 leader epoch</span></div><div class="line">        <span class="comment">// If the leader epoch is valid record the epoch of the controller that made the leadership decision.</span></div><div class="line">        <span class="comment">// This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path</span></div><div class="line">        <span class="keyword">if</span> (partitionLeaderEpoch &lt; stateInfo.leaderEpoch) &#123;</div><div class="line">          <span class="keyword">if</span>(stateInfo.replicas.contains(localBrokerId))</div><div class="line">            partitionState.put(partition, stateInfo)  <span class="comment">//note: 更新 replica 的 stateInfo</span></div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</div><div class="line">              <span class="string">"epoch %d for partition [%s,%d] as itself is not in assigned replica list %s"</span>)</div><div class="line">              .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</div><div class="line">                topicPartition.topic, topicPartition.partition, stateInfo.replicas.asScala.mkString(<span class="string">","</span>)))</div><div class="line">            responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code)</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;  <span class="comment">//note: 忽略这个请求，因为请求的 leader epoch 小于缓存的 epoch</span></div><div class="line">          <span class="comment">// Otherwise record the error code in response</span></div><div class="line">          stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</div><div class="line">            <span class="string">"epoch %d for partition [%s,%d] since its associated leader epoch %d is not higher than the current leader epoch %d"</span>)</div><div class="line">            .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</div><div class="line">              topicPartition.topic, topicPartition.partition, stateInfo.leaderEpoch, partitionLeaderEpoch))</div><div class="line">          responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 3. 过滤出本地副本设置为 leader 的 Partition 列表</span></div><div class="line">      <span class="keyword">val</span> partitionsTobeLeader = partitionState.filter &#123; <span class="keyword">case</span> (_, stateInfo) =&gt;</div><div class="line">        stateInfo.leader == localBrokerId</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 4. 过滤出本地副本设置为 follower 的 Partition 列表</span></div><div class="line">      <span class="keyword">val</span> partitionsToBeFollower = partitionState -- partitionsTobeLeader.keys <span class="comment">//note: 这些 tp 设置为了 follower</span></div><div class="line"></div><div class="line">      <span class="comment">//note: 5. 将为 leader 的副本设置为 leader</span></div><div class="line">      <span class="keyword">val</span> partitionsBecomeLeader = <span class="keyword">if</span> (partitionsTobeLeader.nonEmpty)</div><div class="line">        makeLeaders(controllerId, controllerEpoch, partitionsTobeLeader, correlationId, responseMap)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</div><div class="line"></div><div class="line">      <span class="comment">//note: 6. 将为 follower 的副本设置为 follower</span></div><div class="line">      <span class="keyword">val</span> partitionsBecomeFollower = <span class="keyword">if</span> (partitionsToBeFollower.nonEmpty)</div><div class="line">        makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap, metadataCache)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</div><div class="line"></div><div class="line">      <span class="comment">//note: 7. 如果 hw checkpoint 的线程没有初始化，这里需要进行一次初始化</span></div><div class="line">      <span class="comment">// we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions</span></div><div class="line">      <span class="comment">// have been completely populated before starting the checkpointing there by avoiding weird race conditions</span></div><div class="line">      <span class="keyword">if</span> (!hwThreadInitialized) &#123;</div><div class="line">        startHighWaterMarksCheckPointThread()</div><div class="line">        hwThreadInitialized = <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 8. 检查 replica fetcher 是否需要关闭（有些副本需要关闭因为可能从 follower 变为 leader）</span></div><div class="line">      replicaFetcherManager.shutdownIdleFetcherThreads()</div><div class="line"></div><div class="line">      <span class="comment">//note: 9. 检查是否 __consumer_offset 的 Partition 的 leaderAndIsr 信息，有的话进行相应的操作</span></div><div class="line">      onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)</div><div class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述实现，其处理逻辑总结如下：</p>
<ol>
<li>检查 Controller 的 epoch，如果是来自旧的 Controller，那么就拒绝这个请求；</li>
<li>获取请求的 Partition 列表的 PartitionState 信息，在遍历的过程中，会进行一个检查，如果 leader epoch 小于缓存中的 epoch 值，那么就过滤掉这个 Partition 信息，如果这个 Partition 在本地不存在，那么会初始化这个 Partition 的对象（这时候并不会初始化本地副本）；</li>
<li>获取出本地副本为 leader 的 Partition 列表（partitionsTobeLeader）；</li>
<li>获取出本地副本为 follower 的 Partition 列表（partitionsToBeFollower）；</li>
<li>调用 <code>makeLeaders()</code> 方法将 leader 的副本设置为 leader；</li>
<li>调用 <code>makeFollowers()</code> 方法将 leader 的副本设置为 follower；</li>
<li>检查 HW checkpoint 的线程是否初始化，如果没有，这里需要进行一次初始化；</li>
<li>检查 ReplicaFetcherManager 是否有线程需要关闭（如果这个线程上没有分配要拉取的 Topic Partition，那么在这里这个线程就会被关闭，下次需要时会再次启动）；</li>
<li>检查是否有 <code>__consumer_offset</code> Partition 的 leaderAndIsr 信息，有的话进行相应的操作。</li>
</ol>
<p>这其中，比较复杂的部分是第 5、6、9步，也前面图中标出的 1、2、4步，文章下面接着分析这三部分。</p>
<h3 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h3><p>ReplicaManager 的 <code>makeLeaders()</code> 的作用是将指定的这批 Partition 列表设置为 Leader，并返回是新 leader 对应的 Partition 列表（之前不是 leader，现在选举为了 leader），其实实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 1. 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 2. 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>实现逻辑如下：</p>
<ol>
<li>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 方法移除这些 Partition 的副本同步线程；</li>
<li>遍历这些 Partition，通过 Partition 的 <code>makeLeader()</code> 方法将这个 Partition 设置为 Leader，如果设置成功（如果 leader 没有变化，证明这个 Partition 之前就是 leader，这个方法返回的是 false，这种情况下不会更新到缓存中），那么将 leader 信息更新到缓存中。</li>
</ol>
<p>下面来看下在 Partition 中是如何真正初始化一个 Partition 的 leader？其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将本地副本设置为 leader, 如果 leader 不变,向 ReplicaManager 返回 false</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeLeader</span></span>(controllerId: <span class="type">Int</span>, partitionStateInfo: <span class="type">PartitionState</span>, correlationId: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="keyword">val</span> (leaderHWIncremented, isNewLeader) = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="keyword">val</span> allReplicas = partitionStateInfo.replicas.asScala.map(_.toInt)</div><div class="line">    <span class="comment">// record the epoch of the controller that made the leadership decision. This is useful while updating the isr</span></div><div class="line">    <span class="comment">// to maintain the decision maker controller's epoch in the zookeeper path</span></div><div class="line">    controllerEpoch = partitionStateInfo.controllerEpoch</div><div class="line">    <span class="comment">// add replicas that are new</span></div><div class="line">    <span class="comment">//note: 为了新的 replica 创建副本实例</span></div><div class="line">    allReplicas.foreach(replica =&gt; getOrCreateReplica(replica))</div><div class="line">    <span class="comment">//note: 获取新的 isr 列表</span></div><div class="line">    <span class="keyword">val</span> newInSyncReplicas = partitionStateInfo.isr.asScala.map(r =&gt; getOrCreateReplica(r)).toSet</div><div class="line">    <span class="comment">// remove assigned replicas that have been removed by the controller</span></div><div class="line">    <span class="comment">//note: 将已经在不在 AR 中的副本移除</span></div><div class="line">    (assignedReplicas.map(_.brokerId) -- allReplicas).foreach(removeReplica)</div><div class="line">    inSyncReplicas = newInSyncReplicas</div><div class="line">    leaderEpoch = partitionStateInfo.leaderEpoch</div><div class="line">    zkVersion = partitionStateInfo.zkVersion</div><div class="line">    <span class="comment">//note: 判断是否是新的 leader</span></div><div class="line">    <span class="keyword">val</span> isNewLeader =</div><div class="line">      <span class="keyword">if</span> (leaderReplicaIdOpt.isDefined &amp;&amp; leaderReplicaIdOpt.get == localBrokerId) &#123;<span class="comment">//note: leader 没有更新</span></div><div class="line">        <span class="literal">false</span></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        leaderReplicaIdOpt = <span class="type">Some</span>(localBrokerId)</div><div class="line">        <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">    <span class="keyword">val</span> leaderReplica = getReplica().get <span class="comment">//note: 获取在当前上的副本,也就是 leader replica</span></div><div class="line">    <span class="keyword">val</span> curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset <span class="comment">//note: 获取 leader replica 的 the end offset</span></div><div class="line">    <span class="keyword">val</span> curTimeMs = time.milliseconds</div><div class="line">    <span class="comment">// initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.</span></div><div class="line">    (assignedReplicas - leaderReplica).foreach &#123; replica =&gt; <span class="comment">//note: 对于 isr 中的 replica,更新 LastCaughtUpTime</span></div><div class="line">      <span class="keyword">val</span> lastCaughtUpTimeMs = <span class="keyword">if</span> (inSyncReplicas.contains(replica)) curTimeMs <span class="keyword">else</span> <span class="number">0</span>L</div><div class="line">      replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line">    <span class="keyword">if</span> (isNewLeader) &#123;  <span class="comment">//note: 如果是新的 leader,那么需要</span></div><div class="line">      <span class="comment">// construct the high watermark metadata for the new leader replica</span></div><div class="line">      <span class="comment">//note: 为新的 leader 构造 replica 的 HW metadata</span></div><div class="line">      leaderReplica.convertHWToLocalOffsetMetadata()</div><div class="line">      <span class="comment">// reset log end offset for remote replicas</span></div><div class="line">      <span class="comment">//note: 更新远程副本的副本同步信息（设置为 unKnown）</span></div><div class="line">      assignedReplicas.filter(_.brokerId != localBrokerId).foreach(_.updateLogReadResult(<span class="type">LogReadResult</span>.<span class="type">UnknownLogReadResult</span>))</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 如果满足更新 isr 的条件,就更新 HW 信息</span></div><div class="line">    (maybeIncrementLeaderHW(leaderReplica), isNewLeader)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented) <span class="comment">//note: HW 更新的情况下</span></div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">  isNewLeader</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单总结一下上述的实现：</p>
<ol>
<li>首先更新这个 Partition 的相应信息，包括：isr、AR、leader epoch、zkVersion 等，并为每个副本创建一个 Replica 对象（如果不存在该对象的情况下才会创建，只有本地副本才会初始化相应的日志对象）；</li>
<li>如果这个 Partition 的 leader 本来就是本地副本，那么返回的结果设置为 false，证明这个 leader 并不是新的 leader；</li>
<li>对于 isr 中的所有 Replica，更新 LastCaughtUpTime 值，即最近一次赶得上 leader 的时间；</li>
<li>如果是新的 leader，那么为 leader 初始化相应的 HighWatermarkMetadata 对象，并将所有副本的副本同步信息更新为 UnknownLogReadResult；</li>
<li>检查一下是否需要更新 HW 值。</li>
</ol>
<p>如果这个本地副本是新选举的 leader，那么它所做的事情就是初始化 Leader 应该记录的相关信息。</p>
<h3 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h3><p>ReplicaManager 的 <code>makeFollowers()</code> 方法，是将哪些 Partition 设置为 Follower，返回的结果是那些新的 follower 对应的 Partition 列表（之前是 leader，现在变成了 follower），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 1. 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的 Partition</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: 2. 将 Partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 3. 移除这些 Partition 的副本同步线程,这样在 MakeFollower 期间,这些 Partition 就不会进行副本同步了</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 4. Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 5. 完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 6. 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单总结一下上述的逻辑过程：</p>
<ol>
<li>首先遍历所有的 Partition，获到那些 leader 可用、并且 Partition 可以成功设置为 Follower 的 Partition 列表（partitionsToMakeFollower）；</li>
<li>在上面遍历的过程中，会调用 Partition 的 <code>makeFollower()</code> 方法将 Partition 设置为 Follower（在这里，如果该 Partition 的本地副本不存在，会初始化相应的日志对象，如果该 Partition 的 leader 已经存在，并且没有变化，那么就返回 false，只有 leader 变化的 Partition，才会返回 true，才会加入到 partitionsToMakeFollower 集合中，这是因为 leader 没有变化的 Partition 是不需要变更副本同步线程的）；</li>
<li>移除这些 Partition 的副本同步线程，这样在 MakeFollower 期间，这些 Partition 就不会进行副本同步了；</li>
<li>Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset，因为前面已经移除了这个 Partition 的副本同步线程，所以这里在 checkpoint 后可以保证所有缓存的数据都可以刷新到磁盘；</li>
<li>完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）；</li>
<li>启动相应的副本同步线程。</li>
</ol>
<p>到这里 LeaderAndIsr 请求的大部分处理已经完成，但是有一个比较特殊的 topic（<code>__consumer_offset</code>），如果这 Partition 的 leader 发生变化，是需要一些额外的处理。</p>
<h2 id="consumer-offset-leader-切换处理"><a href="#consumer-offset-leader-切换处理" class="headerlink" title="__consumer_offset leader 切换处理"></a><code>__consumer_offset</code> leader 切换处理</h2><p><code>__consumer_offset</code> 这个 Topic 如果发生了 leader 切换，GroupCoordinator 需要进行相应的处理，其处理过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</div><div class="line">  <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></div><div class="line">  <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></div><div class="line">  <span class="comment">// leadership changes</span></div><div class="line">  <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></div><div class="line">  updatedLeaders.foreach &#123; partition =&gt;</div><div class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">      coordinator.handleGroupImmigration(partition.partitionId)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></div><div class="line">  updatedFollowers.foreach &#123; partition =&gt;</div><div class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">      coordinator.handleGroupEmigration(partition.partitionId)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="成为-leader"><a href="#成为-leader" class="headerlink" title="成为 leader"></a>成为 leader</h3><p>如果当前节点这个 <code>__consumer_offset</code> 有 Partition 成为 leader，GroupCoordinator 通过 <code>handleGroupImmigration()</code> 方法进行相应的处理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 加载这个 Partition 对应的 group offset 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupImmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</div><div class="line">  groupManager.loadGroupsForPartition(offsetTopicPartitionId, onGroupLoaded)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 异步地加载这个 offset Partition 的信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doLoadGroupsAndOffsets</span></span>() &#123;</div><div class="line">    info(<span class="string">s"Loading offsets and group metadata from <span class="subst">$topicPartition</span>"</span>)</div><div class="line"></div><div class="line">    <span class="comment">//note: 添加到  loadingPartitions 集合中</span></div><div class="line">    inLock(partitionLock) &#123;</div><div class="line">      <span class="keyword">if</span> (loadingPartitions.contains(offsetsPartition)) &#123;</div><div class="line">        info(<span class="string">s"Offset load from <span class="subst">$topicPartition</span> already in progress."</span>)</div><div class="line">        <span class="keyword">return</span></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        loadingPartitions.add(offsetsPartition)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 开始加载，加载成功的话，将该 Partition 从 loadingPartitions 集合中移除，添加到 ownedPartition 集合中</span></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      loadGroupsAndOffsets(topicPartition, onGroupLoaded)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt; error(<span class="string">s"Error loading offsets from <span class="subst">$topicPartition</span>"</span>, t)</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      inLock(partitionLock) &#123;</div><div class="line">        ownedPartitions.add(offsetsPartition)</div><div class="line">        loadingPartitions.remove(offsetsPartition)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  scheduler.schedule(topicPartition.toString, doLoadGroupsAndOffsets)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的做的事情是：</p>
<ol>
<li>将正在处理的 Partition 添加到 loadingPartitions 集合中，这个集合内都是当前正在加载的 Partition（特指 <code>__consumer_offset</code> Topic）；</li>
<li>通过 <code>loadGroupsAndOffsets()</code> 加载这个 Partition 的数据，处理完成后，该 Partition 从 loadingPartitions 中清除，并添加到 ownedPartitions 集合中。</li>
</ol>
<p><code>loadGroupsAndOffsets()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 读取该 group offset Partition 数据</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="function"><span class="keyword">def</span> <span class="title">loadGroupsAndOffsets</span></span>(topicPartition: <span class="type">TopicPartition</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="comment">//note: 这个必然有本地副本，现获取 hw（如果本地是 leader 的情况，否则返回-1）</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">highWaterMark</span> </span>= replicaManager.getHighWatermark(topicPartition).getOrElse(<span class="number">-1</span>L)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds()</div><div class="line">  replicaManager.getLog(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      warn(<span class="string">s"Attempted to load offsets and group metadata from <span class="subst">$topicPartition</span>, but found no log"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</div><div class="line">      <span class="keyword">var</span> currOffset = log.logStartOffset <span class="comment">//note: 这副本最起始的 offset</span></div><div class="line">      <span class="keyword">val</span> buffer = <span class="type">ByteBuffer</span>.allocate(config.loadBufferSize) <span class="comment">//note: 默认5MB</span></div><div class="line">      <span class="comment">// loop breaks if leader changes at any time during the load, since getHighWatermark is -1</span></div><div class="line">      <span class="comment">//note: group 与 offset 的对应关系</span></div><div class="line">      <span class="keyword">val</span> loadedOffsets = mutable.<span class="type">Map</span>[<span class="type">GroupTopicPartition</span>, <span class="type">OffsetAndMetadata</span>]()</div><div class="line">      <span class="keyword">val</span> removedOffsets = mutable.<span class="type">Set</span>[<span class="type">GroupTopicPartition</span>]()</div><div class="line">      <span class="comment">//note: Group 对应的 meta 信息</span></div><div class="line">      <span class="keyword">val</span> loadedGroups = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">GroupMetadata</span>]()</div><div class="line">      <span class="keyword">val</span> removedGroups = mutable.<span class="type">Set</span>[<span class="type">String</span>]()</div><div class="line"></div><div class="line">      <span class="keyword">while</span> (currOffset &lt; highWaterMark &amp;&amp; !shuttingDown.get()) &#123; <span class="comment">//note: 直到读取到 hw 位置，或服务关闭</span></div><div class="line">        buffer.clear()</div><div class="line">        <span class="keyword">val</span> fileRecords = log.read(currOffset, config.loadBufferSize, maxOffset = <span class="type">None</span>, minOneMessage = <span class="literal">true</span>)</div><div class="line">          .records.asInstanceOf[<span class="type">FileRecords</span>]</div><div class="line">        <span class="keyword">val</span> bufferRead = fileRecords.readInto(buffer, <span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="type">MemoryRecords</span>.readableRecords(bufferRead).deepEntries.asScala.foreach &#123; entry =&gt;</div><div class="line">          <span class="keyword">val</span> record = entry.record</div><div class="line">          require(record.hasKey, <span class="string">"Group metadata/offset entry key should not be null"</span>)</div><div class="line"></div><div class="line">          <span class="type">GroupMetadataManager</span>.readMessageKey(record.key) <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> offsetKey: <span class="type">OffsetKey</span> =&gt; <span class="comment">//note: GroupTopicPartition，有 group 和 topic-partition</span></div><div class="line">              <span class="comment">// load offset</span></div><div class="line">              <span class="comment">//note: 加载 offset 信息</span></div><div class="line">              <span class="keyword">val</span> key = offsetKey.key</div><div class="line">              <span class="keyword">if</span> (record.hasNullValue) &#123; <span class="comment">//note: value 为空</span></div><div class="line">                loadedOffsets.remove(key)</div><div class="line">                removedOffsets.add(key)</div><div class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 有 commit offset 信息</span></div><div class="line">                <span class="keyword">val</span> value = <span class="type">GroupMetadataManager</span>.readOffsetMessageValue(record.value)</div><div class="line">                loadedOffsets.put(key, value)</div><div class="line">                removedOffsets.remove(key)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> groupMetadataKey: <span class="type">GroupMetadataKey</span> =&gt;</div><div class="line">              <span class="comment">// load group metadata</span></div><div class="line">              <span class="comment">//note: 加载 group metadata 信息</span></div><div class="line">              <span class="keyword">val</span> groupId = groupMetadataKey.key</div><div class="line">              <span class="keyword">val</span> groupMetadata = <span class="type">GroupMetadataManager</span>.readGroupMessageValue(groupId, record.value)</div><div class="line">              <span class="keyword">if</span> (groupMetadata != <span class="literal">null</span>) &#123;</div><div class="line">                trace(<span class="string">s"Loaded group metadata for group <span class="subst">$groupId</span> with generation <span class="subst">$&#123;groupMetadata.generationId&#125;</span>"</span>)</div><div class="line">                removedGroups.remove(groupId)</div><div class="line">                loadedGroups.put(groupId, groupMetadata)</div><div class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 更新最新的信息</span></div><div class="line">                loadedGroups.remove(groupId)</div><div class="line">                removedGroups.add(groupId)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> unknownKey =&gt;</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected message key <span class="subst">$unknownKey</span> while loading offsets and group metadata"</span>)</div><div class="line">          &#125;</div><div class="line"></div><div class="line">          currOffset = entry.nextOffset</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">val</span> (groupOffsets, emptyGroupOffsets) = loadedOffsets</div><div class="line">        .groupBy(_._1.group)</div><div class="line">        .mapValues(_.map &#123; <span class="keyword">case</span> (groupTopicPartition, offset) =&gt; (groupTopicPartition.topicPartition, offset)&#125; )</div><div class="line">        .partition &#123; <span class="keyword">case</span> (group, _) =&gt; loadedGroups.contains(group) &#125; <span class="comment">//note: 把集合根据条件分两个部分</span></div><div class="line"></div><div class="line">      loadedGroups.values.foreach &#123; group =&gt;</div><div class="line">        <span class="keyword">val</span> offsets = groupOffsets.getOrElse(group.groupId, <span class="type">Map</span>.empty[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>])</div><div class="line">        loadGroup(group, offsets) <span class="comment">//note: 在缓存中添加 group 和初始化 offset 信息</span></div><div class="line">        onGroupLoaded(group) <span class="comment">//note: 设置 group 下一次心跳超时时间</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// load groups which store offsets in kafka, but which have no active members and thus no group</span></div><div class="line">      <span class="comment">// metadata stored in the log</span></div><div class="line">      <span class="comment">//note: 加载哪些有 offset 信息但是当前没有活跃的 member 信息的 group</span></div><div class="line">      emptyGroupOffsets.foreach &#123; <span class="keyword">case</span> (groupId, offsets) =&gt;</div><div class="line">        <span class="keyword">val</span> group = <span class="keyword">new</span> <span class="type">GroupMetadata</span>(groupId)</div><div class="line">        loadGroup(group, offsets)</div><div class="line">        onGroupLoaded(group)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      removedGroups.foreach &#123; groupId =&gt;</div><div class="line">        <span class="comment">// if the cache already contains a group which should be removed, raise an error. Note that it</span></div><div class="line">        <span class="comment">// is possible (however unlikely) for a consumer group to be removed, and then to be used only for</span></div><div class="line">        <span class="comment">// offset storage (i.e. by "simple" consumers)</span></div><div class="line">        <span class="keyword">if</span> (groupMetadataCache.contains(groupId) &amp;&amp; !emptyGroupOffsets.contains(groupId))</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected unload of active group <span class="subst">$groupId</span> while "</span> +</div><div class="line">            <span class="string">s"loading partition <span class="subst">$topicPartition</span>"</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (!shuttingDown.get())</div><div class="line">        info(<span class="string">"Finished loading offsets from %s in %d milliseconds."</span></div><div class="line">          .format(topicPartition, time.milliseconds() - startMs))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面方法的实现虽然比较长，但是处理逻辑还是比较简单的，实现结果如下：</p>
<ol>
<li>获取这个 Partition 的 HW 值（如果 leader 不在本地，那么返回-1）；</li>
<li>初始化 loadedOffsets 和 removedOffsets、loadedGroups 和 removedGroups 集合，它们就是 group offset 信息以及 consumer member 信息；</li>
<li>从这个 Partition 第一条数据开始读取，直到读取到 HW 位置，加载相应的 commit offset、consumer member 信息，因为是顺序读取的，所以会新的值会覆盖前面的值；</li>
<li>通过 <code>loadGroup()</code> 加载到 GroupCoordinator 的缓存中。</li>
</ol>
<p>经过上面这些步骤，这个 Partition 的数据就被完整加载缓存中了。</p>
<h3 id="变成-follower"><a href="#变成-follower" class="headerlink" title="变成 follower"></a>变成 follower</h3><p>如果 <code>__consumer_offset</code> 有 Partition 变成了 follower（之前是 leader，如果之前不是 leader，不会走到这一步的），GroupCoordinator 通过 <code>handleGroupEmigration()</code> 移除这个 Partition 相应的缓存信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 移除这个 Partition 对应的 group offset 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupEmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</div><div class="line">  groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>removeGroupsForPartition()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当一个 broker 变成一个 follower 时，清空这个 partition 的相关缓存信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>,</div><div class="line">                             onGroupUnloaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</div><div class="line">  scheduler.schedule(topicPartition.toString, removeGroupsAndOffsets)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">removeGroupsAndOffsets</span></span>() &#123;</div><div class="line">    <span class="keyword">var</span> numOffsetsRemoved = <span class="number">0</span></div><div class="line">    <span class="keyword">var</span> numGroupsRemoved = <span class="number">0</span></div><div class="line"></div><div class="line">    inLock(partitionLock) &#123;</div><div class="line">      <span class="comment">// we need to guard the group removal in cache in the loading partition lock</span></div><div class="line">      <span class="comment">// to prevent coordinator's check-and-get-group race condition</span></div><div class="line">      ownedPartitions.remove(offsetsPartition)</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (group &lt;- groupMetadataCache.values) &#123;</div><div class="line">        <span class="keyword">if</span> (partitionFor(group.groupId) == offsetsPartition) &#123;</div><div class="line">          onGroupUnloaded(group) <span class="comment">//note: 将 group 状态转移成 dead</span></div><div class="line">          groupMetadataCache.remove(group.groupId, group) <span class="comment">//note: 清空 group 的信息</span></div><div class="line">          numGroupsRemoved += <span class="number">1</span></div><div class="line">          numOffsetsRemoved += group.numOffsets</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (numOffsetsRemoved &gt; <span class="number">0</span>)</div><div class="line">      info(<span class="string">s"Removed <span class="subst">$numOffsetsRemoved</span> cached offsets for <span class="subst">$topicPartition</span> on follower transition."</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (numGroupsRemoved &gt; <span class="number">0</span>)</div><div class="line">      info(<span class="string">s"Removed <span class="subst">$numGroupsRemoved</span> cached groups for <span class="subst">$topicPartition</span> on follower transition."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onGroupUnloaded</span></span>(group: <span class="type">GroupMetadata</span>) &#123;</div><div class="line">  group synchronized &#123;</div><div class="line">    info(<span class="string">s"Unloading group metadata for <span class="subst">$&#123;group.groupId&#125;</span> with generation <span class="subst">$&#123;group.generationId&#125;</span>"</span>)</div><div class="line">    <span class="keyword">val</span> previousState = group.currentState</div><div class="line">    group.transitionTo(<span class="type">Dead</span>) <span class="comment">//note: 状态转移成 dead</span></div><div class="line"></div><div class="line">    previousState <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Empty</span> | <span class="type">Dead</span> =&gt;</div><div class="line">      <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt;</div><div class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息返回异常</span></div><div class="line">          <span class="keyword">if</span> (member.awaitingJoinCallback != <span class="literal">null</span>) &#123;</div><div class="line">            member.awaitingJoinCallback(joinError(member.memberId, <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code))</div><div class="line">            member.awaitingJoinCallback = <span class="literal">null</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt;</div><div class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息，返回异常</span></div><div class="line">          <span class="keyword">if</span> (member.awaitingSyncCallback != <span class="literal">null</span>) &#123;</div><div class="line">            member.awaitingSyncCallback(<span class="type">Array</span>.empty[<span class="type">Byte</span>], <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">            member.awaitingSyncCallback = <span class="literal">null</span></div><div class="line">          &#125;</div><div class="line">          heartbeatPurgatory.checkAndComplete(<span class="type">MemberKey</span>(member.groupId, member.memberId))</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于在这个 Partition 上的所有 Group，会按下面的步骤执行：</p>
<ol>
<li>通过 <code>onGroupUnloaded()</code> 方法先将这个 Group 的状态转换为 dead，如果 Group 处在 PreparingRebalance/Stable/AwaitingSync 状态，并且设置了相应的回调函数，那么就在回调函数中返回带有 NOT_COORDINATOR_FOR_GROUP 异常信息的响应，consumer 在收到这个异常信息会重新加入 group；</li>
<li>从缓存中移除这个 Group 的信息。</li>
</ol>
<p>这个遍历执行完成之后，这个 Topic Partition 就从 Leader 变成了 follower 状态。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 发送模型（二十一）</title>
    <link href="http://matt33.com/2018/06/23/controller-request-model/"/>
    <id>http://matt33.com/2018/06/23/controller-request-model/</id>
    <published>2018-06-23T05:26:38.000Z</published>
    <updated>2018-06-23T05:42:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 部分的一个补充，在这篇文章中，将会看到 Controller 在处理 leader 切换、ShutDown 请求时如何向 Broker 发送相应的请求。</p>
<p>Kafka Controller 向 Broker 发送的请求类型主要分为三种：LeaderAndIsr、UpdateMetadata、StopReplica 请求，正如  <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 这里介绍的，Controller 会为每台 Broker 初始化为一个 ControllerBrokerStateInfo 对象，该对象主要包含以下四个内容：</p>
<ol>
<li>NetworkClient：与 Broker 的网络连接对象；</li>
<li>Node：Broker 的节点信息；</li>
<li>MessageQueue：每个 Broker 对应的请求队列，Controller 向 Broker 发送的请求会想放在这个队列里；</li>
<li>RequestSendThread：每台 Broker 对应的请求发送线程。</li>
</ol>
<h2 id="Controller-的请求发送模型"><a href="#Controller-的请求发送模型" class="headerlink" title="Controller 的请求发送模型"></a>Controller 的请求发送模型</h2><p>在讲述 Controller 发送模型之前，先看下 Controller 是如何向 Broker 发送请求的，这里以发送 metadata 更新请求为例，简略的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建新的批量请求</span></div><div class="line">brokerRequestBatch.newBatch()</div><div class="line"><span class="comment">//note: 为目标 Broker 添加相应的请求</span></div><div class="line">brokerRequestBatch.addUpdateMetadataRequestForBrokers(brokers, partitions)</div><div class="line"><span class="comment">//note: 发送请求，实际上只是把请求添加发送线程的 request queue 中</span></div><div class="line">brokerRequestBatch.sendRequestsToBrokers(epoch)</div></pre></td></tr></table></figure>
<p>这里有一个比较重要的对象，就是 ControllerBrokerRequestBatch 对象，可以认为它是一个专门用于批量请求发送的对象，在这个对象中有几个重要成员变量：</p>
<ol>
<li>leaderAndIsrRequestMap：记录每个 broker 与要发送的 LeaderAndIsr 请求集合的 map；</li>
<li>stopReplicaRequestMap：记录每个 broker 与要发送的 StopReplica 集合的 map；</li>
<li>updateMetadataRequestBrokerSet：记录要发送的 update-metadata 请求的 broker 集合；</li>
<li>updateMetadataRequestPartitionInfoMap：记录 update-metadata 请求要更新的 Topic Partition 集合。</li>
</ol>
<p>Controller 可以通过下面这三方法向这些集合添加相应的请求：</p>
<ol>
<li><code>addLeaderAndIsrRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 LeaderAndIsr 请求；</li>
<li><code>addStopReplicaRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 StopReplica 请求；</li>
<li><code>addUpdateMetadataRequestForBrokers()</code>：向给定的 Broker 发送某一批 Partitions 的 UpdateMetadata 请求。</li>
</ol>
<p>Controller 整体的请求模型概况如下图所示：</p>
<p><img src="/images/kafka/controller-request-model.png" alt="Controller 的请求发送模型"></p>
<p>上述三个方法将相应的请求添加到对应的集合中后，然后通过 <code>sendRequestsToBrokers()</code> 方法将该请求添加到该 Broker 对应的请求队列中，接着再由该 Broker 对应的 RequestSendThread 去发送相应的请求。</p>
<h2 id="ControllerBrokerRequestBatch"><a href="#ControllerBrokerRequestBatch" class="headerlink" title="ControllerBrokerRequestBatch"></a>ControllerBrokerRequestBatch</h2><p>这节详细讲述一下关于 ControllerBrokerRequestBatch 的一些方法实现。</p>
<h3 id="newBatch-方法"><a href="#newBatch-方法" class="headerlink" title="newBatch 方法"></a>newBatch 方法</h3><p>Controller 在添加请求前，都会先调用 <code>newBatch()</code> 方法，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建新的请求前,确保前一批请求全部发送完毕</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">newBatch</span></span>() &#123;</div><div class="line">  <span class="comment">// raise error if the previous batch is not empty</span></div><div class="line">  <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating "</span> +</div><div class="line">      <span class="string">"a new one. Some LeaderAndIsr state changes %s might be lost "</span>.format(leaderAndIsrRequestMap.toString()))</div><div class="line">  <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</div><div class="line">      <span class="string">"new one. Some StopReplica state changes %s might be lost "</span>.format(stopReplicaRequestMap.toString()))</div><div class="line">  <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</div><div class="line">      <span class="string">"new one. Some UpdateMetadata state changes to brokers %s with partition info %s might be lost "</span>.format(</div><div class="line">        updateMetadataRequestBrokerSet.toString(), updateMetadataRequestPartitionInfoMap.toString()))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的主要作用是检查上一波的 LeaderAndIsr、UpdateMetadata、StopReplica 请求是否已经发送，正常情况下，Controller 在调用 <code>sendRequestsToBrokers()</code> 方法之后，这些集合中的请求都会被发送，发送之后，会将相应的请求集合清空，当然在异常情况可能会导致部分集合没有被清空，导致无法 <code>newBatch()</code>，这种情况下，通常策略是重启 controller，因为现在 Controller 的设计还是有些复杂，在某些情况下还是可能会导致异常发生，并且有些异常还是无法恢复的。</p>
<h3 id="添加-LeaderAndIsr-请求"><a href="#添加-LeaderAndIsr-请求" class="headerlink" title="添加 LeaderAndIsr 请求"></a>添加 LeaderAndIsr 请求</h3><p>Controller 可以通过 <code>addLeaderAndIsrRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 LeaderAndIsr 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 LeaderAndIsr 添加到对应的 broker 中,还未开始发送数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addLeaderAndIsrRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</div><div class="line">                                     leaderIsrAndControllerEpoch: <span class="type">LeaderIsrAndControllerEpoch</span>,</div><div class="line">                                     replicas: <span class="type">Seq</span>[<span class="type">Int</span>], callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition)</div><div class="line"></div><div class="line">  <span class="comment">//note: 将请求添加到对应的 broker 上</span></div><div class="line">  brokerIds.filter(_ &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</div><div class="line">    <span class="keyword">val</span> result = leaderAndIsrRequestMap.getOrElseUpdate(brokerId, mutable.<span class="type">Map</span>.empty)</div><div class="line">    result.put(topicPartition, <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas.toSet))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 在更新 LeaderAndIsr 信息时,主题的 metadata 相当于也进行了更新,需要发送这个 topic 的 metadata 给所有存活的 broker</span></div><div class="line">  addUpdateMetadataRequestForBrokers(controllerContext.liveOrShuttingDownBrokerIds.toSeq,</div><div class="line">                                     <span class="type">Set</span>(<span class="type">TopicAndPartition</span>(topic, partition)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的处理流程如下：</p>
<ol>
<li>向对应的 Broker 添加 LeaderAndIsr 请求，请求会被添加到 leaderAndIsrRequestMap 集合中；</li>
<li>并通过 <code>addUpdateMetadataRequestForBrokers()</code> 方法向所有的 Broker 添加这个 Topic-Partition 的 UpdateMatedata 请求，leader 或 isr 变动时，会向所有 broker 同步这个 Partition 的 metadata 信息，这样可以保证每台 Broker 上都有最新的 metadata 信息。</li>
</ol>
<h3 id="添加-UpdateMetadata-请求"><a href="#添加-UpdateMetadata-请求" class="headerlink" title="添加 UpdateMetadata 请求"></a>添加 UpdateMetadata 请求</h3><p>Controller 可以通过 <code>addUpdateMetadataRequestForBrokers()</code> 向指定 Broker 列表添加某批 Partitions 的 UpdateMetadata 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向给行的 Broker 发送 UpdateMetadataRequest 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addUpdateMetadataRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>],</div><div class="line">                                       partitions: collection.<span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = <span class="type">Set</span>.empty[<span class="type">TopicAndPartition</span>],</div><div class="line">                                       callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  <span class="comment">//note: 将 Topic-Partition 添加到对应的 map 中</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateMetadataRequestPartitionInfo</span></span>(partition: <span class="type">TopicAndPartition</span>, beingDeleted: <span class="type">Boolean</span>) &#123;</div><div class="line">    <span class="keyword">val</span> leaderIsrAndControllerEpochOpt = controllerContext.partitionLeadershipInfo.get(partition)</div><div class="line">    leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">        <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(partition).toSet</div><div class="line">        <span class="keyword">val</span> partitionStateInfo = <span class="keyword">if</span> (beingDeleted) &#123; <span class="comment">//note: 正在删除的 Partition,设置 leader 为-2</span></div><div class="line">          <span class="keyword">val</span> leaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(<span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>, leaderIsrAndControllerEpoch.leaderAndIsr.isr)</div><div class="line">          <span class="type">PartitionStateInfo</span>(<span class="type">LeaderIsrAndControllerEpoch</span>(leaderAndIsr, leaderIsrAndControllerEpoch.controllerEpoch), replicas)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: 添加到对应的 request map 中</span></div><div class="line">        updateMetadataRequestPartitionInfoMap.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(partition.topic, partition.partition), partitionStateInfo)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        info(<span class="string">"Leader not yet assigned for partition %s. Skip sending UpdateMetadataRequest."</span>.format(partition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note:过滤出要发送的 partition</span></div><div class="line">  <span class="keyword">val</span> filteredPartitions = &#123;</div><div class="line">    <span class="keyword">val</span> givenPartitions = <span class="keyword">if</span> (partitions.isEmpty)</div><div class="line">      controllerContext.partitionLeadershipInfo.keySet <span class="comment">//note: Partitions 为空时，就过滤出所有的 topic</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      partitions</div><div class="line">    <span class="keyword">if</span> (controller.deleteTopicManager.partitionsToBeDeleted.isEmpty)</div><div class="line">      givenPartitions</div><div class="line">    <span class="keyword">else</span></div><div class="line">      givenPartitions -- controller.deleteTopicManager.partitionsToBeDeleted <span class="comment">//note: 将要删除的 topic 过滤掉</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  updateMetadataRequestBrokerSet ++= brokerIds.filter(_ &gt;= <span class="number">0</span>) <span class="comment">//note: 将 broker 列表更新到要发送的集合中</span></div><div class="line">  <span class="comment">//note: 对于要更新 metadata 的 Partition,设置 beingDeleted 为 False</span></div><div class="line">  filteredPartitions.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">false</span>))</div><div class="line">  <span class="comment">//note: 要删除的 Partition 设置 BeingDeleted 为 True</span></div><div class="line">  controller.deleteTopicManager.partitionsToBeDeleted.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">true</span>))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的实现逻辑如下：</p>
<ol>
<li>首先过滤出要发送的 Partition 列表，如果没有指定要发送 partitions 列表，那么默认就是发送全局的 metadata 信息；</li>
<li>接着将已经标记为删除的 Partition 从上面的列表中移除；</li>
<li>将要发送的 Broker 列表添加到 updateMetadataRequestBrokerSet 集合中；</li>
<li>将前面过滤的 Partition 列表对应的 metadata 信息添加到对应的 updateMetadataRequestPartitionInfoMap 集合中;</li>
<li>将当前设置为删除的所有 Partition 的 metadata 信息也添加到 updateMetadataRequestPartitionInfoMap 集合中，添加前会把其 leader 设置为-2，这样 Broker 收到这个 Partition 的 metadata 信息之后就会知道这个 Partition 是设置删除标志。</li>
</ol>
<h3 id="添加-StopReplica-请求"><a href="#添加-StopReplica-请求" class="headerlink" title="添加 StopReplica 请求"></a>添加 StopReplica 请求</h3><p>Controller 可以通过 <code>addStopReplicaRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 StopReplica 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 StopReplica 添加到对应的 Broker 中,还未开始发送数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addStopReplicaRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>, deletePartition: <span class="type">Boolean</span>,</div><div class="line">                                    callback: (<span class="type">AbstractResponse</span>, <span class="type">Int</span>) =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  brokerIds.filter(b =&gt; b &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</div><div class="line">    stopReplicaRequestMap.getOrElseUpdate(brokerId, <span class="type">Seq</span>.empty[<span class="type">StopReplicaRequestInfo</span>])</div><div class="line">    <span class="keyword">val</span> v = stopReplicaRequestMap(brokerId)</div><div class="line">    <span class="keyword">if</span>(callback != <span class="literal">null</span>)</div><div class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</div><div class="line">        deletePartition, (r: <span class="type">AbstractResponse</span>) =&gt; callback(r, brokerId))</div><div class="line">    <span class="keyword">else</span></div><div class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</div><div class="line">        deletePartition)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的实现逻辑比较简单，直接将 StopReplica 添加到 stopReplicaRequestMap 中。</p>
<h3 id="向-Broker-发送请求"><a href="#向-Broker-发送请求" class="headerlink" title="向 Broker 发送请求"></a>向 Broker 发送请求</h3><p>Controller 在添加完相应的请求后，最后一步都会去调用 <code>sendRequestsToBrokers()</code> 方法构造相应的请求，并把请求添加到 Broker 对应的 RequestQueue 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送请求给 broker（只是将对应处理后放入到对应的 queue 中）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequestsToBrokers</span></span>(controllerEpoch: <span class="type">Int</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">//note: LeaderAndIsr 请求</span></div><div class="line">    leaderAndIsrRequestMap.foreach &#123; <span class="keyword">case</span> (broker, partitionStateInfos) =&gt;</div><div class="line">      partitionStateInfos.foreach &#123; <span class="keyword">case</span> (topicPartition, state) =&gt;</div><div class="line">        <span class="keyword">val</span> typeOfRequest = <span class="keyword">if</span> (broker == state.leaderIsrAndControllerEpoch.leaderAndIsr.leader) <span class="string">"become-leader"</span> <span class="keyword">else</span> <span class="string">"become-follower"</span></div><div class="line">        stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending %s LeaderAndIsr request %s to broker %d "</span> +</div><div class="line">                                 <span class="string">"for partition [%s,%d]"</span>).format(controllerId, controllerEpoch, typeOfRequest,</div><div class="line">                                                                 state.leaderIsrAndControllerEpoch, broker,</div><div class="line">                                                                 topicPartition.topic, topicPartition.partition))</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: leader id 集合</span></div><div class="line">      <span class="keyword">val</span> leaderIds = partitionStateInfos.map(_._2.leaderIsrAndControllerEpoch.leaderAndIsr.leader).toSet</div><div class="line">      <span class="keyword">val</span> leaders = controllerContext.liveOrShuttingDownBrokers.filter(b =&gt; leaderIds.contains(b.id)).map &#123;</div><div class="line">        _.getNode(controller.config.interBrokerListenerName)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: requests.PartitionState</span></div><div class="line">      <span class="keyword">val</span> partitionStates = partitionStateInfos.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</div><div class="line">        <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</div><div class="line">        <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</div><div class="line">          leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</div><div class="line">          partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</div><div class="line">        topicPartition -&gt; partitionState</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 构造 LeaderAndIsr 请求,并添加到对应的 queue 中</span></div><div class="line">      <span class="keyword">val</span> leaderAndIsrRequest = <span class="keyword">new</span> <span class="type">LeaderAndIsrRequest</span>.</div><div class="line">          <span class="type">Builder</span>(controllerId, controllerEpoch, partitionStates.asJava, leaders.asJava)</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">LEADER_AND_ISR</span>, leaderAndIsrRequest, <span class="literal">null</span>)</div><div class="line">    &#125;</div><div class="line">    leaderAndIsrRequestMap.clear() <span class="comment">//note: 清空 leaderAndIsr 集合</span></div><div class="line"></div><div class="line">    <span class="comment">//note: update-metadata 请求</span></div><div class="line">    updateMetadataRequestPartitionInfoMap.foreach(p =&gt; stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending UpdateMetadata request %s "</span> +</div><div class="line">      <span class="string">"to brokers %s for partition %s"</span>).format(controllerId, controllerEpoch, p._2.leaderIsrAndControllerEpoch,</div><div class="line">      updateMetadataRequestBrokerSet.toString(), p._1)))</div><div class="line">    <span class="keyword">val</span> partitionStates = updateMetadataRequestPartitionInfoMap.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</div><div class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</div><div class="line">        leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</div><div class="line">        partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</div><div class="line">      topicPartition -&gt; partitionState</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> version: <span class="type">Short</span> =</div><div class="line">      <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_2_IV0</span>) <span class="number">3</span></div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_0_IV1</span>) <span class="number">2</span></div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_9_0</span>) <span class="number">1</span></div><div class="line">      <span class="keyword">else</span> <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment">//note: 构造 update-metadata 请求</span></div><div class="line">    <span class="keyword">val</span> updateMetadataRequest = &#123;</div><div class="line">      <span class="keyword">val</span> liveBrokers = <span class="keyword">if</span> (version == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">// Version 0 of UpdateMetadataRequest only supports PLAINTEXT.</span></div><div class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</div><div class="line">          <span class="keyword">val</span> securityProtocol = <span class="type">SecurityProtocol</span>.<span class="type">PLAINTEXT</span></div><div class="line">          <span class="keyword">val</span> listenerName = <span class="type">ListenerName</span>.forSecurityProtocol(securityProtocol)</div><div class="line">          <span class="keyword">val</span> node = broker.getNode(listenerName)</div><div class="line">          <span class="keyword">val</span> endPoints = <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">EndPoint</span>(node.host, node.port, securityProtocol, listenerName))</div><div class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</div><div class="line">          <span class="keyword">val</span> endPoints = broker.endPoints.map &#123; endPoint =&gt;</div><div class="line">            <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">EndPoint</span>(endPoint.host, endPoint.port, endPoint.securityProtocol, endPoint.listenerName)</div><div class="line">          &#125;</div><div class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Builder</span>(</div><div class="line">        controllerId, controllerEpoch, partitionStates.asJava, liveBrokers.asJava).</div><div class="line">        setVersion(version)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 将请求添加到对应的 queue</span></div><div class="line">    updateMetadataRequestBrokerSet.foreach &#123; broker =&gt;</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">UPDATE_METADATA_KEY</span>, updateMetadataRequest, <span class="literal">null</span>)</div><div class="line">    &#125;</div><div class="line">    updateMetadataRequestBrokerSet.clear() <span class="comment">//note: 清空对应的请求记录</span></div><div class="line">    updateMetadataRequestPartitionInfoMap.clear()</div><div class="line"></div><div class="line">    <span class="comment">//note: StopReplica 请求的处理</span></div><div class="line">    stopReplicaRequestMap.foreach &#123; <span class="keyword">case</span> (broker, replicaInfoList) =&gt;</div><div class="line">      <span class="keyword">val</span> stopReplicaWithDelete = replicaInfoList.filter(_.deletePartition).map(_.replica).toSet</div><div class="line">      <span class="keyword">val</span> stopReplicaWithoutDelete = replicaInfoList.filterNot(_.deletePartition).map(_.replica).toSet</div><div class="line">      debug(<span class="string">"The stop replica request (delete = true) sent to broker %d is %s"</span></div><div class="line">        .format(broker, stopReplicaWithDelete.mkString(<span class="string">","</span>)))</div><div class="line">      debug(<span class="string">"The stop replica request (delete = false) sent to broker %d is %s"</span></div><div class="line">        .format(broker, stopReplicaWithoutDelete.mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">      <span class="keyword">val</span> (replicasToGroup, replicasToNotGroup) = replicaInfoList.partition(r =&gt; !r.deletePartition &amp;&amp; r.callback == <span class="literal">null</span>)</div><div class="line"></div><div class="line">      <span class="comment">// Send one StopReplicaRequest for all partitions that require neither delete nor callback. This potentially</span></div><div class="line">      <span class="comment">// changes the order in which the requests are sent for the same partitions, but that's OK.</span></div><div class="line">      <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(controllerId, controllerEpoch, <span class="literal">false</span>,</div><div class="line">        replicasToGroup.map(r =&gt; <span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).toSet.asJava)</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest)</div><div class="line"></div><div class="line">      replicasToNotGroup.foreach &#123; r =&gt;</div><div class="line">        <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(</div><div class="line">            controllerId, controllerEpoch, r.deletePartition,</div><div class="line">            <span class="type">Set</span>(<span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).asJava)</div><div class="line">        controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest, r.callback)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    stopReplicaRequestMap.clear()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty) &#123;</div><div class="line">        error(<span class="string">"Haven't been able to send leader and isr requests, current state of "</span> +</div><div class="line">            <span class="string">s"the map is <span class="subst">$leaderAndIsrRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty) &#123;</div><div class="line">        error(<span class="string">s"Haven't been able to send metadata update requests to brokers <span class="subst">$updateMetadataRequestBrokerSet</span>, "</span> +</div><div class="line">              <span class="string">s"current state of the partition info is <span class="subst">$updateMetadataRequestPartitionInfoMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty) &#123;</div><div class="line">        error(<span class="string">"Haven't been able to send stop replica requests, current state of "</span> +</div><div class="line">            <span class="string">s"the map is <span class="subst">$stopReplicaRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面这个方法看着很复杂，其实做的事情很明确，就是将三个集合中的请求发送对应 Broker 的请求队列中，这里简单作一个总结：</p>
<ol>
<li>从 leaderAndIsrRequestMap 集合中构造相应的 LeaderAndIsr 请求，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 leaderAndIsrRequestMap 集合；</li>
<li>从 updateMetadataRequestPartitionInfoMap 集合中构造相应的 UpdateMetadata 请求，，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 updateMetadataRequestBrokerSet 和 updateMetadataRequestPartitionInfoMap 集合；</li>
<li>从 stopReplicaRequestMap 集合中构造相应的 StopReplica 请求，在构造时会根据是否设置删除标志将要涉及的 Partition 分成两类，构造对应的请求，对于要删除数据的 StopReplica 会设置相应的回调函数，然后通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 stopReplicaRequestMap 集合。</li>
</ol>
<p>走到这一步，Controller 要发送的请求算是都添加到对应 Broker 的 MessageQueue 中，后台的 RequestSendThread 线程会从这个请求队列中遍历相应的请求，发送给对应的 Broker。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 &lt;a href=&quot;http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager&quot;&gt;Contro
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Topic 的新建/扩容/删除（二十）</title>
    <link href="http://matt33.com/2018/06/18/topic-create-alter-delete/"/>
    <id>http://matt33.com/2018/06/18/topic-create-alter-delete/</id>
    <published>2018-06-18T09:24:21.000Z</published>
    <updated>2018-06-18T09:30:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 <a href="http://matt33.com/2017/07/21/kafka-topic-create/">Kafka 源码解析之 topic 创建过程（三）</a> 中已经讲述过了，本篇与前面不同的是，本篇主要是从 Controller 角度来讲述，而且是把新建、扩容、删除这三个 Topic 级别的操作放在一起做一个总结。</p>
<h2 id="Topic-新建与扩容"><a href="#Topic-新建与扩容" class="headerlink" title="Topic 新建与扩容"></a>Topic 新建与扩容</h2><p>这里把 Topic 新建与扩容放在一起讲解，主要是因为无论 Topic 是新建还是扩容，在 Kafka 内部其实都是 Partition 的新建，底层的实现机制是一样的，Topic 的新建与扩容的整体流程如下图所示：</p>
<p><img src="/images/kafka/topic-create-alter.png" alt="Topic 新建与扩容流程"></p>
<p>Topic 新建与扩容触发条件的不同如下所示：</p>
<ol>
<li>对于 Topic 扩容，监控的节点是 <code>/brokers/topics/TOPIC_NAME</code>，监控的是具体的 Topic 节点，通过 PartitionStateMachine 的 <code>registerPartitionChangeListener(topic)</code> 方法注册的相应 listener；</li>
<li>对于 Topic 新建，监控的节点是 <code>/brokers/topics</code>，监控的是 Topic 列表，通过 PartitionStateMachine 的 <code>registerTopicChangeListener()</code> 方法注册的相应 listener。</li>
</ol>
<p>下面开始详细讲述这两种情况。</p>
<h3 id="Topic-扩容"><a href="#Topic-扩容" class="headerlink" title="Topic 扩容"></a>Topic 扩容</h3><p>Kafka 提供了 Topic 扩容工具，假设一个 Topic（topic_test）只有一个 partition，这时候我们想把它扩容到两个 Partition，可以通过下面两个命令来实现：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --partitions 2</div><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --replica-assignment 1:2,2:1 --partitions 2</div></pre></td></tr></table></figure>
<p>这两种方法的区别是：第二种方法直接指定了要扩容的 Partition 2 的副本需要分配到哪台机器上，这样的话我们可以精确控制到哪些 Topic 放下哪些机器上。</p>
<p>无论是使用哪种方案，上面两条命令产生的结果只有一个，将 Topic 各个 Partition 的副本写入到 ZK 对应的节点上，这样的话 <code>/brokers/topics/topic_test</code> 节点的内容就会发生变化，PartitionModificationsListener 监听器就会被触发，该监听器的处理流程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Partition change 监听器,主要是用于 Partition 扩容的监听</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionModificationsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"AddPartitionsListener"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        info(<span class="string">s"Partition modification triggered <span class="subst">$data</span> for path <span class="subst">$dataPath</span>"</span>)</div><div class="line">        <span class="keyword">val</span> partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(<span class="type">List</span>(topic))</div><div class="line">        <span class="comment">//note: 获取新增的 partition 列表及其对应的分配副本列表</span></div><div class="line">        <span class="keyword">val</span> partitionsToBeAdded = partitionReplicaAssignment.filter(p =&gt;</div><div class="line">          !controllerContext.partitionReplicaAssignment.contains(p._1))</div><div class="line">        <span class="comment">//note: 如果该 topic 被标记为删除,那么直接跳过,不再处理,否则创建该 Partition</span></div><div class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(topic))</div><div class="line">          error(<span class="string">"Skipping adding partitions %s for topic %s since it is currently being deleted"</span></div><div class="line">                .format(partitionsToBeAdded.map(_._1.partition).mkString(<span class="string">","</span>), topic))</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">if</span> (partitionsToBeAdded.nonEmpty) &#123;</div><div class="line">            info(<span class="string">"New partitions to be added %s"</span>.format(partitionsToBeAdded))</div><div class="line">            controllerContext.partitionReplicaAssignment.++=(partitionsToBeAdded)</div><div class="line">            controller.onNewPartitionCreation(partitionsToBeAdded.keySet)<span class="comment">//note: 创建新的 partition</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling add partitions for data path "</span> + dataPath, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// this is not implemented for partition change</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(parentPath: <span class="type">String</span>): <span class="type">Unit</span> = &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其 <code>doHandleDataChange()</code> 方法的处理流程如下：</p>
<ol>
<li>首先获取该 Topic 在 ZK 的 Partition 副本列表，跟本地的缓存做对比，获取新增的 Partition 列表；</li>
<li>检查这个 Topic 是否被标记为删除，如果被标记了，那么直接跳过，不再处理这个 Partition 扩容的请求；</li>
<li>调用 KafkaController 的 <code>onNewPartitionCreation()</code> 新建该 Partition。</li>
</ol>
<p>下面我们看下 <code>onNewPartitionCreation()</code> 方法，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 用于 Topic Partition 的新建</span></div><div class="line"><span class="comment">//note: 1. 将新创建的 partition 状态置为 NewPartition 状态;</span></div><div class="line"><span class="comment">//note: 2. 将新创建的 Replica 状态置为 NewReplica 状态;</span></div><div class="line"><span class="comment">//note: 3. 将该 Partition 从 NewPartition 改为 OnlinePartition 状态,这期间会 为该 Partition 选举 leader 和 isr，更新到 zk 和 controller的缓存中</span></div><div class="line"><span class="comment">//note: 4. 将副本状态从 NewReplica 改为 OnlineReplica 状态。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewPartitionCreation</span></span>(newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"New partition creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">NewPartition</span>)</div><div class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">NewReplica</span>)</div><div class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">OnlinePartition</span>, offlinePartitionSelector)</div><div class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">OnlineReplica</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Partition 的新建，总共分了以下四步：</p>
<ol>
<li>将新创建的 Partition 状态置为 NewPartition 状态，此时 Partition 刚刚创建，只是分配了相应的 Replica 但是还没有 leader 和 isr，不能正常工作;</li>
<li>将该 Partition 对应的 Replica 列表状态设置为 NewReplica 状态，这部分只是将 Replica 的状态设置为了 NewReplica，并没有做其他的处理;</li>
<li>将该 Partition 的状态从 NewPartition 改为 OnlinePartition 状态，这期间会为该 Partition 选举 leader 和 isr，并将结果更新到 ZK 和 Controller 的缓存中，并向该 Partition 的所有副本发送对应的 LeaderAndIsr 信息（发送 LeaderAndIsr 请求的同时也会向所有 Broker 发送该 Topic 的 leader、isr metadata 信息）；</li>
<li>将副本状态从 NewReplica 转移为 OnlineReplica 状态。</li>
</ol>
<p>经过上面几个阶段，一个 Partition 算是真正创建出来，可以正常进行读写工作了，当然上面只是讲述了 Controller 端做的内容，Partition 副本所在节点对 LeaderAndIsr 请求会做更多的工作，这部分会在后面关于 LeaderAndIsr 请求的处理中只能够详细讲述。</p>
<h3 id="Topic-新建"><a href="#Topic-新建" class="headerlink" title="Topic 新建"></a>Topic 新建</h3><p>Kafka 也提供了 Topic 创建的工具，假设我们要创建一个名叫 topic_test，Partition 数为2的 Topic，创建的命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --partitions 2 --replication-factor 2</div><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --replica-assignment 1:2,2:1 --partitions 2</div></pre></td></tr></table></figure>
<p>跟前面的类似，方法二是可以精确控制新建 Topic 每个 Partition 副本所在位置，Topic 创建的本质上是在 <code>/brokers/topics</code> 下新建一个节点信息，并将 Topic 的分区详情写入进去，当 <code>/brokers/topics</code> 有了新增的 Topic 节点后，会触发 TopicChangeListener 监听器，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 监控 zk 上 Topic 子节点的变化 ,KafkaController 会进行相应的处理</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"TopicChangeListener"</span></div><div class="line"></div><div class="line">  <span class="comment">//note: 当 zk 上 topic 节点上有变更时,这个方法就会调用</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> currentChildren = &#123;</div><div class="line">            debug(<span class="string">"Topic change listener fired for path %s with children %s"</span>.format(parentPath, children.mkString(<span class="string">","</span>)))</div><div class="line">            children.toSet</div><div class="line">          &#125;</div><div class="line">          <span class="comment">//note: 新创建的 topic 列表</span></div><div class="line">          <span class="keyword">val</span> newTopics = currentChildren -- controllerContext.allTopics</div><div class="line">          <span class="comment">//note: 已经删除的 topic 列表</span></div><div class="line">          <span class="keyword">val</span> deletedTopics = controllerContext.allTopics -- currentChildren</div><div class="line">          controllerContext.allTopics = currentChildren</div><div class="line"></div><div class="line">          <span class="comment">//note: 新创建 topic 对应的 partition 列表及副本列表添加到 Controller 的缓存中</span></div><div class="line">          <span class="keyword">val</span> addedPartitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(newTopics.toSeq)</div><div class="line">          <span class="comment">//note: Controller 从缓存中把已经删除 partition 过滤掉</span></div><div class="line">          controllerContext.partitionReplicaAssignment = controllerContext.partitionReplicaAssignment.filter(p =&gt;</div><div class="line">            !deletedTopics.contains(p._1.topic))</div><div class="line">          controllerContext.partitionReplicaAssignment.++=(addedPartitionReplicaAssignment)<span class="comment">//note: 将新增的 tp-replicas 更新到缓存中</span></div><div class="line">          info(<span class="string">"New topics: [%s], deleted topics: [%s], new partition replica assignment [%s]"</span>.format(newTopics,</div><div class="line">            deletedTopics, addedPartitionReplicaAssignment))</div><div class="line">          <span class="keyword">if</span> (newTopics.nonEmpty)<span class="comment">//note: 处理新建的 topic</span></div><div class="line">            controller.onNewTopicCreation(newTopics, addedPartitionReplicaAssignment.keySet)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling new topic"</span>, e)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>只要 <code>/brokers/topics</code> 下子节点信息有变化（topic 新增或者删除），TopicChangeListener 都会被触发，其 <code>doHandleChildChange()</code> 方法的处理流程如下：</p>
<ol>
<li>获取 ZK 当前的所有 Topic 列表，根据本地缓存的 Topic 列表记录，可以得到新增的 Topic 记录与已经删除的 Topic 列表；</li>
<li>将新增 Topic 的相信信息更新到 Controller 的缓存中，将已经删除的 Topic 从 Controller 的副本缓存中移除；</li>
<li>调用 KafkaController 的 <code>onNewTopicCreation()</code> 方法创建该 topic。</li>
</ol>
<p>接着看下 <code>onNewTopicCreation()</code> 方法实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 partition state machine 监控到有新 topic 或 partition 时,这个方法将会被调用</span></div><div class="line"><span class="comment">//note: 1. 注册 partition change listener, 监听 Parition 变化;</span></div><div class="line"><span class="comment">//note: 2. 触发 the new partition, 也即是 onNewPartitionCreation()</span></div><div class="line"><span class="comment">//note: 3. 发送 metadata 请求给所有的 Broker</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewTopicCreation</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>], newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"New topic creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">// subscribe to partition changes</span></div><div class="line">  topics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">  onNewPartitionCreation(newPartitions)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法主要做了两件事：</p>
<ol>
<li>注册这个 topic 的 PartitionModificationsListener 监听器；</li>
<li>通过 <code>onNewPartitionCreation()</code> 创建该 Topic 的所有 Partition。</li>
</ol>
<p><code>onNewPartitionCreation()</code> 的实现在前面 Topic 扩容部分已经讲述过，这里不再重复，最好参考前面流程图来梳理 Topic 扩容和新建的整个过程。</p>
<h2 id="Topic-删除"><a href="#Topic-删除" class="headerlink" title="Topic 删除"></a>Topic 删除</h2><p>Kafka Topic 删除这部分的逻辑是一个单独线程去做的，这个线程是在 Controller 启动时初始化和启动的。</p>
<h3 id="TopicDeletionManager-初始化"><a href="#TopicDeletionManager-初始化" class="headerlink" title="TopicDeletionManager 初始化"></a>TopicDeletionManager 初始化</h3><p>TopicDeletionManager 启动实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Invoked at the end of new controller initiation</div><div class="line"> */</div><div class="line"><span class="comment">//note: Controller 初始化完成,触发这个操作,删除 topic 线程启动</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span> (isDeleteTopicEnabled) &#123;</div><div class="line">    deleteTopicsThread = <span class="keyword">new</span> <span class="type">DeleteTopicsThread</span>()</div><div class="line">    <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty)</div><div class="line">      deleteTopicStateChanged.set(<span class="literal">true</span>)</div><div class="line">    deleteTopicsThread.start() <span class="comment">//note: 启动 DeleteTopicsThread</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>TopicDeletionManager 启动时只是初始化了一个 DeleteTopicsThread 线程，并启动该线程。TopicDeletionManager 这个类从名字上去看，它是 Topic 删除的管理器，它是如何实现 Topic 删除管理呢，这里先看下该类的几个重要的成员变量：</p>
<ol>
<li>topicsToBeDeleted：需要删除的 Topic 列表，每当有新的 topic 需要删除时，Controller 就通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到这个列表中，而 DeleteTopicsThread 线程则会从列表拿到需要进行删除的 Topic 信息；</li>
<li>partitionsToBeDeleted：需要删除的 Partition 列表，跟上面的 Topic 列表保持一致，只不过纬度不同；</li>
<li>topicsIneligibleForDeletion：非法删除的 Topic 列表，当一个 Topic 正在进行副本迁移、leader 选举或者有副本 dead 的情况下，该 Topic 都会设置被非法删除状态，只有恢复正常后，这个状态才会解除，处在这个状态的 Topic 是无法删除的。</li>
</ol>
<h3 id="Topic-删除整体流程"><a href="#Topic-删除整体流程" class="headerlink" title="Topic 删除整体流程"></a>Topic 删除整体流程</h3><p>前面一小节，简单介绍了 TopicDeletionManager、DeleteTopicsThread 的启动以及它们之间的关系，这里我们看下一个 Topic 被设置删除后，其处理的整理流程，简单做了一个小图，如下所示：</p>
<p><img src="/images/kafka/topic-delete.png" alt="Topic 删除整理流程"></p>
<p>这里先简单讲述上面的流程，当一个 Topic 设置为删除后：</p>
<ol>
<li>首先 DeleteTopicsListener 会被触发，然后通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表中；</li>
<li>DeleteTopicsThread 这个线程会不断调用 <code>doWork()</code> 方法，这个方法被调用时，它会遍历 <code>topicsToBeDeleted</code> 中的所有 Topic 列表；</li>
<li>对于之前没有处理过的 Topic（之前还没有开始删除），会通过 TopicDeletionManager 的 <code>onTopicDeletion()</code> 方法执行删除操作；</li>
<li>如果 Topic 删除完成（所有 Replica 的状态都变为 ReplicaDeletionSuccessful 状态），那么就执行 TopicDeletionManager 的 <code>completeDeleteTopic()</code> 完成删除流程，即更新状态信息，并将 Topic 的 meta 信息从缓存和 ZK 中清除。</li>
</ol>
<h3 id="Topic-删除详细实现"><a href="#Topic-删除详细实现" class="headerlink" title="Topic 删除详细实现"></a>Topic 删除详细实现</h3><p>先看下 DeleteTopicsListener 的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除 Topic 包括以下操作:</span></div><div class="line"><span class="comment">//note: 1. 如果要删除的 topic 存在,将 Topic 添加到 Topic 将要删除的缓存中;</span></div><div class="line"><span class="comment">//note: 2. 如果有 Topic 将要被删除,那么将触发 Topic 删除线程</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"DeleteTopicsListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when a topic is being deleted</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当 topic 需要被删除时,才会触发</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">var</span> topicsToBeDeleted = children.toSet</div><div class="line">      debug(<span class="string">"Delete topics listener fired for topics %s to be deleted"</span>.format(topicsToBeDeleted.mkString(<span class="string">","</span>)))</div><div class="line">      <span class="comment">//note: 不存在的、需要删除的 topic, 直接清除 zk 上的记录</span></div><div class="line">      <span class="keyword">val</span> nonExistentTopics = topicsToBeDeleted -- controllerContext.allTopics</div><div class="line">      <span class="keyword">if</span> (nonExistentTopics.nonEmpty) &#123;</div><div class="line">        warn(<span class="string">"Ignoring request to delete non-existing topics "</span> + nonExistentTopics.mkString(<span class="string">","</span>))</div><div class="line">        nonExistentTopics.foreach(topic =&gt; zkUtils.deletePathRecursive(getDeleteTopicPath(topic)))</div><div class="line">      &#125;</div><div class="line">      topicsToBeDeleted --= nonExistentTopics</div><div class="line">      <span class="keyword">if</span> (controller.config.deleteTopicEnable) &#123; <span class="comment">//note: 如果允许 topic 删除</span></div><div class="line">        <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 有 Topic 需要删除</span></div><div class="line">          info(<span class="string">"Starting topic deletion for topics "</span> + topicsToBeDeleted.mkString(<span class="string">","</span>))</div><div class="line">          <span class="comment">// mark topic ineligible for deletion if other state changes are in progress</span></div><div class="line">          topicsToBeDeleted.foreach &#123; topic =&gt; <span class="comment">//note: 如果 topic 正在最优 leader 选举或正在迁移,那么将 topic 标记为非法删除状态</span></div><div class="line">            <span class="keyword">val</span> preferredReplicaElectionInProgress =</div><div class="line">              controllerContext.partitionsUndergoingPreferredReplicaElection.map(_.topic).contains(topic)</div><div class="line">            <span class="keyword">val</span> partitionReassignmentInProgress =</div><div class="line">              controllerContext.partitionsBeingReassigned.keySet.map(_.topic).contains(topic)</div><div class="line">            <span class="keyword">if</span> (preferredReplicaElectionInProgress || partitionReassignmentInProgress)</div><div class="line">              controller.deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">          &#125;</div><div class="line">          <span class="comment">// add topic to deletion list</span></div><div class="line">          <span class="comment">//note: 将要删除的 topic 添加到待删除的 topic</span></div><div class="line">          controller.deleteTopicManager.enqueueTopicsForDeletion(topicsToBeDeleted)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// If delete topic is disabled remove entries under zookeeper path : /admin/delete_topics</span></div><div class="line">        <span class="keyword">for</span> (topic &lt;- topicsToBeDeleted) &#123;</div><div class="line">          info(<span class="string">"Removing "</span> + getDeleteTopicPath(topic) + <span class="string">" since delete topic is disabled"</span>)</div><div class="line">          zkUtils.zkClient.delete(getDeleteTopicPath(topic))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其 <code>doHandleChildChange()</code> 的实现逻辑如下：</p>
<ol>
<li>根据要删除的 Topic 列表，过滤出那些不存在的 Topic 列表，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）；</li>
<li>如果集群不允许 Topic 删除，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）这些 Topic 列表，结束流程；</li>
<li>如果这个列表中有正在进行副本迁移或 leader 选举的 Topic，那么先将这些 Topic 加入到 <code>topicsIneligibleForDeletion</code> 中，即标记为非法删除；</li>
<li>通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表（<code>topicsToBeDeleted</code>）、将 Partition 添加到要删除的 Partition 列表中（<code>partitionsToBeDeleted</code>）。</li>
</ol>
<p>接下来，看下 Topic 删除线程 DeleteTopicsThread 的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">/note: topic 删除线程</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsThread</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">ShutdownableThread</span>(<span class="params">name = "delete-topics-thread-" + controller.config.brokerId, isInterruptible = false</span>) </span>&#123;</div><div class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">    awaitTopicDeletionNotification()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (!isRunning.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="comment">//note: 要删除的 topic 列表</span></div><div class="line">      <span class="keyword">val</span> topicsQueuedForDeletion = <span class="type">Set</span>.empty[<span class="type">String</span>] ++ topicsToBeDeleted</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(topicsQueuedForDeletion.nonEmpty)</div><div class="line">        info(<span class="string">"Handling deletion for topics "</span> + topicsQueuedForDeletion.mkString(<span class="string">","</span>))</div><div class="line"></div><div class="line">      topicsQueuedForDeletion.foreach &#123; topic =&gt;</div><div class="line">      <span class="comment">// if all replicas are marked as deleted successfully, then topic deletion is done</span></div><div class="line">        <span class="keyword">if</span>(controller.replicaStateMachine.areAllReplicasForTopicDeleted(topic)) &#123;<span class="comment">//note: 如果 Topic 所有副本都删除成功的情况下</span></div><div class="line">          <span class="comment">// clear up all state for this topic from controller cache and zookeeper</span></div><div class="line">          <span class="comment">//note: 从 controller 的缓存和 zk 中清除这个 topic 的所有记录,这个 topic 彻底删除成功了</span></div><div class="line">          completeDeleteTopic(topic)</div><div class="line">          info(<span class="string">"Deletion of topic %s successfully completed"</span>.format(topic))</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">if</span>(controller.replicaStateMachine.isAtLeastOneReplicaInDeletionStartedState(topic)) &#123;</div><div class="line">            <span class="comment">//note: Topic 的副本至少有一个状态为 ReplicaDeletionStarted 时</span></div><div class="line">            <span class="comment">// ignore since topic deletion is in progress</span></div><div class="line">            <span class="comment">//note: 过滤出 Topic 中副本状态为 ReplicaDeletionStarted 的 Partition 列表</span></div><div class="line">            <span class="keyword">val</span> replicasInDeletionStartedState = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionStarted</span>)</div><div class="line">            <span class="comment">//note: 表明了上面这些副本正在删除中</span></div><div class="line">            <span class="keyword">val</span> replicaIds = replicasInDeletionStartedState.map(_.replica)</div><div class="line">            <span class="keyword">val</span> partitions = replicasInDeletionStartedState.map(r =&gt; <span class="type">TopicAndPartition</span>(r.topic, r.partition))</div><div class="line">            info(<span class="string">"Deletion for replicas %s for partition %s of topic %s in progress"</span>.format(replicaIds.mkString(<span class="string">","</span>),</div><div class="line">              partitions.mkString(<span class="string">","</span>), topic))</div><div class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note:副本既没有全部删除完成、也没有一个副本是在删除过程中，证明这个 topic 还没有开始删除或者删除完成但是至少一个副本删除失败</span></div><div class="line">            <span class="comment">// if you come here, then no replica is in TopicDeletionStarted and all replicas are not in</span></div><div class="line">            <span class="comment">// TopicDeletionSuccessful. That means, that either given topic haven't initiated deletion</span></div><div class="line">            <span class="comment">// or there is at least one failed replica (which means topic deletion should be retried).</span></div><div class="line">            <span class="keyword">if</span>(controller.replicaStateMachine.isAnyReplicaInState(topic, <span class="type">ReplicaDeletionIneligible</span>)) &#123;</div><div class="line">              <span class="comment">//note: 如果有副本删除失败,那么进行重试操作</span></div><div class="line">              <span class="comment">// mark topic for deletion retry</span></div><div class="line">              markTopicForDeletionRetry(topic)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// Try delete topic if it is eligible for deletion.</span></div><div class="line">        <span class="keyword">if</span>(isTopicEligibleForDeletion(topic)) &#123; <span class="comment">//note: 如果 topic 可以被删除</span></div><div class="line">          info(<span class="string">"Deletion of topic %s (re)started"</span>.format(topic))</div><div class="line">          <span class="comment">// topic deletion will be kicked off</span></div><div class="line">          <span class="comment">//note: 开始删除 topic</span></div><div class="line">          onTopicDeletion(<span class="type">Set</span>(topic))</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(isTopicIneligibleForDeletion(topic)) &#123;</div><div class="line">          info(<span class="string">"Not retrying deletion of topic %s at this time since it is marked ineligible for deletion"</span>.format(topic))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>doWork()</code> 方法处理逻辑如下：</p>
<ol>
<li>遍历所有要删除的 Topic，进行如下处理；</li>
<li>如果该 Topic 的所有副本都下线成功（状态为 ReplicaDeletionSuccessful）时，那么执行 <code>completeDeleteTopic()</code> 方法完成 Topic 的删除；</li>
<li>否则，如果 Topic 在删除过程有失败的副本（状态为 ReplicaDeletionIneligible），那么执行 <code>markTopicForDeletionRetry()</code> 将失败的 Replica 状态设置为 OfflineReplica；</li>
<li>判断 Topic 是否允许删除（不在非法删除的集合中就代表运允许），调用 <code>onTopicDeletion()</code> 执行 Topic 删除。</li>
</ol>
<p>先看下 <code>onTopicDeletion()</code> 方法，这是 Topic 最开始删除时的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Topic 删除</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onTopicDeletion</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>]) &#123;</div><div class="line">  info(<span class="string">"Topic deletion callback for %s"</span>.format(topics.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">// send update metadata so that brokers stop serving data for topics to be deleted</span></div><div class="line">  <span class="keyword">val</span> partitions = topics.flatMap(controllerContext.partitionsForTopic) <span class="comment">//note: topic 的所有 Partition</span></div><div class="line">  controller.sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, partitions) <span class="comment">//note: 更新meta</span></div><div class="line">  <span class="keyword">val</span> partitionReplicaAssignmentByTopic = controllerContext.partitionReplicaAssignment.groupBy(p =&gt; p._1.topic)</div><div class="line">  topics.foreach &#123; topic =&gt; <span class="comment">//note:  删除 topic 的每一个 Partition</span></div><div class="line">    onPartitionDeletion(partitionReplicaAssignmentByTopic(topic).keySet)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 这个方法是用于 delete-topic, 用于删除 topic 的所有 partition</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onPartitionDeletion</span></span>(partitionsToBeDeleted: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"Partition deletion callback for %s"</span>.format(partitionsToBeDeleted.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="keyword">val</span> replicasPerPartition = controllerContext.replicasForPartition(partitionsToBeDeleted)</div><div class="line">  startReplicaDeletion(replicasPerPartition)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Topic 的删除的真正实现方法还是在 <code>startReplicaDeletion()</code> 方法中，Topic 删除时，会先调用 <code>onPartitionDeletion()</code> 方法删除所有的 Partition，然后在 Partition 删除时，执行 <code>startReplicaDeletion()</code> 方法删除该 Partition 的副本，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 被 onPartitionDeletion 方法触发,删除副本具体的实现的地方</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startReplicaDeletion</span></span>(replicasForTopicsToBeDeleted: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>]) &#123;</div><div class="line">  replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach &#123; topic =&gt;</div><div class="line">    <span class="comment">//note: topic 所有存活的 replica</span></div><div class="line">    <span class="keyword">val</span> aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p =&gt; p.topic == topic)</div><div class="line">    <span class="comment">//note: topic 的 dead replica</span></div><div class="line">    <span class="keyword">val</span> deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic</div><div class="line">    <span class="comment">//note: topic 中已经处于 ReplicaDeletionSuccessful 状态的副本</span></div><div class="line">    <span class="keyword">val</span> successfullyDeletedReplicas = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">    <span class="comment">//note: 还没有成功删除的、存活的副本</span></div><div class="line">    <span class="keyword">val</span> replicasForDeletionRetry = aliveReplicasForTopic -- successfullyDeletedReplicas</div><div class="line">    <span class="comment">// move dead replicas directly to failed state</span></div><div class="line">    <span class="comment">//note: 将 dead replica 设置为 ReplicaDeletionIneligible（删除无效的状态）</span></div><div class="line">    replicaStateMachine.handleStateChanges(deadReplicasForTopic, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">    <span class="comment">// send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader</span></div><div class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 OfflineReplica（发送 StopReplica 请求）</span></div><div class="line">    replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">OfflineReplica</span>)</div><div class="line">    debug(<span class="string">"Deletion started for replicas %s"</span>.format(replicasForDeletionRetry.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 ReplicaDeletionStarted 状态</span></div><div class="line">    controller.replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">ReplicaDeletionStarted</span>,</div><div class="line">      <span class="keyword">new</span> <span class="type">Callbacks</span>.<span class="type">CallbackBuilder</span>().stopReplicaCallback(deleteTopicStopReplicaCallback).build)</div><div class="line">    <span class="keyword">if</span>(deadReplicasForTopic.nonEmpty) &#123; <span class="comment">//note: 将 topic 标记为不能删除</span></div><div class="line">      debug(<span class="string">"Dead Replicas (%s) found for topic %s"</span>.format(deadReplicasForTopic.mkString(<span class="string">","</span>), topic))</div><div class="line">      markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>该方法的执行逻辑如下：</p>
<ol>
<li>首先获取当前集群所有存活的 broker 信息，根据这个信息可以知道 Topic 哪些副本所在节点是处于 dead 状态；</li>
<li>找到那些已经成功删除的 Replica 列表（状态为 ReplicaDeletionSuccessful），进而可以得到那些还没有成功删除、并且存活的 Replica 列表（<code>replicasForDeletionRetry</code>）；</li>
<li>将处于 dead 节点上的 Replica 的状态设置为 ReplicaDeletionIneligible 状态；</li>
<li>然后重新删除 replicasForDeletionRetry 列表中的副本，先将其状态转移为 OfflineReplica，再转移为 ReplicaDeletionStarted 状态（真正从发送 StopReplica +从物理上删除数据）；</li>
<li>如果有 Replica 所在的机器处于 dead 状态，那么将 Topic 设置为非法删除状态。</li>
</ol>
<p>在将副本状态从 OfflineReplica 转移成 ReplicaDeletionStarted 时，会设置一个回调方法 <code>deleteTopicStopReplicaCallback()</code>，该方法会将删除成功的 Replica 设置为 ReplicaDeletionSuccessful 状态，删除失败的 Replica 设置为 ReplicaDeletionIneligible 状态（需要根据 StopReplica 请求处理的过程，看下哪些情况下 Replica 会删除失败，这个会在后面讲解）。</p>
<p>下面看下这个方法 <code>completeDeleteTopic()</code>，当一个 Topic 的所有 Replica 都删除成功时，即其状态都在 ReplicaDeletionSuccessful 时，会调用这个方法，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: topic 删除后,从 controller 缓存、状态机以及 zk 移除这个 topic 相关记录</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">completeDeleteTopic</span></span>(topic: <span class="type">String</span>) &#123;</div><div class="line">  <span class="comment">// deregister partition change listener on the deleted topic. This is to prevent the partition change listener</span></div><div class="line">  <span class="comment">// firing before the new topic listener when a deleted topic gets auto created</span></div><div class="line">  <span class="comment">//note: 1. 取消 zk 对这个 topic 的 partition-modify-listener</span></div><div class="line">  partitionStateMachine.deregisterPartitionChangeListener(topic)</div><div class="line">  <span class="comment">//note: 2. 过滤出副本状态为 ReplicaDeletionSuccessful 的副本列表</span></div><div class="line">  <span class="keyword">val</span> replicasForDeletedTopic = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">  <span class="comment">// controller will remove this replica from the state machine as well as its partition assignment cache</span></div><div class="line">  <span class="comment">//note: controller 将会从副本状态机移除这些副本</span></div><div class="line">  replicaStateMachine.handleStateChanges(replicasForDeletedTopic, <span class="type">NonExistentReplica</span>)</div><div class="line">  <span class="keyword">val</span> partitionsForDeletedTopic = controllerContext.partitionsForTopic(topic)</div><div class="line">  <span class="comment">// move respective partition to OfflinePartition and NonExistentPartition state</span></div><div class="line">  <span class="comment">//note: 3. 从分区状态机中下线并移除这个 topic 的分区</span></div><div class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">OfflinePartition</span>)</div><div class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">NonExistentPartition</span>)</div><div class="line">  topicsToBeDeleted -= topic <span class="comment">//note: 删除成功,从删除 topic 列表中移除</span></div><div class="line">  partitionsToBeDeleted.retain(_.topic != topic) <span class="comment">//note: 从 partitionsToBeDeleted 移除这个 topic</span></div><div class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line">  <span class="comment">//note: 4. 删除 zk 上关于这个 topic 的相关记录</span></div><div class="line">  zkUtils.zkClient.deleteRecursive(getTopicPath(topic))</div><div class="line">  zkUtils.zkClient.deleteRecursive(getEntityConfigPath(<span class="type">ConfigType</span>.<span class="type">Topic</span>, topic))</div><div class="line">  zkUtils.zkClient.delete(getDeleteTopicPath(topic))</div><div class="line">  <span class="comment">//note: 5. 从 controller 的所有缓存中再次移除关于这个 topic 的信息</span></div><div class="line">  controllerContext.removeTopic(topic)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当一个 Topic 所有副本都删除后，会进行如下处理：</p>
<ol>
<li>取消对该 Topic 的 partition-modify-listener 监听器；</li>
<li>将状态为 ReplicaDeletionSuccessful 的副本状态都转移成 NonExistentReplica；</li>
<li>将该 Topic Partition 状态先后转移成 OfflinePartition、NonExistentPartition 状态，正式下线了该 Partition；</li>
<li>从分区状态机和副本状态机中移除这个 Topic 记录；</li>
<li>从 Controller 缓存和 ZK 中清除这个 Topic 的相关记录。</li>
</ol>
<p>至此，一个 Topic 算是真正删除完成。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 &lt;a href=&quot;ht
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Broker 上线下线（十九）</title>
    <link href="http://matt33.com/2018/06/17/broker-online-offline/"/>
    <id>http://matt33.com/2018/06/17/broker-online-offline/</id>
    <published>2018-06-17T07:04:21.000Z</published>
    <updated>2018-06-17T07:56:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 <code>/brokers/ids</code> 下注册一个节点，节点名字就是 broker id，这个节点是临时节点，该节点内部会有这个 Broker 的详细节点信息。Controller 会监听 <code>/brokers/ids</code> 这个路径下的所有子节点，如果有新的节点出现，那么就代表有新的 Broker 上线，如果有节点消失，就代表有 broker 下线，Controller 会进行相应的处理，Kafka 就是利用 ZK 的这种 watch 机制及临时节点的特性来完成集群 Broker 的上下线，本文将会深入讲解这一过程。</p>
<h2 id="BrokerChangeListener"><a href="#BrokerChangeListener" class="headerlink" title="BrokerChangeListener"></a>BrokerChangeListener</h2><p>KafkaController 在启动时，会通过副本状态机注册一个监控 broker 上下线的监听器，通过 ReplicaStateMachine 的 <code>registerListeners()</code> 方法实现的，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// register ZK listeners of the replica state machine</span></div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">registerListeners</span></span>() &#123;</div><div class="line">   <span class="comment">// register broker change listener</span></div><div class="line">   registerBrokerChangeListener() <span class="comment">//note: 监听【/brokers/ids】，broker 的上线下线</span></div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerBrokerChangeListener</span></span>() = &#123;</div><div class="line">  zkUtils.zkClient.subscribeChildChanges(<span class="type">ZkUtils</span>.<span class="type">BrokerIdsPath</span>, brokerChangeListener)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>BrokerChangeListener 是监听 <code>/brokers/ids</code> 节点的监听器，当该节点有变化时会触发 <code>doHandleChildChange()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果 【/brokers/ids】 目录下子节点有变化将会触发这个操作</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrokerChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"BrokerChangeListener"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, currentBrokerList: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    info(<span class="string">"Broker change listener fired for path %s with children %s"</span>.format(parentPath, currentBrokerList.sorted.mkString(<span class="string">","</span>)))</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">        <span class="type">ControllerStats</span>.leaderElectionTimer.time &#123;</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            <span class="comment">//note: 当前 zk 的 broker 列表</span></div><div class="line">            <span class="keyword">val</span> curBrokers = currentBrokerList.map(_.toInt).toSet.flatMap(zkUtils.getBrokerInfo)</div><div class="line">            <span class="comment">//note: ZK 中的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> curBrokerIds = curBrokers.map(_.id)</div><div class="line">            <span class="comment">//note: Controller 缓存中的 broker 列表</span></div><div class="line">            <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</div><div class="line">            <span class="comment">//note: 新上线的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokerIds = curBrokerIds -- liveOrShuttingDownBrokerIds</div><div class="line">            <span class="comment">//note: 掉线的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> deadBrokerIds = liveOrShuttingDownBrokerIds -- curBrokerIds</div><div class="line">            <span class="comment">//note: 新上线的 Broker 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokers = curBrokers.filter(broker =&gt; newBrokerIds(broker.id))</div><div class="line">            controllerContext.liveBrokers = curBrokers <span class="comment">//note: 更新缓存中当前 broker 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokerIdsSorted = newBrokerIds.toSeq.sorted</div><div class="line">            <span class="keyword">val</span> deadBrokerIdsSorted = deadBrokerIds.toSeq.sorted</div><div class="line">            <span class="keyword">val</span> liveBrokerIdsSorted = curBrokerIds.toSeq.sorted</div><div class="line">            info(<span class="string">"Newly added brokers: %s, deleted brokers: %s, all live brokers: %s"</span></div><div class="line">              .format(newBrokerIdsSorted.mkString(<span class="string">","</span>), deadBrokerIdsSorted.mkString(<span class="string">","</span>), liveBrokerIdsSorted.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="comment">//note: Broker 上线, 在 Controller Channel Manager 中添加该 broker</span></div><div class="line">            newBrokers.foreach(controllerContext.controllerChannelManager.addBroker)</div><div class="line">            <span class="comment">//note: Broker 下线处理, 在 Controller Channel Manager 移除该 broker</span></div><div class="line">            deadBrokerIds.foreach(controllerContext.controllerChannelManager.removeBroker)</div><div class="line">            <span class="keyword">if</span>(newBrokerIds.nonEmpty) <span class="comment">//note: 启动该 Broker</span></div><div class="line">              controller.onBrokerStartup(newBrokerIdsSorted)</div><div class="line">            <span class="keyword">if</span>(deadBrokerIds.nonEmpty) <span class="comment">//note: broker 掉线后开始 leader 选举</span></div><div class="line">              controller.onBrokerFailure(deadBrokerIdsSorted)</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling broker changes"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里需要重点关注 <code>doHandleChildChange()</code> 方法的实现，该方法处理逻辑如下：</p>
<ol>
<li>从 ZK 获取当前的 Broker 列表（<code>curBrokers</code>）及 broker id 的列表（<code>curBrokerIds</code>）；</li>
<li>获取当前 Controller 中缓存的 broker id 列表（<code>liveOrShuttingDownBrokerIds</code>）；</li>
<li>获取新上线 broker id 列表：<code>newBrokerIds</code> = <code>curBrokerIds</code> – <code>liveOrShuttingDownBrokerIds</code>；</li>
<li>获取掉线的 broker id 列表：<code>deadBrokerIds</code> = <code>liveOrShuttingDownBrokerIds</code> – <code>curBrokerIds</code>；</li>
<li>对于新上线的 broker，先在 ControllerChannelManager 中添加该 broker（即建立与该 Broker 的连接、初始化相应的发送线程和请求队列），最后 Controller 调用 <code>onBrokerStartup()</code> 上线该 Broker；</li>
<li>对于掉线的 broker，先在 ControllerChannelManager 中移除该 broker（即关闭与 Broker 的连接、关闭相应的发送线程和清空请求队列），最后 Controller 调用 <code>onBrokerFailure()</code> 下线该 Broker。</li>
</ol>
<p>整体的处理流程如下图所示：</p>
<p><img src="/images/kafka/broker_online_offline.png" alt="Broker 上线下线处理过程"></p>
<h2 id="Broker-上线"><a href="#Broker-上线" class="headerlink" title="Broker 上线"></a>Broker 上线</h2><p>本节主要讲述一台 Broker 上线的过程，如前面图中所示，一台 Broker 上线主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">controllerContext.controllerChannelManager.addBroker</div><div class="line">controller.onBrokerStartup(newBrokerIdsSorted)</div></pre></td></tr></table></figure>
<ol>
<li>在 Controller Channel Manager 中添加该 Broker 节点，主要的内容是：Controller 建立与该 Broker 的连接、初始化相应的请求发送线程与请求队列；</li>
<li>调用 Controller 的 <code>onBrokerStartup()</code> 方法上线该节点。</li>
</ol>
<p>Controller Channel Manager 添加 Broker 的实现如下，这里就不重复讲述了，前面讲述 Controller 服务初始化的文章（ <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager </a>）已经讲述过这部分的内容。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</div><div class="line">  <span class="comment">// be careful here. Maybe the startup() API has already started the request send thread</span></div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(!brokerStateInfo.contains(broker.id)) &#123;</div><div class="line">      addNewBroker(broker)</div><div class="line">      startRequestSendThread(broker.id)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面再看下 Controller 如何在 <code>onBrokerStartup()</code> 方法中实现 Broker 上线操作的，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个是被 副本状态机触发的</span></div><div class="line"><span class="comment">//note: 1. 发送 update-metadata 请求给所有存活的 broker;</span></div><div class="line"><span class="comment">//note: 2. 对于所有 new/offline partition 触发选主操作, 选举成功的, Partition 状态设置为 Online</span></div><div class="line"><span class="comment">//note: 3. 检查是否有分区的重新副本分配分配到了这个台机器上, 如果有, 就进行相应的操作</span></div><div class="line"><span class="comment">//note: 4. 检查这台机器上是否有 Topic 被设置为了删除标志, 如果是, 那么机器启动完成后, 重新尝试删除操作</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerStartup</span></span>(newBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</div><div class="line">  info(<span class="string">"New broker startup callback for %s"</span>.format(newBrokers.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="keyword">val</span> newBrokersSet = newBrokers.toSet <span class="comment">//note: 新启动的 broker</span></div><div class="line">  <span class="comment">// send update metadata request to all live and shutting down brokers. Old brokers will get to know of the new</span></div><div class="line">  <span class="comment">// broker via this update.</span></div><div class="line">  <span class="comment">// In cases of controlled shutdown leaders will not be elected when a new broker comes up. So at least in the</span></div><div class="line">  <span class="comment">// common controlled shutdown case, the metadata will reach the new brokers faster</span></div><div class="line">  <span class="comment">//note: 发送 metadata 更新给所有的 broker, 这样的话旧的 broker 将会知道有机器新上线了</span></div><div class="line">  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line">  <span class="comment">// the very first thing to do when a new broker comes up is send it the entire list of partitions that it is</span></div><div class="line">  <span class="comment">// supposed to host. Based on that the broker starts the high watermark threads for the input list of partitions</span></div><div class="line">  <span class="comment">//note:  获取这个机器上的所有 replica 请求</span></div><div class="line">  <span class="keyword">val</span> allReplicasOnNewBrokers = controllerContext.replicasOnBrokers(newBrokersSet)</div><div class="line">  <span class="comment">//note: 将这些副本的状态设置为 OnlineReplica</span></div><div class="line">  replicaStateMachine.handleStateChanges(allReplicasOnNewBrokers, <span class="type">OnlineReplica</span>)</div><div class="line">  <span class="comment">// when a new broker comes up, the controller needs to trigger leader election for all new and offline partitions</span></div><div class="line">  <span class="comment">// to see if these brokers can become leaders for some/all of those</span></div><div class="line">  <span class="comment">//note: 新的 broker 上线也会触发所有处于 new/offline 的 partition 进行 leader 选举</span></div><div class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</div><div class="line">  <span class="comment">// check if reassignment of some partitions need to be restarted</span></div><div class="line">  <span class="comment">//note: 检查是否副本的重新分配分配到了这台机器上</span></div><div class="line">  <span class="keyword">val</span> partitionsWithReplicasOnNewBrokers = controllerContext.partitionsBeingReassigned.filter &#123;</div><div class="line">    <span class="keyword">case</span> (_, reassignmentContext) =&gt; reassignmentContext.newReplicas.exists(newBrokersSet.contains(_))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 如果需要副本进行迁移的话,就执行副本迁移操作</span></div><div class="line">  partitionsWithReplicasOnNewBrokers.foreach(p =&gt; onPartitionReassignment(p._1, p._2))</div><div class="line">  <span class="comment">// check if topic deletion needs to be resumed. If at least one replica that belongs to the topic being deleted exists</span></div><div class="line">  <span class="comment">// on the newly restarted brokers, there is a chance that topic deletion can resume</span></div><div class="line">  <span class="comment">//note: 检查 topic 删除操作是否需要重新启动</span></div><div class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnNewBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123;</div><div class="line">    info((<span class="string">"Some replicas %s for topics scheduled for deletion %s are on the newly restarted brokers %s. "</span> +</div><div class="line">      <span class="string">"Signaling restart of topic deletion for these topics"</span>).format(replicasForTopicsToBeDeleted.mkString(<span class="string">","</span>),</div><div class="line">      deleteTopicManager.topicsToBeDeleted.mkString(<span class="string">","</span>), newBrokers.mkString(<span class="string">","</span>)))</div><div class="line">    deleteTopicManager.resumeDeletionForTopics(replicasForTopicsToBeDeleted.map(_.topic))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>onBrokerStartup()</code> 方法在实现的逻辑上分为以下几步：</p>
<ol>
<li>调用 <code>sendUpdateMetadataRequest()</code> 方法向当前集群所有存活的 Broker 发送 Update Metadata 请求，这样的话其他的节点就会知道当前的 Broker 已经上线了；</li>
<li>获取当前节点分配的所有的 Replica 列表，并将其状态转移为 OnlineReplica 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 leader 选举，如果 leader 选举成功，那么该 Partition 的状态就会转移到 OnlinePartition 状态，否则状态转移失败；</li>
<li>如果副本迁移中有新的 Replica 落在这台新上线的节点上，那么开始执行副本迁移操作（见<a href="http://matt33.com/2018/06/16/partition-reassignment/">Kafka 源码解析之 Partition 副本迁移实现</a>）;</li>
<li>如果之前由于这个 Topic 设置为删除标志，但是由于其中有 Replica 掉线而导致无法删除，这里在节点启动后，尝试重新执行删除操作。</li>
</ol>
<p>到此为止，一台 Broker 算是真正加入到了 Kafka 的集群中，在上述过程中，涉及到 leader 选举的操作，都会触发 LeaderAndIsr 请求及 Metadata 请求的发送。</p>
<h2 id="Broker-掉线"><a href="#Broker-掉线" class="headerlink" title="Broker 掉线"></a>Broker 掉线</h2><p>本节主要讲述一台 Broker 掉线后的处理过程，正如前面图中所示，一台 Broker 掉线后主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">controllerContext.controllerChannelManager.removeBroker</div><div class="line">controller.onBrokerFailure(deadBrokerIdsSorted)</div></pre></td></tr></table></figure>
<ol>
<li>首先在 Controller Channel Manager 中移除该 Broker 节点，主要的内容是：关闭 Controller  与 Broker 的连接和相应的请求发送线程，并清空请求队列；</li>
<li>调用 Controller 的 <code>onBrokerFailure()</code> 方法下线该节点。</li>
</ol>
<p>Controller Channel Manager 下线 Broker 的处理如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeBroker</span></span>(brokerId: <span class="type">Int</span>) &#123;</div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    removeExistingBroker(brokerStateInfo(brokerId))</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 移除旧的 broker（关闭网络连接、关闭请求发送线程）</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">removeExistingBroker</span></span>(brokerState: <span class="type">ControllerBrokerStateInfo</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    brokerState.networkClient.close()</div><div class="line">    brokerState.messageQueue.clear()</div><div class="line">    brokerState.requestSendThread.shutdown()</div><div class="line">    brokerStateInfo.remove(brokerState.brokerNode.id)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while removing broker by the controller"</span>, e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 Controller Channel Manager 处理完掉线的 Broker 节点后，下面 KafkaController 将会调用 <code>onBrokerFailure()</code> 进行相应的处理，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法会被副本状态机调用（进行 broker 节点下线操作）</span></div><div class="line"><span class="comment">//note: 1. 将 leader 在这台机器上的分区设置为 Offline</span></div><div class="line"><span class="comment">//note: 2. 通过 OfflinePartitionLeaderSelector 为 new/offline partition 选举新的 leader</span></div><div class="line"><span class="comment">//note: 3. leader 选举后,发送 LeaderAndIsr 请求给该分区所有存活的副本;</span></div><div class="line"><span class="comment">//note: 4. 分区选举 leader 后,状态更新为 Online</span></div><div class="line"><span class="comment">//note: 5. 要下线的 broker 上的所有 replica 改为 Offline 状态</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerFailure</span></span>(deadBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</div><div class="line">  info(<span class="string">"Broker failure callback for %s"</span>.format(deadBrokers.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">//note: 从正在下线的 broker 集合中移除已经下线的机器</span></div><div class="line">  <span class="keyword">val</span> deadBrokersThatWereShuttingDown =</div><div class="line">    deadBrokers.filter(id =&gt; controllerContext.shuttingDownBrokerIds.remove(id))</div><div class="line">  info(<span class="string">"Removed %s from list of shutting down brokers."</span>.format(deadBrokersThatWereShuttingDown))</div><div class="line">  <span class="keyword">val</span> deadBrokersSet = deadBrokers.toSet</div><div class="line">  <span class="comment">// trigger OfflinePartition state for all partitions whose current leader is one amongst the dead brokers</span></div><div class="line">  <span class="comment">//note: 1. 将 leader 在这台机器上的、并且未设置删除的分区状态设置为 Offline</span></div><div class="line">  <span class="keyword">val</span> partitionsWithoutLeader = controllerContext.partitionLeadershipInfo.filter(partitionAndLeader =&gt;</div><div class="line">    deadBrokersSet.contains(partitionAndLeader._2.leaderAndIsr.leader) &amp;&amp;</div><div class="line">      !deleteTopicManager.isTopicQueuedUpForDeletion(partitionAndLeader._1.topic)).keySet</div><div class="line">  partitionStateMachine.handleStateChanges(partitionsWithoutLeader, <span class="type">OfflinePartition</span>)</div><div class="line">  <span class="comment">// trigger OnlinePartition state changes for offline or new partitions</span></div><div class="line">  <span class="comment">//note: 2. 选举 leader, 选举成功后设置为 Online 状态</span></div><div class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</div><div class="line">  <span class="comment">// filter out the replicas that belong to topics that are being deleted</span></div><div class="line">  <span class="comment">//note: 过滤出 replica 在这个机器上、并且没有被设置为删除的 topic 列表</span></div><div class="line">  <span class="keyword">var</span> allReplicasOnDeadBrokers = controllerContext.replicasOnBrokers(deadBrokersSet)</div><div class="line">  <span class="keyword">val</span> activeReplicasOnDeadBrokers = allReplicasOnDeadBrokers.filterNot(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="comment">// handle dead replicas</span></div><div class="line">  <span class="comment">//note: 将这些 replica 状态转为 Offline</span></div><div class="line">  replicaStateMachine.handleStateChanges(activeReplicasOnDeadBrokers, <span class="type">OfflineReplica</span>)</div><div class="line">  <span class="comment">// check if topic deletion state for the dead replicas needs to be updated</span></div><div class="line">  <span class="comment">//note: 过滤设置为删除的 replica</span></div><div class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnDeadBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 将上面这个 topic 列表的 topic 标记为删除失败</span></div><div class="line">    <span class="comment">// it is required to mark the respective replicas in TopicDeletionFailed state since the replica cannot be</span></div><div class="line">    <span class="comment">// deleted when the broker is down. This will prevent the replica from being in TopicDeletionStarted state indefinitely</span></div><div class="line">    <span class="comment">// since topic deletion cannot be retried until at least one replica is in TopicDeletionStarted state</span></div><div class="line">    deleteTopicManager.failReplicaDeletion(replicasForTopicsToBeDeleted)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// If broker failure did not require leader re-election, inform brokers of failed broker</span></div><div class="line">  <span class="comment">// Note that during leader re-election, brokers update their metadata</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithoutLeader.isEmpty) &#123;</div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 对于掉线 Broker 的处理过程主要有以下几步：</p>
<ol>
<li>首先找到 Leader 在该 Broker 上所有 Partition 列表，然后将这些 Partition 的状态全部转移为 OfflinePartition 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 Leader 选举，如果 Leader 选举成功，那么该 Partition 的状态就会迁移到 OnlinePartition 状态，否则状态转移失败（Broker 上线/掉线、Controller 初始化时都会触发这个方法）；</li>
<li>获取在该 Broker 上的所有 Replica 列表，将其状态转移成 OfflineReplica 状态；</li>
<li>过滤出设置为删除、并且有副本在该节点上的 Topic 列表，先将该 Replica 的转移成 ReplicaDeletionIneligible 状态，然后再将该 Topic 标记为非法删除，即因为有 Replica 掉线导致该 Topic 无法删除；</li>
<li>如果 leader 在该 Broker 上所有 Partition 列表不为空，证明有 Partition 的 leader 需要选举，在最后一步会触发全局 metadata 信息的更新。</li>
</ol>
<p>到这里，一台掉线的 Broker 算是真正下线完成了。</p>
<h2 id="Broker-优雅下线"><a href="#Broker-优雅下线" class="headerlink" title="Broker 优雅下线"></a>Broker 优雅下线</h2><p>前面部分是关于通过监听节点变化来实现对 Broker 的上下线，这也是 Kafka 上下线 Broker 的主要流程，但是还有一种情况是：主动关闭 Kafka 服务，这种情况又被称为 Broker 的优雅关闭。</p>
<p>优雅关闭的节点会向 Controller 发送 ControlledShutdownRequest 请求，Controller 在收到这个情况会进行相应的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleControlledShutdownRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></div><div class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></div><div class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></div><div class="line">  <span class="keyword">val</span> controlledShutdownRequest = request.requestObj.asInstanceOf[<span class="type">ControlledShutdownRequest</span>]</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断该连接是否经过认证</span></div><div class="line">  authorizeClusterAction(request)</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理该请求</span></div><div class="line">  <span class="keyword">val</span> partitionsRemaining = controller.shutdownBroker(controlledShutdownRequest.brokerId)</div><div class="line">  <span class="comment">//note: 返回的 response</span></div><div class="line">  <span class="keyword">val</span> controlledShutdownResponse = <span class="keyword">new</span> <span class="type">ControlledShutdownResponse</span>(controlledShutdownRequest.correlationId,</div><div class="line">    <span class="type">Errors</span>.<span class="type">NONE</span>.code, partitionsRemaining)</div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, <span class="keyword">new</span> <span class="type">RequestOrResponseSend</span>(request.connectionId, controlledShutdownResponse)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 在接收这个关闭服务的请求，通过 <code>shutdownBroker()</code> 方法进行处理，实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 优雅地关闭 Broker</span></div><div class="line"><span class="comment">//note: controller 首先决定将这个 broker 上的 leader 迁移到其他可用的机器上</span></div><div class="line"><span class="comment">//note: 返回还没有 leader 的迁移的 TopicPartition 集合</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownBroker</span></span>(id: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (!isActive) &#123;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(<span class="string">"Controller moved to another broker. Aborting controlled shutdown"</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  controllerContext.brokerShutdownLock synchronized &#123; <span class="comment">//note: 拿到 broker shutdown 的唯一锁</span></div><div class="line">    info(<span class="string">"Shutting down broker "</span> + id)</div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 拿到 controllerLock 的排它锁</span></div><div class="line">      <span class="keyword">if</span> (!controllerContext.liveOrShuttingDownBrokerIds.contains(id))</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BrokerNotAvailableException</span>(<span class="string">"Broker id %d does not exist."</span>.format(id))</div><div class="line"></div><div class="line">      controllerContext.shuttingDownBrokerIds.add(id) <span class="comment">//note: 将 broker id 添加到正在关闭的 broker 列表中</span></div><div class="line">      debug(<span class="string">"All shutting down brokers: "</span> + controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>))</div><div class="line">      debug(<span class="string">"Live brokers: "</span> + controllerContext.liveBrokerIds.mkString(<span class="string">","</span>))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 获取这个 broker 上所有 Partition 与副本数的 map</span></div><div class="line">    <span class="keyword">val</span> allPartitionsAndReplicationFactorOnBroker: <span class="type">Set</span>[(<span class="type">TopicAndPartition</span>, <span class="type">Int</span>)] =</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        controllerContext.partitionsOnBroker(id)</div><div class="line">          .map(topicAndPartition =&gt; (topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition).size))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 处理这些 TopicPartition，更新 Partition 或 Replica 的状态，必要时进行 leader 选举</span></div><div class="line">    allPartitionsAndReplicationFactorOnBroker.foreach &#123;</div><div class="line">      <span class="keyword">case</span>(topicAndPartition, replicationFactor) =&gt;</div><div class="line">        <span class="comment">// Move leadership serially to relinquish lock.</span></div><div class="line">        inLock(controllerContext.controllerLock) &#123;</div><div class="line">          controllerContext.partitionLeadershipInfo.get(topicAndPartition).foreach &#123; currLeaderIsrAndControllerEpoch =&gt;</div><div class="line">            <span class="keyword">if</span> (replicationFactor &gt; <span class="number">1</span>) &#123; <span class="comment">//note: 副本数大于1</span></div><div class="line">              <span class="keyword">if</span> (currLeaderIsrAndControllerEpoch.leaderAndIsr.leader == id) &#123; <span class="comment">//note: leader 正好是下线的节点</span></div><div class="line">                <span class="comment">// If the broker leads the topic partition, transition the leader and update isr. Updates zk and</span></div><div class="line">                <span class="comment">// notifies all affected brokers</span></div><div class="line">                <span class="comment">//todo: 这种情况下 Replica 的状态不需要修改么？（Replica 的处理还是通过监听器还实现的,这里只是在服务关闭前进行 leader 切换和停止副本同步）</span></div><div class="line">                <span class="comment">//note: 状态变化（变为 OnlinePartition，并且进行 leader 选举，使用 controlledShutdownPartitionLeaderSelector 算法）</span></div><div class="line">                partitionStateMachine.handleStateChanges(<span class="type">Set</span>(topicAndPartition), <span class="type">OnlinePartition</span>,</div><div class="line">                  controlledShutdownPartitionLeaderSelector)</div><div class="line">              &#125; <span class="keyword">else</span> &#123;</div><div class="line">                <span class="comment">// Stop the replica first. The state change below initiates ZK changes which should take some time</span></div><div class="line">                <span class="comment">// before which the stop replica request should be completed (in most cases)</span></div><div class="line">                <span class="keyword">try</span> &#123; <span class="comment">//note: 要下线的机器停止副本迁移，发送 StopReplica 请求</span></div><div class="line">                  brokerRequestBatch.newBatch()</div><div class="line">                  brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">Seq</span>(id), topicAndPartition.topic,</div><div class="line">                    topicAndPartition.partition, deletePartition = <span class="literal">false</span>)</div><div class="line">                  brokerRequestBatch.sendRequestsToBrokers(epoch)</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> e : <span class="type">IllegalStateException</span> =&gt; &#123;</div><div class="line">                    <span class="comment">// Resign if the controller is in an illegal state</span></div><div class="line">                    error(<span class="string">"Forcing the controller to resign"</span>)</div><div class="line">                    brokerRequestBatch.clear()</div><div class="line">                    controllerElector.resign()</div><div class="line"></div><div class="line">                    <span class="keyword">throw</span> e</div><div class="line">                  &#125;</div><div class="line">                &#125;</div><div class="line">                <span class="comment">// If the broker is a follower, updates the isr in ZK and notifies the current leader</span></div><div class="line">                <span class="comment">//note: 更新这个副本的状态，变为 OfflineReplica</span></div><div class="line">                replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="type">PartitionAndReplica</span>(topicAndPartition.topic,</div><div class="line">                  topicAndPartition.partition, id)), <span class="type">OfflineReplica</span>)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 返回 leader 在这个要下线节点上并且副本数大于 1 的 TopicPartition 集合</span></div><div class="line">    <span class="comment">//note: 在已经进行前面 leader 迁移后</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replicatedPartitionsBrokerLeads</span></span>() = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      trace(<span class="string">"All leaders = "</span> + controllerContext.partitionLeadershipInfo.mkString(<span class="string">","</span>))</div><div class="line">      controllerContext.partitionLeadershipInfo.filter &#123;</div><div class="line">        <span class="keyword">case</span> (topicAndPartition, leaderIsrAndControllerEpoch) =&gt;</div><div class="line">          leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &amp;&amp; controllerContext.partitionReplicaAssignment(topicAndPartition).size &gt; <span class="number">1</span></div><div class="line">      &#125;.keys</div><div class="line">    &#125;</div><div class="line">    replicatedPartitionsBrokerLeads().toSet</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法的处理逻辑如下：</p>
<ol>
<li>先将要下线的 Broker 添加到 shuttingDownBrokerIds 集合中，该集合记录了当前正在进行关闭的 broker 列表；</li>
<li>获取副本在该节点上的所有 Partition 的列表集合；</li>
<li>遍历上述 Partition 列表进行处理：如果该 Partition 的 leader 是要下线的节点，那么通过 PartitionStateMachine 进行状态转移（OnlinePartition –&gt; OnlinePartition）触发 leader 选举，使用的 leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#ControlledShutdownLeaderSelector">ControlledShutdownLeaderSelector</a>，它会选举 isr 中第一个没有正在关闭的 Replica 作为 leader，否则抛出 StateChangeFailedException 异常；</li>
<li>否则的话，即要下线的节点不是 leader，那么就向要下线的节点发送 StopReplica 请求停止副本同步，并将该副本设置为 OfflineReplica 状态，这里对 Replica 进行处理的原因是为了让要下线的机器关闭副本同步流程，这样 Kafka 服务才能正常关闭。</li>
</ol>
<p>我在看这部分的代码是有一个疑问的，那就是如果要下线的节点是 Partition leader 的情况下，并没有对 Replica 进行相应的处理，这里的原因是，这部分 Replica 的处理可以放在 <code>onBrokerFailure()</code> 方法中处理，即使通过优雅下线的方法下线了 Broker，但是监听 ZK 的 BrokerChangeListener 监听器还是会被触发的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 &lt;code&gt;/brokers/ids&lt;/code&gt; 下注册一个节点，节点名字就是 brok
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Partition 副本迁移实现（十八）</title>
    <link href="http://matt33.com/2018/06/16/partition-reassignment/"/>
    <id>http://matt33.com/2018/06/16/partition-reassignment/</id>
    <published>2018-06-16T15:40:00.000Z</published>
    <updated>2018-06-18T05:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Controller 在初始化时，会利用 ZK 的 watch 机制注册很多不同类型的监听器，当监听的事件被触发时，Controller 就会触发相应的操作。</p>
<p>Controller 在初始化时，会注册多种类型的监听器，主要有以下6种：</p>
<ol>
<li>监听 <code>/admin/reassign_partitions</code> 节点，用于分区副本迁移的监听；</li>
<li>监听 <code>/isr_change_notification</code> 节点，用于 Partition Isr 变动的监听，；</li>
<li>监听 <code>/admin/preferred_replica_election</code> 节点，用于需要进行 Partition 最优 leader 选举的监听；</li>
<li>监听 <code>/brokers/topics</code> 节点，用于 Topic 新建的监听；</li>
<li>监听 <code>/brokers/topics/TOPIC_NAME</code> 节点，用于 Topic Partition 扩容的监听；</li>
<li>监听 <code>/admin/delete_topics</code> 节点，用于 Topic 删除的监听；</li>
<li>监听 <code>/brokers/ids</code> 节点，用于 Broker 上下线的监听。</li>
</ol>
<p>本文主要讲解第一部分，也就是 Controller 对 Partition 副本迁移的处理，后续会单独一篇文章讲述 Topic 的新建、扩容和删除，再单独一篇文章讲述 Broker 的上下线，另外两部分将会在对 LeaderAndIsr 请求处理的文章中讲述。</p>
<h2 id="Partition-副本迁移整体流程"><a href="#Partition-副本迁移整体流程" class="headerlink" title="Partition 副本迁移整体流程"></a>Partition 副本迁移整体流程</h2><p>Partition 的副本迁移实际上就是将分区的副本重新分配到不同的代理节点上，如果 zk 中新副本的集合与 Partition 原来的副本集合相同，那么这个副本就不需要重新分配了。</p>
<p>Partition 的副本迁移是通过监听 zk 的 <code>/admin/reassign_partitions</code> 节点触发的，Kafka 也向用户提供相应的脚本工具进行副本迁移，副本迁移的脚本使用方法如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-reassign-partitions.sh --zookeeper XXX --reassignment-json-file XXX.json --execute</div></pre></td></tr></table></figure>
<p>其中 XXX.json 为要进行 Partition 副本迁移的 json 文件，json 文件的格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"version"</span>:<span class="number">1</span>,</div><div class="line">    <span class="attr">"partitions"</span>:[</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">19</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">9</span>,</div><div class="line">                <span class="number">2</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">26</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">2</span>,</div><div class="line">                <span class="number">6</span>,</div><div class="line">                <span class="number">4</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">27</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">5</span>,</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">8</span></div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个 json 文件的意思是将 Topic <code>__consumer_offsets</code> Partition 19 的副本迁移到 {3, 2, 9} 上，Partition 26 的副本迁移到 {6, 2, 4} 上，Partition 27 的副本迁移到 {5, 3, 8} 上。</p>
<p>在调用脚本向 zk 提交 Partition 的迁移计划时，迁移计划更新到 zk 前需要进行一步判断，如果该节点（写入迁移计划的节点）已经存在，即副本迁移还在进行，那么本次副本迁移计划是无法提交的，实现的逻辑如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">executeAssignment</span></span>(zkUtils: <span class="type">ZkUtils</span>, reassignmentJsonString: <span class="type">String</span>, throttle: <span class="type">Long</span> = <span class="number">-1</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsToBeReassigned = parseAndValidate(zkUtils, reassignmentJsonString)</div><div class="line">  <span class="keyword">val</span> reassignPartitionsCommand = <span class="keyword">new</span> <span class="type">ReassignPartitionsCommand</span>(zkUtils, partitionsToBeReassigned.toMap)</div><div class="line"></div><div class="line">  <span class="comment">// If there is an existing rebalance running, attempt to change its throttle</span></div><div class="line">  <span class="comment">//note: 如果副本迁移正在进行,那么这次的副本迁移计划是无法提交的</span></div><div class="line">  <span class="keyword">if</span> (zkUtils.pathExists(<span class="type">ZkUtils</span>.<span class="type">ReassignPartitionsPath</span>)) &#123;</div><div class="line">    println(<span class="string">"There is an existing assignment running."</span>)</div><div class="line">    reassignPartitionsCommand.maybeLimit(throttle)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    printCurrentAssignment(zkUtils, partitionsToBeReassigned)</div><div class="line">    <span class="keyword">if</span> (throttle &gt;= <span class="number">0</span>)</div><div class="line">      println(<span class="type">String</span>.format(<span class="string">"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value."</span>))</div><div class="line">    <span class="comment">//note: 将迁移计划更新到 zk 上</span></div><div class="line">    <span class="keyword">if</span> (reassignPartitionsCommand.reassignPartitions(throttle)) &#123;</div><div class="line">      println(<span class="string">"Successfully started reassignment of partitions."</span>)</div><div class="line">    &#125; <span class="keyword">else</span></div><div class="line">      println(<span class="string">"Failed to reassign partitions %s"</span>.format(partitionsToBeReassigned))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在迁移计划提交到 zk 之后，Controller 的 PartitionsReassignedListener 就会被触发，Controller 开始 Partition 的副本迁移，触发之后 Controller 的处理流程大体如下图所示：</p>
<p><img src="/images/kafka/partition_reassignment.png" alt="Partition 迁移过程"></p>
<h2 id="PartitionsReassignedListener-副本迁移处理"><a href="#PartitionsReassignedListener-副本迁移处理" class="headerlink" title="PartitionsReassignedListener 副本迁移处理"></a>PartitionsReassignedListener 副本迁移处理</h2><p>在 zk 的 <code>/admin/reassign_partitions</code> 节点数据有变化时，就会触发 PartitionsReassignedListener 的 <code>doHandleDataChange()</code> 方法，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 开始进行 partition reassignment 除非这三种情况发生:</span></div><div class="line"><span class="comment">//note: 1. 这个 partition 的 reassignment 之前已经存在, 即正在迁移中;</span></div><div class="line"><span class="comment">//note: 2. new replica 与已经存在的 replicas 相同;</span></div><div class="line"><span class="comment">//note: 3. Partition 所有新分配 replica 都已经 dead;</span></div><div class="line"><span class="comment">//note: 这种情况发生时,会输出一条日志,并从 zk 移除该 Partition 的迁移计划。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionsReassignedListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"PartitionsReassignedListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when some partitions are reassigned by the admin command</div><div class="line">   *</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当一些分区需要进行迁移时</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    debug(<span class="string">"Partitions reassigned listener fired for path %s. Record partitions to be reassigned %s"</span></div><div class="line">      .format(dataPath, data))</div><div class="line">    <span class="keyword">val</span> partitionsReassignmentData = <span class="type">ZkUtils</span>.parsePartitionReassignmentData(data.toString)</div><div class="line">    <span class="keyword">val</span> partitionsToBeReassigned = inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 需要迁移的新副本</span></div><div class="line">      <span class="comment">//note: 过滤掉正在迁移的副本,如果 Partition 正在迁移,这一波迁移完之前不允许再次迁移</span></div><div class="line">      partitionsReassignmentData.filterNot(p =&gt; controllerContext.partitionsBeingReassigned.contains(p._1))</div><div class="line">    &#125;</div><div class="line">    partitionsToBeReassigned.foreach &#123; partitionToBeReassigned =&gt;</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(partitionToBeReassigned._1.topic)) &#123;</div><div class="line">          <span class="comment">//note: 如果这个 topic 已经设置了删除，那么就不会进行迁移了（从需要副本迁移的集合中移除）</span></div><div class="line">          error(<span class="string">"Skipping reassignment of partition %s for topic %s since it is currently being deleted"</span></div><div class="line">            .format(partitionToBeReassigned._1, partitionToBeReassigned._1.topic))</div><div class="line">          controller.removePartitionFromReassignedPartitions(partitionToBeReassigned._1)</div><div class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 添加到需要迁移的副本集合中</span></div><div class="line">          <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">ReassignedPartitionsContext</span>(partitionToBeReassigned._2)</div><div class="line">          controller.initiateReassignReplicasForTopicPartition(partitionToBeReassigned._1, context)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果 Partition 出现下面的情况，将不会进行副本迁移，直接将 Partition 的迁移计划从 ZK 移除：</p>
<ol>
<li>这个 Partition 的 reassignment 之前已经存在, 即正在迁移中;</li>
<li>这个 Partition 新分配的 replica 与之前的 replicas 相同;</li>
<li>这个 Partition 所有新分配 replica 都已经 dead;</li>
<li>这个 Partition 已经被设置了删除标志。</li>
</ol>
<p>对于可以进行副本迁移的 Partition 集合，这里将会调用 Kafka Controller 的 <code>initiateReassignReplicasForTopicPartition()</code> 方法对每个 Partition 进行处理。</p>
<h2 id="副本迁移初始化"><a href="#副本迁移初始化" class="headerlink" title="副本迁移初始化"></a>副本迁移初始化</h2><p>进行了前面的判断后，这个 Partition 满足了可以迁移的条件，Controller 会首先初始化副本迁移的流程，实现如下所示：</p>
<blockquote>
<p>如果 Partition 新分配的 replica 与之前的 replicas 相同，那么不会进行副本迁移，这部分的判断实际上是在这里实现的，前面只是为了更好地讲述。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化 Topic-Partition 的副本迁移</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initiateReassignReplicasForTopicPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>,</div><div class="line">                                      reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</div><div class="line">  <span class="comment">//note: 要迁移的 topic-partition，及新的副本</span></div><div class="line">  <span class="keyword">val</span> newReplicas = reassignedPartitionContext.newReplicas</div><div class="line">  <span class="keyword">val</span> topic = topicAndPartition.topic</div><div class="line">  <span class="keyword">val</span> partition = topicAndPartition.partition</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> assignedReplicasOpt = controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="comment">//note: partition 的 AR</span></div><div class="line">    assignedReplicasOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</div><div class="line">        <span class="keyword">if</span> (assignedReplicas == newReplicas) &#123; <span class="comment">//note: 不需要迁移</span></div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Partition %s to be reassigned is already assigned to replicas"</span>.format(topicAndPartition) +</div><div class="line">            <span class="string">" %s. Ignoring request for partition reassignment"</span>.format(newReplicas.mkString(<span class="string">","</span>)))</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          info(<span class="string">"Handling reassignment of partition %s to new replicas %s"</span>.format(topicAndPartition, newReplicas.mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// first register ISR change listener</span></div><div class="line">          <span class="comment">//note: 首先注册 ISR 监听的变化</span></div><div class="line">          watchIsrChangesForReassignedPartition(topic, partition, reassignedPartitionContext)</div><div class="line">          <span class="comment">//note: 正在迁移 Partition 添加到缓存中</span></div><div class="line">          controllerContext.partitionsBeingReassigned.put(topicAndPartition, reassignedPartitionContext)</div><div class="line">          <span class="comment">// mark topic ineligible for deletion for the partitions being reassigned</span></div><div class="line">          <span class="comment">//note: 设置正在迁移的副本为不能删除</span></div><div class="line">          deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">          <span class="comment">//note: 进行副本迁移</span></div><div class="line">          onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</div><div class="line">        &#125;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Attempt to reassign partition %s that doesn't exist"</span></div><div class="line">        .format(topicAndPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error completing reassignment of partition %s"</span>.format(topicAndPartition), e)</div><div class="line">    <span class="comment">// remove the partition from the admin path to unblock the admin client</span></div><div class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于副本迁移流程初始化如下：</p>
<ol>
<li>通过 <code>watchIsrChangesForReassignedPartition()</code> 方法监控这个 Partition 的 LeaderAndIsr 变化，如果有新的副本数据同步完成，那么 leader 会将其加到 isr 中更新到 zk 中，这时候 Controller 是可以接收到相关的信息通知的；</li>
<li>将正在迁移的 Partition 添加到 partitionsBeingReassigned 中，它会记录当前正在迁移的 Partition 列表；</li>
<li>将要迁移的 Topic 设置为非法删除删除状态，在这个状态的 Topic 是无法进行删除的；</li>
<li>调用 <code>onPartitionReassignment()</code>，进行副本迁移。</li>
</ol>
<p>在第一步中，会向这个 Partition 注册一个额外的监听器，监听其 LeaderAndIsr 信息变化，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: ISR 变动的监听器（这个不是由 leader 主动触发的，而是 controller 自己触发的，主要用于 partition 迁移时，isr 变动的监听处理）</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionsIsrChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</span></span></div><div class="line">                                            reassignedReplicas: <span class="type">Set</span>[<span class="type">Int</span>]) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> &#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controller.controllerContext.zkUtils</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"ReassignedPartitionsIsrChangeListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when some partitions need to move leader to preferred replica</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      debug(<span class="string">"Reassigned partitions isr change listener fired for path %s with children %s"</span>.format(dataPath, data))</div><div class="line">      <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// check if this partition is still being reassigned or not</span></div><div class="line">        <span class="comment">//note: 检查这个副本是不是还在迁移中（这个方法只用于副本迁移中）</span></div><div class="line">        controllerContext.partitionsBeingReassigned.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(reassignedPartitionContext) =&gt;</div><div class="line">            <span class="comment">// need to re-read leader and isr from zookeeper since the zkclient callback doesn't return the Stat object</span></div><div class="line">            <span class="comment">//note: 从 zk 获取最新的 leader 和 isr 信息</span></div><div class="line">            <span class="keyword">val</span> newLeaderAndIsrOpt = zkUtils.getLeaderAndIsrForPartition(topic, partition)</div><div class="line">            newLeaderAndIsrOpt <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Some</span>(leaderAndIsr) =&gt; <span class="comment">// check if new replicas have joined ISR</span></div><div class="line">                <span class="keyword">val</span> caughtUpReplicas = reassignedReplicas &amp; leaderAndIsr.isr.toSet</div><div class="line">                <span class="keyword">if</span>(caughtUpReplicas == reassignedReplicas) &#123; <span class="comment">//note: 新分配的副本已经全部在 isr 中了</span></div><div class="line">                  <span class="comment">// resume the partition reassignment process</span></div><div class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></div><div class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</div><div class="line">                    <span class="string">"Resuming partition reassignment"</span>)</div><div class="line">                  <span class="comment">//note: 再次触发 onPartitionReassignment 方法,副本已经迁移完成</span></div><div class="line">                  controller.onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span> &#123;  <span class="comment">//note: 否则不进行任何处理</span></div><div class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></div><div class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</div><div class="line">                    <span class="string">"Replica(s) %s still need to catch up"</span>.format((reassignedReplicas -- leaderAndIsr.isr.toSet).mkString(<span class="string">","</span>)))</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">None</span> =&gt; error(<span class="string">"Error handling reassignment of partition %s to replicas %s as it was never created"</span></div><div class="line">                .format(topicAndPartition, reassignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">            &#125;</div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling partition reassignment"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果该 Partition 的 LeaderAndIsr 信息有变动，那么就会触发这个 listener 的 <code>doHandleDataChange()</code> 方法：</p>
<ol>
<li>首先检查这个 Partition 是否在还在迁移中，不在的话直接结束流程，因为这个监听器本来就是为了 Partition 副本迁移而服务的；</li>
<li>从 zk 获取最新的 leader 和 isr 信息，如果新分配的副本全部都在 isr 中，那么就再次触发 controller 的 <code>onPartitionReassignment()</code> 方法，再次调用时实际上已经证明了这个 Partition 的副本迁移已经完成，否则的话就会不进行任何处理，等待新分配的所有副本迁移完成。</li>
</ol>
<h2 id="副本迁移"><a href="#副本迁移" class="headerlink" title="副本迁移"></a>副本迁移</h2><p>Partition 副本迁移真正实际处理是在 Controller 的 <code>onPartitionReassignment()</code> 方法完成的，在看这个方法之前，先介绍几个基本的概念（假设一个 Partition 原来的 replica 是 {1、2、3}，新分配的副本列表是：{2、3、4}）：</p>
<ul>
<li>RAR = Reassigned replicas，即新分配的副本列表，也就是 {2、3、4}；</li>
<li>OAR = Original list of replicas for partition，即这个 Partition 原来的副本列表，也就是 {1、2、3}；</li>
<li>AR = current assigned replicas，该 Partition 当前的副本列表，这个会随着阶段的不同而变化；</li>
<li>RAR-OAR：需要创建、数据同步的新副本，也就是 {4}；</li>
<li>OAR-RAR：不需要创建、数据同步的副本，也就是{2、3}</li>
</ul>
<p>这个方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个回调方法被 reassigned partitions listener 触发,当需要进行分区副本迁移时,会在【/admin/reassign_partitions】下创建一个节点来触发操作</span></div><div class="line"><span class="comment">//note: RAR: 重新分配的副本, OAR: 这个分区原来的副本列表, AR: 当前的分配的副本</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPartitionReassignment</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</div><div class="line">  <span class="keyword">val</span> reassignedReplicas = reassignedPartitionContext.newReplicas</div><div class="line">  <span class="keyword">if</span> (!areReplicasInIsr(topicAndPartition.topic, topicAndPartition.partition, reassignedReplicas)) &#123;</div><div class="line">    <span class="comment">//note: 新分配的并没有权限在 isr 中</span></div><div class="line">    info(<span class="string">"New replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</div><div class="line">      <span class="string">"reassigned not yet caught up with the leader"</span>)</div><div class="line">    <span class="comment">//note: RAR-OAR</span></div><div class="line">    <span class="keyword">val</span> newReplicasNotInOldReplicaList = reassignedReplicas.toSet -- controllerContext.partitionReplicaAssignment(topicAndPartition).toSet</div><div class="line">    <span class="comment">//note: RAR+OAR</span></div><div class="line">    <span class="keyword">val</span> newAndOldReplicas = (reassignedPartitionContext.newReplicas ++ controllerContext.partitionReplicaAssignment(topicAndPartition)).toSet</div><div class="line">    <span class="comment">//1. Update AR in ZK with OAR + RAR.</span></div><div class="line">    updateAssignedReplicasForPartition(topicAndPartition, newAndOldReplicas.toSeq)</div><div class="line">    <span class="comment">//2. Send LeaderAndIsr request to every replica in OAR + RAR (with AR as OAR + RAR).</span></div><div class="line">    updateLeaderEpochAndSendRequest(topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition),</div><div class="line">      newAndOldReplicas.toSeq)</div><div class="line">    <span class="comment">//3. replicas in RAR - OAR -&gt; NewReplica</span></div><div class="line">    <span class="comment">//note: 新分配的副本状态更新为 NewReplica（在第二步中发送 LeaderAndIsr 请求时,新的副本会开始创建并且同步数据）</span></div><div class="line">    startNewReplicasForReassignedPartition(topicAndPartition, reassignedPartitionContext, newReplicasNotInOldReplicaList)</div><div class="line">    info(<span class="string">"Waiting for new replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</div><div class="line">      <span class="string">"reassigned to catch up with the leader"</span>)</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 新副本全在 isr 中了</span></div><div class="line">    <span class="comment">//4. Wait until all replicas in RAR are in sync with the leader.</span></div><div class="line">   <span class="comment">//note: 【OAR-RAR】</span></div><div class="line">    <span class="keyword">val</span> oldReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).toSet -- reassignedReplicas.toSet</div><div class="line">    <span class="comment">//5. replicas in RAR -&gt; OnlineReplica</span></div><div class="line">    <span class="comment">//note: RAR 中的副本都在 isr 中了,将副本状态设置为 OnlineReplica</span></div><div class="line">    reassignedReplicas.foreach &#123; replica =&gt;</div><div class="line">      replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="keyword">new</span> <span class="type">PartitionAndReplica</span>(topicAndPartition.topic, topicAndPartition.partition,</div><div class="line">        replica)), <span class="type">OnlineReplica</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//6. Set AR to RAR in memory.</span></div><div class="line">    <span class="comment">//7. Send LeaderAndIsr request with a potential new leader (if current leader not in RAR) and</span></div><div class="line">    <span class="comment">//   a new AR (using RAR) and same isr to every broker in RAR</span></div><div class="line">    <span class="comment">//note: 到这一步,新加入的 replica 已经同步完成,leader和isr都更新到最新的结果</span></div><div class="line">    moveReassignedPartitionLeaderIfRequired(topicAndPartition, reassignedPartitionContext)</div><div class="line">    <span class="comment">//8. replicas in OAR - RAR -&gt; Offline (force those replicas out of isr)</span></div><div class="line">    <span class="comment">//9. replicas in OAR - RAR -&gt; NonExistentReplica (force those replicas to be deleted)</span></div><div class="line">    <span class="comment">//note: 下线旧的副本</span></div><div class="line">    stopOldReplicasOfReassignedPartition(topicAndPartition, reassignedPartitionContext, oldReplicas)</div><div class="line">    <span class="comment">//10. Update AR in ZK with RAR.</span></div><div class="line">    updateAssignedReplicasForPartition(topicAndPartition, reassignedReplicas)</div><div class="line">    <span class="comment">//11. Update the /admin/reassign_partitions path in ZK to remove this partition.</span></div><div class="line">    <span class="comment">//note: partition 迁移完成,从待迁移的集合中移除该 Partition</span></div><div class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</div><div class="line">    info(<span class="string">"Removed partition %s from the list of reassigned partitions in zookeeper"</span>.format(topicAndPartition))</div><div class="line">    controllerContext.partitionsBeingReassigned.remove(topicAndPartition)</div><div class="line">    <span class="comment">//12. After electing leader, the replicas and isr information changes, so resend the update metadata request to every broker</span></div><div class="line">    <span class="comment">//note: 发送 metadata 更新请求给所有存活的 broker</span></div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, <span class="type">Set</span>(topicAndPartition))</div><div class="line">    <span class="comment">// signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed</span></div><div class="line">    <span class="comment">//note: topic 删除恢复（如果当前 topic 设置了删除,之前由于无法删除）</span></div><div class="line">    deleteTopicManager.resumeDeletionForTopics(<span class="type">Set</span>(topicAndPartition.topic))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法整体分为以下12个步骤：</p>
<ol>
<li>把 AR = OAR+RAR （{1、2、3、4}）更新到 zk 及本地 Controller 缓存中;</li>
<li>发送 LeaderAndIsr 给 AR 中每一个副本,并且会强制更新 zk 中 leader 的 epoch;</li>
<li>创建需要新建的副本（【RAR-OAR】，即 {4}）,将其状态设置为 NewReplica；</li>
<li>等待直到 RAR（{2、3、4}） 中的所有副本都在 ISR 中;</li>
<li>把 RAR（{2、3、4}） 中的所有副本设置为 OnReplica 状态;</li>
<li>将缓存中 AR 更新为 RAR（重新分配的副本列表，即 {2、3、4}）;</li>
<li>如果 leader 不在 RAR 中, 就从 RAR 选择对应的 leader, 然后发送 LeaderAndIsr 请求；如果不需要，那么只会更新 leader epoch，然后发送 LeaderAndIsr 请求; 在发送 LeaderAndIsr 请求前设置了 AR=RAR, 这将确保了 leader 在 isr 中不会添加任何 【RAR-OAR】中的副本（old replica，即 {1}）；</li>
<li>将【OAR-RAR】（{1}）中的副本设置为 OfflineReplica 状态，OfflineReplica 状态的变化，将会从 ISR 中删除【OAR-RAR】的副本，更新到 zk 中并发送 LeaderAndIsr 请求给 leader，通知 leader isr 变动。之后再发送 StopReplica 请求（delete=false）给【OAR-RAR】中的副本；</li>
<li>将【OAR-RAR】中的副本设置为 NonExistentReplica 状态。这将发送 StopReplica 请求（delete=true）给【OAR-RAR】中的副本，这些副本将会从本地上删除数据；</li>
<li>在 zk 中更新 AR 为 RAR；</li>
<li>更新 zk 中路径 【/admin/reassign_partitions】信息，移除已经成功迁移的 Partition；</li>
<li>leader 选举之后，这个 replica 和 isr 信息将会变动，发送 metadata 更新给所有的 broker。</li>
</ol>
<p>上面的流程简单来说，就是先创建新的 replica，开始同步数据，等待所有新的分配都加入到了 isr 中后，开始进行 leader 选举（需要的情况下），下线不需要的副本（OAR-RAR），下线完成后将 Partition 的最新 AR （即 RAR）信息更新到 zk 中，最后发送相应的请求给 broker，到这里一个 Partition 的副本迁移算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Co
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本状态机与分区状态机（十七）</title>
    <link href="http://matt33.com/2018/06/16/controller-state-machine/"/>
    <id>http://matt33.com/2018/06/16/controller-state-machine/</id>
    <published>2018-06-16T03:04:14.000Z</published>
    <updated>2018-06-18T07:20:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副本状态机则是记录着当前集群所有 Replica 的状态信息以及如何对 Replica 状态转变进行相应的处理。</p>
<h2 id="ReplicaStateMachine"><a href="#ReplicaStateMachine" class="headerlink" title="ReplicaStateMachine"></a>ReplicaStateMachine</h2><p>ReplicaStateMachine 记录着集群所有 Replica 的状态信息，它决定着一个 replica 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中副本的状态总共有以下七种类型：</p>
<ol>
<li>NewReplica：这种状态下 Controller 可以创建这个 Replica，这种状态下该 Replica 只能作为 follower，它可以是 Replica 删除后的一个临时状态，它有效的前置状态是 NonExistentReplica；</li>
<li>OnlineReplica：一旦这个 Replica 被分配到指定的 Partition 上，并且 Replica 创建完成，那么它将会被置为这个状态，在这个状态下，这个 Replica 既可以作为 leader 也可以作为 follower，它有效的前置状态是  NewReplica、OnlineReplica 或 OfflineReplica；</li>
<li>OfflineReplica：如果一个 Replica 挂掉（所在的节点宕机或者其他情况），该 Replica 将会被转换到这个状态，它有的效前置状态是 NewReplica、OfflineReplica 或者 OnlineReplica；</li>
<li>ReplicaDeletionStarted：Replica 开始删除时被置为的状态，它有效的前置状态是 OfflineReplica；</li>
<li>ReplicaDeletionSuccessful：如果 Replica 在删除时没有遇到任何错误信息，它将被置为这个状态，这个状态代表该 Replica 的数据已经从节点上清除了，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>ReplicaDeletionIneligible：如果 Replica 删除失败，它将会转移到这个状态，这个状态意思是非法删除，也就是删除是无法成功的，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>NonExistentReplica：如果 Replica 删除成功，它将被转移到这个状态，它有效的前置状态是：ReplicaDeletionSuccessful。</li>
</ol>
<p>上面的状态中其中后面4是专门为 Replica 删除而服务的，副本状态机转移图如下所示：</p>
<p><img src="/images/kafka/replica_state.png" alt="副本状态机"></p>
<p>这张图是副本状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 ReplicaStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="ReplicaStateMachine-初始化"><a href="#ReplicaStateMachine-初始化" class="headerlink" title="ReplicaStateMachine 初始化"></a>ReplicaStateMachine 初始化</h3><p>副本状态机初始化的过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 重新选举后触发的操作</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// initialize replica state</span></div><div class="line">  <span class="comment">//note: 初始化 zk 上所有的 Replica 状态信息（replica 存活的话设置为 Online,不存活的设置为 ReplicaDeletionIneligible）</span></div><div class="line">  initializeReplicaState()</div><div class="line">  <span class="comment">// set started flag</span></div><div class="line">  hasStarted.set(<span class="literal">true</span>)</div><div class="line">  <span class="comment">// move all Online replicas to Online</span></div><div class="line">  <span class="comment">//note: 将存活的副本状态转变为 OnlineReplica</span></div><div class="line">  handleStateChanges(controllerContext.allLiveReplicas(), <span class="type">OnlineReplica</span>)</div><div class="line"></div><div class="line">  info(<span class="string">"Started replica state machine with initial state -&gt; "</span> + replicaState.toString())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，ReplicaStateMachine 先调用 <code>initializeReplicaState()</code> 方法初始化集群中所有 Replica 的状态信息，如果 Replica 所在机器是 alive 的，那么将其状态设置为 OnlineReplica，否则设置为 ReplicaDeletionIneligible 状态，这里只是将 Replica 的状态信息更新副本状态机的缓存 <code>replicaState</code> 中，并没有真正进行状态转移的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化所有副本的状态信息</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeReplicaState</span></span>() &#123;</div><div class="line">  <span class="keyword">for</span>((topicPartition, assignedReplicas) &lt;- controllerContext.partitionReplicaAssignment) &#123;</div><div class="line">    <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">    <span class="keyword">val</span> partition = topicPartition.partition</div><div class="line">    assignedReplicas.foreach &#123; replicaId =&gt;</div><div class="line">      <span class="keyword">val</span> partitionAndReplica = <span class="type">PartitionAndReplica</span>(topic, partition, replicaId)</div><div class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(replicaId)) <span class="comment">//note: 如果副本是存活,那么将状态都设置为 OnlineReplica</span></div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">// mark replicas on dead brokers as failed for topic deletion, if they belong to a topic to be deleted.</span></div><div class="line">        <span class="comment">// This is required during controller failover since during controller failover a broker can go down,</span></div><div class="line">        <span class="comment">// so the replicas on that broker should be moved to ReplicaDeletionIneligible to be on the safer side.</span></div><div class="line">        <span class="comment">//note: 将不存活的副本状态设置为 ReplicaDeletionIneligible</span></div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接着第二步调用 <code>handleStateChanges()</code> 将所有存活的副本状态转移为 OnlineReplica 状态，这里才是真正进行状态转移的地方，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 用于处理 Replica 状态的变化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</div><div class="line">                       callbacks: <span class="type">Callbacks</span> = (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build) &#123;</div><div class="line">  <span class="keyword">if</span>(replicas.nonEmpty) &#123;</div><div class="line">    info(<span class="string">"Invoking state change to %s for replicas %s"</span>.format(targetState, replicas.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      brokerRequestBatch.newBatch()</div><div class="line">      <span class="comment">//note: 状态转变</span></div><div class="line">      replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks))</div><div class="line">      <span class="comment">//note: 向 broker 发送相应请求</span></div><div class="line">      brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">    &#125;<span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some replicas to %s state"</span>.format(targetState), e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里是副本状态机 <code>startup()</code> 方法的最后一步，它的目的是将所有 alive 的 Replica 状态转移到 OnlineReplica 状态，由于前面已经这些 alive replica 的状态设置成了 OnlineReplica，所以这里 Replica 的状态转移情况是：<strong>OnlineReplica –&gt; OnlineReplica</strong>，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="副本的状态转移"><a href="#副本的状态转移" class="headerlink" title="副本的状态转移"></a>副本的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewReplica、ReplicaDeletionStarted、ReplicaDeletionIneligible、ReplicaDeletionSuccessful、NonExistentReplica、OnlineReplica 或者 OfflineReplica 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewReplica"><a href="#TargetState-NewReplica" class="headerlink" title="TargetState: NewReplica"></a>TargetState: NewReplica</h4><p>NewReplica 这个状态是 Replica 准备开始创建是的一个状态，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> currState = replicaState.getOrElseUpdate(partitionAndReplica, <span class="type">NonExistentReplica</span>)<span class="comment">//note: Replica 不存在的话,状态初始化为 NonExistentReplica</span></div><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">NonExistentReplica</span>), targetState)<span class="comment">//note: 验证</span></div><div class="line"><span class="comment">// start replica as a follower to the current leader for its partition</span></div><div class="line"><span class="comment">//note: 从 zk 获取 Partition 的 leaderAndIsr 信息</span></div><div class="line"><span class="keyword">val</span> leaderIsrAndControllerEpochOpt = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topic, partition)</div><div class="line">leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">    <span class="keyword">if</span>(leaderIsrAndControllerEpoch.leaderAndIsr.leader == replicaId)<span class="comment">//note: 这个状态的 Replica 不能作为 leader</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Replica %d for partition %s cannot be moved to NewReplica"</span></div><div class="line">        .format(replicaId, topicAndPartition) + <span class="string">"state as it is being requested to become leader"</span>)</div><div class="line">    <span class="comment">//note: 向该 replicaId 发送 LeaderAndIsr 请求,这个方法同时也会向所有的 broker 发送 updateMeta 请求</span></div><div class="line">    brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId),</div><div class="line">                                                        topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">                                                        replicaAssignment)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// new leader request will be sent to this replica when one gets elected</span></div><div class="line">&#125;</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">NewReplica</span>)<span class="comment">//note: 缓存这个 replica 对象的状态</span></div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                  targetState))</div></pre></td></tr></table></figure>
<p>当想要把 Replica 的状态转移为 NewReplica 时，副本状态机的处理逻辑如下：</p>
<ol>
<li>校验 Replica 的前置状态，只有处于 NonExistentReplica 状态的副本才能转移到 NewReplica 状态；</li>
<li>从 zk 中获取该 Topic-Partition 的 LeaderIsrAndControllerEpoch 信息；</li>
<li>如果获取不到上述信息，直接将该 Replica 的状态转移成 NewReplica，然后结束流程（对与新建的 Partition，处于这个状态时，该 Partition 是没有相应的 LeaderAndIsr 信息的）；</li>
<li>获取到 Partition 的 LeaderIsrAndControllerEpoch 信息，如果发现该 Partition 的 leader 是当前副本，那么就抛出 StateChangeFailedException 异常，因为处在这个状态的 Replica 是不能被选举为 leader 的；</li>
<li>获取到了 Partition 的 LeaderIsrAndControllerEpoch 信息，并且该 Partition 的 leader 不是当前 replica，那么向该 Partition 的所有 Replica 添加一个 LeaderAndIsr 请求（添加 LeaderAndIsr 请求时，实际上也会向所有的 Broker 都添加一个 Update-Metadata 请求）；</li>
<li>最后将该 Replica 的状态转移成 NewReplica，然后结束流程。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionStarted"><a href="#TargetState-ReplicaDeletionStarted" class="headerlink" title="TargetState: ReplicaDeletionStarted"></a>TargetState: ReplicaDeletionStarted</h4><p>这是 Replica 开始删除时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">OfflineReplica</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionStarted</span>)</div><div class="line"><span class="comment">// send stop replica command</span></div><div class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,并设置 deletePartition=true</span></div><div class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">true</span>,</div><div class="line">  callbacks.stopReplicaResponseCallback)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>这部分的实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 OfflineReplica 的情况下才能转移到这种状态；</li>
<li>更新向该 Replica 的状态为 ReplicaDeletionStarted；</li>
<li>向该 replica 发送 StopReplica 请求（deletePartition = true），收到这请求后，broker 会从物理存储上删除这个 Replica 的数据内容；</li>
<li>如果请求返回的话会触发其回调函数（这部分会在 topic 删除部分讲解）。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionIneligible"><a href="#TargetState-ReplicaDeletionIneligible" class="headerlink" title="TargetState: ReplicaDeletionIneligible"></a>TargetState: ReplicaDeletionIneligible</h4><p>ReplicaDeletionIneligible 是副本删除失败时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionIneligible。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionSuccessful"><a href="#TargetState-ReplicaDeletionSuccessful" class="headerlink" title="TargetState: ReplicaDeletionSuccessful"></a>TargetState: ReplicaDeletionSuccessful</h4><p>ReplicaDeletionSuccessful 是副本删除成功时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionSuccessful。</li>
</ol>
<h4 id="TargetState-NonExistentReplica"><a href="#TargetState-NonExistentReplica" class="headerlink" title="TargetState: NonExistentReplica"></a>TargetState: NonExistentReplica</h4><p>NonExistentReplica 是副本完全删除、不存在这个副本的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionSuccessful</span>), targetState)</div><div class="line"><span class="comment">// remove this replica from the assigned replicas list for its partition</span></div><div class="line"><span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line"><span class="comment">//note: 从 controller 和副本状态机的缓存中清除这个 Replica 的记录西溪</span></div><div class="line">controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas.filterNot(_ == replicaId))</div><div class="line">replicaState.remove(partitionAndReplica)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionSuccessful 下才能转移这种状态；</li>
<li>在 controller 的 partitionReplicaAssignment 删除这个 Partition 对应的 replica 信息；</li>
<li>从 Controller 和副本状态机中将这个 Topic 从缓存中删除。</li>
</ol>
<h4 id="TargetState-OnlineReplica"><a href="#TargetState-OnlineReplica" class="headerlink" title="TargetState: OnlineReplica"></a>TargetState: OnlineReplica</h4><p>OnlineReplica 是副本正常工作时的状态，此时的 Replica 既可以作为 leader 也可以作为 follower，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica,</div><div class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line">replicaState(partitionAndReplica) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NewReplica</span> =&gt; <span class="comment">//note: NewReplica --&gt; OnlineReplica</span></div><div class="line">    <span class="comment">// add this replica to the assigned replicas list for its partition</span></div><div class="line">    <span class="comment">//note: 向 the assigned replicas list 添加这个 replica（正常情况下这些 replicas 已经更新到 list 中了）</span></div><div class="line">    <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">if</span>(!currentAssignedReplicas.contains(replicaId))</div><div class="line">      controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas :+ replicaId)</div><div class="line">    stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                              .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                      targetState))</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">//note: OnlineReplica/OfflineReplica/ReplicaDeletionIneligible --&gt; OnlineReplica</span></div><div class="line">    <span class="comment">// check if the leader for this partition ever existed</span></div><div class="line">    <span class="comment">//note: 如果该 Partition 的 LeaderIsrAndControllerEpoch 信息存在,那么就更新副本的状态,并发送相应的请求</span></div><div class="line">    controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">        brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">          replicaAssignment)</div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div><div class="line">        stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// that means the partition was never in OnlinePartition state, this means the broker never</span></div><div class="line">        <span class="comment">// started a log for that partition and does not have a high watermark value for this partition</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div></pre></td></tr></table></figure>
<p>从前面的状态转移图中可以看出，当 Replica 处在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，Replica 是可以转移到 OnlineReplica 状态的，下面分两种情况讲述：</p>
<p><strong>NewReplica –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionReplicaAssignment 中获取这个 Partition 的 AR；</li>
<li>如果 Replica 不在 AR 中的话，那么就将其添加到 Partition 的 AR 中；</li>
<li>最后将 Replica 的状态设置为 OnlineReplica 状态。</li>
</ol>
<p><strong>OnlineReplica/OfflineReplica/ReplicaDeletionIneligible –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionLeadershipInfo 中获取 Partition 的 LeaderAndIsr 信息；</li>
<li>如果该信息存在，那么就向这个 Replica 所在 broker 添加这个 Partition 的 LeaderAndIsr 请求，并将 Replica 的状态设置为 OnlineReplica 状态；</li>
<li>否则不做任务处理；</li>
<li>最后更新R Replica 的状态为 OnlineReplica。</li>
</ol>
<h4 id="TargetState-OfflineReplica"><a href="#TargetState-OfflineReplica" class="headerlink" title="TargetState: OfflineReplica"></a>TargetState: OfflineReplica</h4><p>OfflineReplica 是 Replica 所在 Broker 掉线时 Replica 的状态，转移到这种状态的处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica,</div><div class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line"><span class="comment">// send stop replica command to the replica so that it stops fetching from the leader</span></div><div class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,先停止副本同步</span></div><div class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">false</span>)</div><div class="line"><span class="comment">// As an optimization, the controller removes dead replicas from the ISR</span></div><div class="line"><span class="keyword">val</span> leaderAndIsrIsEmpty: <span class="type">Boolean</span> =</div><div class="line">  controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</div><div class="line">      controller.removeReplicaFromIsr(topic, partition, replicaId) <span class="keyword">match</span> &#123; <span class="comment">//note: 从 isr 中移除这个副本（前提是 ISR 有其他有效副本）</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(updatedLeaderIsrAndControllerEpoch) =&gt;</div><div class="line">          <span class="comment">// send the shrunk ISR state change request to all the remaining alive replicas of the partition.</span></div><div class="line">          <span class="comment">//note: 发送 LeaderAndIsr 请求给剩余的其他副本,因为 ISR 变动了</span></div><div class="line">          <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">          <span class="keyword">if</span> (!controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition)) &#123;</div><div class="line">            brokerRequestBatch.addLeaderAndIsrRequestForBrokers(currentAssignedReplicas.filterNot(_ == replicaId),</div><div class="line">              topic, partition, updatedLeaderIsrAndControllerEpoch, replicaAssignment)</div><div class="line">          &#125;</div><div class="line">          replicaState.put(partitionAndReplica, <span class="type">OfflineReplica</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">            .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div><div class="line">          <span class="literal">false</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="literal">true</span></div><div class="line">  &#125;</div><div class="line"><span class="keyword">if</span> (leaderAndIsrIsEmpty &amp;&amp; !controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition))</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(</div><div class="line">    <span class="string">"Failed to change state of replica %d for partition %s since the leader and isr path in zookeeper is empty"</span></div><div class="line">    .format(replicaId, topicAndPartition))</div></pre></td></tr></table></figure>
<p>处理逻辑如下：</p>
<ol>
<li>校验其前置状态，只有 Replica 在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，才能转移到这种状态；</li>
<li>向该 Replica 所在节点发送 StopReplica 请求（deletePartition = false）；</li>
<li>调用 Controller 的 <code>removeReplicaFromIsr()</code> 方法将该 replica 从 Partition 的 isr 移除这个 replica（前提 isr 中还有其他有效副本），然后向该 Partition 的其他副本发送 LeaderAndIsr 请求；</li>
<li>更新这个 Replica 的状态为 OfflineReplica。</li>
</ol>
<h3 id="状态转移触发的条件"><a href="#状态转移触发的条件" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Replica 各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onBrokerStartup()</td>
<td>Broker 启动时，目的是将在该节点的 Replica 状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>新建 Partition 时，Replica 初始化及 Partition 状态变成 OnlinePartition 后，新创建的 Replica 状态也变为 OnlineReplica；</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onPartitionReassignment()</td>
<td>副本迁移完成后，RAR 中的副本设置为 OnlineReplica 状态</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>ReplicaStateMachine 的 startup()</td>
<td>副本状态机刚初始化启动时，将存活的副本状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的  markTopicForDeletionRetry()</td>
<td>将删除失败的 Replica 设置为 OfflineReplica，重新进行删除</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的 startReplicaDeletion()</td>
<td>开始副本删除时，先将副本设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 shutdownBroker() 方法</td>
<td>优雅关闭 broker 时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 onBrokerFailure()</td>
<td>broker 掉线时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>Partition 新建时，当 Partition 状态变为 NewPartition 后，副本的状态变为 NewReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 startNewReplicasForReassignedPartition()</td>
<td>Partition 副本迁移时，将新分配的副本状态设置为 NewReplica；</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>下线副本时，将成功设置为 OfflineReplica 的 Replica 设置为 ReplicaDeletionStarted 状态，开始物理上删除副本数据（也是发送 StopReplica）</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，目的是下线那些 old replica，新的 replica 已经迁移到新分配的副本上了</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>物理将数据成功删除的 Replica 状态会变为这个</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，在下线那些旧 Replica 时的一个状态，删除成功</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>开始副本删除时，删除失败的副本会设置成这个状态</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 副本迁移时，在下线那些旧的 Replica 时的一个状态，删除失败</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>副本删除成功后（状态为 ReplicaDeletionSuccessful），从状态机和 Controller 的缓存中清除该副本的记录；</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本成功迁移、旧副本成功删除后，从状态机和 Controller 的缓存中清除旧副本的记录</td>
</tr>
</tbody>
</table>
<h2 id="PartitionStateMachine"><a href="#PartitionStateMachine" class="headerlink" title="PartitionStateMachine"></a>PartitionStateMachine</h2><p>PartitionStateMachine 记录着集群所有 Partition 的状态信息，它决定着一个 Partition 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中副本的状态总共有以下四种类型：</p>
<ol>
<li>NonExistentPartition：这个代表着这个 Partition 之前没有被创建过或者之前创建了现在又被删除了，它有效的前置状态是 OfflinePartition；</li>
<li>NewPartition：Partition 创建后，它将处于这个状态，这个状态的 Partition 还没有 leader 和 isr，它有效的前置状态是 NonExistentPartition；</li>
<li>OnlinePartition：一旦这个 Partition 的 leader 被选举出来了，它将处于这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition；</li>
<li>OfflinePartition：如果这个 Partition 的 leader 掉线，这个 Partition 将被转移到这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition。</li>
</ol>
<p>分区状态机转移图如下所示：</p>
<p><img src="/images/kafka/partition_state.png" alt="分区状态机"></p>
<p>这张图是分区状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 PartitionStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="PartitionStateMachine-初始化"><a href="#PartitionStateMachine-初始化" class="headerlink" title="PartitionStateMachine 初始化"></a>PartitionStateMachine 初始化</h3><p>PartitionStateMachine 的初始化方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 启动时触发</span></div><div class="line"><span class="comment">//note: 初始化所有 Partition 的状态（从 zk 获取）, 然后对于 new/offline Partition 触发选主（选主成功的话,变为 OnlinePartition）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// initialize partition state</span></div><div class="line">  <span class="comment">//note: 初始化 partition 的状态,如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></div><div class="line">  initializePartitionState()</div><div class="line">  <span class="comment">// set started flag</span></div><div class="line">  hasStarted.set(<span class="literal">true</span>)</div><div class="line">  <span class="comment">// try to move partitions to online state</span></div><div class="line">  <span class="comment">//note: 为所有处理 NewPartition 或 OnlinePartition 状态 Partition 选举 leader</span></div><div class="line">  triggerOnlinePartitionStateChange()</div><div class="line"></div><div class="line">  info(<span class="string">"Started partition state machine with initial state -&gt; "</span> + partitionState.toString())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，PartitionStateMachine 先调用 <code>initializePartitionState()</code> 方法初始化集群中所有 Partition 的状态信息：</p>
<ol>
<li>如果该 Partition 有 LeaderAndIsr 信息，那么如果 Partition leader 所在的机器是 alive 的，那么将其状态设置为 OnlinePartition，否则设置为 OfflinePartition 状态；</li>
<li>如果该 Partition 没有 LeaderAndIsr 信息，那么将其状态设置为 NewPartition。</li>
</ol>
<p>这里只是将 Partition 的状态信息更新分区状态机的缓存 <code>partitionState</code> 中，并没有真正进行状态的转移。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 根据从 zk 获取的所有 Partition,进行状态初始化</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializePartitionState</span></span>() &#123;</div><div class="line">  <span class="keyword">for</span> (topicPartition &lt;- controllerContext.partitionReplicaAssignment.keys) &#123;</div><div class="line">    <span class="comment">// check if leader and isr path exists for partition. If not, then it is in NEW state</span></div><div class="line">    controllerContext.partitionLeadershipInfo.get(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(currentLeaderIsrAndEpoch) =&gt;</div><div class="line">        <span class="comment">// else, check if the leader for partition is alive. If yes, it is in Online state, else it is in Offline state</span></div><div class="line">        <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(currentLeaderIsrAndEpoch.leaderAndIsr.leader))</div><div class="line">          <span class="comment">// leader is alive</span></div><div class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,并且 leader 存活,设置为 OnlinePartition 状态</span></div><div class="line">          partitionState.put(topicPartition, <span class="type">OnlinePartition</span>)</div><div class="line">        <span class="keyword">else</span></div><div class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,但是 leader 不存活,设置为 OfflinePartition 状态</span></div><div class="line">          partitionState.put(topicPartition, <span class="type">OfflinePartition</span>)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">//note: 没有 LeaderAndIsr 信息,设置为 NewPartition 状态（这个 Partition 还没有）</span></div><div class="line">        partitionState.put(topicPartition, <span class="type">NewPartition</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在初始化的第二步，将会调用 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有的状态为 NewPartition/OnlinePartition 的 Partition 进行 leader 选举，选举成功后的话，其状态将会设置为 OnlinePartition，调用的 Leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#OfflinePartitionLeaderSelector">OfflinePartitionLeaderSelector</a>（具体实现参考链接）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是在 controller 选举后或 broker 上线或下线时时触发的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">triggerOnlinePartitionStateChange</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    brokerRequestBatch.newBatch()</div><div class="line">    <span class="comment">// try to move all partitions in NewPartition or OfflinePartition state to OnlinePartition state except partitions</span></div><div class="line">    <span class="comment">// that belong to topics to be deleted</span></div><div class="line">    <span class="comment">//note: 开始为所有状态在 NewPartition or OfflinePartition 状态的 partition 更新状态（除去将要被删除的 topic）</span></div><div class="line">    <span class="keyword">for</span>((topicAndPartition, partitionState) &lt;- partitionState</div><div class="line">        <span class="keyword">if</span> !controller.deleteTopicManager.isTopicQueuedUpForDeletion(topicAndPartition.topic)) &#123;</div><div class="line">      <span class="keyword">if</span>(partitionState.equals(<span class="type">OfflinePartition</span>) || partitionState.equals(<span class="type">NewPartition</span>))</div><div class="line">        <span class="comment">//note: 尝试为处在 OfflinePartition 或 NewPartition 状态的 Partition 选主,成功后转换为 OnlinePartition</span></div><div class="line">        handleStateChange(topicAndPartition.topic, topicAndPartition.partition, <span class="type">OnlinePartition</span>, controller.offlinePartitionSelector,</div><div class="line">                          (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 发送请求给所有的 broker,包括 LeaderAndIsr 请求和 UpdateMetadata 请求（这里只是添加到 Broker 对应的 RequestQueue 中,后台有线程去发送）</span></div><div class="line">    brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some partitions to the online state"</span>, e)</div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> It is not enough to bail out and log an error, it is important to trigger leader election for those partitions</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面方法的目的是为尝试将所有的状态为 NewPartition/OnlinePartition 的 Partition 状态转移到 OnlinePartition，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="分区的状态转移"><a href="#分区的状态转移" class="headerlink" title="分区的状态转移"></a>分区的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewPartition、OfflinePartition、NonExistentPartition 或者 OnlinePartition 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewPartition"><a href="#TargetState-NewPartition" class="headerlink" title="TargetState: NewPartition"></a>TargetState: NewPartition</h4><p>NewPartition 是 Partition 刚创建时的一个状态，其处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果该 Partition 的状态不存在,默认为 NonExistentPartition</span></div><div class="line"><span class="keyword">val</span> currState = partitionState.getOrElseUpdate(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line"><span class="comment">// pre: partition did not exist before this</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</div><div class="line">partitionState.put(topicAndPartition, <span class="type">NewPartition</span>) <span class="comment">//note: 缓存 partition 的状态</span></div><div class="line"><span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</div><div class="line">                                  assignedReplicas))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NonExistentPartition；</li>
<li>将该 Partition 的状态转移为 NewPartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-OnlinePartition"><a href="#TargetState-OnlinePartition" class="headerlink" title="TargetState: OnlinePartition"></a>TargetState: OnlinePartition</h4><p>OnlinePartition 是一个 Partition 正常工作时的状态，这个状态下的 Partition 已经成功选举出了 leader 和 isr 信息，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 判断 Partition 之前的状态是否可以转换为目的状态</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</div><div class="line">partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt; <span class="comment">//note: 新建的 Partition</span></div><div class="line">    <span class="comment">//note: 选举 leader 和 isr,更新到 zk 和 controller 中,如果没有存活的 replica,抛出异常</span></div><div class="line">    <span class="comment">// initialize leader and isr path for new partition</span></div><div class="line">    initializeLeaderAndIsrForPartition(topicAndPartition)</div><div class="line">  <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt; <span class="comment">//note: leader 挂掉的 Partition</span></div><div class="line">    <span class="comment">//note: 进行 leader 选举,更新到 zk 及 controller 缓存中,失败的抛出异常</span></div><div class="line">    electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">  <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></div><div class="line">    <span class="comment">//note:这种只有在 leader 需要重新选举时才会触发</span></div><div class="line">    electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></div><div class="line">&#125;</div><div class="line">partitionState.put(topicAndPartition, <span class="type">OnlinePartition</span>)</div><div class="line"><span class="keyword">val</span> leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s from %s to %s with leader %d"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验这个 Partition 的前置状态，有效的前置状态是：NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>如果前置状态是 NewPartition，那么为该 Partition 选举 leader 和 isr，更新到 zk 和 controller 的缓存中，如果副本没有处于 alive 状态的话，就抛出异常；</li>
<li>如果前置状态是 OnlinePartition，那么只是触发 leader 选举，在 OnlinePartition –&gt; OnlinePartition 这种状态转移时，需要传入 leader 选举的方法，触发该 Partition 的 leader 选举；</li>
<li>如果前置状态是 OfflinePartition，同上，也是触发 leader 选举。</li>
<li>更新 Partition 的状态为 OnlinePartition。</li>
</ol>
<p>对于以上这几种情况，无论前置状态是什么，最后都会触发这个 Partition 的 leader 选举，leader 成功后，都会触发向这个 Partition 的所有 replica 发送 LeaderAndIsr 请求。</p>
<h4 id="TargetState-OfflinePartition"><a href="#TargetState-OfflinePartition" class="headerlink" title="TargetState: OfflinePartition"></a>TargetState: OfflinePartition</h4><p>OfflinePartition 是这个 Partition 的 leader 挂掉时转移的一个状态，如果 Partition 转移到这个状态，那么就意味着这个 Partition 没有了可用 leader。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// pre: partition should be in New or Online state</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OfflinePartition</span>)</div><div class="line"><span class="comment">// should be called when the leader for a partition is no longer alive</span></div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">partitionState.put(topicAndPartition, <span class="type">OfflinePartition</span>)</div><div class="line"><span class="comment">// post: partition has no alive leader</span></div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 OfflinePartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-NonExistentPartition"><a href="#TargetState-NonExistentPartition" class="headerlink" title="TargetState: NonExistentPartition"></a>TargetState: NonExistentPartition</h4><p>NonExistentPartition 代表了已经处于 OfflinePartition 状态的 Partition 已经从 metadata 和 zk 中删除后进入的状态。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// pre: partition should be in Offline state</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">OfflinePartition</span>), <span class="type">NonExistentPartition</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">partitionState.put(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line"><span class="comment">// post: partition state is deleted from all brokers and zookeeper</span></div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 NonExistentPartition 状态，并且更新到缓存中。</li>
</ol>
<h3 id="状态转移触发的条件-1"><a href="#状态转移触发的条件-1" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Partition   各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 shutdownBroker()</td>
<td>优雅关闭 Broker 时调用，因为要下线的节点是 leader，所以需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 新建时，这个是在 Replica 已经变为 NewPartition 状态后进行的，为新建的 Partition 初始化 leader 和 isr</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 onPreferredReplicaElection()</td>
<td>对 Partition 进行最优 leader 选举，目的是触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 moveReassignedPartitionLeaderIfRequired()</td>
<td>分区副本迁移完成后，1. 当前的 leader 不在 RAR 中，需要触发 leader 选举；2. 当前 leader 在 RAR 但是掉线了，也需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 triggerOnlinePartitionStateChange()</td>
<td>当 Controller 重新选举出来或 broker 有变化时，目的为了那些状态为 NewPartition/OfflinePartition 的 Partition 重新选举 leader，选举成功后状态变为 OnlinePartition</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 initializePartitionState()</td>
<td>Controller 初始化时，遍历 zk 的所有的分区，如果有 LeaderAndIsr 信息并且 leader 在 alive broker 上，那么就将状态转为 OnlinePartition。</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>controller 的 onBrokerFailure()</td>
<td>当有 broker 掉线时，将 leader 在这个机器上的 Partition 设置为 OfflinePartition</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，中间会将该 Partition 的状态先转变为 OfflinePartition</td>
</tr>
<tr>
<td>NonExistentPartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，最后会将该 Partition 的状态转移为 NonExistentPartition</td>
</tr>
<tr>
<td>NewPartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 刚创建时的一个中间状态 ，此时还没选举 leader 和设置 isr 信息</td>
</tr>
</tbody>
</table>
<p>上面就是副本状态机与分区状态机的所有内容，这里只是单纯地讲述了一下这两种状态机，后续文章会开始介绍 Controller 一些其他内容，包括 Partition 迁移、Topic 新建、Topic 下线等，这些内容都会用到这篇文章讲述的内容。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 选举及服务启动流程（十六）</title>
    <link href="http://matt33.com/2018/06/15/kafka-controller-start/"/>
    <id>http://matt33.com/2018/06/15/kafka-controller-start/</id>
    <published>2018-06-15T03:04:14.000Z</published>
    <updated>2018-06-23T03:28:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>从本篇文章开始，Kafka 源码解析就正式进入了 Controller 部分，Controller 作为 Kafka Server 端一个重要的组件，它的角色类似于其他分布式系统 Master 的角色，跟其他系统不一样的是，Kafka 集群的任何一台 Broker 都可以作为 Controller，但是在一个集群中同时只会有一个 Controller 是 alive 状态。Controller 在集群中负责的事务很多，比如：集群 meta 信息的一致性保证、Partition leader 的选举、broker 上下线等都是由 Controller 来具体负责。Controller 部分的内容还是比较多的，计划分5篇左右的文章讲述，本文先来看下 Controller 的简介、Controller 的选举、Controller 选举后服务的启动流程以及 Controller 的四种不同 leader 选举机制。分区状态机、副本副本状态机以及对各种 listener 的处理将在后续的文章中展开。</p>
<h2 id="Controller-简介"><a href="#Controller-简介" class="headerlink" title="Controller 简介"></a>Controller 简介</h2><p>在于分布式系统中，总会有一个地方需要对全局 meta 做一个统一的维护，Kafka 的 Controller 就是充当这个角色的。Kafka 简单的框架图如下所示</p>
<p><img src="/images/kafka/kafka-framwoker.png" alt="Kafka架构简图"></p>
<p>Controller 是运行在 Broker 上的，任何一台 Broker 都可以作为 Controller，但是一个集群同时只能存在一个 Controller，也就意味着 Controller 与数据节点是在一起的，Controller 做的主要事情如下：</p>
<ol>
<li>Broker 的上线、下线处理；</li>
<li>新创建的 topic 或已有 topic 的分区扩容，处理分区副本的分配、leader 选举；</li>
<li>管理所有副本的状态机和分区的状态机，处理状态机的变化事件；</li>
<li>topic 删除、副本迁移、leader 切换等处理。</li>
</ol>
<h2 id="Controller-选举过程"><a href="#Controller-选举过程" class="headerlink" title="Controller 选举过程"></a>Controller 选举过程</h2><p>Kafka 的每台 Broker 在启动过程中，都会启动 Controller 服务，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  info(<span class="string">"starting"</span>)</div><div class="line">  <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">  <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">    <span class="comment">/* start kafka controller */</span></div><div class="line">    <span class="comment">//note: 启动 controller</span></div><div class="line">    kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">    kafkaController.startup()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Controller-启动"><a href="#Controller-启动" class="headerlink" title="Controller 启动"></a>Controller 启动</h3><p>Kafka Server 在启动的过程中，都会去启动 Controller 服务，Controller 启动方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 当 broker 的 controller 模块启动时触发,它比并不保证当前 broker 是 controller,它仅仅是注册 registerSessionExpirationListener 和启动 controllerElector</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() = &#123;</div><div class="line">  inLock(controllerContext.controllerLock) &#123;</div><div class="line">    info(<span class="string">"Controller starting up"</span>)</div><div class="line">    registerSessionExpirationListener() <span class="comment">// note: 注册回话失效的监听器</span></div><div class="line">    isRunning = <span class="literal">true</span></div><div class="line">    controllerElector.startup <span class="comment">//note: 启动选举过程</span></div><div class="line">    info(<span class="string">"Controller startup complete"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 在 <code>startup()</code> 方法中主要实现以下两部分功能：</p>
<ol>
<li><code>registerSessionExpirationListener()</code> 方法注册连接 zk 的超时监听器；</li>
<li><code>controllerElector.startup()</code> 方法，监听 zk 上 controller 节点的变化，并触发 controller 选举方法。</li>
</ol>
<h3 id="Controller-选举"><a href="#Controller-选举" class="headerlink" title="Controller 选举"></a>Controller 选举</h3><p>Controller 在启动时，会初始化 ZookeeperLeaderElector 对象，并调用其 <code>startup()</code> 启动相应的流程，具体过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span> </span>&#123;</div><div class="line">  inLock(controllerContext.controllerLock) &#123;</div><div class="line">    controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)</div><div class="line">    elect</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>startup()</code> 方法中，主要做了下面两件事情：</p>
<ol>
<li>监听 zk 的 <code>/controller</code> 节点的数据变化，一旦节点有变化，立刻通过 LeaderChangeListener 的方法进行相应的处理；</li>
<li><code>elect</code> 在 controller 不存在的情况下选举 controller，存在的话，就是从 zk 获取当前的 controller 节点信息。</li>
</ol>
<h4 id="Controller-选举方法-elect"><a href="#Controller-选举方法-elect" class="headerlink" title="Controller 选举方法 elect"></a>Controller 选举方法 elect</h4><p>ZookeeperLeaderElector 的 <code>elect</code> 方法实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 从 zk 获取当前的 controller 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getControllerID</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  controllerContext.zkUtils.readDataMaybeNull(electionPath)._1 <span class="keyword">match</span> &#123;</div><div class="line">     <span class="keyword">case</span> <span class="type">Some</span>(controller) =&gt; <span class="type">KafkaController</span>.parseControllerId(controller)</div><div class="line">     <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="number">-1</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 进行 controller 选举</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">elect</span></span>: <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="keyword">val</span> timestamp = time.milliseconds.toString</div><div class="line">  <span class="keyword">val</span> electString = <span class="type">Json</span>.encode(<span class="type">Map</span>(<span class="string">"version"</span> -&gt; <span class="number">1</span>, <span class="string">"brokerid"</span> -&gt; brokerId, <span class="string">"timestamp"</span> -&gt; timestamp))</div><div class="line"></div><div class="line"> leaderId = getControllerID</div><div class="line">  <span class="comment">/*</span></div><div class="line">   * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,</div><div class="line">   * it's possible that the controller has already been elected when we get here. This check will prevent the following</div><div class="line">   * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.</div><div class="line">   */</div><div class="line">  <span class="keyword">if</span>(leaderId != <span class="number">-1</span>) &#123;</div><div class="line">     debug(<span class="string">"Broker %d has been elected as leader, so stopping the election process."</span>.format(leaderId))</div><div class="line">     <span class="keyword">return</span> amILeader</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,</div><div class="line">                                                    electString,</div><div class="line">                                                    controllerContext.zkUtils.zkConnection.getZookeeper,</div><div class="line">                                                    <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">    zkCheckedEphemeral.create() <span class="comment">//note: 没有异常的话就是创建成功了</span></div><div class="line">    info(brokerId + <span class="string">" successfully elected as leader"</span>)</div><div class="line">    leaderId = brokerId</div><div class="line">    onBecomingLeader() <span class="comment">//note: 成为了 controller</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt; <span class="comment">//note: 在创建时,发现已经有 broker 提前注册成功</span></div><div class="line">      <span class="comment">// If someone else has written the path, then</span></div><div class="line">      leaderId = getControllerID</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (leaderId != <span class="number">-1</span>)</div><div class="line">        debug(<span class="string">"Broker %d was elected as leader instead of broker %d"</span>.format(leaderId, brokerId))</div><div class="line">      <span class="keyword">else</span></div><div class="line">        warn(<span class="string">"A leader has been elected but just resigned, this will result in another round of election"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> e2: <span class="type">Throwable</span> =&gt; <span class="comment">//note: 抛出了其他异常，那么重新选举 controller</span></div><div class="line">      error(<span class="string">"Error while electing or becoming leader on broker %d"</span>.format(brokerId), e2)</div><div class="line">      resign()</div><div class="line">  &#125;</div><div class="line">  amILeader</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">amILeader</span> </span>: <span class="type">Boolean</span> = leaderId == brokerId</div></pre></td></tr></table></figure>
<p>其实现逻辑如下：</p>
<ol>
<li>先获取 zk 的 <code>/cotroller</code> 节点的信息，获取 controller 的 broker id，如果该节点不存在（比如集群刚创建时），那么获取的 controller id 为-1；</li>
<li>如果 controller id 不为-1，即 controller 已经存在，直接结束流程；</li>
<li>如果 controller id 为-1，证明 controller 还不存在，这时候当前 broker 开始在 zk 注册 controller；</li>
<li>如果注册成功，那么当前 broker 就成为了 controller，这时候开始调用 <code>onBecomingLeader()</code> 方法，正式初始化 controller（注意：<strong>controller 节点是临时节点</strong>，如果当前 controller 与 zk 的 session 断开，那么 controller 的临时节点会消失，会触发 controller 的重新选举）；</li>
<li>如果注册失败（刚好 controller 被其他 broker 创建了、抛出异常等），那么直接返回。</li>
</ol>
<p>在这里 controller 算是成功被选举出来了，controller 选举过程实际上就是各个 Broker 抢占式注册该节点，注册成功的便为 Controller。</p>
<h4 id="controller-节点监听-LeaderChangeListener"><a href="#controller-节点监听-LeaderChangeListener" class="headerlink" title="controller 节点监听 LeaderChangeListener"></a>controller 节点监听 LeaderChangeListener</h4><p>LeaderChangeListener 主要是监听 zk 上的 Controller 节点变化，如果该节点内容变化或者节点被删除，那么会触发 <code>handleDataChange()</code> 和 <code>handleDataDeleted()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 监控 controller 内容的变化</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeaderChangeListener</span> <span class="keyword">extends</span> <span class="title">IZkDataListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Called when the leader information stored in zookeeper has changed. Record the new leader in memory</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">Object</span>) &#123;</div><div class="line">    <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">val</span> amILeaderBeforeDataChange = amILeader</div><div class="line">      leaderId = <span class="type">KafkaController</span>.parseControllerId(data.toString)</div><div class="line">      info(<span class="string">"New leader is %d"</span>.format(leaderId))</div><div class="line">      <span class="comment">// The old leader needs to resign leadership if it is no longer the leader</span></div><div class="line">      amILeaderBeforeDataChange &amp;&amp; !amILeader</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 之前是 controller,现在不是了</span></div><div class="line">    <span class="keyword">if</span> (shouldResign)</div><div class="line">      onResigningAsLeader() <span class="comment">//note: 关闭 controller 服务</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader</div><div class="line">   * @throws Exception</div><div class="line">   *             On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 如果之前是 controller,现在这个节点被删除了,那么首先退出 controller 进程,然后开始重新选举 controller</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;</div><div class="line">    <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      debug(<span class="string">"%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"</span></div><div class="line">        .format(brokerId, dataPath))</div><div class="line">      amILeader</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (shouldResign)</div><div class="line">      onResigningAsLeader()</div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      elect</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>处理过程如下：</p>
<ol>
<li>如果 <code>/controller</code> 节点内容变化，那么更新一下 controller 最新的节点信息，如果该节点刚好之前是 controller，现在不是了，那么需要执行 controller 关闭操作，即 <code>onResigningAsLeader()</code> 方法；</li>
<li>如果 <code>/controller</code> 节点被删除，如果该节点刚好之前是 controller，那么需要执行 controller 关闭操作，即 <code>onResigningAsLeader()</code> 方法，然后再执行 <code>elect</code> 方法重新去选举 controller；</li>
</ol>
<h2 id="Controller-服务启动流程"><a href="#Controller-服务启动流程" class="headerlink" title="Controller 服务启动流程"></a>Controller 服务启动流程</h2><p>Controller 节点选举出来之后，ZookeeperLeaderElector 就会调用 <code>onBecomingLeader()</code> 方法初始化 KafkaController 的相关内容，在 KafkaController 对 ZookeeperLeaderElector 的初始化中可以看到 <code>onBecomingLeader()</code> 这个方法实际上是 KafkaController 的 <code>onControllerFailover()</code> 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaController</span></span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controllerElector = <span class="keyword">new</span> <span class="type">ZookeeperLeaderElector</span>(controllerContext, <span class="type">ZkUtils</span>.<span class="type">ControllerPath</span>, onControllerFailover,</div><div class="line">                                                               onControllerResignation, config.brokerId, time) <span class="comment">//note: controller 通过 zk 选举</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: controller 临时节点监控及 controller 选举</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZookeeperLeaderElector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>,</span></span></div><div class="line">                             electionPath: <span class="type">String</span>, //note: 路径是 /controller</div><div class="line">                             onBecomingLeader: () <span class="title">=&gt;</span> <span class="title">Unit</span>, <span class="title">//note</span>: onControllerFailover() 方法</div><div class="line">                             onResigningAsLeader: () =&gt; <span class="type">Unit</span>, <span class="comment">//note: onControllerResignation() 方法</span></div><div class="line">                             brokerId: <span class="type">Int</span>,</div><div class="line">                             time: <span class="type">Time</span>)</div></pre></td></tr></table></figure>
<h3 id="onControllerFailover-启动及初始化"><a href="#onControllerFailover-启动及初始化" class="headerlink" title="onControllerFailover 启动及初始化"></a>onControllerFailover 启动及初始化</h3><p>下面开始进入 KafkaController 正式初始化的讲解过程中，<code>onControllerFailover()</code> 方法实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果当前 Broker 被选为 controller 时, 当被选为 controller,它将会做以下操作</span></div><div class="line"><span class="comment">//note: 1. 注册 controller epoch changed listener;</span></div><div class="line"><span class="comment">//note: 2. controller epoch 自增加1;</span></div><div class="line"><span class="comment">//note: 3. 初始化 KafkaController 的上下文信息 ControllerContext,它包含了当前的 topic、存活的 broker 以及已经存在的 partition 的 leader;</span></div><div class="line"><span class="comment">//note: 4. 启动 controller 的 channel 管理: 建立与其他 broker 的连接的,负责与其他 broker 之间的通信;</span></div><div class="line"><span class="comment">//note: 5. 启动 ReplicaStateMachine（副本状态机,管理副本的状态）;</span></div><div class="line"><span class="comment">//note: 6. 启动 PartitionStateMachine（分区状态机,管理分区的状态）;</span></div><div class="line"><span class="comment">//note: 如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onControllerFailover</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span>(isRunning) &#123;</div><div class="line">    info(<span class="string">"Broker %d starting become controller state transition"</span>.format(config.brokerId))</div><div class="line">    readControllerEpochFromZookeeper() <span class="comment">//note: 从 zk 获取 controllrt 的 epoch 和 zkVersion 值</span></div><div class="line">    incrementControllerEpoch(zkUtils.zkClient) <span class="comment">//note: 更新 Controller 的 epoch 和 zkVersion 值，可能会抛出异常</span></div><div class="line"></div><div class="line">    <span class="comment">// before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks</span></div><div class="line">    <span class="comment">//note: 再从 zk 获取数据初始化前，注册一些关于 broker/topic 的回调监听器</span></div><div class="line">    registerReassignedPartitionsListener() <span class="comment">//note: 监控路径【/admin/reassign_partitions】，分区迁移监听</span></div><div class="line">    registerIsrChangeNotificationListener() <span class="comment">//note: 监控路径【/isr_change_notification】，isr 变动监听</span></div><div class="line">    registerPreferredReplicaElectionListener() <span class="comment">//note: 监听路径【/admin/preferred_replica_election】，最优 leader 选举</span></div><div class="line">    partitionStateMachine.registerListeners()<span class="comment">//note: 监听 Topic 的创建与删除</span></div><div class="line">    replicaStateMachine.registerListeners() <span class="comment">//note: 监听 broker 的上下线</span></div><div class="line"></div><div class="line">    <span class="comment">//note: 初始化 controller 相关的变量信息:包括 alive broker 列表、partition 的详细信息等</span></div><div class="line">    initializeControllerContext() <span class="comment">//note: 初始化 controller 相关的变量信息</span></div><div class="line"></div><div class="line">    <span class="comment">// We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines</span></div><div class="line">    <span class="comment">// are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before</span></div><div class="line">    <span class="comment">// they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and</span></div><div class="line">    <span class="comment">// partitionStateMachine.startup().</span></div><div class="line">    <span class="comment">//note: 在 controller contest 初始化之后,我们需要发送 UpdateMetadata 请求在状态机启动之前,这是因为 broker 需要从 UpdateMetadata 请求</span></div><div class="line">    <span class="comment">//note: 获取当前存活的 broker list, 因为它们需要处理来自副本状态机或分区状态机启动发送的 LeaderAndIsr 请求</span></div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line"></div><div class="line">    <span class="comment">//note: 初始化 replica 的状态信息: replica 是存活状态时是 OnlineReplica, 否则是 ReplicaDeletionIneligible</span></div><div class="line">    replicaStateMachine.startup() <span class="comment">//note: 初始化 replica 的状态信息</span></div><div class="line">    <span class="comment">//note: 初始化 partition 的状态信息:如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></div><div class="line">    <span class="comment">//note: 并状态为 OfflinePartition 的 topic 选举 leader</span></div><div class="line">    partitionStateMachine.startup() <span class="comment">//note: 初始化 partition 的状态信息</span></div><div class="line"></div><div class="line">    <span class="comment">// register the partition change listeners for all existing topics on failover</span></div><div class="line">    <span class="comment">//note: 为所有的 topic 注册 partition change 监听器</span></div><div class="line">    controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">    info(<span class="string">"Broker %d is ready to serve as the new controller with epoch %d"</span>.format(config.brokerId, epoch))</div><div class="line">    maybeTriggerPartitionReassignment() <span class="comment">//note: 触发一次分区副本迁移的操作</span></div><div class="line">    maybeTriggerPreferredReplicaElection() <span class="comment">//note: 触发一次分区的最优 leader 选举操作</span></div><div class="line">    <span class="keyword">if</span> (config.autoLeaderRebalanceEnable) &#123; <span class="comment">//note: 如果开启自动均衡</span></div><div class="line">      info(<span class="string">"starting the partition rebalance scheduler"</span>)</div><div class="line">      autoRebalanceScheduler.startup()</div><div class="line">      autoRebalanceScheduler.schedule(<span class="string">"partition-rebalance-thread"</span>, checkAndTriggerPartitionRebalance,</div><div class="line">        <span class="number">5</span>, config.leaderImbalanceCheckIntervalSeconds.toLong, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">//note: 发送最新的 meta 信息</span></div><div class="line">    &#125;</div><div class="line">    deleteTopicManager.start() <span class="comment">//note: topic 删除线程启动</span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span></div><div class="line">    info(<span class="string">"Controller has been shut down, aborting startup/failover"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>onControllerFailover()</code> 所做的事情如下：</p>
<ol>
<li><code>readControllerEpochFromZookeeper()</code> 方法更新 controller 的 epoch 及 zkVersion 信息，<code>incrementControllerEpoch()</code> 方法将 controller 的 epoch 字增加1，并更新到 zk 中；</li>
<li>在控制器中注册相关的监听器，主要有6类类型，如下面表格中所列；</li>
<li>通过 <code>initializeControllerContext()</code> 方法初始化 Controller 的上下文信息，更新 Controller 的相关缓存信息、并启动 ControllerChannelManager 等；</li>
<li>向所有 alive 的 broker 发送 Update-Metadata 请求，broker 通过这个请求获取当前集群中 alive 的 broker 列表；</li>
<li>启动副本状态机，初始化所有 Replica 的状态信息，如果 Replica 所在节点是 alive 的，那么状态更新为 OnlineReplica, 否则更新为 ReplicaDeletionIneligible；</li>
<li>启动分区状态机，初始化所有 Partition 的状态信息，如果 leader 所在 broker 是 alive 的，那么状态更新为 OnlinePartition，否则更新为 OfflinePartition；</li>
<li>为当前所有 topic 注册一个 PartitionModificationsListener 监听器，监听所有 Topic 分区数的变化；</li>
<li>KafkaController 初始化完成，正式启动；</li>
<li>KafkaController 启动后，触发一次副本迁移，如果需要的情况下；</li>
<li>KafkaController 启动后，触发一次最优 leader 选举操作，如果需要的情况下；</li>
<li>KafkaController 启动后，如果开启了自动 leader 均衡，启动自动 leader 均衡线程，它会根据配置的信息定期运行。</li>
</ol>
<p>KafkaController 需要监听的 zk 节点、触发的监听方法及作用如下：</p>
<table>
<thead>
<tr>
<th>监听方法</th>
<th>监听路径</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>registerReassignedPartitionsListener</td>
<td>/admin/reassign_partitions</td>
<td>用于分区副本迁移</td>
</tr>
<tr>
<td>registerIsrChangeNotificationListener</td>
<td>/isr_change_notification</td>
<td>用于 Partition ISR 变动</td>
</tr>
<tr>
<td>registerPreferredReplicaElectionListener</td>
<td>/admin/preferred_replica_election</td>
<td>用于 Partition 最优 leader 选举</td>
</tr>
<tr>
<td>partitionStateMachine.registerTopicChangeListener()</td>
<td>/brokers/topics</td>
<td>用于 Topic 新建的监听</td>
</tr>
<tr>
<td>partitionStateMachine.registerDeleteTopicListener()</td>
<td>/admin/delete_topics</td>
<td>用于 Topic 删除的监听</td>
</tr>
<tr>
<td>replicaStateMachine.registerBrokerChangeListener()</td>
<td>/brokers/ids</td>
<td>用于 broker 上下线的监听</td>
</tr>
<tr>
<td>partitionStateMachine.registerPartitionChangeListener(topic)</td>
<td>/brokers/topics/TOPIC_NAME</td>
<td>用于 Topic Partition 扩容的监听</td>
</tr>
</tbody>
</table>
<p>在 KafkaController 中</p>
<ul>
<li>有两个状态机：分区状态机和副本状态机；</li>
<li>一个管理器：Channel 管理器，负责管理所有的 Broker 通信；</li>
<li>相关缓存：Partition 信息、Topic 信息、broker id 信息等；</li>
<li>四种 leader 选举机制：分别是用 leader offline、broker 掉线、partition reassign、最优 leader 选举时触发；</li>
</ul>
<p>如下图所示：</p>
<p><img src="/images/kafka/controller-cache.png" alt="Kafka Controller 的重要内容"></p>
<h3 id="initializeControllerContext-初始化-Controller-上下文信息"><a href="#initializeControllerContext-初始化-Controller-上下文信息" class="headerlink" title="initializeControllerContext 初始化 Controller 上下文信息"></a>initializeControllerContext 初始化 Controller 上下文信息</h3><p>在 <code>initializeControllerContext()</code> 初始化 KafkaController 上下文信息的方法中，主要做了以下事情：</p>
<ol>
<li>从 zk 获取所有 alive broker 列表，记录到 <code>liveBrokers</code>；</li>
<li>从 zk 获取所有的 topic 列表，记录到 <code>allTopic</code> 中；</li>
<li>从 zk 获取所有 Partition 的 replica 信息，更新到 <code>partitionReplicaAssignment</code> 中；</li>
<li>从 zk 获取所有 Partition 的 LeaderAndIsr 信息，更新到 <code>partitionLeadershipInfo</code> 中；</li>
<li>调用 <code>startChannelManager()</code> 启动 Controller 的 Channel Manager；</li>
<li>通过 <code>initializePreferredReplicaElection()</code> 初始化需要最优 leader 选举的 Partition 列表，记录到 <code>partitionsUndergoingPreferredReplicaElection</code> 中；</li>
<li>通过 <code>initializePartitionReassignment()</code> 方法初始化需要进行副本迁移的 Partition 列表，记录到 <code>partitionsBeingReassigned</code> 中；</li>
<li>通过 <code>initializeTopicDeletion()</code> 方法初始化需要删除的 topic 列表及 TopicDeletionManager 对象；</li>
</ol>
<p>综上，这个方法最主要的作用就是相关的 meta 信息及启动 Channel 管理器，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化 KafkaController 的上下文数据</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeControllerContext</span></span>() &#123;</div><div class="line">  <span class="comment">// update controller cache with delete topic information</span></div><div class="line">  controllerContext.liveBrokers = zkUtils.getAllBrokersInCluster().toSet <span class="comment">//note: 初始化 zk 的 broker_list 信息</span></div><div class="line">  controllerContext.allTopics = zkUtils.getAllTopics().toSet <span class="comment">//note: 初始化所有的 topic 信息</span></div><div class="line">  <span class="comment">//note: 初始化所有 topic 的所有 partition 的 replica 分配</span></div><div class="line">  controllerContext.partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(controllerContext.allTopics.toSeq)</div><div class="line">  <span class="comment">//note: 下面两个都是新创建的空集合</span></div><div class="line">  controllerContext.partitionLeadershipInfo = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicAndPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>]</div><div class="line">  controllerContext.shuttingDownBrokerIds = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</div><div class="line">  <span class="comment">// update the leader and isr cache for all existing partitions from Zookeeper</span></div><div class="line">  updateLeaderAndIsrCache() <span class="comment">//note: 获取 topic-partition 的详细信息,更新到 partitionLeadershipInfo 中</span></div><div class="line">  <span class="comment">// start the channel manager</span></div><div class="line">  startChannelManager() <span class="comment">//note: 启动连接所有的 broker 的线程, 根据 broker/ids 的临时去判断要连接哪些 broker</span></div><div class="line">  initializePreferredReplicaElection() <span class="comment">//note: 初始化需要进行最优 leader 选举的 partition</span></div><div class="line">  initializePartitionReassignment() <span class="comment">//note: 初始化需要进行分区副本迁移的 partition</span></div><div class="line">  initializeTopicDeletion() <span class="comment">//note: 初始化要删除的 topic 及后台的 topic 删除线程,还有不能删除的 topic 集合</span></div><div class="line">  info(<span class="string">"Currently active brokers in the cluster: %s"</span>.format(controllerContext.liveBrokerIds))</div><div class="line">  info(<span class="string">"Currently shutting brokers in the cluster: %s"</span>.format(controllerContext.shuttingDownBrokerIds))</div><div class="line">  info(<span class="string">"Current list of topics in the cluster: %s"</span>.format(controllerContext.allTopics))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>最优 leader 选举：就是默认选择 Replica 分配中第一个 replica 作为 leader，为什么叫做最优 leader 选举呢？因为 Kafka 在给每个 Partition 分配副本时，它会保证分区的主副本会均匀分布在所有的 broker 上，这样的话只要保证第一个 replica 被选举为 leader，读写流量就会均匀分布在所有的 Broker 上，当然这是有一个前提的，那就是每个 Partition 的读写流量相差不多，但是在实际的生产环境，这是不太可能的，所以一般情况下，大集群是不建议开自动 leader 均衡的，可以通过额外的算法计算、手动去触发最优 leader 选举。</p>
</blockquote>
<h3 id="Controller-Channel-Manager"><a href="#Controller-Channel-Manager" class="headerlink" title="Controller Channel Manager"></a>Controller Channel Manager</h3><p><code>initializeControllerContext()</code> 方法会通过 <code>startChannelManager()</code> 方法初始化 ControllerChannelManager 对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 启动 ChannelManager 线程</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startChannelManager</span></span>() &#123;</div><div class="line">  controllerContext.controllerChannelManager = <span class="keyword">new</span> <span class="type">ControllerChannelManager</span>(controllerContext, config, time, metrics, threadNamePrefix)</div><div class="line">  controllerContext.controllerChannelManager.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ControllerChannelManager 在初始化时，会为集群中的每个节点初始化一个 ControllerBrokerStateInfo 对象，该对象包含四个部分：</p>
<ol>
<li>NetworkClient：网络连接对象；</li>
<li>Node：节点信息；</li>
<li>BlockingQueue：请求队列；</li>
<li>RequestSendThread：请求的发送线程。</li>
</ol>
<p>其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 控制所有已经存活 broker 的网络连接</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControllerChannelManager</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>, config: <span class="type">KafkaConfig</span>, time: <span class="type">Time</span>, metrics: <span class="type">Metrics</span>, threadNamePrefix: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">protected</span> <span class="keyword">val</span> brokerStateInfo = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ControllerBrokerStateInfo</span>]</div><div class="line">  controllerContext.liveBrokers.foreach(addNewBroker) <span class="comment">//note: 获取目前已经存活的所有 broker</span></div><div class="line">  <span class="comment">//note: 添加一个新的 broker（初始化时,这个方法相当于连接当前存活的所有 broker）</span></div><div class="line">  <span class="comment">//note: 建立网络连接、启动请求发送线程</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addNewBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</div><div class="line">    <span class="keyword">val</span> messageQueue = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">QueueItem</span>]</div><div class="line">    debug(<span class="string">"Controller %d trying to connect to broker %d"</span>.format(config.brokerId, broker.id))</div><div class="line">    <span class="keyword">val</span> brokerEndPoint = broker.getBrokerEndPoint(config.interBrokerListenerName)</div><div class="line">    <span class="keyword">val</span> brokerNode = <span class="keyword">new</span> <span class="type">Node</span>(broker.id, brokerEndPoint.host, brokerEndPoint.port)</div><div class="line">    <span class="keyword">val</span> networkClient = &#123; <span class="comment">//note: 初始化 NetworkClient</span></div><div class="line">      <span class="keyword">val</span> channelBuilder = <span class="type">ChannelBuilders</span>.clientChannelBuilder(</div><div class="line">        config.interBrokerSecurityProtocol,</div><div class="line">        <span class="type">LoginType</span>.<span class="type">SERVER</span>,</div><div class="line">        config.values,</div><div class="line">        config.saslMechanismInterBrokerProtocol,</div><div class="line">        config.saslInterBrokerHandshakeRequestEnable</div><div class="line">      )</div><div class="line">      <span class="keyword">val</span> selector = <span class="keyword">new</span> <span class="type">Selector</span>(</div><div class="line">        <span class="type">NetworkReceive</span>.<span class="type">UNLIMITED</span>,</div><div class="line">        <span class="type">Selector</span>.<span class="type">NO_IDLE_TIMEOUT_MS</span>,</div><div class="line">        metrics,</div><div class="line">        time,</div><div class="line">        <span class="string">"controller-channel"</span>,</div><div class="line">        <span class="type">Map</span>(<span class="string">"broker-id"</span> -&gt; broker.id.toString).asJava,</div><div class="line">        <span class="literal">false</span>,</div><div class="line">        channelBuilder</div><div class="line">      )</div><div class="line">      <span class="keyword">new</span> <span class="type">NetworkClient</span>(</div><div class="line">        selector,</div><div class="line">        <span class="keyword">new</span> <span class="type">ManualMetadataUpdater</span>(<span class="type">Seq</span>(brokerNode).asJava),</div><div class="line">        config.brokerId.toString,</div><div class="line">        <span class="number">1</span>,</div><div class="line">        <span class="number">0</span>,</div><div class="line">        <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>,</div><div class="line">        <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>,</div><div class="line">        config.requestTimeoutMs,</div><div class="line">        time,</div><div class="line">        <span class="literal">false</span></div><div class="line">      )</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="string">"Controller-%d-to-broker-%d-send-thread"</span>.format(config.brokerId, broker.id)</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(name) =&gt; <span class="string">"%s:Controller-%d-to-broker-%d-send-thread"</span>.format(name, config.brokerId, broker.id)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> requestThread = <span class="keyword">new</span> <span class="type">RequestSendThread</span>(config.brokerId, controllerContext, messageQueue, networkClient,</div><div class="line">      brokerNode, config, time, threadName) <span class="comment">//note: 初始化 requestThread</span></div><div class="line">    requestThread.setDaemon(<span class="literal">false</span>) <span class="comment">//note: 非守护进程</span></div><div class="line">    brokerStateInfo.put(broker.id, <span class="keyword">new</span> <span class="type">ControllerBrokerStateInfo</span>(networkClient, brokerNode, messageQueue, requestThread))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清楚了上面的逻辑，再来看 KafkaController 部分是如何向 Broker 发送请求的？</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sendRequest(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</div><div class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) = &#123;</div><div class="line">  controllerContext.controllerChannelManager.sendRequest(brokerId, apiKey, request, callback)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>KafkaController 实际上是调用的 ControllerChannelManager 的 <code>sendRequest()</code> 方法向 Broker 发送请求信息，其实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向 broker 发送请求（并没有真正发送,只是添加到对应的 queue 中）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</div><div class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> stateInfoOpt = brokerStateInfo.get(brokerId)</div><div class="line">    stateInfoOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(stateInfo) =&gt;</div><div class="line">        stateInfo.messageQueue.put(<span class="type">QueueItem</span>(apiKey, request, callback))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"Not sending request %s to broker %d, since it is offline."</span>.format(request, brokerId))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它实际上只是把对应的请求添加到该 Broker 对应的 MessageQueue 中，并没有真正的去发送请求，请求的的发送是在 每台 Broker 对应的 RequestSendThread 中处理的。</p>
<h2 id="Controller-原生的四种-leader-选举机制"><a href="#Controller-原生的四种-leader-选举机制" class="headerlink" title="Controller 原生的四种 leader 选举机制"></a>Controller 原生的四种 leader 选举机制</h2><p>KafkaController 在初始化时，也会初始化四种不同的 leader 选举机制，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: partition leader 挂掉时，选举 leader</span></div><div class="line"><span class="keyword">val</span> offlinePartitionSelector = <span class="keyword">new</span> <span class="type">OfflinePartitionLeaderSelector</span>(controllerContext, config)</div><div class="line"><span class="comment">//note: 重新分配分区时，leader 选举</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> reassignedPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ReassignedPartitionLeaderSelector</span>(controllerContext)</div><div class="line"><span class="comment">//note: 使用最优的副本作为 leader</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">PreferredReplicaPartitionLeaderSelector</span>(controllerContext)</div><div class="line"><span class="comment">//note: broker 掉线时，重新选举 leader</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> controlledShutdownPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ControlledShutdownLeaderSelector</span>(controllerContext)</div></pre></td></tr></table></figure>
<p>四种 leader 选举实现类及对应触发条件如下所示：</p>
<table>
<thead>
<tr>
<th>实现</th>
<th>触发条件</th>
</tr>
</thead>
<tbody>
<tr>
<td>OfflinePartitionLeaderSelector</td>
<td>leader 掉线时触发</td>
</tr>
<tr>
<td>ReassignedPartitionLeaderSelector</td>
<td>分区的副本重新分配数据同步完成后触发的</td>
</tr>
<tr>
<td>PreferredReplicaPartitionLeaderSelector</td>
<td>最优 leader 选举，手动触发或自动 leader 均衡调度时触发</td>
</tr>
<tr>
<td>ControlledShutdownLeaderSelector</td>
<td>broker 发送 ShutDown 请求主动关闭服务时触发</td>
</tr>
</tbody>
</table>
<h3 id="OfflinePartitionLeaderSelector"><a href="#OfflinePartitionLeaderSelector" class="headerlink" title="OfflinePartitionLeaderSelector"></a>OfflinePartitionLeaderSelector</h3><p>OfflinePartitionLeaderSelector Partition leader 选举的逻辑是：</p>
<ol>
<li>如果 isr 中至少有一个副本是存活的，那么从该 Partition 存活的 isr 中选举第一个副本作为新的 leader，存活的 isr 作为新的 isr；</li>
<li>否则，如果脏选举（unclear elect）是禁止的，那么就抛出 NoReplicaOnlineException 异常；</li>
<li>否则，即允许脏选举的情况下，从存活的、所分配的副本（不在 isr 中的副本）中选出一个副本作为新的 leader 和新的 isr 集合；</li>
<li>否则，即是 Partition 分配的副本没有存活的，抛出 NoReplicaOnlineException 异常；</li>
</ol>
<p>一旦 leader 被成功注册到 zk 中，它将会更新到 KafkaController 缓存中的 allLeaders 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于 LeaderAndIsrRequest， 选举一个新的 leader、isr 和 receiving replicas</span></div><div class="line"><span class="comment">//note: 1.如果 isr 中至少有一个副本是存活的，那么存活的 isr 中选举一个副本作为新的 leader，存活的 isr 作为新的 isr；</span></div><div class="line"><span class="comment">//note: 2.否则，如果脏选举（unclear elect）是禁止的，那么就抛出 NoReplicaOnlineException 异常；</span></div><div class="line"><span class="comment">//note: 3.否则，从存活的、所分配的副本中选出一个副本作为新的 leader 和新的 isr 集合；</span></div><div class="line"><span class="comment">//note: 4.否则，partition 分配的副本没有存活的，抛出 NoReplicaOnlineException 异常；</span></div><div class="line"><span class="comment">//note: 一旦 leader 被成功注册到 zk 中，它将更新缓存中的 allLeaders。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">OfflinePartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>, config: <span class="type">KafkaConfig</span></span>)</span></div><div class="line">  <span class="keyword">extends</span> <span class="type">PartitionLeaderSelector</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[OfflinePartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="comment">//note: leader 选举，过程如上面所述</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</div><div class="line">        <span class="comment">//note: AR 中还存活的副本</span></div><div class="line">        <span class="keyword">val</span> liveAssignedReplicas = assignedReplicas.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">        <span class="comment">//note: 当前 isr 中还存活的副本</span></div><div class="line">        <span class="keyword">val</span> liveBrokersInIsr = currentLeaderAndIsr.isr.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">        <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch <span class="comment">//note: epoch</span></div><div class="line">        <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion <span class="comment">//note: zkVersion</span></div><div class="line">        <span class="comment">//note: 选取新的 leader 和 isr</span></div><div class="line">        <span class="keyword">val</span> newLeaderAndIsr =</div><div class="line">          <span class="keyword">if</span> (liveBrokersInIsr.isEmpty) &#123; <span class="comment">//note: 当前 isr 中副本都挂了</span></div><div class="line">            <span class="comment">// Prior to electing an unclean (i.e. non-ISR) leader, ensure that doing so is not disallowed by the configuration</span></div><div class="line">            <span class="comment">// for unclean leader election.</span></div><div class="line">            <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(config.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(controllerContext.zkUtils,</div><div class="line">              <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicAndPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许脏选举的话，抛异常</span></div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>((<span class="string">"No broker in ISR for partition "</span> +</div><div class="line">                <span class="string">"%s is alive. Live brokers are: [%s],"</span>.format(topicAndPartition, controllerContext.liveBrokerIds)) +</div><div class="line">                <span class="string">" ISR brokers are: [%s]"</span>.format(currentLeaderAndIsr.isr.mkString(<span class="string">","</span>)))</div><div class="line">            &#125;</div><div class="line">            debug(<span class="string">"No broker in ISR is alive for %s. Pick the leader from the alive assigned replicas: %s"</span></div><div class="line">              .format(topicAndPartition, liveAssignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="keyword">if</span> (liveAssignedReplicas.isEmpty) &#123; <span class="comment">//note: 副本全挂了，抛异常</span></div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>((<span class="string">"No replica for partition "</span> +</div><div class="line">                <span class="string">"%s is alive. Live brokers are: [%s],"</span>.format(topicAndPartition, controllerContext.liveBrokerIds)) +</div><div class="line">                <span class="string">" Assigned replicas are: [%s]"</span>.format(assignedReplicas))</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 从存活的副本中选举 leader（不能保证选举的是 LEO 最大的副本），并将该副本作为 isr</span></div><div class="line">              <span class="type">ControllerStats</span>.uncleanLeaderElectionRate.mark()</div><div class="line">              <span class="keyword">val</span> newLeader = liveAssignedReplicas.head <span class="comment">//note: 选择第一个作为 leader</span></div><div class="line">              warn(<span class="string">"No broker in ISR is alive for %s. Elect leader %d from live brokers %s. There's potential data loss."</span></div><div class="line">                .format(topicAndPartition, newLeader, liveAssignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">              <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, <span class="type">List</span>(newLeader), currentLeaderIsrZkPathVersion + <span class="number">1</span>)</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 isr 中还有副本存活</span></div><div class="line">            <span class="keyword">val</span> liveReplicasInIsr = liveAssignedReplicas.filter(r =&gt; liveBrokersInIsr.contains(r))</div><div class="line">            <span class="keyword">val</span> newLeader = liveReplicasInIsr.head <span class="comment">//note: 第一个作为 leader</span></div><div class="line">            debug(<span class="string">"Some broker in ISR is alive for %s. Select %d from ISR %s to be the leader."</span></div><div class="line">              .format(topicAndPartition, newLeader, liveBrokersInIsr.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, liveBrokersInIsr.toList, currentLeaderIsrZkPathVersion + <span class="number">1</span>)</div><div class="line">          &#125;</div><div class="line">        info(<span class="string">"Selected new leader and ISR %s for offline partition %s"</span>.format(newLeaderAndIsr.toString(), topicAndPartition))</div><div class="line">        (newLeaderAndIsr, liveAssignedReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"Partition %s doesn't have replicas assigned to it"</span>.format(topicAndPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">`</div></pre></td></tr></table></figure>
<h3 id="ReassignedPartitionLeaderSelector"><a href="#ReassignedPartitionLeaderSelector" class="headerlink" title="ReassignedPartitionLeaderSelector"></a>ReassignedPartitionLeaderSelector</h3><p>ReassignedPartitionLeaderSelector 是在 Partition 副本迁移后，副本同步完成（RAR 都处在 isr 中，RAR 指的是该 Partition 新分配的副本）后触发的，其 leader 选举逻辑如下：</p>
<ol>
<li>leader 选择存活的 RAR 中的第一个副本，此时 RAR 都在 isr 中了；</li>
<li>new isr 是所有存活的 RAR 副本列表；</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 重新分配分区时，partition 的 leader 选举策略</span></div><div class="line"><span class="comment">//note: new leader = 新分配并且在 isr 中的一个副本</span></div><div class="line"><span class="comment">//note: new isr = 当前的 isr</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的副本 = reassigned replicas</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>) <span class="keyword">extends</span> <span class="title">PartitionLeaderSelector</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[ReassignedPartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * The reassigned replicas are already in the ISR when selectLeader is called.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当这个方法被调用时，要求新分配的副本已经在 isr 中了</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="comment">//note: 新分配的 replica 列表</span></div><div class="line">    <span class="keyword">val</span> reassignedInSyncReplicas = controllerContext.partitionsBeingReassigned(topicAndPartition).newReplicas</div><div class="line">    <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch</div><div class="line">    <span class="comment">//note: 当前的 zk version</span></div><div class="line">    <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion</div><div class="line">    <span class="comment">//note: 新分配的 replica 列表，并且其 broker 存活、且在 isr 中</span></div><div class="line">    <span class="keyword">val</span> aliveReassignedInSyncReplicas = reassignedInSyncReplicas.filter(r =&gt; controllerContext.liveBrokerIds.contains(r) &amp;&amp;</div><div class="line">                                                                             currentLeaderAndIsr.isr.contains(r))</div><div class="line">    <span class="comment">//note: 选择第一个作为新的 leader</span></div><div class="line">    <span class="keyword">val</span> newLeaderOpt = aliveReassignedInSyncReplicas.headOption</div><div class="line">    newLeaderOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(newLeader) =&gt; (<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, currentLeaderAndIsr.isr,</div><div class="line">        currentLeaderIsrZkPathVersion + <span class="number">1</span>), reassignedInSyncReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        reassignedInSyncReplicas.size <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"List of reassigned replicas for partition "</span> +</div><div class="line">              <span class="string">" %s is empty. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">          <span class="keyword">case</span> _ =&gt;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"None of the reassigned replicas for partition "</span> +</div><div class="line">              <span class="string">"%s are in-sync with the leader. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="PreferredReplicaPartitionLeaderSelector"><a href="#PreferredReplicaPartitionLeaderSelector" class="headerlink" title="PreferredReplicaPartitionLeaderSelector"></a>PreferredReplicaPartitionLeaderSelector</h3><p>PreferredReplicaPartitionLeaderSelector 是最优 leader 选举，选择 AR（assign replica）中的第一个副本作为 leader，前提是该 replica 在是存活的、并且在 isr 中，否则会抛出 StateChangeFailedException 的异常。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 最优的 leader 选举策略（主要用于自动 leader 均衡，选择 AR 中第一个为 leader，前提是它在 isr 中，这样整个集群的 leader 是均衡的,否则抛出异常）</span></div><div class="line"><span class="comment">//note: new leader = 第一个 replica（alive and in isr）</span></div><div class="line"><span class="comment">//note: new isr = 当前 isr</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的 replica = AR</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PreferredReplicaPartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>) <span class="keyword">extends</span> <span class="title">PartitionLeaderSelector</span></span></div><div class="line"><span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[PreferredReplicaPartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="comment">//note: Partition 的 AR</span></div><div class="line">    <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="comment">//note: preferredReplica，第一个 replica</span></div><div class="line">    <span class="keyword">val</span> preferredReplica = assignedReplicas.head</div><div class="line">    <span class="comment">// check if preferred replica is the current leader</span></div><div class="line">    <span class="comment">//note: 当前的 leader</span></div><div class="line">    <span class="keyword">val</span> currentLeader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">    <span class="keyword">if</span> (currentLeader == preferredReplica) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">LeaderElectionNotNeededException</span>(<span class="string">"Preferred replica %d is already the current leader for partition %s"</span></div><div class="line">                                                   .format(preferredReplica, topicAndPartition))</div><div class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 leader 不是 preferredReplica 的情况</span></div><div class="line">      info(<span class="string">"Current leader %d for partition %s is not the preferred replica."</span>.format(currentLeader, topicAndPartition) +</div><div class="line">        <span class="string">" Triggering preferred replica leader election"</span>)</div><div class="line">      <span class="comment">// check if preferred replica is not the current leader and is alive and in the isr</span></div><div class="line">      <span class="comment">//note: preferredReplica 是 alive 并且在 isr 中</span></div><div class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(preferredReplica) &amp;&amp; currentLeaderAndIsr.isr.contains(preferredReplica)) &#123;</div><div class="line">        (<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(preferredReplica, currentLeaderAndIsr.leaderEpoch + <span class="number">1</span>, currentLeaderAndIsr.isr,</div><div class="line">          currentLeaderAndIsr.zkVersion + <span class="number">1</span>), assignedReplicas)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Preferred replica %d for partition "</span>.format(preferredReplica) +</div><div class="line">          <span class="string">"%s is either not alive or not in the isr. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ControlledShutdownLeaderSelector"><a href="#ControlledShutdownLeaderSelector" class="headerlink" title="ControlledShutdownLeaderSelector"></a>ControlledShutdownLeaderSelector</h3><p>ControlledShutdownLeaderSelector 是在处理 broker 下线时调用的 leader 选举方法，它会选举 isr 中第一个没有正在关闭的 replica 作为 leader，否则抛出 StateChangeFailedException 异常。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Broker 掉线时，重新选举 leader 调用的 leader 选举方法</span></div><div class="line"><span class="comment">//note: new leader = 在 isr 中，并且没有正在 shutdown 的 replica</span></div><div class="line"><span class="comment">//note: new isr = 当前 isr 除去关闭的 replica</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的 replica = 存活的 AR</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControlledShutdownLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>)</span></div><div class="line">        <span class="keyword">extends</span> <span class="type">PartitionLeaderSelector</span></div><div class="line">        <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[ControlledShutdownLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch</div><div class="line">    <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion</div><div class="line"></div><div class="line">    <span class="keyword">val</span> currentLeader = currentLeaderAndIsr.leader</div><div class="line"></div><div class="line">    <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</div><div class="line">    <span class="comment">//note: 存活的 AR</span></div><div class="line">    <span class="keyword">val</span> liveAssignedReplicas = assignedReplicas.filter(r =&gt; liveOrShuttingDownBrokerIds.contains(r))</div><div class="line"></div><div class="line">    <span class="comment">//note: 从当前 isr 中过滤掉正在 shutdown 的 broker</span></div><div class="line">    <span class="keyword">val</span> newIsr = currentLeaderAndIsr.isr.filter(brokerId =&gt; !controllerContext.shuttingDownBrokerIds.contains(brokerId))</div><div class="line">    liveAssignedReplicas.find(newIsr.contains) <span class="keyword">match</span> &#123; <span class="comment">//note: find 方法返回的是第一满足条件的元素，AR 中第一个在 newIsr 集合中的元素被选为 leader</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(newLeader) =&gt;</div><div class="line">        debug(<span class="string">"Partition %s : current leader = %d, new leader = %d"</span>.format(topicAndPartition, currentLeader, newLeader))</div><div class="line">        (<span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, newIsr, currentLeaderIsrZkPathVersion + <span class="number">1</span>), liveAssignedReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>((<span class="string">"No other replicas in ISR %s for %s besides"</span> +</div><div class="line">          <span class="string">" shutting down brokers %s"</span>).format(currentLeaderAndIsr.isr.mkString(<span class="string">","</span>), topicAndPartition, controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>)))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从本篇文章开始，Kafka 源码解析就正式进入了 Controller 部分，Controller 作为 Kafka Server 端一个重要的组件，它的角色类似于其他分布式系统 Master 的角色，跟其他系统不一样的是，Kafka 集群的任何一台 Broker 都可以作
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 ReplicaManager 详解（十五）</title>
    <link href="http://matt33.com/2018/05/01/kafka-replica-manager/"/>
    <id>http://matt33.com/2018/05/01/kafka-replica-manager/</id>
    <published>2018-05-01T04:07:01.000Z</published>
    <updated>2018-05-01T04:35:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类为入口，通过对 ReplicaManager 的详解，顺便再把 Kafka 存储层的内容做一个简单的总结。</p>
<h2 id="ReplicaManager-简介"><a href="#ReplicaManager-简介" class="headerlink" title="ReplicaManager 简介"></a>ReplicaManager 简介</h2><p>前面三篇文章，关于 Produce 请求、Fetch 请求以及副本同步流程的启动都是由 ReplicaManager 来控制的，ReplicaManager 可以说是 Server 端重要的组成部分，回头再仔细看下 KafkaApi 这个类，就会发现 Server 端要处理的多种类型的请求都是 ReplicaManager 来处理的，ReplicaManager 需要处理的请求的有以下六种：</p>
<ol>
<li>LeaderAndIsr 请求；</li>
<li>StopReplica 请求；</li>
<li>UpdateMetadata 请求；</li>
<li>Produce 请求；</li>
<li>Fetch 请求；</li>
<li>ListOffset 请求；</li>
</ol>
<p>其中后面三个已经在前面的文章中介绍过，前面三个都是 Controller 发送的请求，虽然是由 ReplicaManager 中处理的，也会在 Controller 部分展开详细的介绍。</p>
<p>这里先看下面这张图，这张图把 ReplicaManager、Partition、Replica、LogManager、Log、logSegment 这几个抽象的类之间的调用关系简单地串了起来，也算是对存储层这几个重要的部分简单总结了一下：</p>
<p><img src="/images/kafka/replica-manager.png" alt="存储层各个类之间关系"></p>
<p>对着上面的图，简单总结一下：</p>
<ol>
<li>ReplicaManager 是最外层暴露的一个实例，前面说的那几种类型的请求都是由这个实例来处理的；</li>
<li>LogManager 负责管理本节点上所有的日志（Log）实例，它作为 ReplicaManager 的变量传入到了 ReplicaManager 中，ReplicaManager 通过 LogManager 可以对相应的日志实例进行操作；</li>
<li>在 ReplicaManager 中有一个变量：allPartitions，它负责管理本节点所有的 Partition 实例（只要本节点有这个 partition 的日志实例，就会有一个对应的 Partition 对对象实例）；</li>
<li>在创建 Partition 实例时，ReplicaManager 也会作为成员变量传入到 Partition 实例中，Partition 通过 ReplicaManager 可以获得 LogManager 实例、brokerId 等；</li>
<li>Partition 会为它的每一个副本创建一个 Replica 对象实例，但只会为那个在本地副本创建 Log 对象实例（LogManager 不存在这个 Log 对象的情况下，有的话直接引用），这样的话，本地的 Replica 也就与 Log 实例建立了一个联系。</li>
</ol>
<p>关于 ReplicaManager 的 allPartitions 变量可以看下面这张图（假设 Partition 设置的是3副本）：</p>
<p><img src="/images/kafka/all-partition.png" alt="ReplicaManager 的 allPartitions 变量"></p>
<p>allPartitions 管理的 Partition 实例，因为是 3 副本，所以每个 Partition 实例又会管理着三个 Replica，其中只有本地副本（对于上图，就是值 replica.id = 1 的副本）才有对应的 Log 实例对象（HW 和 LEO 的介绍参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Offset 那些事</a>）。</p>
<h2 id="ReplicaManager-启动"><a href="#ReplicaManager-启动" class="headerlink" title="ReplicaManager 启动"></a>ReplicaManager 启动</h2><p>KafkaServer 在启动时，就初始化了 ReplicaManager 实例，如下所示，KafkaServer 在初始化 logManager 后，将 logManager 作为参数传递给了 ReplicaManager。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(isShuttingDown.get)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(startupComplete.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">      brokerState.newState(<span class="type">Starting</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* start scheduler */</span></div><div class="line">      kafkaScheduler.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* setup zookeeper */</span></div><div class="line">      zkUtils = initZk()</div><div class="line"></div><div class="line">      <span class="comment">/* Get or create cluster_id */</span></div><div class="line">      _clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">      info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* generate brokerId */</span></div><div class="line">      config.brokerId =  getBrokerId</div><div class="line">      <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">      <span class="comment">/* start log manager */</span></div><div class="line">      <span class="comment">//note: 启动日志管理线程</span></div><div class="line">      logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">      logManager.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* start replica manager */</span></div><div class="line">      <span class="comment">//note: 启动 replica manager</span></div><div class="line">      replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">        isShuttingDown, quotaManagers.follower)</div><div class="line">      replicaManager.startup()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">      isStartingUp.set(<span class="literal">false</span>)</div><div class="line">      shutdown()</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">    &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h3 id="startup"><a href="#startup" class="headerlink" title="startup"></a>startup</h3><p>ReplicaManager <code>startup()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// start ISR expiration thread</span></div><div class="line">  <span class="comment">// A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR</span></div><div class="line">  <span class="comment">//note: 周期性检查 isr 是否有 replica 过期需要从 isr 中移除</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-expiration"</span>, maybeShrinkIsr, period = config.replicaLagTimeMaxMs / <span class="number">2</span>, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  <span class="comment">//note: 周期性检查是不是有 topic-partition 的 isr 需要变动,如果需要,就更新到 zk 上,来触发 controller</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-change-propagation"</span>, maybePropagateIsrChanges, period = <span class="number">2500</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法与 LogManager 的 <code>startup()</code> 方法类似，也是启动了相应的定时任务，这里，ReplicaManger 启动了两个周期性的任务：</p>
<ol>
<li>maybeShrinkIsr: 判断 topic-partition 的 isr 是否有 replica 因为延迟或 hang 住需要从 isr 中移除；</li>
<li>maybePropagateIsrChanges：判断是不是需要对 isr 进行更新，如果有 topic-partition 的 isr 发生了变动需要进行更新，那么这个方法就会被调用，它会触发 zk 的相应节点，进而触发 controller 进行相应的操作。</li>
</ol>
<p>关于 ReplicaManager 这两个方法的处理过程及 topic-partition isr 变动情况的触发，下面这张流程图做了简单的说明，如下所示：</p>
<p><img src="/images/kafka/replica-manager-startup.png" alt="ReplicaManager 的 Startup 方法启动两个周期性任务及 isr 扩充的情况"></p>
<h3 id="maybeShrinkIsr"><a href="#maybeShrinkIsr" class="headerlink" title="maybeShrinkIsr"></a>maybeShrinkIsr</h3><p>如前面流程图所示， ReplicaManager 的 <code>maybeShrinkIsr()</code> 实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code>  会遍历本节点所有的 Partition 实例，来检查它们 isr 中的 replica 是否需要从 isr 中移除，Partition 中这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查这个 isr 中的每个 replcia</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(replicaMaxLagTimeMs: <span class="type">Long</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123; <span class="comment">//note: 只有本地副本是 leader, 才会做这个操作</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="comment">//note: 检查当前 isr 的副本是否需要从 isr 中移除</span></div><div class="line">        <span class="keyword">val</span> outOfSyncReplicas = getOutOfSyncReplicas(leaderReplica, replicaMaxLagTimeMs)</div><div class="line">        <span class="keyword">if</span>(outOfSyncReplicas.nonEmpty) &#123;</div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas <span class="comment">//note: new isr</span></div><div class="line">          assert(newInSyncReplicas.nonEmpty)</div><div class="line">          info(<span class="string">"Shrinking ISR for partition [%s,%d] from %s to %s"</span>.format(topic, partitionId,</div><div class="line">            inSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>), newInSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// update ISR in zk and in cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新 isr 到 zk</span></div><div class="line">          <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line"></div><div class="line">          replicaManager.isrShrinkRate.mark() <span class="comment">//note: 更新 metrics</span></div><div class="line">          maybeIncrementLeaderHW(leaderReplica) <span class="comment">//note: isr 变动了,判断是否需要更新 partition 的 hw</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="literal">false</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// do nothing if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 检查 isr 中的副本是否需要从 isr 中移除</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOutOfSyncReplicas</span></span>(leaderReplica: <span class="type">Replica</span>, maxLagMs: <span class="type">Long</span>): <span class="type">Set</span>[<span class="type">Replica</span>] = &#123;</div><div class="line">  <span class="comment">//note: 获取那些不应该咋 isr 中副本的列表</span></div><div class="line">  <span class="comment">//note: 1. hang 住的 replica: replica 的 LEO 超过 maxLagMs 没有更新, 那么这个 replica 将会被从 isr 中移除;</span></div><div class="line">  <span class="comment">//note: 2. 数据同步慢的 replica: 副本在 maxLagMs 内没有追上 leader 当前的 LEO, 那么这个 replica 讲会从 ist 中移除;</span></div><div class="line">  <span class="comment">//note: 都是通过 lastCaughtUpTimeMs 来判断的</span></div><div class="line">  <span class="keyword">val</span> candidateReplicas = inSyncReplicas - leaderReplica</div><div class="line"></div><div class="line">  <span class="keyword">val</span> laggingReplicas = candidateReplicas.filter(r =&gt; (time.milliseconds - r.lastCaughtUpTimeMs) &gt; maxLagMs)</div><div class="line">  <span class="keyword">if</span> (laggingReplicas.nonEmpty)</div><div class="line">    debug(<span class="string">"Lagging replicas for partition %s are %s"</span>.format(topicPartition, laggingReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">  laggingReplicas</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code> 这个方法的实现可以简单总结为以下几步：</p>
<ol>
<li>先判断本地副本是不是这个 partition 的 leader，<strong>这个操作只会在 leader 上进行</strong>，如果不是 leader 直接跳过；</li>
<li>通过 <code>getOutOfSyncReplicas()</code> 方法遍历除 leader 外 isr 的所有 replica，找到那些满足条件（<strong>落后超过 maxLagMs 时间的副本</strong>）需要从 isr 中移除的 replica；</li>
<li>得到了新的 isr 列表，调用 <code>updateIsr()</code> 将新的 isr 更新到 zk 上，并且这个方法内部又调用了 ReplicaManager 的 <code>recordIsrChange()</code> 方法来告诉 ReplicaManager 当前这个 topic-partition 的 isr 发生了变化（<strong>可以看出，zk 上这个 topic-partition 的 isr 信息虽然变化了，但是实际上 controller 还是无法感知的</strong>）；</li>
<li>因为 isr 发生了变动，所以这里会通过 <code>maybeIncrementLeaderHW()</code> 方法来检查一下这个 partition 的 HW 是否需要更新。</li>
</ol>
<p><code>updateIsr()</code> 和 <code>maybeIncrementLeaderHW()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查是否需要更新 partition 的 HW,这个方法将在两种情况下触发:</span></div><div class="line"><span class="comment">//note: 1.Partition ISR 变动; 2. 任何副本的 LEO 改变;</span></div><div class="line"><span class="comment">//note: 在获取 HW 时,是从 isr 和认为能追得上的副本中选择最小的 LEO,之所以也要从能追得上的副本中选择,是为了等待 follower 追上 HW,否则可能没机会追上了</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeIncrementLeaderHW</span></span>(leaderReplica: <span class="type">Replica</span>, curTime: <span class="type">Long</span> = time.milliseconds): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取 isr 以及能够追上 isr （认为最近一次 fetch 的时间在 replica.lag.time.max.time 之内） 副本的 LEO 信息。</span></div><div class="line">  <span class="keyword">val</span> allLogEndOffsets = assignedReplicas.filter &#123; replica =&gt;</div><div class="line">    curTime - replica.lastCaughtUpTimeMs &lt;= replicaManager.config.replicaLagTimeMaxMs || inSyncReplicas.contains(replica)</div><div class="line">  &#125;.map(_.logEndOffset)</div><div class="line">  <span class="keyword">val</span> newHighWatermark = allLogEndOffsets.min(<span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>.<span class="type">OffsetOrdering</span>) <span class="comment">//note: 新的 HW</span></div><div class="line">  <span class="keyword">val</span> oldHighWatermark = leaderReplica.highWatermark</div><div class="line">  <span class="keyword">if</span> (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset || oldHighWatermark.onOlderSegment(newHighWatermark)) &#123;</div><div class="line">    leaderReplica.highWatermark = newHighWatermark</div><div class="line">    debug(<span class="string">"High watermark for partition [%s,%d] updated to %s"</span>.format(topic, partitionId, newHighWatermark))</div><div class="line">    <span class="literal">true</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    debug(<span class="string">"Skipping update high watermark since Old hw %s is larger than new hw %s for partition [%s,%d]. All leo's are %s"</span></div><div class="line">      .format(oldHighWatermark, newHighWatermark, topic, partitionId, allLogEndOffsets.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="literal">false</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateIsr</span></span>(newIsr: <span class="type">Set</span>[<span class="type">Replica</span>]) &#123;</div><div class="line">  <span class="keyword">val</span> newLeaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(localBrokerId, leaderEpoch, newIsr.map(r =&gt; r.brokerId).toList, zkVersion)</div><div class="line">  <span class="keyword">val</span> (updateSucceeded,newVersion) = <span class="type">ReplicationUtils</span>.updateLeaderAndIsr(zkUtils, topic, partitionId,</div><div class="line">    newLeaderAndIsr, controllerEpoch, zkVersion) <span class="comment">//note: 执行更新操作</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span>(updateSucceeded) &#123; <span class="comment">//note: 成功更新到 zk 上</span></div><div class="line">    replicaManager.recordIsrChange(topicPartition) <span class="comment">//note: 告诉 replicaManager 这个 partition 的 isr 需要更新</span></div><div class="line">    inSyncReplicas = newIsr</div><div class="line">    zkVersion = newVersion</div><div class="line">    trace(<span class="string">"ISR updated to [%s] and zkVersion updated to [%d]"</span>.format(newIsr.mkString(<span class="string">","</span>), zkVersion))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    info(<span class="string">"Cached zkVersion [%d] not equal to that in zookeeper, skip updating ISR"</span>.format(zkVersion))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="maybePropagateIsrChanges"><a href="#maybePropagateIsrChanges" class="headerlink" title="maybePropagateIsrChanges"></a>maybePropagateIsrChanges</h3><p>ReplicaManager <code>maybePropagateIsrChanges()</code> 方法的作用是将那些 isr 变动的 topic-partition 列表（<code>isrChangeSet</code>）通过 ReplicationUtils 的 <code>propagateIsrChanges()</code> 方法更新 zk 上，这时候 Controller 才能知道哪些 topic-partition 的 isr 发生了变动。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是周期性的运行,来判断 partition 的 isr 是否需要更新,</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybePropagateIsrChanges</span></span>() &#123;</div><div class="line">  <span class="keyword">val</span> now = <span class="type">System</span>.currentTimeMillis()</div><div class="line">  isrChangeSet synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (isrChangeSet.nonEmpty &amp;&amp; <span class="comment">//note:  有 topic-partition 的 isr 需要更新</span></div><div class="line">      (lastIsrChangeMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationBlackOut</span> &lt; now || <span class="comment">//note: 5s 内没有触发过</span></div><div class="line">        lastIsrPropagationMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationInterval</span> &lt; now)) &#123; <span class="comment">//note: 距离上次触发有60s</span></div><div class="line">      <span class="type">ReplicationUtils</span>.propagateIsrChanges(zkUtils, isrChangeSet) <span class="comment">//note: 在 zk 创建 isr 变动的提醒</span></div><div class="line">      isrChangeSet.clear() <span class="comment">//note: 清空 isrChangeSet,它记录着 isr 变动的 topic-partition 信息</span></div><div class="line">      lastIsrPropagationMs.set(now) <span class="comment">//note: 最近一次触发这个方法的时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Partition-ISR-变化"><a href="#Partition-ISR-变化" class="headerlink" title="Partition ISR 变化"></a>Partition ISR 变化</h2><p>前面讲述了 ReplicaManager 周期性调度的两个方法：<code>maybeShrinkIsr()</code> 和 <code>maybePropagateIsrChanges()</code> ，其中 <code>maybeShrinkIsr()</code> 是来检查 isr 中是否有 replica 需要从 isr 中移除，也就是说这个方法只会减少 isr 中的副本数，那么 isr 中副本数的增加是在哪里触发的呢？</p>
<p>观察上面流程图的第三部分，ReplicaManager 在处理来自 replica 的 Fetch 请求时，会将 Fetch 的相关信息到更新 Partition 中，Partition 调用 <code>maybeExpandIsr()</code> 方法来判断 isr 是否需要更新。</p>
<p>举一个例子，一个 topic 的 partition 1有三个副本，其中 replica 1 为 leader replica，那么这个副本之间关系图如下所示：</p>
<p><img src="/images/kafka/partition_replica.png" alt="Leader replica 与 follower replica"></p>
<p>简单分析一下上面的图：</p>
<ol>
<li>对于 replica 1 而言，它是 leader，首先 replica 1 有对应的 Log 实例对象，而且它会记录其他远程副本的 LEO，以便更新这个 Partition 的 HW；</li>
<li>对于 replica 2 而言，它是 follower，replica 2 有对应的 Log 实例对象，它只会有本地的 LEO 和 HW 记录，没有其他副本的 LEO 记录。</li>
<li>replica 2 和 replica 3 从 replica 1 上拉取数据，进行数据同步。</li>
</ol>
<p>再来看前面的流程图，ReplicaManager 在 <code>FetchMessages()</code> 方法对来自副本的 Fetch 请求进行处理的，实际上是会更新相应 replica 的 LEO 信息的，这时候 leader 可以根据副本 LEO 信息的变动来判断 这个副本是否满足加入 isr 的条件，下面详细来看下这个过程。</p>
<h3 id="updateFollowerLogReadResults"><a href="#updateFollowerLogReadResults" class="headerlink" title="updateFollowerLogReadResults"></a>updateFollowerLogReadResults</h3><p>在 ReplicaManager 的 <code>FetchMessages()</code> 方法中，如果 Fetch 请求是来自副本，那么会调用 <code>updateFollowerLogReadResults()</code> 更新远程副本的信息，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的作用就是找到本节点这个 Partition 对象，然后调用其 <code>updateReplicaLogReadResult()</code> 方法更新副本的 LEO 信息和拉取时间信息。</p>
<h3 id="updateReplicaLogReadResult"><a href="#updateReplicaLogReadResult" class="headerlink" title="updateReplicaLogReadResult"></a>updateReplicaLogReadResult</h3><p>这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新这个 partition replica 的 the end offset</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateReplicaLogReadResult</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  getReplica(replicaId) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(replica) =&gt;</div><div class="line">      <span class="comment">//note: 更新副本的信息</span></div><div class="line">      replica.updateLogReadResult(logReadResult)</div><div class="line">      <span class="comment">// check if we need to expand ISR to include this replica</span></div><div class="line">      <span class="comment">// if it is not in the ISR yet</span></div><div class="line">      <span class="comment">//note: 如果该副本不在 isr 中,检查是否需要进行更新</span></div><div class="line">      maybeExpandIsr(replicaId, logReadResult)</div><div class="line"></div><div class="line">      debug(<span class="string">"Recorded replica %d log end offset (LEO) position %d for partition %s."</span></div><div class="line">        .format(replicaId, logReadResult.info.fetchOffsetMetadata.messageOffset, topicPartition))</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotAssignedReplicaException</span>((<span class="string">"Leader %d failed to record follower %d's position %d since the replica"</span> +</div><div class="line">        <span class="string">" is not recognized to be one of the assigned replicas %s for partition %s."</span>)</div><div class="line">        .format(localBrokerId,</div><div class="line">                replicaId,</div><div class="line">                logReadResult.info.fetchOffsetMetadata.messageOffset,</div><div class="line">                assignedReplicas.map(_.brokerId).mkString(<span class="string">","</span>),</div><div class="line">                topicPartition))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法分为以下两步：</p>
<ol>
<li><code>updateLogReadResult()</code>：更新副本的相关信息，这里是更新该副本的 LEO、lastFetchLeaderLogEndOffset 和 lastFetchTimeMs；</li>
<li><code>maybeExpandIsr()</code>：判断 isr 是否需要扩充，即是否有不在 isr 内的副本满足进入 isr 的条件。</li>
</ol>
<h3 id="maybeExpandIsr"><a href="#maybeExpandIsr" class="headerlink" title="maybeExpandIsr"></a>maybeExpandIsr</h3><p><code>maybeExpandIsr()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查当前 Partition 是否需要扩充 ISR, 副本的 LEO 大于等于 hw 的副本将会被添加到 isr 中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeExpandIsr</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="comment">// check if this replica needs to be added to the ISR</span></div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> replica = getReplica(replicaId).get</div><div class="line">        <span class="keyword">val</span> leaderHW = leaderReplica.highWatermark</div><div class="line">        <span class="keyword">if</span>(!inSyncReplicas.contains(replica) &amp;&amp;</div><div class="line">           assignedReplicas.map(_.brokerId).contains(replicaId) &amp;&amp;</div><div class="line">           replica.logEndOffset.offsetDiff(leaderHW) &gt;= <span class="number">0</span>) &#123; <span class="comment">//note: replica LEO 大于 HW 的情况下,加入 isr 列表</span></div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas + replica</div><div class="line">          info(<span class="string">s"Expanding ISR for partition <span class="subst">$topicPartition</span> from <span class="subst">$&#123;inSyncReplicas.map(_.brokerId).mkString(",")&#125;</span> "</span> +</div><div class="line">            <span class="string">s"to <span class="subst">$&#123;newInSyncReplicas.map(_.brokerId).mkString(",")&#125;</span>"</span>)</div><div class="line">          <span class="comment">// update ISR in ZK and cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新到 zk</span></div><div class="line">          replicaManager.isrExpandRate.mark()</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// check if the HW of the partition can now be incremented</span></div><div class="line">        <span class="comment">// since the replica may already be in the ISR and its LEO has just incremented</span></div><div class="line">        <span class="comment">//note: 检查 HW 是否需要更新</span></div><div class="line">        maybeIncrementLeaderHW(leaderReplica, logReadResult.fetchTimeMs)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// nothing to do if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法会根据这个 replica 的 LEO 来判断它是否满足进入 ISR 的条件，如果满足的话，就添加到 ISR 中（前提是这个 replica 在 AR：assign replica 中，并且不在 ISR 中），之后再调用 <code>updateIsr()</code> 更新这个 topic-partition 的 isr 信息和更新 HW 信息。</p>
<h2 id="Updata-Metadata-请求的处理"><a href="#Updata-Metadata-请求的处理" class="headerlink" title="Updata-Metadata 请求的处理"></a>Updata-Metadata 请求的处理</h2><p>这里顺便讲述一下 Update-Metadata 请求的处理流程，先看下在 KafkaApis 中对 Update-Metadata 请求的处理流程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 update-metadata 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleUpdateMetadataRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> updateMetadataRequest = request.body.asInstanceOf[<span class="type">UpdateMetadataRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">val</span> updateMetadataResponse =</div><div class="line">    <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata, 并返回需要删除的 Partition</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = replicaManager.maybeUpdateMetadataCache(correlationId, updateMetadataRequest, metadataCache)</div><div class="line">      <span class="keyword">if</span> (deletedPartitions.nonEmpty)</div><div class="line">        coordinator.handleDeletedPartitions(deletedPartitions) <span class="comment">//note: GroupCoordinator 会清除相关 partition 的信息</span></div><div class="line"></div><div class="line">      <span class="keyword">if</span> (adminManager.hasDelayedTopicOperations) &#123;</div><div class="line">        updateMetadataRequest.partitionStates.keySet.asScala.map(_.topic).foreach &#123; topic =&gt;</div><div class="line">          adminManager.tryCompleteDelayedTopicOperations(topic)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, updateMetadataResponse))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个请求的处理还是调用 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法进行处理的，这个方法会先更新相关的 meta 信息，然后返回需要删除的 topic-partition 信息，GroupCoordinator 再从它的 meta 删除这个 topic-partition 的相关信息。</p>
<h3 id="maybeUpdateMetadataCache"><a href="#maybeUpdateMetadataCache" class="headerlink" title="maybeUpdateMetadataCache"></a>maybeUpdateMetadataCache</h3><p>先看下 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 向所有的 Broker 发送请求,让它们去更新各自的 meta 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeUpdateMetadataCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>, metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Seq</span>[<span class="type">TopicPartition</span>] =  &#123;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(updateMetadataRequest.controllerEpoch &lt; controllerEpoch) &#123; <span class="comment">//note: 来自过期的 controller</span></div><div class="line">      <span class="keyword">val</span> stateControllerEpochErrorMessage = (<span class="string">"Broker %d received update metadata request with correlation id %d from an "</span> +</div><div class="line">        <span class="string">"old controller %d with epoch %d. Latest known controller epoch is %d"</span>).format(localBrokerId,</div><div class="line">        correlationId, updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch,</div><div class="line">        controllerEpoch)</div><div class="line">      stateChangeLogger.warn(stateControllerEpochErrorMessage)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(stateControllerEpochErrorMessage)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata 信息,并返回需要删除的 Partition 信息</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = metadataCache.updateCache(correlationId, updateMetadataRequest)</div><div class="line">      controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      deletedPartitions</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法就是：调用 <code>metadataCache.updateCache()</code> 方法更新 meta 缓存，然后返回需要删除的 topic-partition 列表。</p>
<h3 id="updateCache"><a href="#updateCache" class="headerlink" title="updateCache"></a>updateCache</h3><p>MetadataCache 的 <code>updateCache()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新本地的 meta,并返回要删除的 topic-partition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = &#123;</div><div class="line">  inWriteLock(partitionMetadataLock) &#123;</div><div class="line">    controllerId = updateMetadataRequest.controllerId <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> id <span class="keyword">if</span> id &lt; <span class="number">0</span> =&gt; <span class="type">None</span></div><div class="line">        <span class="keyword">case</span> id =&gt; <span class="type">Some</span>(id)</div><div class="line">      &#125;</div><div class="line">    <span class="comment">//note: 清空 aliveNodes 和 aliveBrokers 记录,并更新成最新的记录</span></div><div class="line">    aliveNodes.clear()</div><div class="line">    aliveBrokers.clear()</div><div class="line">    updateMetadataRequest.liveBrokers.asScala.foreach &#123; broker =&gt;</div><div class="line">      <span class="comment">// `aliveNodes` is a hot path for metadata requests for large clusters, so we use java.util.HashMap which</span></div><div class="line">      <span class="comment">// is a bit faster than scala.collection.mutable.HashMap. When we drop support for Scala 2.10, we could</span></div><div class="line">      <span class="comment">// move to `AnyRefMap`, which has comparable performance.</span></div><div class="line">      <span class="keyword">val</span> nodes = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]</div><div class="line">      <span class="keyword">val</span> endPoints = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">EndPoint</span>]</div><div class="line">      broker.endPoints.asScala.foreach &#123; ep =&gt;</div><div class="line">        endPoints += <span class="type">EndPoint</span>(ep.host, ep.port, ep.listenerName, ep.securityProtocol)</div><div class="line">        nodes.put(ep.listenerName, <span class="keyword">new</span> <span class="type">Node</span>(broker.id, ep.host, ep.port))</div><div class="line">      &#125;</div><div class="line">      aliveBrokers(broker.id) = <span class="type">Broker</span>(broker.id, endPoints, <span class="type">Option</span>(broker.rack))</div><div class="line">      aliveNodes(broker.id) = nodes.asScala</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> deletedPartitions = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">TopicPartition</span>] <span class="comment">//note:</span></div><div class="line">    updateMetadataRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (tp, info) =&gt;</div><div class="line">      <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId</div><div class="line">      <span class="keyword">val</span> controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      <span class="keyword">if</span> (info.leader == <span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>) &#123; <span class="comment">//note: partition 被标记为了删除</span></div><div class="line">        removePartitionInfo(tp.topic, tp.partition) <span class="comment">//note: 从 cache 中删除</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> deleted partition <span class="subst">$tp</span> from metadata cache in response to UpdateMetadata "</span> +</div><div class="line">          <span class="string">s"request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">        deletedPartitions += tp</div><div class="line">      &#125; <span class="keyword">else</span> &#123;<span class="comment">//note: 更新</span></div><div class="line">        <span class="keyword">val</span> partitionInfo = partitionStateToPartitionStateInfo(info)</div><div class="line">        addOrUpdatePartitionInfo(tp.topic, tp.partition, partitionInfo) <span class="comment">//note: 更新 topic-partition meta</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> cached leader info <span class="subst">$partitionInfo</span> for partition <span class="subst">$tp</span> in response to "</span> +</div><div class="line">          <span class="string">s"UpdateMetadata request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    deletedPartitions</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它的处理流程如下：</p>
<ol>
<li>清空本节点的 aliveNodes 和 aliveBrokers 记录，并更新为最新的记录；</li>
<li>对于要删除的 topic-partition，从缓存中删除，并记录下来作为这个方法的返回；</li>
<li>对于其他的 topic-partition，执行 updateOrCreate 操作。</li>
</ol>
<p>到这里 ReplicaManager 算是讲述完了，Kafka 存储层的内容基本也介绍完了，后面会开始讲述 Kafka Controller 部分的内容，争取这部分能够在一个半月内总结完。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本同步机制实现（十四）</title>
    <link href="http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/"/>
    <id>http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/</id>
    <published>2018-04-29T10:36:52.000Z</published>
    <updated>2018-06-24T23:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。本篇文章我们就来看一下 Kafka 副本同步这块的内容，对于每个 broker 来说，它上面的 replica 对象，除了 leader 就是 follower，只要这台 broker 有 follower replica，broker 就会启动副本同步流程从 leader 同步数据，副本同步机制的实现是 Kafka Server 端非常重要的内容，在这篇文章中，主要会从以下几块来讲解：</p>
<ol>
<li>Kafka 在什么情况下会启动副本同步线程？</li>
<li>Kafka 副本同步线程启动流程及付副本同步流程的处理逻辑；</li>
<li>Kafka 副本同步需要解决的问题以及 Kafka 是如何解决这些问题的？</li>
<li>Kafka 在什么情况下会关闭一个副本同步线程。</li>
</ol>
<blockquote>
<p>小插曲：本来想先介绍一下与 LeaderAndIsr 请求相关的，因为副本同步线程的启动与这部分是息息相关的，但是发现涉及到了很多 controller 端的内容，而 controller 这部分还没开始涉及，所以本篇文章涉及到 LeaderAndIsr 请求的部分先简单讲述一下其处理逻辑，在 controller 这块再详细介绍。</p>
</blockquote>
<h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>Kafka Server 端的副本同步，是由 replica fetcher 线程来负责的，而它又是由 ReplicaManager 来控制的。关于 ReplicaManger，不知道大家还记不记得在 <a href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a> 有一个简单的表格，如下所示。ReplicaManager 通过对 Partition 对象的管理，来控制着 Partition 对应的 Replica 实例，而 Replica 实例又是通过 Log 对象实例来管理着其底层的存储内容。</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 的内容准备专门写一篇文章来介绍，刚好也作为对 Kafka 存储层内容的一个总结。</p>
<p>下面回到这篇文章的主题 —— 副本同步机制，在 ReplicaManager 中有一个实例变量 <code>replicaFetcherManager</code>，它负责管理所有副本同步线程，副本同步线程的启动和关闭都是由这个实例来操作的，关于副本同步相关处理逻辑，下面这张图可以作为一个整体流程，包括了 replica fetcher 线程的启动、工作流程、关闭三个部分，如下图所示：</p>
<p><img src="/images/kafka/fetcher_thread.png" alt="副本同步机制"></p>
<p>后面的讲述会围绕着这张图开始，这里看不懂或不理解也没有关系，后面会一一讲解。</p>
<h2 id="replica-fetcher-线程何时启动"><a href="#replica-fetcher-线程何时启动" class="headerlink" title="replica fetcher 线程何时启动"></a>replica fetcher 线程何时启动</h2><p>Broker 会在什么情况下启动副本同步线程呢？简单想一下这部分的逻辑：首先 broker 分配的任何一个 partition 都是以 Replica 对象实例的形式存在，而 Replica 在 Kafka 上是有两个角色： leader 和 follower，只要这个 Replica 是 follower，它便会向 leader 进行数据同步。</p>
<p>反应在 ReplicaManager 上就是如果 Broker 的本地副本被选举为 follower，那么它将会启动副本同步线程，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于给定的这些副本，将本地副本设置为 follower</span></div><div class="line"><span class="comment">//note: 1. 从 leader partition 集合移除这些 partition；</span></div><div class="line"><span class="comment">//note: 2. 将这些 partition 标记为 follower，之后这些 partition 就不会再接收 produce 的请求了；</span></div><div class="line"><span class="comment">//note: 3. 停止对这些 partition 的副本同步，这样这些副本就不会再有（来自副本请求线程）的数据进行追加了；</span></div><div class="line"><span class="comment">//note: 4. 对这些 partition 的 offset 进行 checkpoint，如果日志需要截断就进行截断操作；</span></div><div class="line"><span class="comment">//note: 5. 清空 purgatory 中的 produce 和 fetch 请求；</span></div><div class="line"><span class="comment">//note: 6. 如果 broker 没有掉线，向这些 partition 的新 leader 启动副本同步线程；</span></div><div class="line"><span class="comment">//note: 上面这些操作的顺序性，保证了这些副本在 offset checkpoint 之前将不会接收新的数据，这样的话，在 checkpoint 之前这些数据都可以保证刷到磁盘</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 删除对这些 partition 的副本同步线程</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 完成那些延迟请求的处理</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>makeFollowers()</code> 的处理过程如下：</p>
<ol>
<li>先从本地记录 leader partition 的集合中将这些 partition 移除，因为这些 partition 已经被选举为了 follower；</li>
<li>将这些 partition 的本地副本设置为 follower，后面就不会接收关于这个 partition 的 Produce 请求了，如果依然有 client 在向这台 broker 发送数据，那么它将会返回相应的错误；</li>
<li>先停止关于这些 partition 的副本同步线程（如果本地副本之前是 follower 现在还是 follower，先关闭的原因是：这个 partition 的 leader 发生了变化，如果 leader 没有发生变化，那么 <code>makeFollower</code> 方法返回的是 False，这个 Partition 就不会被添加到 partitionsToMakeFollower 集合中），这样的话可以保证这些 partition 的本地副本将不会再有新的数据追加；</li>
<li>对这些 partition 本地副本日志文件进行截断操作并进行 checkpoint 操作；</li>
<li>完成那些延迟处理的 Produce 和 Fetch 请求；</li>
<li>如果本地的 broker 没有掉线，那么向这些 partition 新选举出来的 leader 启动副本同步线程。</li>
</ol>
<p>关于第6步，并不一定会为每一个 partition 都启动一个 fetcher 线程，对于一个目的 broker，只会启动 <code>num.replica.fetchers</code> 个线程，具体这个 topic-partition 会分配到哪个 fetcher 线程上，是根据 topic 名和 partition id 进行计算得到，实现所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取分配到这个 topic-partition 的 fetcher 线程 id</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFetcherId</span></span>(topic: <span class="type">String</span>, partitionId: <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="type">Utils</span>.abs(<span class="number">31</span> * topic.hashCode() + partitionId) % numFetchers</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="replica-fetcher-线程参数设置"><a href="#replica-fetcher-线程参数设置" class="headerlink" title="replica fetcher 线程参数设置"></a>replica fetcher 线程参数设置</h3><p>关于副本同步线程有一些参数配置，具体如下表所示：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>num.replica.fetchers</td>
<td>从一个 broker 同步数据的 fetcher 线程数，增加这个值时也会增加该 broker 的 Io 并行度（也就是说：从一台 broker 同步数据，最多能开这么大的线程数）</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.wait.max.ms</td>
<td>对于 follower replica 而言，每个 Fetch 请求的最大等待时间，这个值应该比 <code>replica.lag.time.max.ms</code> 要小，否则对于那些吞吐量特别低的 topic 可能会导致 isr 频繁抖动</td>
<td>500</td>
</tr>
<tr>
<td>replica.high.watermark.checkpoint.interval.ms</td>
<td>hw 刷到磁盘频率</td>
<td>500</td>
</tr>
<tr>
<td>replica.lag.time.max.ms</td>
<td>如果一个 follower 在这个时间内没有发送任何 fetch 请求或者在这个时间内没有追上 leader 当前的 log end offset，那么将会从 isr 中移除</td>
<td>10000</td>
</tr>
<tr>
<td>replica.fetch.min.bytes</td>
<td>每次 fetch 请求最少拉取的数据量，如果不满足这个条件，那么要等待 replicaMaxWaitTimeMs</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.backoff.ms</td>
<td>拉取时，如果遇到错误，下次拉取等待的时间</td>
<td>1000</td>
</tr>
<tr>
<td>replica.fetch.max.bytes</td>
<td>在对每个 partition 拉取时，最大的拉取数量，这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>1048576</td>
</tr>
<tr>
<td>replica.fetch.response.max.bytes</td>
<td>对于一个 fetch 请求，返回的最大数据量（可能会涉及多个 partition），这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>10MB</td>
</tr>
</tbody>
</table>
<h2 id="replica-fetcher-线程启动"><a href="#replica-fetcher-线程启动" class="headerlink" title="replica fetcher 线程启动"></a>replica fetcher 线程启动</h2><p>如上面的图所示，在 ReplicaManager 调用 <code>makeFollowers()</code> 启动 replica fetcher 线程后，它实际上是通过 ReplicaFetcherManager 实例进行相关 topic-partition 同步线程的启动和关闭，其启动过程分为下面两步：</p>
<ol>
<li>ReplicaFetcherManager 调用 <code>addFetcherForPartitions()</code> 添加对这些 topic-partition 的数据同步流程；</li>
<li>ReplicaFetcherManager 调用 <code>createFetcherThread()</code> 初始化相应的 ReplicaFetcherThread 线程。</li>
</ol>
<h3 id="addFetcherForPartitions"><a href="#addFetcherForPartitions" class="headerlink" title="addFetcherForPartitions"></a>addFetcherForPartitions</h3><p><code>addFetcherForPartitions()</code> 的具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 为一个 topic-partition 添加 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addFetcherForPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">BrokerAndInitialOffset</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="comment">//note: 为这些 topic-partition 分配相应的 fetch 线程 id</span></div><div class="line">    <span class="keyword">val</span> partitionsPerFetcher = partitionAndOffsets.groupBy &#123; <span class="keyword">case</span>(topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">      <span class="type">BrokerAndFetcherId</span>(brokerAndInitialOffset.broker, getFetcherId(topicPartition.topic, topicPartition.partition))&#125;</div><div class="line">    <span class="keyword">for</span> ((brokerAndFetcherId, partitionAndOffsets) &lt;- partitionsPerFetcher) &#123;</div><div class="line">      <span class="comment">//note: 为 BrokerAndFetcherId 构造 fetcherThread 线程</span></div><div class="line">      <span class="keyword">var</span> fetcherThread: <span class="type">AbstractFetcherThread</span> = <span class="literal">null</span></div><div class="line">      fetcherThreadMap.get(brokerAndFetcherId) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(f) =&gt; fetcherThread = f</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">//note: 创建 fetcher 线程</span></div><div class="line">          fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)</div><div class="line">          fetcherThreadMap.put(brokerAndFetcherId, fetcherThread)</div><div class="line">          fetcherThread.start</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 添加 topic-partition 列表</span></div><div class="line">      fetcherThreadMap(brokerAndFetcherId).addPartitions(partitionAndOffsets.map &#123; <span class="keyword">case</span> (tp, brokerAndInitOffset) =&gt;</div><div class="line">        tp -&gt; brokerAndInitOffset.initOffset</div><div class="line">      &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  info(<span class="string">"Added fetcher for partitions %s"</span>.format(partitionAndOffsets.map &#123; <span class="keyword">case</span> (topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">    <span class="string">"["</span> + topicPartition + <span class="string">", initOffset "</span> + brokerAndInitialOffset.initOffset + <span class="string">" to broker "</span> + brokerAndInitialOffset.broker + <span class="string">"] "</span>&#125;))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法其实是做了下面这几件事：</p>
<ol>
<li>先计算这个 topic-partition 对应的 fetcher id；</li>
<li>根据 leader 和 fetcher id 获取对应的 replica fetcher 线程，如果没有找到，就调用 <code>createFetcherThread()</code> 创建一个新的 fetcher 线程；</li>
<li>如果是新启动的 replica fetcher 线程，那么就启动这个线程；</li>
<li>将 topic-partition 记录到 <code>fetcherThreadMap</code> 中，这个变量记录每个 replica fetcher 线程要同步的 topic-partition 列表。</li>
</ol>
<h3 id="createFetcherThread"><a href="#createFetcherThread" class="headerlink" title="createFetcherThread"></a>createFetcherThread</h3><p>ReplicaFetcherManager 创建 replica Fetcher 线程的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建 replica-fetch 线程</span></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createFetcherThread</span></span>(fetcherId: <span class="type">Int</span>, sourceBroker: <span class="type">BrokerEndPoint</span>): <span class="type">AbstractFetcherThread</span> = &#123;</div><div class="line">  <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="string">"ReplicaFetcherThread-%d-%d"</span>.format(fetcherId, sourceBroker.id)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt;</div><div class="line">      <span class="string">"%s:ReplicaFetcherThread-%d-%d"</span>.format(p, fetcherId, sourceBroker.id)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">ReplicaFetcherThread</span>(threadName, fetcherId, sourceBroker, brokerConfig,</div><div class="line">    replicaMgr, metrics, time, quotaManager) <span class="comment">//note: replica-fetch 线程</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="replica-fetcher-线程处理过程"><a href="#replica-fetcher-线程处理过程" class="headerlink" title="replica fetcher 线程处理过程"></a>replica fetcher 线程处理过程</h2><p>replica fetcher 线程在启动之后就开始进行正常数据同步流程了，在文章最开始流程图中的第二部分（线程处理过程）已经给出了大概的处理过程，这节会详细介绍一下，这个过程都是在 ReplicaFetcherThread 线程中实现的。</p>
<h3 id="doWoker"><a href="#doWoker" class="headerlink" title="doWoker"></a>doWoker</h3><p>ReplicaFetcherThread 的 <code>doWork()</code> 方法是一直在这个线程中的 <code>run()</code> 中调用的，实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  info(<span class="string">"Starting "</span>)</div><div class="line">  <span class="keyword">try</span>&#123;</div><div class="line">    <span class="keyword">while</span>(isRunning.get())&#123;</div><div class="line">      doWork()</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span>&#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span>(isRunning.get())</div><div class="line">        error(<span class="string">"Error due to "</span>, e)</div><div class="line">  &#125;</div><div class="line">  shutdownLatch.countDown()</div><div class="line">  info(<span class="string">"Stopped "</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">  <span class="comment">//note: 构造 fetch request</span></div><div class="line">  <span class="keyword">val</span> fetchRequest = inLock(partitionMapLock) &#123;</div><div class="line">    <span class="keyword">val</span> fetchRequest = buildFetchRequest(partitionStates.partitionStates.asScala.map &#123; state =&gt;</div><div class="line">      state.topicPartition -&gt; state.value</div><div class="line">    &#125;)</div><div class="line">    <span class="keyword">if</span> (fetchRequest.isEmpty) &#123; <span class="comment">//note: 如果没有活跃的 partition，在下次调用之前，sleep fetchBackOffMs 时间</span></div><div class="line">      trace(<span class="string">"There are no active partitions. Back off for %d ms before sending a fetch request"</span>.format(fetchBackOffMs))</div><div class="line">      partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    &#125;</div><div class="line">    fetchRequest</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!fetchRequest.isEmpty)</div><div class="line">    processFetchRequest(fetchRequest) <span class="comment">//note: 发送 fetch 请求，处理 fetch 的结果</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>doWork()</code> 方法中主要做了两件事：</p>
<ol>
<li>构造相应的 Fetch 请求（<code>buildFetchRequest()</code>）；</li>
<li>通过 <code>processFetchRequest()</code> 方法发送 Fetch 请求，并对其结果进行相应的处理。</li>
</ol>
<h3 id="buildFetchRequest"><a href="#buildFetchRequest" class="headerlink" title="buildFetchRequest"></a>buildFetchRequest</h3><p>通过 <code>buildFetchRequest()</code> 方法构造相应的 Fetcher 请求时，会设置 replicaId，该值会代表了这个 Fetch 请求是来自副本同步，而不是来自 consumer。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, 对于 consumer, 该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="processFetchRequest"><a href="#processFetchRequest" class="headerlink" title="processFetchRequest"></a>processFetchRequest</h3><p><code>processFetchRequest()</code> 这个方法的作用是发送 Fetch 请求，并对返回的结果进行处理，最终写入到本地副本的 Log 实例中，其具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processFetchRequest</span></span>(fetchRequest: <span class="type">REQ</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsWithError = mutable.<span class="type">Set</span>[<span class="type">TopicPartition</span>]()</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updatePartitionsWithError</span></span>(partition: <span class="type">TopicPartition</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    partitionsWithError += partition</div><div class="line">    partitionStates.moveToEnd(partition)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> responseData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PD</span>)] = <span class="type">Seq</span>.empty</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    trace(<span class="string">"Issuing to broker %d of fetch request %s"</span>.format(sourceBroker.id, fetchRequest))</div><div class="line">    responseData = fetch(fetchRequest) <span class="comment">//note: 发送 fetch 请求，获取 fetch 结果</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">        warn(<span class="string">s"Error in fetch <span class="subst">$fetchRequest</span>"</span>, t)</div><div class="line">        inLock(partitionMapLock) &#123; <span class="comment">//note: fetch 时发生错误，sleep 一会</span></div><div class="line">          partitionStates.partitionSet.asScala.foreach(updatePartitionsWithError)</div><div class="line">          <span class="comment">// there is an error occurred while fetching partitions, sleep a while</span></div><div class="line">          <span class="comment">// note that `ReplicaFetcherThread.handlePartitionsWithError` will also introduce the same delay for every</span></div><div class="line">          <span class="comment">// partition with error effectively doubling the delay. It would be good to improve this.</span></div><div class="line">          partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  fetcherStats.requestRate.mark()</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (responseData.nonEmpty) &#123; <span class="comment">//note: fetch 结果不为空</span></div><div class="line">    <span class="comment">// process fetched data</span></div><div class="line">    inLock(partitionMapLock) &#123;</div><div class="line"></div><div class="line">      responseData.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionData) =&gt;</div><div class="line">        <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">        <span class="keyword">val</span> partitionId = topicPartition.partition</div><div class="line">        <span class="type">Option</span>(partitionStates.stateValue(topicPartition)).foreach(currentPartitionFetchState =&gt;</div><div class="line">          <span class="comment">// we append to the log if the current offset is defined and it is the same as the offset requested during fetch</span></div><div class="line">          <span class="comment">//note: 如果 fetch 的 offset 与返回结果的 offset 相同，并且返回没有异常，那么就将拉取的数据追加到对应的 partition 上</span></div><div class="line">          <span class="keyword">if</span> (fetchRequest.offset(topicPartition) == currentPartitionFetchState.offset) &#123;</div><div class="line">            <span class="type">Errors</span>.forCode(partitionData.errorCode) <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NONE</span> =&gt;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line">                  <span class="keyword">val</span> newOffset = records.shallowEntries.asScala.lastOption.map(_.nextOffset).getOrElse(</div><div class="line">                    currentPartitionFetchState.offset)</div><div class="line"></div><div class="line">                  fetcherLagStats.getAndMaybePut(topic, partitionId).lag = <span class="type">Math</span>.max(<span class="number">0</span>L, partitionData.highWatermark - newOffset)</div><div class="line">                  <span class="comment">// Once we hand off the partition data to the subclass, we can't mess with it any more in this thread</span></div><div class="line">                  <span class="comment">//note: 将 fetch 的数据追加到日志文件中</span></div><div class="line">                  processPartitionData(topicPartition, currentPartitionFetchState.offset, partitionData)</div><div class="line"></div><div class="line">                  <span class="keyword">val</span> validBytes = records.validBytes</div><div class="line">                  <span class="keyword">if</span> (validBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">                    <span class="comment">// Update partitionStates only if there is no exception during processPartitionData</span></div><div class="line">                    <span class="comment">//note: 更新 fetch 的 offset 位置</span></div><div class="line">                    partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                    fetcherStats.byteRate.mark(validBytes) <span class="comment">//note: 更新 metrics</span></div><div class="line">                  &#125;</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> ime: <span class="type">CorruptRecordException</span> =&gt;</div><div class="line">                    <span class="comment">// we log the error and continue. This ensures two things</span></div><div class="line">                    <span class="comment">// 1. If there is a corrupt message in a topic partition, it does not bring the fetcher thread down and cause other topic partition to also lag</span></div><div class="line">                    <span class="comment">// 2. If the message is corrupt due to a transient state in the log (truncation, partial writes can cause this), we simply continue and</span></div><div class="line">                    <span class="comment">// should get fixed in the subsequent fetches</span></div><div class="line">                    <span class="comment">//note: CRC 验证失败时，打印日志，并继续进行（这个线程还会有其他的 tp 拉取，防止影响其他副本同步）</span></div><div class="line">                    logger.error(<span class="string">"Found invalid messages during fetch for partition ["</span> + topic + <span class="string">","</span> + partitionId + <span class="string">"] offset "</span> + currentPartitionFetchState.offset  + <span class="string">" error "</span> + ime.getMessage)</div><div class="line">                    updatePartitionsWithError(topicPartition);</div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    <span class="comment">//note: 这里还会抛出异常，是 RUNTimeException</span></div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"error processing data for partition [%s,%d] offset %d"</span></div><div class="line">                      .format(topic, partitionId, currentPartitionFetchState.offset), e)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">OFFSET_OUT_OF_RANGE</span> =&gt; <span class="comment">//note: Out-of-range 的情况处理</span></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> newOffset = handleOffsetOutOfRange(topicPartition)</div><div class="line">                  partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                  error(<span class="string">"Current offset %d for partition [%s,%d] out of range; reset offset to %d"</span></div><div class="line">                    .format(currentPartitionFetchState.offset, topic, partitionId, newOffset))</div><div class="line">                &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理 out-of-range 是抛出的异常</span></div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    error(<span class="string">"Error getting offset for partition [%s,%d] to broker %d"</span>.format(topic, partitionId, sourceBroker.id), e)</div><div class="line">                    updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> _ =&gt; <span class="comment">//note: 其他的异常情况</span></div><div class="line">                <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">                  error(<span class="string">"Error for partition [%s,%d] to broker %d:%s"</span>.format(topic, partitionId, sourceBroker.id,</div><div class="line">                    partitionData.exception.get))</div><div class="line">                  updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理拉取遇到的错误读的 tp</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithError.nonEmpty) &#123;</div><div class="line">    debug(<span class="string">"handling partitions with error for %s"</span>.format(partitionsWithError))</div><div class="line">    handlePartitionsWithErrors(partitionsWithError)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其处理过程简单总结一下：</p>
<ol>
<li>通过 <code>fetch()</code> 方法，发送 Fetch 请求，获取相应的 response（如果遇到异常，那么在下次发送 Fetch 请求之前，会 sleep 一段时间再发）；</li>
<li>如果返回的结果 不为空，并且 Fetch 请求的 offset 信息与返回结果的 offset 信息对得上，那么就会调用 <code>processPartitionData()</code> 方法将拉取到的数据追加本地副本的日志文件中，如果返回结果有错误信息，那么就对相应错误进行相应的处理；</li>
<li>对在 Fetch 过程中遇到异常或返回错误的 topic-partition，会进行 delay 操作，下次 Fetch 请求的发生至少要间隔 <code>replica.fetch.backoff.ms</code> 时间。</li>
</ol>
<h4 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h4><p><code>fetch()</code> 方法作用是发送 Fetch 请求，并返回相应的结果，其具体的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送 fetch 请求，获取拉取结果</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span></span>(fetchRequest: <span class="type">FetchRequest</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)] = &#123;</div><div class="line">  <span class="keyword">val</span> clientResponse = sendRequest(fetchRequest.underlying)</div><div class="line">  <span class="keyword">val</span> fetchResponse = clientResponse.responseBody.asInstanceOf[<span class="type">FetchResponse</span>]</div><div class="line">  fetchResponse.responseData.asScala.toSeq.map &#123; <span class="keyword">case</span> (key, value) =&gt;</div><div class="line">    key -&gt; <span class="keyword">new</span> <span class="type">PartitionData</span>(value)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 发送请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(requestBuilder: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>]): <span class="type">ClientResponse</span> = &#123;</div><div class="line">  <span class="keyword">import</span> kafka.utils.<span class="type">NetworkClientBlockingOps</span>._</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">if</span> (!networkClient.blockingReady(sourceNode, socketTimeout)(time))</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SocketTimeoutException</span>(<span class="string">s"Failed to connect within <span class="subst">$socketTimeout</span> ms"</span>)</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> clientRequest = networkClient.newClientRequest(sourceBroker.id.toString, requestBuilder,</div><div class="line">        time.milliseconds(), <span class="literal">true</span>)</div><div class="line">      networkClient.blockingSendAndReceive(clientRequest)(time) <span class="comment">//note: 阻塞直到获取返回结果</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      networkClient.close(sourceBroker.id.toString)</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processPartitionData"><a href="#processPartitionData" class="headerlink" title="processPartitionData"></a>processPartitionData</h4><p>这个方法的作用是，处理 Fetch 请求的具体数据内容，简单来说就是：检查一下数据大小是否超过限制、将数据追加到本地副本的日志文件中、更新本地副本的 hw 值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// process fetched data</span></div><div class="line"><span class="comment">//note: 处理 fetch 的数据，将 fetch 的数据追加的日志文件中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">processPartitionData</span></span>(topicPartition: <span class="type">TopicPartition</span>, fetchOffset: <span class="type">Long</span>, partitionData: <span class="type">PartitionData</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</div><div class="line">    <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line"></div><div class="line">    <span class="comment">//note: 检查 records</span></div><div class="line">    maybeWarnIfOversizedRecords(records, topicPartition)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (fetchOffset != replica.logEndOffset.messageOffset)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Offset mismatch for partition %s: fetched offset = %d, log end offset = %d."</span>.format(topicPartition, fetchOffset, replica.logEndOffset.messageOffset))</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d for partition %s. Received %d messages and leader hw %d"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))</div><div class="line">    replica.log.get.append(records, assignOffsets = <span class="literal">false</span>) <span class="comment">//note: 将 fetch 的数据追加到 log 中</span></div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d after appending %d bytes of messages for partition %s"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, records.sizeInBytes, topicPartition))</div><div class="line">    <span class="comment">//note: 更新 replica 的 hw（logEndOffset 在追加数据后也会立马进行修改)</span></div><div class="line">    <span class="keyword">val</span> followerHighWatermark = replica.logEndOffset.messageOffset.min(partitionData.highWatermark)</div><div class="line">    <span class="comment">// for the follower replica, we do not need to keep</span></div><div class="line">    <span class="comment">// its segment base offset the physical position,</span></div><div class="line">    <span class="comment">// these values will be computed upon making the leader</span></div><div class="line">    <span class="comment">//note: 这个值主要是用在 leader replica 上的</span></div><div class="line">    replica.highWatermark = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(followerHighWatermark)</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">s"Follower <span class="subst">$&#123;replica.brokerId&#125;</span> set replica high watermark for partition <span class="subst">$topicPartition</span> to <span class="subst">$followerHighWatermark</span>"</span>)</div><div class="line">    <span class="keyword">if</span> (quota.isThrottled(topicPartition))</div><div class="line">      quota.record(records.sizeInBytes)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">s"Disk error while replicating data for <span class="subst">$topicPartition</span>"</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="副本同步异常情况的处理"><a href="#副本同步异常情况的处理" class="headerlink" title="副本同步异常情况的处理"></a>副本同步异常情况的处理</h2><p>在副本同步的过程中，会遇到哪些异常情况呢？</p>
<p>大家一定会想到关于 offset 的问题，在 Kafka 中，关于 offset 的处理，无论是 producer 端、consumer 端还是其他地方，offset 似乎都是一个形影不离的问题。在副本同步时，关于 offset，会遇到什么问题呢？下面举两个异常的场景：</p>
<ol>
<li>假如当前本地（id：1）的副本现在是 leader，其 LEO 假设为1000，而另一个在 isr 中的副本（id：2）其 LEO 为800，此时出现网络抖动，id 为1 的机器掉线后又上线了，但是此时副本的 leader 实际上已经变成了 2，而2的 LEO 为800，这时候1启动副本同步线程去2上拉取数据，希望从 offset=1000 的地方开始拉取，但是2上最大的 offset 才是800，这种情况该如何处理呢？</li>
<li>假设一个 replica （id：1）其 LEO 是10，它已经掉线好几天，这个 partition leader 的 offset 范围是 [100, 800]，那么 1 重启启动时，它希望从 offset=10 的地方开始拉取数据时，这时候发生了 OutOfRange，不过跟上面不同的是这里是小于了 leader offset 的范围，这种情况又该怎么处理？</li>
</ol>
<p>以上两种情况都是 offset OutOfRange 的情况，只不过：一是 Fetch Offset 超过了 leader 的 LEO，二是 Fetch Offset 小于 leader 最小的 offset，在介绍 Kafka 解决方案之前，我们先来自己思考一下这两种情况应该怎么处理？</p>
<ol>
<li>如果 fetch offset 超过 leader 的 offset，这时候副本应该是回溯到 leader 的 LEO 位置（超过这个值的数据删除），然后再去进行副本同步，当然这种解决方案其实是无法保证 leader 与 follower 数据的完全一致，再次发生 leader 切换时，可能会导致数据的可见性不一致，但既然用户允许了脏选举的发生，其实我们是可以认为用户是可以接收这种情况发生的；</li>
<li>这种就比较容易处理，首先清空本地的数据，因为本地的数据都已经过期了，然后从 leader 的最小 offset 位置开始拉取数据。</li>
</ol>
<p>上面是我们比较容易想出的解决方案，而在 Kafka 中，其解决方案也很类似，不过遇到情况比上面我们列出的两种情况多了一些复杂，其解决方案如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">   * Unclean leader election: A follower goes down, in the meanwhile the leader keeps appending messages. The follower comes back up</div><div class="line">   * and before it has completely caught up with the leader's logs, all replicas in the ISR go down. The follower is now uncleanly</div><div class="line">   * elected as the new leader, and it starts appending messages from the client. The old leader comes back up, becomes a follower</div><div class="line">   * and it may discover that the current leader's end offset is behind its own end offset.</div><div class="line">   *</div><div class="line">   * In such a case, truncate the current follower's log to the current leader's end offset and continue fetching.</div><div class="line">   *</div><div class="line">   * There is a potential for a mismatch between the logs of the two replicas here. We don't fix this mismatch as of now.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 脏选举的发生</span></div><div class="line">  <span class="comment">//note: 获取最新的 offset</span></div><div class="line">  <span class="keyword">val</span> leaderEndOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">LATEST_TIMESTAMP</span>,</div><div class="line">    brokerConfig.brokerId)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (leaderEndOffset &lt; replica.logEndOffset.messageOffset) &#123; <span class="comment">//note: leaderEndOffset 小于 副本 LEO 的情况</span></div><div class="line">    <span class="comment">// Prior to truncating the follower's log, ensure that doing so is not disallowed by the configuration for unclean leader election.</span></div><div class="line">    <span class="comment">// This situation could only happen if the unclean election configuration for a topic changes while a replica is down. Otherwise,</span></div><div class="line">    <span class="comment">// we should never encounter this situation since a non-ISR leader cannot be elected if disallowed by the broker configuration.</span></div><div class="line">    <span class="comment">//note: 这种情况只是发生在 unclear election 的情况下</span></div><div class="line">    <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(brokerConfig.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(replicaMgr.zkUtils,</div><div class="line">      <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许 unclear elect 时,直接退出进程</span></div><div class="line">      <span class="comment">// Log a fatal error and shutdown the broker to ensure that data loss does not unexpectedly occur.</span></div><div class="line">      fatal(<span class="string">"Exiting because log truncation is not allowed for partition %s,"</span>.format(topicPartition) +</div><div class="line">        <span class="string">" Current leader %d's latest offset %d is less than replica %d's latest offset %d"</span></div><div class="line">        .format(sourceBroker.id, leaderEndOffset, brokerConfig.brokerId, replica.logEndOffset.messageOffset))</div><div class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: warn 日志信息</span></div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's latest offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderEndOffset))</div><div class="line">    <span class="comment">//note: 进行截断操作,将offset 大于等于targetOffset 的数据和索引删除</span></div><div class="line">    replicaMgr.logManager.truncateTo(<span class="type">Map</span>(topicPartition -&gt; leaderEndOffset))</div><div class="line">    leaderEndOffset</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: leader 的 LEO 大于 follower 的 LEO 的情况下,还发生了 OutOfRange</span></div><div class="line">    <span class="comment">//note: 1. follower 下线了很久,其 LEO 已经小于了 leader 的 StartOffset;</span></div><div class="line">    <span class="comment">//note: 2. 脏选举发生时, 如果 old leader 的 HW 大于 new leader 的 LEO,此时 old leader 回溯到 HW,并且这个位置开始拉取数据发生了 Out of range</span></div><div class="line">    <span class="comment">//note:    当这个方法调用时,随着 produce 持续产生数据,可能出现 leader LEO 大于 Follower LEO 的情况（不做任何处理,重试即可解决,但</span></div><div class="line">    <span class="comment">//note:    无法保证数据的一致性）。</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * If the leader's log end offset is greater than the follower's log end offset, there are two possibilities:</div><div class="line">     * 1. The follower could have been down for a long time and when it starts up, its end offset could be smaller than the leader's</div><div class="line">     * start offset because the leader has deleted old logs (log.logEndOffset &lt; leaderStartOffset).</div><div class="line">     * 2. When unclean leader election occurs, it is possible that the old leader's high watermark is greater than</div><div class="line">     * the new leader's log end offset. So when the old leader truncates its offset to its high watermark and starts</div><div class="line">     * to fetch from the new leader, an OffsetOutOfRangeException will be thrown. After that some more messages are</div><div class="line">     * produced to the new leader. While the old leader is trying to handle the OffsetOutOfRangeException and query</div><div class="line">     * the log end offset of the new leader, the new leader's log end offset becomes higher than the follower's log end offset.</div><div class="line">     *</div><div class="line">     * In the first case, the follower's current log end offset is smaller than the leader's log start offset. So the</div><div class="line">     * follower should truncate all its logs, roll out a new segment and start to fetch from the current leader's log</div><div class="line">     * start offset.</div><div class="line">     * In the second case, the follower should just keep the current log segments and retry the fetch. In the second</div><div class="line">     * case, there will be some inconsistency of data between old and new leader. We are not solving it here.</div><div class="line">     * If users want to have strong consistency guarantees, appropriate configurations needs to be set for both</div><div class="line">     * brokers and producers.</div><div class="line">     *</div><div class="line">     * Putting the two cases together, the follower should fetch from the higher one of its replica log end offset</div><div class="line">     * and the current leader's log start offset.</div><div class="line">     *</div><div class="line">     */</div><div class="line">    <span class="keyword">val</span> leaderStartOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">EARLIEST_TIMESTAMP</span>,</div><div class="line">      brokerConfig.brokerId)</div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's start offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderStartOffset))</div><div class="line">    <span class="keyword">val</span> offsetToFetch = <span class="type">Math</span>.max(leaderStartOffset, replica.logEndOffset.messageOffset)</div><div class="line">    <span class="comment">// Only truncate log when current leader's log start offset is greater than follower's log end offset.</span></div><div class="line">    <span class="keyword">if</span> (leaderStartOffset &gt; replica.logEndOffset.messageOffset) <span class="comment">//note: 如果 leader 的 startOffset 大于副本的最大 offset</span></div><div class="line">      <span class="comment">//note: 将这个 log 的数据全部清空,并且从 leaderStartOffset 开始拉取数据</span></div><div class="line">      replicaMgr.logManager.truncateFullyAndStartAt(topicPartition, leaderStartOffset)</div><div class="line">    offsetToFetch</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>针对第一种情况，在 Kafka 中，实际上还会发生这样一种情况，1 在收到 OutOfRange 错误时，这时去 leader 上获取的 LEO 值与最小的 offset 值，这时候却发现 leader 的 LEO 已经从 800 变成了 1100（这个 topic-partition 的数据量增长得比较快），再按照上面的解决方案就不太合理，Kafka 这边的解决方案是：遇到这种情况，进行重试就可以了，下次同步时就会正常了，但是依然会有上面说的那个问题。</p>
<h2 id="replica-fetcher-线程的关闭"><a href="#replica-fetcher-线程的关闭" class="headerlink" title="replica fetcher 线程的关闭"></a>replica fetcher 线程的关闭</h2><p>最后我们再来介绍一下 replica fetcher 线程在什么情况下会关闭，同样，看一下最开始那张图的第三部分，图中已经比较清晰地列出了 replica fetcher 线程关闭的条件，在三种情况下会关闭对这个 topic-partition 的拉取操作（<code>becomeLeaderOrFollower()</code> 这个方法会在对 LeaderAndIsr 请求处理的文章中讲解，这里先忽略）：</p>
<ol>
<li><code>stopReplica()</code>：broker 收到了 controller 发来的 StopReplica 请求，这时会开始关闭对指定 topic-partition 的同步线程；</li>
<li><code>makeLeaders</code>：这些 partition 的本地副本被选举成了 leader，这时候就会先停止对这些 topic-partition 副本同步线程；</li>
<li><code>makeFollowers()</code>：前面已经介绍过，这里实际上停止副本同步，然后再开启副本同步线程，因为这些 topic-partition 的 leader 可能发生了切换。</li>
</ol>
<blockquote>
<p>这里直接说线程关闭，其实不是很准确，因为每个 replica fetcher 线程操作的是多个 topic-partition，而在关闭的粒度是 partition 级别，只有这个线程分配的 partition 全部关闭后，这个线程才会真正被关闭。</p>
</blockquote>
<h3 id="关闭副本同步"><a href="#关闭副本同步" class="headerlink" title="关闭副本同步"></a>关闭副本同步</h3><p>看下 ReplicaManager 中触发 replica fetcher 线程关闭的三个方法。</p>
<h4 id="stopReplica"><a href="#stopReplica" class="headerlink" title="stopReplica"></a>stopReplica</h4><p>StopReplica 的请求实际上是 Controller 发送过来的，这个在 controller 部分会讲述，它触发的条件有多种，比如：broker 下线、partition replica 迁移等等，ReplicaManager 这里的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 tp 的 leader replica</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLeaderReplicaIfLocal</span></span>(topicPartition: <span class="type">TopicPartition</span>): <span class="type">Replica</span> =  &#123;</div><div class="line">  <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取对应的 Partiion 对象</span></div><div class="line">  partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">s"Partition <span class="subst">$topicPartition</span> doesn't exist on <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">      partition.leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt; leaderReplica <span class="comment">//note: 返回 leader 对应的副本</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">s"Leader not local for partition <span class="subst">$topicPartition</span> on broker <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h4><p><code>makeLeaders()</code> 方法的调用是在 broker 上这个 partition 的副本被设置为 leader 时触发的，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> * Make the current broker to become leader for a given set of partitions by:</div><div class="line"> *</div><div class="line"> * 1. Stop fetchers for these partitions</div><div class="line"> * 2. Update the partition metadata in cache</div><div class="line"> * 3. Add these partitions to the leader partitions set</div><div class="line"> *</div><div class="line"> * If an unexpected error is thrown in this function, it will be propagated to KafkaApis where</div><div class="line"> * the error message will be set on each partition since we do not know which partition caused it. Otherwise,</div><div class="line"> * return the set of partitions that are made leader due to this method</div><div class="line"> *</div><div class="line"> *  <span class="doctag">TODO:</span> the above may need to be fixed later</div><div class="line"> */</div><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，这个方法的过程逻辑如下：</p>
<ol>
<li>先停止对这些 partition 的副本同步流程，因为这些 partition 的本地副本已经被选举成为了 leader；</li>
<li>将这些 partition 的本地副本设置为 leader，并且开始更新相应 meta 信息（主要是记录其他 follower 副本的相关信息）；</li>
<li>将这些 partition 添加到本地记录的 leader partition 集合中。</li>
</ol>
<h4 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h4><p>这个在前面已经讲述过了，参考前面的讲述。</p>
<h3 id="removeFetcherForPartitions"><a href="#removeFetcherForPartitions" class="headerlink" title="removeFetcherForPartitions"></a>removeFetcherForPartitions</h3><p>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 删除对这些 topic-partition 的副本同步设置，这里在实现时，会遍历所有的 replica fetcher 线程，都执行 <code>removePartitions()</code> 方法来移除对应的 topic-partition 集合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除一个 partition 的 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeFetcherForPartitions</span></span>(partitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">for</span> (fetcher &lt;- fetcherThreadMap.values) <span class="comment">//note: 遍历所有的 fetchThread 去移除这个 topic-partition 集合</span></div><div class="line">      fetcher.removePartitions(partitions)</div><div class="line">  &#125;</div><div class="line">  info(<span class="string">"Removed fetcher for partitions %s"</span>.format(partitions.mkString(<span class="string">","</span>)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="removePartitions"><a href="#removePartitions" class="headerlink" title="removePartitions"></a>removePartitions</h3><p>这个方法的作用是：ReplicaFetcherThread 将这些 topic-partition 从自己要拉取的 partition 列表中移除。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removePartitions</span></span>(topicPartitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  partitionMapLock.lockInterruptibly()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    topicPartitions.foreach &#123; topicPartition =&gt;</div><div class="line">      partitionStates.remove(topicPartition)</div><div class="line">      fetcherLagStats.unregister(topicPartition.topic, topicPartition.partition)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> partitionMapLock.unlock()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ReplicaFetcherThread-的关闭"><a href="#ReplicaFetcherThread-的关闭" class="headerlink" title="ReplicaFetcherThread 的关闭"></a>ReplicaFetcherThread 的关闭</h3><p>前面介绍那么多，似乎还是没有真正去关闭，那么 ReplicaFetcherThread 真正关闭是哪里操作的呢？</p>
<p>实际上 ReplicaManager 每次处理完 LeaderAndIsr 请求后，都会调用 ReplicaFetcherManager 的 <code>shutdownIdleFetcherThreads()</code> 方法，如果 fetcher 线程要拉取的 topic-partition 集合为空，那么就会关闭掉对应的 fetcher 线程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 关闭没有拉取 topic-partition 任务的拉取线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownIdleFetcherThreads</span></span>() &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> keysToBeRemoved = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">BrokerAndFetcherId</span>]</div><div class="line">    <span class="keyword">for</span> ((key, fetcher) &lt;- fetcherThreadMap) &#123;</div><div class="line">      <span class="keyword">if</span> (fetcher.partitionCount &lt;= <span class="number">0</span>) &#123; <span class="comment">//note: 如果该线程拉取的 partition 数小于 0</span></div><div class="line">        fetcher.shutdown()</div><div class="line">        keysToBeRemoved += key</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    fetcherThreadMap --= keysToBeRemoved</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Replica Fetcher 线程这部分的内容终于讲解完了，希望能对大家有所帮助，有问题欢迎通过留言、微博或邮件进行交流。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</title>
    <link href="http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/"/>
    <id>http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/</id>
    <published>2018-04-15T15:21:16.000Z</published>
    <updated>2018-04-17T12:40:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底层的 Log 实例部分。与 Produce 请求不一样的地方是，对于 Fetch 请求，是有两种不同的来源：consumer 和 follower，consumer 读取数据与副本同步数据都是通过向 leader 发送 Fetch 请求来实现的，在对这两种不同情况处理过程中，其底层的实现是统一的，只是实现方法的参数不同而已，在本文中会详细讲述对这两种不同情况的处理。</p>
<h2 id="Fetch-请求处理的整体流程"><a href="#Fetch-请求处理的整体流程" class="headerlink" title="Fetch 请求处理的整体流程"></a>Fetch 请求处理的整体流程</h2><p>Fetch 请求（读请求）的处理与 Produce 请求（写请求）的整体流程非常类似，读和写由最上面的抽象层做入口，最终还是在存储层的 Log 对象实例进行真正的读写操作，在这一点上，Kafka 封装的非常清晰，这样的系统设计是非常值得学习的，甚至可以作为分布式系统的模范系统来学习。</p>
<p>Fetch 请求处理的整体流程如下图所示，与 Produce 请求的处理流程非常相似。</p>
<p><img src="/images/kafka/kafka_fetch_request.png" alt="Server 端处理 Fetch 请求的总体过程"></p>
<h3 id="Fetch-请求的来源"><a href="#Fetch-请求的来源" class="headerlink" title="Fetch 请求的来源"></a>Fetch 请求的来源</h3><p>那 Server 要处理的 Fetch 请求有几种类型呢？来自于哪里呢？第一个来源肯定是 Consumer，Consumer 在消费数据时会向 Server 端发送 Fetch 请求，那么是不是还没有其他的类型，对 Kafka 比较熟悉的同学大概会猜到，还有一种就是：副本同步，follower 在从 leader 同步数据时，也是发送的 Fetch 请求，下面看下这两种情况的具体实现（代码会进行简化，并不完全与源码一致，便于理解）。</p>
<h4 id="Consumer-Fetch-请求"><a href="#Consumer-Fetch-请求" class="headerlink" title="Consumer Fetch 请求"></a>Consumer Fetch 请求</h4><p>Consumer 的 Fetch 请求是在 poll 方法中调用的，Fetcher 请求的构造过程及发送如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Set-up a fetch request for any node that we have assigned partitions for which doesn't already have</div><div class="line"> * an in-flight fetch or pending fetch data.</div><div class="line"> * <span class="doctag">@return</span> number of fetches sent</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向订阅的所有 partition （只要该 leader 暂时没有拉取请求）所在 leader 发送 fetch请求</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">//note: 1 创建 Fetch Request</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; fetchRequestMap = createFetchRequests();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchRequest.Builder&gt; fetchEntry : fetchRequestMap.entrySet()) &#123;</div><div class="line">        <span class="keyword">final</span> FetchRequest.Builder request = fetchEntry.getValue();</div><div class="line">        <span class="keyword">final</span> Node fetchTarget = fetchEntry.getKey();</div><div class="line"></div><div class="line">        log.debug(<span class="string">"Sending fetch for partitions &#123;&#125; to broker &#123;&#125;"</span>, request.fetchData().keySet(), fetchTarget);</div><div class="line">        <span class="comment">//note: 2 发送 Fetch Request</span></div><div class="line">        client.send(fetchTarget, request)</div><div class="line">                .addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</div><div class="line">                        ...</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">                        ...</div><div class="line">                    &#125;</div><div class="line">                &#125;);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> fetchRequestMap.size();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Create fetch requests for all nodes for which we have assigned partitions</div><div class="line"> * that have no existing requests in flight.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 为所有 node 创建 fetch request</span></div><div class="line"><span class="keyword">private</span> Map&lt;Node, FetchRequest.Builder&gt; createFetchRequests() &#123;</div><div class="line">    <span class="comment">// create the fetch info</span></div><div class="line">    Cluster cluster = metadata.fetch();</div><div class="line">    Map&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; fetchable = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (TopicPartition partition : fetchablePartitions()) &#123;</div><div class="line">        Node node = cluster.leaderFor(partition);</div><div class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</div><div class="line">            metadata.requestUpdate();</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.client.pendingRequestCount(node) == <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">// if there is a leader and no in-flight requests, issue a new fetch</span></div><div class="line">            LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt; fetch = fetchable.get(node);</div><div class="line">            <span class="keyword">if</span> (fetch == <span class="keyword">null</span>) &#123;</div><div class="line">                fetch = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</div><div class="line">                fetchable.put(node, fetch);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">long</span> position = <span class="keyword">this</span>.subscriptions.position(partition);</div><div class="line">            <span class="comment">//note: 要 fetch 的 position 以及 fetch 的大小</span></div><div class="line">            fetch.put(partition, <span class="keyword">new</span> FetchRequest.PartitionData(position, <span class="keyword">this</span>.fetchSize));</div><div class="line">            log.trace(<span class="string">"Added fetch request for partition &#123;&#125; at offset &#123;&#125; to node &#123;&#125;"</span>, partition, position, node);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            log.trace(<span class="string">"Skipping fetch for partition &#123;&#125; because there is an in-flight request to &#123;&#125;"</span>, partition, node);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// create the fetches</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; requests = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; entry : fetchable.entrySet()) &#123;</div><div class="line">        Node node = entry.getKey();</div><div class="line">        <span class="comment">// 构造 Fetch 请求</span></div><div class="line">        FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</div><div class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></div><div class="line">        requests.put(node, fetch);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> requests;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出，Consumer 的 Fetcher 请求构造为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</div><div class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></div></pre></td></tr></table></figure>
<h4 id="Replica-同步-Fetch-请求"><a href="#Replica-同步-Fetch-请求" class="headerlink" title="Replica 同步 Fetch 请求"></a>Replica 同步 Fetch 请求</h4><p>在 Replica 同步（Replica 同步流程的讲解将会在下篇文章中详细展开）的 Fetch 请求中，其 Fetch 请求的构造如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, consumer 的该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>与 Consumer Fetch 请求进行对比，这里区别仅在于在构造 FetchRequest 时，调用了 <code>setReplicaId()</code> 方法设置了对应的 replicaId，而 Consumer 在构造时则没有进行设置，该值默认为 <code>CONSUMER_REPLICA_ID</code>，即 <strong>-1</strong>，这个值是作为 Consumer 的 Fetch 请求与 Replica 同步的 Fetch 请求的区分。</p>
<h2 id="Server-端的处理"><a href="#Server-端的处理" class="headerlink" title="Server 端的处理"></a>Server 端的处理</h2><p>这里开始真正讲解 Fetch 请求的处理过程，会按照前面图中的处理流程开始讲解，本节主要是 Server 端抽象层的内容。</p>
<h3 id="KafkaApis-如何处理-Fetch-请求"><a href="#KafkaApis-如何处理-Fetch-请求" class="headerlink" title="KafkaApis 如何处理 Fetch 请求"></a>KafkaApis 如何处理 Fetch 请求</h3><p>关于 Fetch 请求的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle a fetch request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> fetchRequest = request.body.asInstanceOf[<span class="type">FetchRequest</span>]</div><div class="line">  <span class="keyword">val</span> versionId = request.header.apiVersion</div><div class="line">  <span class="keyword">val</span> clientId = request.header.clientId</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断 tp 是否存在以及是否有 Describe 权限</span></div><div class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = fetchRequest.fetchData.asScala.toSeq.partition &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic)) &amp;&amp; metadataCache.contains(tp.topic)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断 tp 是否有 Read 权限</span></div><div class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForReadRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Read</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 不存在或没有 Describe 权限的 topic 返回 UNKNOWN_TOPIC_OR_PARTITION 错误</span></div><div class="line">  <span class="keyword">val</span> nonExistingOrUnauthorizedForDescribePartitionData = nonExistingOrUnauthorizedForDescribeTopics.map &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 没有 Read 权限的 topic 返回 TOPIC_AUTHORIZATION_FAILED 错误</span></div><div class="line">  <span class="keyword">val</span> unauthorizedForReadPartitionData = unauthorizedForReadRequestInfo.map &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// the callback for sending a fetch response</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responsePartitionData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)]) &#123;</div><div class="line">    ....</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetchResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</div><div class="line">      trace(<span class="string">s"Sending fetch response to client <span class="subst">$clientId</span> of "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;convertedPartitionData.map &#123; case (_, v) =&gt; v.records.sizeInBytes &#125;</span>.sum&#125; bytes"</span>)</div><div class="line">      <span class="keyword">val</span> fetchResponse = <span class="keyword">if</span> (delayTimeMs &gt; <span class="number">0</span>) <span class="keyword">new</span> <span class="type">FetchResponse</span>(versionId, fetchedPartitionData, delayTimeMs) <span class="keyword">else</span> response</div><div class="line">      requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, fetchResponse))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></div><div class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</div><div class="line"></div><div class="line">    <span class="comment">//note: 配额情况的处理</span></div><div class="line">    <span class="keyword">if</span> (fetchRequest.isFromFollower) &#123;</div><div class="line">      <span class="comment">// We've already evaluated against the quota and are good to go. Just need to record it now.</span></div><div class="line">      <span class="keyword">val</span> responseSize = sizeOfThrottledPartitions(versionId, fetchRequest, mergedPartitionData, quotas.leader)</div><div class="line">      quotas.leader.record(responseSize)</div><div class="line">      fetchResponseCallback(<span class="number">0</span>)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      quotas.fetch.recordAndMaybeThrottle(request.session.sanitizedUser, clientId, response.sizeOf, fetchResponseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</div><div class="line">    sendResponseCallback(<span class="type">Seq</span>.empty)</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// call the replica manager to fetch messages from the local replica</span></div><div class="line">    <span class="comment">//note: 从 replica 上拉取数据,满足条件后调用回调函数进行返回</span></div><div class="line">    replicaManager.fetchMessages(</div><div class="line">      fetchRequest.maxWait.toLong, <span class="comment">//note: 拉取请求最长的等待时间</span></div><div class="line">      fetchRequest.replicaId, <span class="comment">//note: Replica 编号，Consumer 的为 -1</span></div><div class="line">      fetchRequest.minBytes, <span class="comment">//note: 拉取请求设置的最小拉取字节</span></div><div class="line">      fetchRequest.maxBytes, <span class="comment">//note: 拉取请求设置的最大拉取字节</span></div><div class="line">      versionId &lt;= <span class="number">2</span>,</div><div class="line">      authorizedRequestInfo,</div><div class="line">      replicationQuota(fetchRequest),</div><div class="line">      sendResponseCallback)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Fetch 请求处理的真正实现是在 replicaManager 的 <code>fetchMessages()</code> 方法中，在这里，可以看出，无论是 Fetch 请求还是 Produce 请求，都是通过副本管理器来实现的，副本管理器（ReplicaManager）管理的对象是分区实例（Partition），而每个分区都会与相应的副本实例对应（Replica），在这个节点上的副本又会与唯一的 Log 实例对应，正如流程图的上半部分一样，Server 就是通过这几部分抽象概念来管理真正存储层的内容。</p>
<h3 id="ReplicaManager-如何处理-Fetch-请求"><a href="#ReplicaManager-如何处理-Fetch-请求" class="headerlink" title="ReplicaManager 如何处理 Fetch 请求"></a>ReplicaManager 如何处理 Fetch 请求</h3><p>ReplicaManger 处理 Fetch 请求的入口在 <code>fetchMessages()</code> 方法。</p>
<h4 id="fetchMessages"><a href="#fetchMessages" class="headerlink" title="fetchMessages"></a>fetchMessages</h4><p><code>fetchMessages()</code> 方法的具体如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Fetch messages from the leader replica, and wait until enough data can be fetched and return;</div><div class="line"> * the callback function will be triggered either when timeout or required fetch info is satisfied</div><div class="line"> */</div><div class="line"><span class="comment">//note: 从 leader 拉取数据,等待拉取到足够的数据或者达到 timeout 时间后返回拉取的结果</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetchMessages</span></span>(timeout: <span class="type">Long</span>,</div><div class="line">                  replicaId: <span class="type">Int</span>,</div><div class="line">                  fetchMinBytes: <span class="type">Int</span>,</div><div class="line">                  fetchMaxBytes: <span class="type">Int</span>,</div><div class="line">                  hardMaxBytesLimit: <span class="type">Boolean</span>,</div><div class="line">                  fetchInfos: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</div><div class="line">                  quota: <span class="type">ReplicaQuota</span> = <span class="type">UnboundedQuota</span>,</div><div class="line">                  responseCallback: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> isFromFollower = replicaId &gt;= <span class="number">0</span> <span class="comment">//note: 判断请求是来自 consumer （这个值为 -1）还是副本同步</span></div><div class="line">  <span class="comment">//note: 默认都是从 leader 拉取，推测这个值只是为了后续能从 follower 消费数据而设置的</span></div><div class="line">  <span class="keyword">val</span> fetchOnlyFromLeader: <span class="type">Boolean</span> = replicaId != <span class="type">Request</span>.<span class="type">DebuggingConsumerId</span></div><div class="line">  <span class="comment">//note: 如果拉取请求来自 consumer（true）,只拉取 HW 以内的数据,如果是来自 Replica 同步,则没有该限制（false）。</span></div><div class="line">  <span class="keyword">val</span> fetchOnlyCommitted: <span class="type">Boolean</span> = ! <span class="type">Request</span>.isValidBrokerId(replicaId)</div><div class="line"></div><div class="line">  <span class="comment">// read from local logs</span></div><div class="line">  <span class="comment">//note：获取本地日志</span></div><div class="line">  <span class="keyword">val</span> logReadResults = readFromLocalLog(</div><div class="line">    replicaId = replicaId,</div><div class="line">    fetchOnlyFromLeader = fetchOnlyFromLeader,</div><div class="line">    readOnlyCommitted = fetchOnlyCommitted,</div><div class="line">    fetchMaxBytes = fetchMaxBytes,</div><div class="line">    hardMaxBytesLimit = hardMaxBytesLimit,</div><div class="line">    readPartitionInfo = fetchInfos,</div><div class="line">    quota = quota)</div><div class="line"></div><div class="line">  <span class="comment">// if the fetch comes from the follower,</span></div><div class="line">  <span class="comment">// update its corresponding log end offset</span></div><div class="line">  <span class="comment">//note: 如果 fetch 来自 broker 的副本同步,那么就更新相关的 log end offset</span></div><div class="line">  <span class="keyword">if</span>(<span class="type">Request</span>.isValidBrokerId(replicaId))</div><div class="line">    updateFollowerLogReadResults(replicaId, logReadResults)</div><div class="line"></div><div class="line">  <span class="comment">// check if this fetch request can be satisfied right away</span></div><div class="line">  <span class="keyword">val</span> logReadResultValues = logReadResults.map &#123; <span class="keyword">case</span> (_, v) =&gt; v &#125;</div><div class="line">  <span class="keyword">val</span> bytesReadable = logReadResultValues.map(_.info.records.sizeInBytes).sum</div><div class="line">  <span class="keyword">val</span> errorReadingData = logReadResultValues.foldLeft(<span class="literal">false</span>) ((errorIncurred, readResult) =&gt;</div><div class="line">    errorIncurred || (readResult.error != <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line"></div><div class="line">  <span class="comment">// respond immediately if 1) fetch request does not want to wait</span></div><div class="line">  <span class="comment">//                        2) fetch request does not require any data</span></div><div class="line">  <span class="comment">//                        3) has enough data to respond</span></div><div class="line">  <span class="comment">//                        4) some error happens while reading data</span></div><div class="line">  <span class="comment">//note: 如果满足以下条件的其中一个,将会立马返回结果:</span></div><div class="line">  <span class="comment">//note: 1. timeout 达到; 2. 拉取结果为空; 3. 拉取到足够的数据; 4. 拉取是遇到 error</span></div><div class="line">  <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span> || fetchInfos.isEmpty || bytesReadable &gt;= fetchMinBytes || errorReadingData) &#123;</div><div class="line">    <span class="keyword">val</span> fetchPartitionData = logReadResults.map &#123; <span class="keyword">case</span> (tp, result) =&gt;</div><div class="line">      tp -&gt; <span class="type">FetchPartitionData</span>(result.error, result.hw, result.info.records)</div><div class="line">    &#125;</div><div class="line">    responseCallback(fetchPartitionData)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">//note： 其他情况下,延迟发送结果</span></div><div class="line">    <span class="comment">// construct the fetch results from the read results</span></div><div class="line">    <span class="keyword">val</span> fetchPartitionStatus = logReadResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</div><div class="line">      <span class="keyword">val</span> fetchInfo = fetchInfos.collectFirst &#123;</div><div class="line">        <span class="keyword">case</span> (tp, v) <span class="keyword">if</span> tp == topicPartition =&gt; v</div><div class="line">      &#125;.getOrElse(sys.error(<span class="string">s"Partition <span class="subst">$topicPartition</span> not found in fetchInfos"</span>))</div><div class="line">      (topicPartition, <span class="type">FetchPartitionStatus</span>(result.info.fetchOffsetMetadata, fetchInfo))</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> fetchMetadata = <span class="type">FetchMetadata</span>(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, fetchOnlyFromLeader,</div><div class="line">      fetchOnlyCommitted, isFromFollower, replicaId, fetchPartitionStatus)</div><div class="line">    <span class="keyword">val</span> delayedFetch = <span class="keyword">new</span> <span class="type">DelayedFetch</span>(timeout, fetchMetadata, <span class="keyword">this</span>, quota, responseCallback)</div><div class="line"></div><div class="line">    <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed fetch operation</span></div><div class="line">    <span class="keyword">val</span> delayedFetchKeys = fetchPartitionStatus.map &#123; <span class="keyword">case</span> (tp, _) =&gt; <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(tp) &#125;</div><div class="line"></div><div class="line">    <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory;</span></div><div class="line">    <span class="comment">// this is because while the delayed fetch operation is being created, new requests</span></div><div class="line">    <span class="comment">// may arrive and hence make this operation completable.</span></div><div class="line">    delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>整体来说，分为以下几步：</p>
<ol>
<li><code>readFromLocalLog()</code>：调用该方法，从本地日志拉取相应的数据；</li>
<li>判断 Fetch 请求来源，如果来自副本同步，那么更新该副本的 the end offset 记录，如果该副本不在 isr 中，并判断是否需要更新 isr；</li>
<li>返回结果，满足条件的话立马返回，否则的话，通过延迟操作，延迟返回结果。</li>
</ol>
<h4 id="readFromLocalLog"><a href="#readFromLocalLog" class="headerlink" title="readFromLocalLog"></a>readFromLocalLog</h4><p><code>readFromLocalLog()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Read from multiple topic partitions at the given offset up to maxSize bytes</div><div class="line"> */</div><div class="line"><span class="comment">//note: 按 offset 从 tp 列表中读取相应的数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readFromLocalLog</span></span>(replicaId: <span class="type">Int</span>,</div><div class="line">                     fetchOnlyFromLeader: <span class="type">Boolean</span>,</div><div class="line">                     readOnlyCommitted: <span class="type">Boolean</span>,</div><div class="line">                     fetchMaxBytes: <span class="type">Int</span>,</div><div class="line">                     hardMaxBytesLimit: <span class="type">Boolean</span>,</div><div class="line">                     readPartitionInfo: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</div><div class="line">                     quota: <span class="type">ReplicaQuota</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = &#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(tp: <span class="type">TopicPartition</span>, fetchInfo: <span class="type">PartitionData</span>, limitBytes: <span class="type">Int</span>, minOneMessage: <span class="type">Boolean</span>): <span class="type">LogReadResult</span> = &#123;</div><div class="line">    <span class="keyword">val</span> offset = fetchInfo.offset</div><div class="line">    <span class="keyword">val</span> partitionFetchSize = fetchInfo.maxBytes</div><div class="line"></div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).totalFetchRequestRate.mark()</div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalFetchRequestRate.mark()</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      trace(<span class="string">s"Fetching log segment for partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>, partition fetch size <span class="subst">$partitionFetchSize</span>, "</span> +</div><div class="line">        <span class="string">s"remaining response limit <span class="subst">$limitBytes</span>"</span> +</div><div class="line">        (<span class="keyword">if</span> (minOneMessage) <span class="string">s", ignoring response/partition size limits"</span> <span class="keyword">else</span> <span class="string">""</span>))</div><div class="line"></div><div class="line">      <span class="comment">// decide whether to only fetch from leader</span></div><div class="line">      <span class="comment">//note: 根据决定 [是否只从 leader 读取数据] 来获取相应的副本</span></div><div class="line">      <span class="comment">//note: 根据 tp 获取 Partition 对象, 在获取相应的 Replica 对象</span></div><div class="line">      <span class="keyword">val</span> localReplica = <span class="keyword">if</span> (fetchOnlyFromLeader)</div><div class="line">        getLeaderReplicaIfLocal(tp)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        getReplicaOrException(tp)</div><div class="line"></div><div class="line">      <span class="comment">// decide whether to only fetch committed data (i.e. messages below high watermark)</span></div><div class="line">      <span class="comment">//note: 获取 hw 位置，副本同步不设置这个值</span></div><div class="line">      <span class="keyword">val</span> maxOffsetOpt = <span class="keyword">if</span> (readOnlyCommitted)</div><div class="line">        <span class="type">Some</span>(localReplica.highWatermark.messageOffset)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">None</span></div><div class="line"></div><div class="line">      <span class="comment">/* Read the LogOffsetMetadata prior to performing the read from the log.</span></div><div class="line">       * We use the LogOffsetMetadata to determine if a particular replica is in-sync or not.</div><div class="line">       * Using the log end offset after performing the read can lead to a race condition</div><div class="line">       * where data gets appended to the log immediately after the replica has consumed from it</div><div class="line">       * This can cause a replica to always be out of sync.</div><div class="line">       */</div><div class="line">      <span class="keyword">val</span> initialLogEndOffset = localReplica.logEndOffset.messageOffset <span class="comment">//note: the end offset</span></div><div class="line">      <span class="keyword">val</span> initialHighWatermark = localReplica.highWatermark.messageOffset <span class="comment">//note: hw</span></div><div class="line">      <span class="keyword">val</span> fetchTimeMs = time.milliseconds</div><div class="line">      <span class="keyword">val</span> logReadInfo = localReplica.log <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</div><div class="line">          <span class="keyword">val</span> adjustedFetchSize = math.min(partitionFetchSize, limitBytes)</div><div class="line"></div><div class="line">          <span class="comment">// Try the read first, this tells us whether we need all of adjustedFetchSize for this partition</span></div><div class="line">          <span class="comment">//note: 从指定的 offset 位置开始读取数据，副本同步不需要 maxOffsetOpt</span></div><div class="line">          <span class="keyword">val</span> fetch = log.read(offset, adjustedFetchSize, maxOffsetOpt, minOneMessage)</div><div class="line"></div><div class="line">          <span class="comment">// If the partition is being throttled, simply return an empty set.</span></div><div class="line">          <span class="keyword">if</span> (shouldLeaderThrottle(quota, tp, replicaId)) <span class="comment">//note: 如果被限速了,那么返回 空 集合</span></div><div class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">          <span class="comment">// For FetchRequest version 3, we replace incomplete message sets with an empty one as consumers can make</span></div><div class="line">          <span class="comment">// progress in such cases and don't need to report a `RecordTooLargeException`</span></div><div class="line">          <span class="keyword">else</span> <span class="keyword">if</span> (!hardMaxBytesLimit &amp;&amp; fetch.firstEntryIncomplete)</div><div class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">          <span class="keyword">else</span> fetch</div><div class="line"></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          error(<span class="string">s"Leader for partition <span class="subst">$tp</span> does not have a local log"</span>)</div><div class="line">          <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 返回最后的结果,返回的都是 LogReadResult 对象</span></div><div class="line">      <span class="type">LogReadResult</span>(info = logReadInfo,</div><div class="line">                    hw = initialHighWatermark,</div><div class="line">                    leaderLogEndOffset = initialLogEndOffset,</div><div class="line">                    fetchTimeMs = fetchTimeMs,</div><div class="line">                    readSize = partitionFetchSize,</div><div class="line">                    exception = <span class="type">None</span>)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// <span class="doctag">NOTE:</span> Failed fetch requests metric is not incremented for known exceptions since it</span></div><div class="line">      <span class="comment">// is supposed to indicate un-expected failure of a broker in handling a fetch request</span></div><div class="line">      <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</div><div class="line">               _: <span class="type">NotLeaderForPartitionException</span> |</div><div class="line">               _: <span class="type">ReplicaNotAvailableException</span> |</div><div class="line">               _: <span class="type">OffsetOutOfRangeException</span>) =&gt;</div><div class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</div><div class="line">                      hw = <span class="number">-1</span>L,</div><div class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</div><div class="line">                      fetchTimeMs = <span class="number">-1</span>L,</div><div class="line">                      readSize = partitionFetchSize,</div><div class="line">                      exception = <span class="type">Some</span>(e))</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).failedFetchRequestRate.mark()</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().failedFetchRequestRate.mark()</div><div class="line">        error(<span class="string">s"Error processing fetch operation on partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>"</span>, e)</div><div class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</div><div class="line">                      hw = <span class="number">-1</span>L,</div><div class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</div><div class="line">                      fetchTimeMs = <span class="number">-1</span>L,</div><div class="line">                      readSize = partitionFetchSize,</div><div class="line">                      exception = <span class="type">Some</span>(e))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> limitBytes = fetchMaxBytes</div><div class="line">  <span class="keyword">val</span> result = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]</div><div class="line">  <span class="keyword">var</span> minOneMessage = !hardMaxBytesLimit</div><div class="line">  readPartitionInfo.foreach &#123; <span class="keyword">case</span> (tp, fetchInfo) =&gt;</div><div class="line">    <span class="keyword">val</span> readResult = read(tp, fetchInfo, limitBytes, minOneMessage) <span class="comment">//note: 读取该 tp 的数据</span></div><div class="line">    <span class="keyword">val</span> messageSetSize = readResult.info.records.sizeInBytes</div><div class="line">    <span class="comment">// Once we read from a non-empty partition, we stop ignoring request and partition level size limits</span></div><div class="line">    <span class="keyword">if</span> (messageSetSize &gt; <span class="number">0</span>)</div><div class="line">      minOneMessage = <span class="literal">false</span></div><div class="line">    limitBytes = math.max(<span class="number">0</span>, limitBytes - messageSetSize)</div><div class="line">    result += (tp -&gt; readResult)</div><div class="line">  &#125;</div><div class="line">  result</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>readFromLocalLog()</code> 方法的处理过程：</p>
<ol>
<li>先根据要拉取的 topic-partition 获取对应的 Partition 对象，根据 Partition 对象获取对应的 Replica 对象；</li>
<li>根据 Replica 对象找到对应的 Log 对象，然后调用其 <code>read()</code> 方法从指定的位置读取数据。</li>
</ol>
<h2 id="存储层对-Fetch-请求的处理"><a href="#存储层对-Fetch-请求的处理" class="headerlink" title="存储层对 Fetch 请求的处理"></a>存储层对 Fetch 请求的处理</h2><p>接着前面的流程开始往下走。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>每个 Replica 会对应一个 log 对象，而每个 log 对象会管理相应的 LogSegment 实例。</p>
<h4 id="read"><a href="#read" class="headerlink" title="read()"></a>read()</h4><p>Log 对象的 <code>read()</code> 方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 从指定 offset 开始读取数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxLength: <span class="type">Int</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>] = <span class="type">None</span>, minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</div><div class="line">  trace(<span class="string">"Reading %d bytes from offset %d in log %s of length %d bytes"</span>.format(maxLength, startOffset, name, size))</div><div class="line"></div><div class="line">  <span class="comment">// Because we don't use lock for reading, the synchronization is a little bit tricky.</span></div><div class="line">  <span class="comment">// We create the local variables to avoid race conditions with updates to the log.</span></div><div class="line">  <span class="keyword">val</span> currentNextOffsetMetadata = nextOffsetMetadata</div><div class="line">  <span class="keyword">val</span> next = currentNextOffsetMetadata.messageOffset</div><div class="line">  <span class="keyword">if</span>(startOffset == next)</div><div class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(currentNextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line"></div><div class="line">  <span class="comment">//note: 先查找对应的日志分段（segment）</span></div><div class="line">  <span class="keyword">var</span> entry = segments.floorEntry(startOffset)</div><div class="line"></div><div class="line">  <span class="comment">// attempt to read beyond the log end offset is an error</span></div><div class="line">  <span class="keyword">if</span>(startOffset &gt; next || entry == <span class="literal">null</span>)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(<span class="string">"Request for offset %d but we only have log segments in the range %d to %d."</span>.format(startOffset, segments.firstKey, next))</div><div class="line"></div><div class="line">  <span class="comment">// Do the read on the segment with a base offset less than the target offset</span></div><div class="line">  <span class="comment">// but if that segment doesn't contain any messages with an offset greater than that</span></div><div class="line">  <span class="comment">// continue to read from successive segments until we get some messages or we reach the end of the log</span></div><div class="line">  <span class="keyword">while</span>(entry != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">// If the fetch occurs on the active segment, there might be a race condition where two fetch requests occur after</span></div><div class="line">    <span class="comment">// the message is appended but before the nextOffsetMetadata is updated. In that case the second fetch may</span></div><div class="line">    <span class="comment">// cause OffsetOutOfRangeException. To solve that, we cap the reading up to exposed position instead of the log</span></div><div class="line">    <span class="comment">// end of the active segment.</span></div><div class="line">    <span class="comment">//note: 如果 Fetch 请求刚好发生在 the active segment 上,当多个 Fetch 请求同时处理,如果 nextOffsetMetadata 更新不及时,可能会导致</span></div><div class="line">    <span class="comment">//note: 发送 OffsetOutOfRangeException 异常; 为了解决这个问题, 这里能读取的最大位置是对应的物理位置（exposedPos）</span></div><div class="line">    <span class="comment">//note: 而不是 the log end of the active segment.</span></div><div class="line">    <span class="keyword">val</span> maxPosition = &#123;</div><div class="line">      <span class="keyword">if</span> (entry == segments.lastEntry) &#123;</div><div class="line">        <span class="comment">//note: nextOffsetMetadata 对应的实际物理位置</span></div><div class="line">        <span class="keyword">val</span> exposedPos = nextOffsetMetadata.relativePositionInSegment.toLong</div><div class="line">        <span class="comment">// Check the segment again in case a new segment has just rolled out.</span></div><div class="line">        <span class="keyword">if</span> (entry != segments.lastEntry) <span class="comment">//note: 可能会有新的 segment 产生,所以需要再次判断</span></div><div class="line">          <span class="comment">// New log segment has rolled out, we can read up to the file end.</span></div><div class="line">          entry.getValue.size</div><div class="line">        <span class="keyword">else</span></div><div class="line">          exposedPos</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        entry.getValue.size</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 从 segment 中读取相应的数据</span></div><div class="line">    <span class="keyword">val</span> fetchInfo = entry.getValue.read(startOffset, maxOffset, maxLength, maxPosition, minOneMessage)</div><div class="line">    <span class="keyword">if</span>(fetchInfo == <span class="literal">null</span>) &#123; <span class="comment">//note: 如果该日志分段没有读取到数据,则读取更高的日志分段</span></div><div class="line">      entry = segments.higherEntry(entry.getKey)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">return</span> fetchInfo</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// okay we are beyond the end of the last segment with no data fetched although the start offset is in range,</span></div><div class="line">  <span class="comment">// this can happen when all messages with offset larger than start offsets have been deleted.</span></div><div class="line">  <span class="comment">// In this case, we will return the empty set with log end offset metadata</span></div><div class="line">  <span class="type">FetchDataInfo</span>(nextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从实现可以看出，该方法会先查找对应的 Segment 对象（日志分段），然后循环直到读取到数据结束，如果当前的日志分段没有读取到相应的数据，那么会更新日志分段及对应的最大位置。</p>
<p>日志分段实际上是逻辑概念，它管理了物理概念的一个数据文件、一个时间索引文件和一个 offset 索引文件，读取日志分段时，会先读取 offset 索引文件再读取数据文件，具体步骤如下：</p>
<ol>
<li>根据要读取的起始偏移量（startOffset）读取 offset 索引文件中对应的物理位置；</li>
<li>查找 offset 索引文件最后返回：起始偏移量对应的最近物理位置（startPosition）；</li>
<li>根据 startPosition 直接定位到数据文件，然后读取数据文件内容；</li>
<li>最多能读到数据文件的结束位置（maxPosition）。</li>
</ol>
<h3 id="LogSegment"><a href="#LogSegment" class="headerlink" title="LogSegment"></a>LogSegment</h3><p>关乎 数据文件、offset 索引文件和时间索引文件真正的操作都是在 LogSegment 对象中的，日志读取也与这个方法息息相关。</p>
<h4 id="read-1"><a href="#read-1" class="headerlink" title="read()"></a>read()</h4><p><code>read()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 读取日志分段（副本同步不会设置 maxSize）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>], maxSize: <span class="type">Int</span>, maxPosition: <span class="type">Long</span> = size,</div><div class="line">         minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (maxSize &lt; <span class="number">0</span>)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Invalid max size for log read (%d)"</span>.format(maxSize))</div><div class="line"></div><div class="line">  <span class="comment">//note: log 文件物理长度</span></div><div class="line">  <span class="keyword">val</span> logSize = log.sizeInBytes <span class="comment">// this may change, need to save a consistent copy</span></div><div class="line">  <span class="comment">//note: 将起始的 offset 转换为起始的实际物理位置</span></div><div class="line">  <span class="keyword">val</span> startOffsetAndSize = translateOffset(startOffset)</div><div class="line"></div><div class="line">  <span class="comment">// if the start position is already off the end of the log, return null</span></div><div class="line">  <span class="keyword">if</span> (startOffsetAndSize == <span class="literal">null</span>)</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> startPosition = startOffsetAndSize.position.toInt</div><div class="line">  <span class="keyword">val</span> offsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(startOffset, <span class="keyword">this</span>.baseOffset, startPosition)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> adjustedMaxSize =</div><div class="line">    <span class="keyword">if</span> (minOneMessage) math.max(maxSize, startOffsetAndSize.size)</div><div class="line">    <span class="keyword">else</span> maxSize</div><div class="line"></div><div class="line">  <span class="comment">// return a log segment but with zero size in the case below</span></div><div class="line">  <span class="keyword">if</span> (adjustedMaxSize == <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line"></div><div class="line">  <span class="comment">// calculate the length of the message set to read based on whether or not they gave us a maxOffset</span></div><div class="line">  <span class="comment">//note: 计算读取的长度</span></div><div class="line">  <span class="keyword">val</span> length = maxOffset <span class="keyword">match</span> &#123;</div><div class="line">    <span class="comment">//note: 副本同步时的计算方式</span></div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="comment">// no max offset, just read until the max position</span></div><div class="line">      min((maxPosition - startPosition).toInt, adjustedMaxSize) <span class="comment">//note: 直接读取到最大的位置</span></div><div class="line">    <span class="comment">//note: consumer 拉取时,计算方式</span></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt;</div><div class="line">      <span class="comment">// there is a max offset, translate it to a file position and use that to calculate the max read size;</span></div><div class="line">      <span class="comment">// when the leader of a partition changes, it's possible for the new leader's high watermark to be less than the</span></div><div class="line">      <span class="comment">// true high watermark in the previous leader for a short window. In this window, if a consumer fetches on an</span></div><div class="line">      <span class="comment">// offset between new leader's high watermark and the log end offset, we want to return an empty response.</span></div><div class="line">      <span class="keyword">if</span> (offset &lt; startOffset)</div><div class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>, firstEntryIncomplete = <span class="literal">false</span>)</div><div class="line">      <span class="keyword">val</span> mapping = translateOffset(offset, startPosition)</div><div class="line">      <span class="keyword">val</span> endPosition =</div><div class="line">        <span class="keyword">if</span> (mapping == <span class="literal">null</span>)</div><div class="line">          logSize <span class="comment">// the max offset is off the end of the log, use the end of the file</span></div><div class="line">        <span class="keyword">else</span></div><div class="line">          mapping.position</div><div class="line">      min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 根据起始的物理位置和读取长度读取数据文件</span></div><div class="line">  <span class="type">FetchDataInfo</span>(offsetMetadata, log.read(startPosition, length),</div><div class="line">    firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的实现来看，上述过程分为以下三部分：</p>
<ol>
<li>根据 startOffset 得到实际的物理位置（<code>translateOffset()</code>）；</li>
<li>计算要读取的实际物理长度；</li>
<li>根据实际起始物理位置和要读取实际物理长度读取数据文件。</li>
</ol>
<h4 id="translateOffset"><a href="#translateOffset" class="headerlink" title="translateOffset()"></a>translateOffset()</h4><p><code>translateOffset()</code> 方法的实现过程主要分为两部分：</p>
<ol>
<li>查找 offset 索引文件：调用 offset 索引文件的 <code>lookup()</code> 查找方法，获取离 startOffset 最接近的物理位置；</li>
<li>调用数据文件的 <code>searchFor()</code> 方法，从指定的物理位置开始读取每条数据，知道找到对应 offset 的物理位置。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">translateOffset</span></span>(offset: <span class="type">Long</span>, startingFilePosition: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogEntryPosition</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取离 offset 最新的物理位置,返回包括 offset 和物理位置（不是准确值）</span></div><div class="line">  <span class="keyword">val</span> mapping = index.lookup(offset)</div><div class="line">  <span class="comment">//note: 从指定的位置开始消费,直到找到 offset 对应的实际物理位置,返回包括 offset 和物理位置（准确值）</span></div><div class="line">  log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="查找-offset-索引文件"><a href="#查找-offset-索引文件" class="headerlink" title="查找 offset 索引文件"></a>查找 offset 索引文件</h5><p>offset 索引文件是使用内存映射（不了解的，可以阅读 <a href="http://matt33.com/2018/02/04/linux-mmap/">操作系统之共享对象学习</a>）的方式加载到内存中的，在查询的过程中，内存映射是会发生变化，所以在 <code>lookup()</code> 中先拷贝出来了一个（idx），然后再进行查询，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 查找小于等于指定 offset 的最大 offset,并且返回对应的 offset 和实际物理位置</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(targetOffset: <span class="type">Long</span>): <span class="type">OffsetPosition</span> = &#123;</div><div class="line">  maybeLock(lock) &#123;</div><div class="line">    <span class="keyword">val</span> idx = mmap.duplicate <span class="comment">//note: 查询时,mmap 会发生变化,先复制出来一个</span></div><div class="line">    <span class="keyword">val</span> slot = indexSlotFor(idx, targetOffset, <span class="type">IndexSearchType</span>.<span class="type">KEY</span>) <span class="comment">//note: 二分查找</span></div><div class="line">    <span class="keyword">if</span>(slot == <span class="number">-1</span>)</div><div class="line">      <span class="type">OffsetPosition</span>(baseOffset, <span class="number">0</span>)</div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="comment">//note: 先计算绝对偏移量,再计算物理位置</span></div><div class="line">      parseEntry(idx, slot).asInstanceOf[<span class="type">OffsetPosition</span>]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">parseEntry</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">IndexEntry</span> = &#123;</div><div class="line">    <span class="type">OffsetPosition</span>(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">relativeOffset</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize)</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">physical</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize + <span class="number">4</span>)</div></pre></td></tr></table></figure>
<p>关于 relativeOffset 和 physical 的计算方法，可以参考下面这张图（来自《Kafka 计算内幕》）：</p>
<p><img src="/images/kafka/offset-physical.png" alt="根据索引条目编号查找偏移量的值和物理位置的值"></p>
<h5 id="搜索数据文件获取准确的物理位置"><a href="#搜索数据文件获取准确的物理位置" class="headerlink" title="搜索数据文件获取准确的物理位置"></a>搜索数据文件获取准确的物理位置</h5><p>前面通过 offset 索引文件获取的物理位置是一个接近值，下面通过实际读取数据文件将会得到一个真正的准确值，它是通过遍历数据文件实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Search forward for the file position of the last offset that is greater than or equal to the target offset</div><div class="line"> * and return its physical position and the size of the message (including log overhead) at the returned offset. If</div><div class="line"> * no such offsets are found, return null.</div><div class="line"> *</div><div class="line"> * @param targetOffset The offset to search for.</div><div class="line"> * @param startingPosition The starting position in the file to begin searching from.</div><div class="line"> */</div><div class="line">public <span class="type">LogEntryPosition</span> searchForOffsetWithSize(long targetOffset, int startingPosition) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="type">FileChannelLogEntry</span> entry : shallowEntriesFrom(startingPosition)) &#123;</div><div class="line">        long offset = entry.offset();</div><div class="line">        <span class="keyword">if</span> (offset &gt;= targetOffset)</div><div class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">LogEntryPosition</span>(offset, entry.position(), entry.sizeInBytes());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里，一个 Fetch 请求的处理过程算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</title>
    <link href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/"/>
    <id>http://matt33.com/2018/03/18/kafka-server-handle-produce-request/</id>
    <published>2018-03-18T08:32:01.000Z</published>
    <updated>2018-03-18T08:45:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer Client 的 Produce 请求为入口。但是 Server 端的内容较多，本篇文章并不能全部涵盖，涉及到其他内容，在本篇文章暂时先不详细讲述，后面会再分析，本篇文章会以 Server 处理 produce 为主线，主要详细讲解 Kafka 存储层的内容。</p>
<h2 id="produce-请求处理整体流程"><a href="#produce-请求处理整体流程" class="headerlink" title="produce 请求处理整体流程"></a>produce 请求处理整体流程</h2><p>根据在这篇 <a href="http://matt33.com/2017/06/25/kafka-producer-send-module/">Kafka 源码解析之 Producer 发送模型（一）</a> 中的讲解，在 Producer Client 端，Producer 会维护一个 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches</code> 的变量，然后会根据 topic-partition 的 leader 信息，将 leader 在同一台机器上的 batch 放在一个 request 中，发送到 server，这样可以节省很多网络开销，提高发送效率。</p>
<p>Producer Client 发送请求的方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 produce 请求</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;RecordBatch&gt; batches)</span> </span>&#123;</div><div class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</div><div class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</div><div class="line">    <span class="keyword">for</span> (RecordBatch batch : batches) &#123;</div><div class="line">        TopicPartition tp = batch.topicPartition;</div><div class="line">        produceRecordsByPartition.put(tp, batch.records());</div><div class="line">        recordsByPartition.put(tp, batch);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    ProduceRequest.Builder requestBuilder =</div><div class="line">            <span class="keyword">new</span> ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</div><div class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</div><div class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</div><div class="line">        &#125;</div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    String nodeId = Integer.toString(destination);</div><div class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, callback);</div><div class="line">    client.send(clientRequest, now);</div><div class="line">    log.trace(<span class="string">"Sent produce request to &#123;&#125;: &#123;&#125;"</span>, nodeId, requestBuilder);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在发送 Produce 的请求里，Client 是把一个 <code>Map&lt;TopicPartition, MemoryRecords&gt;</code> 类型的 <code>produceRecordsByPartition</code> 作为内容发送给了 Server 端，那么 Server 端是如何处理这个请求的呢？这就是本篇文章要讲述的内容，Server 处理这个请求的总体逻辑如下图所示：</p>
<p><img src="/images/kafka/kafka_produce_process.png" alt="Server 端处理 produce 请求的总体过程"></p>
<p>Broker 在收到 Produce 请求后，会有一个 KafkaApis 进行处理，KafkaApis 是 Server 端处理所有请求的入口，它会负责将请求的具体处理交给相应的组件进行处理，从上图可以看到 Produce 请求是交给了 ReplicaManager 对象进行处理了。</p>
<h2 id="Server-端处理"><a href="#Server-端处理" class="headerlink" title="Server 端处理"></a>Server 端处理</h2><p>Server 端的处理过程会按照上图的流程一块一块去介绍。</p>
<h3 id="KafkaApis-处理-Produce-请求"><a href="#KafkaApis-处理-Produce-请求" class="headerlink" title="KafkaApis 处理 Produce 请求"></a>KafkaApis 处理 Produce 请求</h3><p>KafkaApis 处理 produce 请求是在 <code>handleProducerRequest()</code> 方法中完成，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle a produce request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleProducerRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> produceRequest = request.body.asInstanceOf[<span class="type">ProduceRequest</span>]</div><div class="line">  <span class="keyword">val</span> numBytesAppended = request.header.sizeOf + produceRequest.sizeOf</div><div class="line"></div><div class="line">  <span class="comment">//note: 按 exist 和有 Describe 权限进行筛选</span></div><div class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = produceRequest.partitionRecords.asScala.partition &#123;</div><div class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic)) &amp;&amp; metadataCache.contains(topicPartition.topic)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断有没有 Write 权限</span></div><div class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForWriteRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</div><div class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Write</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// the callback for sending a produce response</span></div><div class="line">  <span class="comment">//note: 回调函数</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> mergedResponseStatus = responseStatus ++</div><div class="line">      unauthorizedForWriteRequestInfo.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)) ++</div><div class="line">      nonExistingOrUnauthorizedForDescribeTopics.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>))</div><div class="line"></div><div class="line">    <span class="keyword">var</span> errorInResponse = <span class="literal">false</span></div><div class="line"></div><div class="line">    mergedResponseStatus.foreach &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</div><div class="line">      <span class="keyword">if</span> (status.error != <span class="type">Errors</span>.<span class="type">NONE</span>) &#123;</div><div class="line">        errorInResponse = <span class="literal">true</span></div><div class="line">        debug(<span class="string">"Produce request with correlation id %d from client %s on partition %s failed due to %s"</span>.format(</div><div class="line">          request.header.correlationId,</div><div class="line">          request.header.clientId,</div><div class="line">          topicPartition,</div><div class="line">          status.error.exceptionName))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">produceResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</div><div class="line">      <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">// no operation needed if producer request.required.acks = 0; however, if there is any error in handling</span></div><div class="line">        <span class="comment">// the request, since no response is expected by the producer, the server will close socket server so that</span></div><div class="line">        <span class="comment">// the producer client will know that some error has happened and will refresh its metadata</span></div><div class="line">        <span class="comment">//note: 因为设置的 ack=0, 相当于 client 会默认发送成功了,如果 server 在处理的过程出现了错误,那么就会关闭 socket 连接来间接地通知 client</span></div><div class="line">        <span class="comment">//note: client 会重新刷新 meta,重新建立相应的连接</span></div><div class="line">        <span class="keyword">if</span> (errorInResponse) &#123;</div><div class="line">          <span class="keyword">val</span> exceptionsSummary = mergedResponseStatus.map &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</div><div class="line">            topicPartition -&gt; status.error.exceptionName</div><div class="line">          &#125;.mkString(<span class="string">", "</span>)</div><div class="line">          info(</div><div class="line">            <span class="string">s"Closing connection due to error during produce request with correlation id <span class="subst">$&#123;request.header.correlationId&#125;</span> "</span> +</div><div class="line">              <span class="string">s"from client id <span class="subst">$&#123;request.header.clientId&#125;</span> with ack=0\n"</span> +</div><div class="line">              <span class="string">s"Topic and partition to exceptions: <span class="subst">$exceptionsSummary</span>"</span></div><div class="line">          )</div><div class="line">          requestChannel.closeConnection(request.processor, request)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          requestChannel.noOperation(request.processor, request)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> respBody = request.header.apiVersion <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava)</div><div class="line">          <span class="keyword">case</span> version@(<span class="number">1</span> | <span class="number">2</span>) =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava, delayTimeMs, version)</div><div class="line">          <span class="comment">// This case shouldn't happen unless a new version of ProducerRequest is added without</span></div><div class="line">          <span class="comment">// updating this part of the code to handle it properly.</span></div><div class="line">          <span class="keyword">case</span> version =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Version `<span class="subst">$version</span>` of ProduceRequest is not handled. Code must be updated."</span>)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, respBody))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></div><div class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</div><div class="line"></div><div class="line">    quotas.produce.recordAndMaybeThrottle(</div><div class="line">      request.session.sanitizedUser,</div><div class="line">      request.header.clientId,</div><div class="line">      numBytesAppended,</div><div class="line">      produceResponseCallback)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</div><div class="line">    sendResponseCallback(<span class="type">Map</span>.empty)</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">val</span> internalTopicsAllowed = request.header.clientId == <span class="type">AdminUtils</span>.<span class="type">AdminClientId</span></div><div class="line"></div><div class="line">    <span class="comment">// call the replica manager to append messages to the replicas</span></div><div class="line">    <span class="comment">//note: 追加 Record</span></div><div class="line">    replicaManager.appendRecords(</div><div class="line">      produceRequest.timeout.toLong,</div><div class="line">      produceRequest.acks,</div><div class="line">      internalTopicsAllowed,</div><div class="line">      authorizedRequestInfo,</div><div class="line">      sendResponseCallback)</div><div class="line"></div><div class="line">    <span class="comment">// if the request is put into the purgatory, it will have a held reference</span></div><div class="line">    <span class="comment">// and hence cannot be garbage collected; hence we clear its data here in</span></div><div class="line">    <span class="comment">// order to let GC re-claim its memory since it is already appended to log</span></div><div class="line">    produceRequest.clearPartitionRecords()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>总体来说，处理过程是（在权限系统的情况下）：</p>
<ol>
<li>查看 topic 是否存在，以及 client 是否有相应的 Desribe 权限；</li>
<li>对于已经有 Describe 权限的 topic 查看是否有 Write 权限；</li>
<li>调用 <code>replicaManager.appendRecords()</code> 方法向有 Write 权限的 topic-partition 追加相应的 record。</li>
</ol>
<h3 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h3><p>ReplicaManager 顾名思义，它就是副本管理器，副本管理器的作用是管理这台 broker 上的所有副本（replica）。在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。</p>
<p>Kafka Server 在启动的时候，会创建 ReplicaManager 对象，如下所示。在 ReplicaManager 的构造方法中，它需要 LogManager 作为成员变量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//kafka.server.KafkaServer</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line">    <span class="comment">/* start replica manager */</span></div><div class="line">    replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)</div><div class="line">    replicaManager.startup()</div><div class="line">  &#125;<span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">    isStartingUp.set(<span class="literal">false</span>)</div><div class="line">    shutdown()</div><div class="line">    <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ReplicaManager 的<strong>并不负责具体的日志创建，它只是管理 Broker 上的所有分区</strong>（也就是图中下一步的那个 Partition 对象）。在创建 Partition 对象时，它需要 ReplicaManager 的 logManager 对象，Partition 会通过这个 logManager 对象为每个 replica 创建对应的日志。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR</div><div class="line"> */</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Partition</span>(<span class="params">val topic: <span class="type">String</span>,</span></span></div><div class="line">                val partitionId: <span class="type">Int</span>,</div><div class="line">                time: <span class="type">Time</span>,</div><div class="line">                replicaManager: <span class="type">ReplicaManager</span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partitionId)</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> localBrokerId = replicaManager.config.brokerId</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logManager = replicaManager.logManager <span class="comment">//note: 日志管理器</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ReplicaManager 与 LogManger 对比如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 后面还会介绍，这篇文章先不详细展开。</p>
<h4 id="appendRecords-实现"><a href="#appendRecords-实现" class="headerlink" title="appendRecords() 实现"></a><code>appendRecords()</code> 实现</h4><p>下面我们来看 <code>appendRecords()</code> 方法的具体实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向 partition 的 leader 写入数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecords</span></span>(timeout: <span class="type">Long</span>,</div><div class="line">                  requiredAcks: <span class="type">Short</span>,</div><div class="line">                  internalTopicsAllowed: <span class="type">Boolean</span>,</div><div class="line">                  entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</div><div class="line">                  responseCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (isValidRequiredAcks(requiredAcks)) &#123; <span class="comment">//note: acks 设置有效</span></div><div class="line">    <span class="keyword">val</span> sTime = time.milliseconds</div><div class="line">    <span class="comment">//note: 向本地的副本 log 追加数据</span></div><div class="line">    <span class="keyword">val</span> localProduceResults = appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)</div><div class="line">    debug(<span class="string">"Produce to local log in %d ms"</span>.format(time.milliseconds - sTime))</div><div class="line"></div><div class="line">    <span class="keyword">val</span> produceStatus = localProduceResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</div><div class="line">      topicPartition -&gt;</div><div class="line">              <span class="type">ProducePartitionStatus</span>(</div><div class="line">                result.info.lastOffset + <span class="number">1</span>, <span class="comment">// required offset</span></div><div class="line">                <span class="keyword">new</span> <span class="type">PartitionResponse</span>(result.error, result.info.firstOffset, result.info.logAppendTime)) <span class="comment">// response status</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</div><div class="line">      <span class="comment">//note: 处理 ack=-1 的情况,需要等到 isr 的 follower 都写入成功的话,才能返回最后结果</span></div><div class="line">      <span class="comment">// create delayed produce operation</span></div><div class="line">      <span class="keyword">val</span> produceMetadata = <span class="type">ProduceMetadata</span>(requiredAcks, produceStatus)</div><div class="line">      <span class="comment">//note: 延迟 produce 请求</span></div><div class="line">      <span class="keyword">val</span> delayedProduce = <span class="keyword">new</span> <span class="type">DelayedProduce</span>(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback)</div><div class="line"></div><div class="line">      <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed produce operation</span></div><div class="line">      <span class="keyword">val</span> producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(_)).toSeq</div><div class="line"></div><div class="line">      <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory</span></div><div class="line">      <span class="comment">// this is because while the delayed produce operation is being created, new</span></div><div class="line">      <span class="comment">// requests may arrive and hence make this operation completable.</span></div><div class="line">      delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</div><div class="line"></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we can respond immediately</span></div><div class="line">      <span class="comment">//note: 通过回调函数直接返回结果</span></div><div class="line">      <span class="keyword">val</span> produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</div><div class="line">      responseCallback(produceResponseStatus)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// If required.acks is outside accepted range, something is wrong with the client</span></div><div class="line">    <span class="comment">// Just return an error and don't handle the request at all</span></div><div class="line">    <span class="comment">//note: 返回 INVALID_REQUIRED_ACKS 错误</span></div><div class="line">    <span class="keyword">val</span> responseStatus = entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, _) =&gt;</div><div class="line">      topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">INVALID_REQUIRED_ACKS</span>,</div><div class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.firstOffset, <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span>)</div><div class="line">    &#125;</div><div class="line">    responseCallback(responseStatus)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的实现来看，<code>appendRecords()</code> 的实现主要分为以下几步：</p>
<ol>
<li>首先判断 acks 设置是否有效（-1，0，1三个值有效），无效的话直接返回异常，不再处理；</li>
<li>acks 设置有效的话，调用 <code>appendToLocalLog()</code> 方法将 records 追加到本地对应的 log 对象中；</li>
<li><code>appendToLocalLog()</code> 处理完后，如果发现 clients 设置的 acks=-1，即需要 isr 的其他的副本同步完成才能返回 response，那么就会创建一个 DelayedProduce 对象，等待 isr 的其他副本进行同步，否则的话直接返回追加的结果。</li>
</ol>
<h4 id="appendToLocalLog-的实现"><a href="#appendToLocalLog-的实现" class="headerlink" title="appendToLocalLog() 的实现"></a><code>appendToLocalLog()</code> 的实现</h4><p>追加日志的实际操作是在 <code>appendToLocalLog()</code>  中完成的，这里看下它的具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Append the messages to the local replica logs</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向本地的 replica 写入数据</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">appendToLocalLog</span></span>(internalTopicsAllowed: <span class="type">Boolean</span>,</div><div class="line">                             entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</div><div class="line">                             requiredAcks: <span class="type">Short</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">LogAppendResult</span>] = &#123;</div><div class="line">  trace(<span class="string">"Append [%s] to local log "</span>.format(entriesPerPartition))</div><div class="line">  entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, records) =&gt; <span class="comment">//note: 遍历要写的所有 topic-partition</span></div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).totalProduceRequestRate.mark()</div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalProduceRequestRate.mark()</div><div class="line"></div><div class="line">    <span class="comment">// reject appending to internal topics if it is not allowed</span></div><div class="line">    <span class="comment">//note: 不能向 kafka 内部使用的 topic 追加数据</span></div><div class="line">    <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topicPartition.topic) &amp;&amp; !internalTopicsAllowed) &#123;</div><div class="line">      (topicPartition, <span class="type">LogAppendResult</span>(</div><div class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>,</div><div class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">InvalidTopicException</span>(<span class="string">s"Cannot append to internal topic <span class="subst">$&#123;topicPartition.topic&#125;</span>"</span>))))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">//note: 查找对应的 Partition,并向分区对应的副本写入数据文件</span></div><div class="line">        <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取 topic-partition 的 Partition 对象</span></div><div class="line">        <span class="keyword">val</span> info = partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">            partition.appendRecordsToLeader(records, requiredAcks) <span class="comment">//note: 如果找到了这个对象,就开始追加日志</span></div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">"Partition %s doesn't exist on %d"</span></div><div class="line">            .format(topicPartition, localBrokerId)) <span class="comment">//note: 没有找到的话,返回异常</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 追加的 msg 数</span></div><div class="line">        <span class="keyword">val</span> numAppendedMessages =</div><div class="line">          <span class="keyword">if</span> (info.firstOffset == <span class="number">-1</span>L || info.lastOffset == <span class="number">-1</span>L)</div><div class="line">            <span class="number">0</span></div><div class="line">          <span class="keyword">else</span></div><div class="line">            info.lastOffset - info.firstOffset + <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment">// update stats for successfully appended bytes and messages as bytesInRate and messageInRate</span></div><div class="line">        <span class="comment">//note:  更新 metrics</span></div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesInRate.mark(records.sizeInBytes)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesInRate.mark(records.sizeInBytes)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).messagesInRate.mark(numAppendedMessages)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.messagesInRate.mark(numAppendedMessages)</div><div class="line"></div><div class="line">        trace(<span class="string">"%d bytes written to log %s-%d beginning at offset %d and ending at offset %d"</span></div><div class="line">          .format(records.sizeInBytes, topicPartition.topic, topicPartition.partition, info.firstOffset, info.lastOffset))</div><div class="line">        (topicPartition, <span class="type">LogAppendResult</span>(info))</div><div class="line">      &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理追加过程中出现的异常</span></div><div class="line">        <span class="comment">// <span class="doctag">NOTE:</span> Failed produce requests metric is not incremented for known exceptions</span></div><div class="line">        <span class="comment">// it is supposed to indicate un-expected failures of a broker in handling a produce request</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">          fatal(<span class="string">"Halting due to unrecoverable I/O error while handling produce request: "</span>, e)</div><div class="line">          <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">          (topicPartition, <span class="literal">null</span>)</div><div class="line">        <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</div><div class="line">                 _: <span class="type">NotLeaderForPartitionException</span> |</div><div class="line">                 _: <span class="type">RecordTooLargeException</span> |</div><div class="line">                 _: <span class="type">RecordBatchTooLargeException</span> |</div><div class="line">                 _: <span class="type">CorruptRecordException</span> |</div><div class="line">                 _: <span class="type">InvalidTimestampException</span>) =&gt;</div><div class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(e)))</div><div class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">          <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).failedProduceRequestRate.mark()</div><div class="line">          <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.failedProduceRequestRate.mark()</div><div class="line">          error(<span class="string">"Error processing append operation on partition %s"</span>.format(topicPartition), t)</div><div class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(t)))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看到 <code>appendToLocalLog()</code> 的实现如下：</p>
<ol>
<li>首先判断要写的 topic 是不是 Kafka 内置的 topic，内置的 topic 是不允许 Producer 写入的；</li>
<li>先查找 topic-partition 对应的 Partition 对象，如果在 <code>allPartitions</code> 中查找到了对应的 partition，那么直接调用 <code>partition.appendRecordsToLeader()</code> 方法追加相应的 records，否则会向 client 抛出异常。</li>
</ol>
<h3 id="Partition-appendRecordsToLeader-方法"><a href="#Partition-appendRecordsToLeader-方法" class="headerlink" title="Partition.appendRecordsToLeader() 方法"></a>Partition.appendRecordsToLeader() 方法</h3><p>ReplicaManager 在追加 records 时，调用的是 Partition 的 <code>appendRecordsToLeader()</code> 方法，其具体的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecordsToLeader</span></span>(records: <span class="type">MemoryRecords</span>, requiredAcks: <span class="type">Int</span> = <span class="number">0</span>) = &#123;</div><div class="line">  <span class="keyword">val</span> (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> log = leaderReplica.log.get <span class="comment">//note: 获取对应的 Log 对象</span></div><div class="line">        <span class="keyword">val</span> minIsr = log.config.minInSyncReplicas</div><div class="line">        <span class="keyword">val</span> inSyncSize = inSyncReplicas.size</div><div class="line"></div><div class="line">        <span class="comment">// Avoid writing to leader if there are not enough insync replicas to make it safe</span></div><div class="line">        <span class="comment">//note: 如果 ack 设置为-1, isr 数小于设置的 min.isr 时,就会向 producer 抛出相应的异常</span></div><div class="line">        <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == <span class="number">-1</span>) &#123;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotEnoughReplicasException</span>(<span class="string">"Number of insync replicas for partition %s is [%d], below required minimum [%d]"</span></div><div class="line">            .format(topicPartition, inSyncSize, minIsr))</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 向副本对应的 log 追加响应的数据</span></div><div class="line">        <span class="keyword">val</span> info = log.append(records, assignOffsets = <span class="literal">true</span>)</div><div class="line">        <span class="comment">// probably unblock some follower fetch requests since log end offset has been updated</span></div><div class="line">        replicaManager.tryCompleteDelayedFetch(<span class="type">TopicPartitionOperationKey</span>(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</div><div class="line">        <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line">        <span class="comment">//note: 判断是否需要增加 HHW（追加日志后会进行一次判断）</span></div><div class="line">        (info, maybeIncrementLeaderHW(leaderReplica))</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">//note: leader 不在本台机器上</span></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">"Leader not local for partition %s on broker %d"</span></div><div class="line">          .format(topicPartition, localBrokerId))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line"></div><div class="line">  info</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法里，会根据 topic 的 <code>min.isrs</code> 配置以及当前这个 partition 的 isr 情况判断是否可以写入，如果不满足条件，就会抛出 <code>NotEnoughReplicasException</code> 的异常，如果满足条件，就会调用 <code>log.append()</code> 向 replica 追加日志。</p>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><p>跟着最开始图中的流程及代码分析，走到这里，才算是到了 Kafka 的存储层部分，在这里会详细讲述在存储层 Kafka 如何写入日志。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>在上面有过一些介绍，每个 replica 会对应一个 log 对象，log 对象是管理当前分区的一个单位，它会包含这个分区的所有 segment 文件（包括对应的 offset 索引和时间戳索引文件），它会提供一些增删查的方法。</p>
<p>在 Log 对象的初始化时，有三个变量是比较重要的：</p>
<ol>
<li><code>nextOffsetMetadata</code>：可以叫做下一个偏移量元数据，它包括 activeSegment 的下一条消息的偏移量，该 activeSegment 的基准偏移量及日志分段的大小；</li>
<li><code>activeSegment</code>：指的是该 Log 管理的 segments 中那个最新的 segment（这里叫做活跃的 segment），一个 Log 中只会有一个活跃的 segment，其他的 segment 都已经被持久化到磁盘了；</li>
<li><code>logEndOffset</code>：表示下一条消息的 offset，它取自 <code>nextOffsetMetadata</code> 的 offset，实际上就是活动日志分段的下一个偏移量。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: nextOffsetMetadata 声明为 volatile，如果该值被修改，其他使用此变量的线程就可以立刻见到变化后的值，在生产和消费都会使用到这个值</span></div><div class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> nextOffsetMetadata: <span class="type">LogOffsetMetadata</span> = _</div><div class="line"></div><div class="line"><span class="comment">/* Calculate the offset of the next message */</span></div><div class="line"><span class="comment">//note: 下一个偏移量元数据</span></div><div class="line"><span class="comment">//note: 第一个参数：下一条消息的偏移量；第二个参数：日志分段的基准偏移量；第三个参数：日志分段大小</span></div><div class="line">nextOffsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(activeSegment.nextOffset(), activeSegment.baseOffset, activeSegment.size.toInt)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* The active segment that is currently taking appends</div><div class="line">*/</div><div class="line"><span class="comment">//note: 任何时刻，只会有一个活动的日志分段</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">activeSegment</span> </span>= segments.lastEntry.getValue</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">*  The offset of the next message that will be appended to the log</div><div class="line">*/</div><div class="line"><span class="comment">//note: 下一条消息的 offset，从 nextOffsetMetadata 中获取的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logEndOffset</span></span>: <span class="type">Long</span> = nextOffsetMetadata.messageOffset</div></pre></td></tr></table></figure>
<h4 id="日志写入"><a href="#日志写入" class="headerlink" title="日志写入"></a>日志写入</h4><p>在 Log 中一个重要的方法就是日志的写入方法，下面来看下这个方法的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Append this message set to the active segment of the log, rolling over to a fresh segment if necessary.</div><div class="line"> *</div><div class="line"> * This method will generally be responsible for assigning offsets to the messages,</div><div class="line"> * however if the assignOffsets=false flag is passed we will only check that the existing offsets are valid.</div><div class="line"> *</div><div class="line"> * @param records The log records to append</div><div class="line"> * @param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given</div><div class="line"> * @throws KafkaStorageException If the append fails due to an I/O error.</div><div class="line"> * @return Information about the appended messages including the first and last offset.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向 active segment 追加 log,必要的情况下,滚动创建新的 segment</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</div><div class="line">  <span class="keyword">val</span> appendInfo = analyzeAndValidateRecords(records) <span class="comment">//note: 返回这批消息的该要信息,并对这批 msg 进行校验</span></div><div class="line"></div><div class="line">  <span class="comment">// if we have any valid messages, append them to the log</span></div><div class="line">  <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> appendInfo</div><div class="line"></div><div class="line">  <span class="comment">// trim any invalid bytes or partial messages before appending it to the on-disk log</span></div><div class="line">  <span class="comment">//note: 删除这批消息中无效的消息</span></div><div class="line">  <span class="keyword">var</span> validRecords = trimInvalidBytes(records, appendInfo)</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// they are valid, insert them in the log</span></div><div class="line">    lock synchronized &#123;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (assignOffsets) &#123;</div><div class="line">        <span class="comment">// assign offsets to the message set</span></div><div class="line">        <span class="comment">//note: 计算这个消息集起始 offset，对 offset 的操作是一个原子操作</span></div><div class="line">        <span class="keyword">val</span> offset = <span class="keyword">new</span> <span class="type">LongRef</span>(nextOffsetMetadata.messageOffset)</div><div class="line">        appendInfo.firstOffset = offset.value <span class="comment">//note: 作为消息集的第一个 offset</span></div><div class="line">        <span class="keyword">val</span> now = time.milliseconds <span class="comment">//note: 设置的时间错以 server 收到的时间戳为准</span></div><div class="line">        <span class="comment">//note: 验证消息,并为没条 record 设置相应的 offset 和 timestrap</span></div><div class="line">        <span class="keyword">val</span> validateAndOffsetAssignResult = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(validRecords,</div><div class="line">                                                        offset,</div><div class="line">                                                        now,</div><div class="line">                                                        appendInfo.sourceCodec,</div><div class="line">                                                        appendInfo.targetCodec,</div><div class="line">                                                        config.compact,</div><div class="line">                                                        config.messageFormatVersion.messageFormatVersion,</div><div class="line">                                                        config.messageTimestampType,</div><div class="line">                                                        config.messageTimestampDifferenceMaxMs)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Error in validating messages while appending to log '%s'"</span>.format(name), e)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: 返回已经计算好 offset 和 timestrap 的 MemoryRecords</span></div><div class="line">        validRecords = validateAndOffsetAssignResult.validatedRecords</div><div class="line">        appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</div><div class="line">        appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</div><div class="line">        appendInfo.lastOffset = offset.value - <span class="number">1</span> <span class="comment">//note: 最后一条消息的 offset</span></div><div class="line">        <span class="keyword">if</span> (config.messageTimestampType == <span class="type">TimestampType</span>.<span class="type">LOG_APPEND_TIME</span>)</div><div class="line">          appendInfo.logAppendTime = now</div><div class="line"></div><div class="line">        <span class="comment">// re-validate message sizes if there's a possibility that they have changed (due to re-compression or message</span></div><div class="line">        <span class="comment">// format conversion)</span></div><div class="line">        <span class="comment">//note: 更新 metrics 的记录</span></div><div class="line">        <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) &#123;</div><div class="line">          <span class="keyword">for</span> (logEntry &lt;- validRecords.shallowEntries.asScala) &#123;</div><div class="line">            <span class="keyword">if</span> (logEntry.sizeInBytes &gt; config.maxMessageSize) &#123;</div><div class="line">              <span class="comment">// we record the original message set size instead of the trimmed size</span></div><div class="line">              <span class="comment">// to be consistent with pre-compression bytesRejectedRate recording</span></div><div class="line">              <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</div><div class="line">              <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(<span class="string">"Message size is %d bytes which exceeds the maximum configured message size of %d."</span></div><div class="line">                .format(logEntry.sizeInBytes, config.maxMessageSize))</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// we are taking the offsets we are given</span></div><div class="line">        <span class="keyword">if</span> (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &lt; nextOffsetMetadata.messageOffset)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Out of order offsets found in "</span> + records.deepEntries.asScala.map(_.offset))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// check messages set size may be exceed config.segmentSize</span></div><div class="line">      <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordBatchTooLargeException</span>(<span class="string">"Message set size is %d bytes which exceeds the maximum configured segment size of %d."</span></div><div class="line">          .format(validRecords.sizeInBytes, config.segmentSize))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// maybe roll the log if this segment is full</span></div><div class="line">      <span class="comment">//note: 如果当前 segment 满了，就需要重新新建一个 segment</span></div><div class="line">      <span class="keyword">val</span> segment = maybeRoll(messagesSize = validRecords.sizeInBytes,</div><div class="line">        maxTimestampInMessages = appendInfo.maxTimestamp,</div><div class="line">        maxOffsetInMessages = appendInfo.lastOffset)</div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// now append to the log</span></div><div class="line">      <span class="comment">//note: 追加消息到当前 segment</span></div><div class="line">      segment.append(firstOffset = appendInfo.firstOffset,</div><div class="line">        largestOffset = appendInfo.lastOffset,</div><div class="line">        largestTimestamp = appendInfo.maxTimestamp,</div><div class="line">        shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</div><div class="line">        records = validRecords)</div><div class="line"></div><div class="line">      <span class="comment">// increment the log end offset</span></div><div class="line">      <span class="comment">//note: 修改最新的 next_offset</span></div><div class="line">      updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</div><div class="line"></div><div class="line">      trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></div><div class="line">        .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)<span class="comment">//note: 满足条件的话，刷新磁盘</span></div><div class="line">        flush()</div><div class="line"></div><div class="line">      appendInfo</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaStorageException</span>(<span class="string">"I/O exception in append to log '%s'"</span>.format(name), e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Server 将每个分区的消息追加到日志中时，是以 segment 为单位的，当 segment 的大小到达阈值大小之后，会滚动新建一个日志分段（segment）保存新的消息，而分区的消息总是追加到最新的日志分段（也就是 activeSegment）中。每个日志分段都会有一个基准偏移量（segmentBaseOffset，或者叫做 baseOffset），这个基准偏移量就是分区级别的绝对偏移量，而且这个值在日志分段是固定的。有了这个基准偏移量，就可以计算出来每条消息在分区中的绝对偏移量，最后把数据以及对应的绝对偏移量写到日志文件中。<code>append()</code> 方法的过程可以总结如下：</p>
<ol>
<li><code>analyzeAndValidateRecords()</code>：对这批要写入的消息进行检测，主要是检查消息的大小及 crc 校验；</li>
<li><code>trimInvalidBytes()</code>：会将这批消息中无效的消息删除，返回一个都是有效消息的 MemoryRecords；</li>
<li><code>LogValidator.validateMessagesAndAssignOffsets()</code>：为每条消息设置相应的 offset（绝对偏移量） 和 timestrap；</li>
<li><code>maybeRoll()</code>：判断是否需要新建一个 segment 的，如果当前的 segment 放不下这批消息的话，需要新建一个 segment；</li>
<li><code>segment.append()</code>：向 segment 中添加消息；</li>
<li>更新 logEndOffset 和判断是否需要刷新磁盘（如果需要的话，调用 <code>flush()</code> 方法刷到磁盘）。</li>
</ol>
<p>关于 timestrap 的设置，这里也顺便介绍一下，在新版的 Kafka 中，每条 msg 都会有一个对应的时间戳记录，producer 端可以设置这个字段 <code>message.timestamp.type</code> 来选择 timestrap 的类型，默认是按照创建时间，只能选择从下面的选择中二选一：</p>
<ol>
<li><code>CreateTime</code>，默认值；</li>
<li><code>LogAppendTime</code>。</li>
</ol>
<h4 id="日志分段"><a href="#日志分段" class="headerlink" title="日志分段"></a>日志分段</h4><p>在 Log 的 <code>append()</code> 方法中，会调用 <code>maybeRoll()</code> 方法来判断是否需要进行相应日志分段操作，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Roll the log over to a new empty log segment if necessary.</div><div class="line"> *</div><div class="line"> * @param messagesSize The messages set size in bytes</div><div class="line"> * @param maxTimestampInMessages The maximum timestamp in the messages.</div><div class="line"> * logSegment will be rolled if one of the following conditions met</div><div class="line"> * &lt;ol&gt;</div><div class="line"> * &lt;li&gt; The logSegment is full</div><div class="line"> * &lt;li&gt; The maxTime has elapsed since the timestamp of first message in the segment (or since the create time if</div><div class="line"> * the first message does not have a timestamp)</div><div class="line"> * &lt;li&gt; The index is full</div><div class="line"> * &lt;/ol&gt;</div><div class="line"> * @return The currently active segment after (perhaps) rolling to a new segment</div><div class="line"> */</div><div class="line"><span class="comment">//note: 判断是否需要创建日志分段,如果不需要返回当前分段,需要的话,返回新创建的日志分段</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeRoll</span></span>(messagesSize: <span class="type">Int</span>, maxTimestampInMessages: <span class="type">Long</span>, maxOffsetInMessages: <span class="type">Long</span>): <span class="type">LogSegment</span> = &#123;</div><div class="line">  <span class="keyword">val</span> segment = activeSegment <span class="comment">//note: 对活跃的日志分段进行判断,它也是最新的一个日志分段</span></div><div class="line">  <span class="keyword">val</span> now = time.milliseconds</div><div class="line">  <span class="comment">//note: 距离上次日志分段的时间是否达到了设置的阈值（log.roll.hours）</span></div><div class="line">  <span class="keyword">val</span> reachedRollMs = segment.timeWaitedForRoll(now, maxTimestampInMessages) &gt; config.segmentMs - segment.rollJitterMs</div><div class="line">  <span class="comment">//note: 这是五个条件: 1. 文件满了,不足以放心这么大的 messageSet; 2. 文件有数据,并且到分段的时间阈值; 3. 索引文件满了;</span></div><div class="line">  <span class="comment">//note: 4. 时间索引文件满了; 5. 最大的 offset，其相对偏移量超过了正整数的阈值</span></div><div class="line">  <span class="keyword">if</span> (segment.size &gt; config.segmentSize - messagesSize ||</div><div class="line">      (segment.size &gt; <span class="number">0</span> &amp;&amp; reachedRollMs) ||</div><div class="line">      segment.index.isFull || segment.timeIndex.isFull || !segment.canConvertToRelativeOffset(maxOffsetInMessages)) &#123;</div><div class="line">    debug(<span class="string">s"Rolling new log segment in <span class="subst">$name</span> (log_size = <span class="subst">$&#123;segment.size&#125;</span>/<span class="subst">$&#123;config.segmentSize&#125;</span>&#125;, "</span> +</div><div class="line">        <span class="string">s"index_size = <span class="subst">$&#123;segment.index.entries&#125;</span>/<span class="subst">$&#123;segment.index.maxEntries&#125;</span>, "</span> +</div><div class="line">        <span class="string">s"time_index_size = <span class="subst">$&#123;segment.timeIndex.entries&#125;</span>/<span class="subst">$&#123;segment.timeIndex.maxEntries&#125;</span>, "</span> +</div><div class="line">        <span class="string">s"inactive_time_ms = <span class="subst">$&#123;segment.timeWaitedForRoll(now, maxTimestampInMessages)&#125;</span>/<span class="subst">$&#123;config.segmentMs - segment.rollJitterMs&#125;</span>)."</span>)</div><div class="line">    roll(maxOffsetInMessages - <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>) <span class="comment">//note: 创建新的日志分段</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    segment <span class="comment">//note: 使用当前的日志分段</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从 <code>maybeRoll()</code> 的实现可以看到，是否需要创建新的日志分段，有下面几种情况：</p>
<ol>
<li>当前日志分段的大小加上消息的大小超过了日志分段的阈值（<code>log.segment.bytes</code>）；</li>
<li>距离上次创建日志分段的时间达到了一定的阈值（<code>log.roll.hours</code>），并且数据文件有数据；</li>
<li>索引文件满了；</li>
<li>时间索引文件满了；</li>
<li>最大的 offset，其相对偏移量超过了正整数的阈值。</li>
</ol>
<p>如果上面的其中一个条件，就会创建新的 segment 文件，见 <code>roll()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Roll the log over to a new active segment starting with the current logEndOffset.</div><div class="line"> * This will trim the index to the exact size of the number of entries it currently contains.</div><div class="line"> *</div><div class="line"> * @return The newly rolled segment</div><div class="line"> */</div><div class="line"><span class="comment">//note: 滚动创建日志,并添加到日志管理的映射表中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">roll</span></span>(expectedNextOffset: <span class="type">Long</span> = <span class="number">0</span>): <span class="type">LogSegment</span> = &#123;</div><div class="line">  <span class="keyword">val</span> start = time.nanoseconds</div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> newOffset = <span class="type">Math</span>.max(expectedNextOffset, logEndOffset) <span class="comment">//note: 选择最新的 offset 作为基准偏移量</span></div><div class="line">    <span class="keyword">val</span> logFile = logFilename(dir, newOffset) <span class="comment">//note: 创建数据文件</span></div><div class="line">    <span class="keyword">val</span> indexFile = indexFilename(dir, newOffset) <span class="comment">//note: 创建 offset 索引文件</span></div><div class="line">    <span class="keyword">val</span> timeIndexFile = timeIndexFilename(dir, newOffset) <span class="comment">//note: 创建 time 索引文件</span></div><div class="line">    <span class="keyword">for</span>(file &lt;- <span class="type">List</span>(logFile, indexFile, timeIndexFile); <span class="keyword">if</span> file.exists) &#123;</div><div class="line">      warn(<span class="string">"Newly rolled segment file "</span> + file.getName + <span class="string">" already exists; deleting it first"</span>)</div><div class="line">      file.delete()</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    segments.lastEntry() <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="literal">null</span> =&gt;</div><div class="line">      <span class="keyword">case</span> entry =&gt; &#123;</div><div class="line">        <span class="keyword">val</span> seg = entry.getValue</div><div class="line">        seg.onBecomeInactiveSegment()</div><div class="line">        seg.index.trimToValidSize()</div><div class="line">        seg.timeIndex.trimToValidSize()</div><div class="line">        seg.log.trim()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 创建一个 segment 对象</span></div><div class="line">    <span class="keyword">val</span> segment = <span class="keyword">new</span> <span class="type">LogSegment</span>(dir,</div><div class="line">                                 startOffset = newOffset,</div><div class="line">                                 indexIntervalBytes = config.indexInterval,</div><div class="line">                                 maxIndexSize = config.maxIndexSize,</div><div class="line">                                 rollJitterMs = config.randomSegmentJitter,</div><div class="line">                                 time = time,</div><div class="line">                                 fileAlreadyExists = <span class="literal">false</span>,</div><div class="line">                                 initFileSize = initFileSize,</div><div class="line">                                 preallocate = config.preallocate)</div><div class="line">    <span class="keyword">val</span> prev = addSegment(segment) <span class="comment">//note: 添加到日志管理中</span></div><div class="line">    <span class="keyword">if</span>(prev != <span class="literal">null</span>)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Trying to roll a new log segment for topic partition %s with start offset %d while it already exists."</span>.format(name, newOffset))</div><div class="line">    <span class="comment">// We need to update the segment base offset and append position data of the metadata when log rolls.</span></div><div class="line">    <span class="comment">// The next offset should not change.</span></div><div class="line">    updateLogEndOffset(nextOffsetMetadata.messageOffset) <span class="comment">//note: 更新 offset</span></div><div class="line">    <span class="comment">// schedule an asynchronous flush of the old segment</span></div><div class="line">    scheduler.schedule(<span class="string">"flush-log"</span>, () =&gt; flush(newOffset), delay = <span class="number">0</span>L)</div><div class="line"></div><div class="line">    info(<span class="string">"Rolled new log segment for '"</span> + name + <span class="string">"' in %.0f ms."</span>.format((<span class="type">System</span>.nanoTime - start) / (<span class="number">1000.0</span>*<span class="number">1000.0</span>)))</div><div class="line"></div><div class="line">    segment</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>创建一个 segment 对象，真正的实现是在 Log 的 <code>roll()</code> 方法中，也就是上面的方法中，创建 segment 对象，主要包括三部分：数据文件、offset 索引文件和 time 索引文件。</p>
<h4 id="offset-索引文件"><a href="#offset-索引文件" class="headerlink" title="offset 索引文件"></a>offset 索引文件</h4><p>这里顺便讲述一下 offset 索引文件，Kafka 的索引文件有下面一个特点：</p>
<ol>
<li>采用 <strong>绝对偏移量+相对偏移量</strong> 的方式进行存储的，每个 segment 最开始绝对偏移量也是其基准偏移量；</li>
<li>数据文件每隔一定的大小创建一个索引条目，而不是每条消息会创建索引条目，通过 <code>index.interval.bytes</code> 来配置，默认是 4096，也就是4KB；</li>
</ol>
<p>这样做的好处也非常明显：</p>
<ol>
<li>因为不是每条消息都创建相应的索引条目，所以索引条目是稀疏的；</li>
<li>索引的相对偏移量占据4个字节，而绝对偏移量占据8个字节，加上物理位置的4个字节，使用相对索引可以将每条索引条目的大小从12字节减少到8个字节；</li>
<li>因为偏移量有序的，再读取数据时，可以按照二分查找的方式去快速定位偏移量的位置；</li>
<li>这样的稀疏索引是可以完全放到内存中，加快偏移量的查找。</li>
</ol>
<h3 id="LogSegment-写入"><a href="#LogSegment-写入" class="headerlink" title="LogSegment 写入"></a>LogSegment 写入</h3><p>真正的日志写入，还是在 LogSegment 的 <code>append()</code> 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line"> * Append the given messages starting with the given offset. Add</div><div class="line"> * an entry to the index if needed.</div><div class="line"> *</div><div class="line"> * It is assumed this method is being called from within a lock.</div><div class="line"> *</div><div class="line"> * @param firstOffset The first offset in the message set.</div><div class="line"> * @param largestTimestamp The largest timestamp in the message set.</div><div class="line"> * @param shallowOffsetOfMaxTimestamp The offset of the message that has the largest timestamp in the messages to append.</div><div class="line"> * @param records The log entries to append.</div><div class="line"> */</div><div class="line"> <span class="comment">//note: 在指定的 offset 处追加指定的 msgs, 需要的情况下追加相应的索引</span></div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(firstOffset: <span class="type">Long</span>, largestOffset: <span class="type">Long</span>, largestTimestamp: <span class="type">Long</span>, shallowOffsetOfMaxTimestamp: <span class="type">Long</span>, records: <span class="type">MemoryRecords</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (records.sizeInBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">    trace(<span class="string">"Inserting %d bytes at offset %d at position %d with largest timestamp %d at shallow offset %d"</span></div><div class="line">        .format(records.sizeInBytes, firstOffset, log.sizeInBytes(), largestTimestamp, shallowOffsetOfMaxTimestamp))</div><div class="line">    <span class="keyword">val</span> physicalPosition = log.sizeInBytes()</div><div class="line">    <span class="keyword">if</span> (physicalPosition == <span class="number">0</span>)</div><div class="line">      rollingBasedTimestamp = <span class="type">Some</span>(largestTimestamp)</div><div class="line">    <span class="comment">// append the messages</span></div><div class="line">    require(canConvertToRelativeOffset(largestOffset), <span class="string">"largest offset in message set can not be safely converted to relative offset."</span>)</div><div class="line">    <span class="keyword">val</span> appendedBytes = log.append(records) <span class="comment">//note: 追加到数据文件中</span></div><div class="line">    trace(<span class="string">s"Appended <span class="subst">$appendedBytes</span> to <span class="subst">$&#123;log.file()&#125;</span> at offset <span class="subst">$firstOffset</span>"</span>)</div><div class="line">    <span class="comment">// Update the in memory max timestamp and corresponding offset.</span></div><div class="line">    <span class="keyword">if</span> (largestTimestamp &gt; maxTimestampSoFar) &#123;</div><div class="line">      maxTimestampSoFar = largestTimestamp</div><div class="line">      offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// append an entry to the index (if needed)</span></div><div class="line">    <span class="comment">//note: 判断是否需要追加索引（数据每次都会添加到数据文件中,但不是每次都会添加索引的,间隔 indexIntervalBytes 大小才会写入一个索引文件）</span></div><div class="line">    <span class="keyword">if</span>(bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123;</div><div class="line">      index.append(firstOffset, physicalPosition) <span class="comment">//note: 添加索引</span></div><div class="line">      timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</div><div class="line">      bytesSinceLastIndexEntry = <span class="number">0</span> <span class="comment">//note: 重置为0</span></div><div class="line">    &#125;</div><div class="line">    bytesSinceLastIndexEntry += records.sizeInBytes</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>经过上面的分析，一个消息集（MemoryRecords）在 Kafka 存储层的调用情况如下图所示：</p>
<p><img src="/images/kafka/log_append.png" alt="MemoryRecords 追加过程"></p>
<p>最后还是利用底层的 Java NIO 实现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之日志管理（十一）</title>
    <link href="http://matt33.com/2018/03/12/kafka-log-manager/"/>
    <id>http://matt33.com/2018/03/12/kafka-log-manager/</id>
    <published>2018-03-11T16:48:13.000Z</published>
    <updated>2018-03-11T16:26:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是比较多的，会分为三篇左右的文章介绍，本篇先介绍最简单的部分，主要是日志的基本概念、日志管理、日志刷新和日志清理四部分（后两个其实也属于日志管理，为便于讲解，这里分开讲述），日志的读写和分段将在下一篇讲述。</p>
<p>本篇主要的内容如下：</p>
<ol>
<li>Kafka 中 Log 的基本概念；</li>
<li>日志管理；</li>
<li>日志刷新；</li>
<li>日志清理；</li>
</ol>
<h2 id="日志的基本概念"><a href="#日志的基本概念" class="headerlink" title="日志的基本概念"></a>日志的基本概念</h2><p>在 Kafka 的官方文档中，最开始介绍 Kafka 的一句话是：</p>
<blockquote>
<p>Kafka is a distributed, partitioned, replicated commit log service. （0.10.0 之前）</p>
<p>Apache Kafka is a distributed streaming platform. （0.10.0 及之后）</p>
</blockquote>
<p>可以说在 KafkaStream 之前，Kafka 最开始的应用场景就是日志场景或 mq 场景，更多的扮演着一个存储系统，这是 Kafka 立家之本。</p>
<p>Kafka 是一个分布式的（distributed）、可分区的（partitioned）、支持多副本（replicated）的日志提交系统，分布式这个概念很好理解，Kafka 本身就是一个分布式系统，那另外两个概念什么意思呢？</p>
<ul>
<li>可分区的：一个 topic 是可以设置多个分区的，可分区解决了单 topic 线性扩展的问题（也解决了负载均衡的问题）；</li>
<li>支持多副本的：使得 topic 可以做到更多容错性，牺牲性能与空间去换取更高的可靠性。</li>
</ul>
<p>一个 Topic 基本结果如下：</p>
<p><img src="/images/2016-03-07-KafkaMessage/topic.png" alt="Topic"></p>
<p>图中的 topic 由三个 partition 组成，topic 在创建开始，每个 partition 在写入时，其 offset 值是从0开始逐渐增加。topic 的 partition 是可以分配到 Kafka 集群的任何节点上，在实际存储时，每个 partition 是按 segment 文件去存储的（segment 的大小是在 server 端配置的，这就是日志的分段），如下图所示：</p>
<p><img src="/images/2016-03-07-KafkaMessage/segment.png" alt="Segment"></p>
<blockquote>
<p>注：上图是 0.8.2.1 版的 segment 的结构，0.10.2.0 版每个 segment 还会有一个对应的 timestrap 文件。</p>
</blockquote>
<p>再简单介绍一下 topic 的副本的概念，kafka 中为了保证一定可靠性，一般会为设置多个副本，假设一个 topic 设置了三个副本：</p>
<ul>
<li>每个 partition 都会有三个副本，这个三个副本需要分配在不同的 broker 上，在同一台 broker 上的话，就没有什么意义了；</li>
<li>这个三个副本中，会有选举出来了一个 leader，另外两个就是 follower，topic 的读写都是在 leader 上进行的，follower 从 leader 同步 partition 的数据。</li>
</ul>
<blockquote>
<p>follower 不支持读的原因，个人感觉是对于流式系统而言，如果允许 follower 也可以读的话，数据一致性、可见性将会很难保证，对最初 Kafka 的设计将会带来很大的复杂性。</p>
</blockquote>
<p>有了对 topic、partition、副本（replica）、segment、leader、follower 概念的理解之后，下面再看 Kafka 存储层的内容，就不会那么云里雾里了。 </p>
<h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><p>Kafka 的日志管理（LogManager）主要的作用是负责日志的创建、检索、清理，日志相关的读写操作实际上是由日志实例对象（Log）来处理的。</p>
<h3 id="KafkaServer-启动-LogManager-线程"><a href="#KafkaServer-启动-LogManager-线程" class="headerlink" title="KafkaServer 启动 LogManager 线程"></a>KafkaServer 启动 LogManager 线程</h3><p>LogManager 线程是在节点的 Kafka 服务启动时启动的，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//kafka.server.KafkaServer</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line">    <span class="comment">/* start log manager */</span></div><div class="line">    <span class="comment">//note: 启动日志管理线程</span></div><div class="line">    logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">    logManager.startup()</div><div class="line">    &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">    isStartingUp.set(<span class="literal">false</span>)</div><div class="line">    shutdown()</div><div class="line">    <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createLogManager</span></span>(zkClient: <span class="type">ZkClient</span>, brokerState: <span class="type">BrokerState</span>): <span class="type">LogManager</span> = &#123;</div><div class="line">  <span class="keyword">val</span> defaultProps = <span class="type">KafkaServer</span>.copyKafkaConfigToLog(config)</div><div class="line">  <span class="keyword">val</span> defaultLogConfig = <span class="type">LogConfig</span>(defaultProps)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchAllTopicConfigs(zkUtils).map &#123; <span class="keyword">case</span> (topic, configs) =&gt;</div><div class="line">    topic -&gt; <span class="type">LogConfig</span>.fromProps(defaultProps, configs)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// read the log configurations from zookeeper</span></div><div class="line">  <span class="keyword">val</span> cleanerConfig = <span class="type">CleanerConfig</span>(numThreads = config.logCleanerThreads, <span class="comment">//note: 日志清理线程数,默认是1</span></div><div class="line">                                    dedupeBufferSize = config.logCleanerDedupeBufferSize, <span class="comment">//note: 日志清理使用的总内容,默认128MB</span></div><div class="line">                                    dedupeBufferLoadFactor = config.logCleanerDedupeBufferLoadFactor, <span class="comment">//note:  buffer load factor</span></div><div class="line">                                    ioBufferSize = config.logCleanerIoBufferSize, <span class="comment">//note:</span></div><div class="line">                                    maxMessageSize = config.messageMaxBytes, <span class="comment">//note:</span></div><div class="line">                                    maxIoBytesPerSecond = config.logCleanerIoMaxBytesPerSecond, <span class="comment">//note:</span></div><div class="line">                                    backOffMs = config.logCleanerBackoffMs, <span class="comment">//note: 没有日志清理时的 sleep 时间,默认 15s</span></div><div class="line">                                    enableCleaner = config.logCleanerEnable) <span class="comment">//note: 是否允许对 compact 日志进行清理</span></div><div class="line">  <span class="keyword">new</span> <span class="type">LogManager</span>(logDirs = config.logDirs.map(<span class="keyword">new</span> <span class="type">File</span>(_)).toArray, <span class="comment">//note: 日志目录列表</span></div><div class="line">                 topicConfigs = configs,</div><div class="line">                 defaultConfig = defaultLogConfig,</div><div class="line">                 cleanerConfig = cleanerConfig,</div><div class="line">                 ioThreads = config.numRecoveryThreadsPerDataDir,<span class="comment">//note: 每个日志目录在开始时用日志恢复以及关闭时日志flush的线程数,默认1</span></div><div class="line">                 flushCheckMs = config.logFlushSchedulerIntervalMs,</div><div class="line">                 flushCheckpointMs = config.logFlushOffsetCheckpointIntervalMs, <span class="comment">//note: 更新 check-point 的频率,默认是60s</span></div><div class="line">                 retentionCheckMs = config.logCleanupIntervalMs, <span class="comment">//note: log-cleaner 检查 topic 是否需要删除的频率,默认是5min</span></div><div class="line">                 scheduler = kafkaScheduler,</div><div class="line">                 brokerState = brokerState,</div><div class="line">                 time = time)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LogManager-初始化"><a href="#LogManager-初始化" class="headerlink" title="LogManager 初始化"></a>LogManager 初始化</h3><p>LogManager 在初始化时，首先会检查 server 端配置的日志目录信息，然后会加载日志目录下的所有分区日志，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogManager</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="comment">//note: 检查点表示日志已经刷新到磁盘的位置，主要是用于数据恢复</span></div><div class="line">  <span class="keyword">val</span> <span class="type">RecoveryPointCheckpointFile</span> = <span class="string">"recovery-point-offset-checkpoint"</span> <span class="comment">//note: 检查点文件</span></div><div class="line">  </div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logs = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">TopicPartition</span>, <span class="type">Log</span>]() <span class="comment">//note: 分区与日志实例的对应关系</span></div><div class="line"></div><div class="line">  createAndValidateLogDirs(logDirs) <span class="comment">//note: 检查日志目录</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dirLocks = lockLogDirs(logDirs)</div><div class="line">  <span class="comment">//note: 每个数据目录都有一个检查点文件,存储这个数据目录下所有分区的检查点信息</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> recoveryPointCheckpoints = logDirs.map(dir =&gt; (dir, <span class="keyword">new</span> <span class="type">OffsetCheckpoint</span>(<span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">RecoveryPointCheckpointFile</span>)))).toMap</div><div class="line">  loadLogs()</div><div class="line">  </div><div class="line">  <span class="comment">//note: 创建指定的数据目录,并做相应的检查:</span></div><div class="line">  <span class="comment">//note: 1.确保数据目录中没有重复的数据目录;</span></div><div class="line">  <span class="comment">//note: 2.数据不存在的话就创建相应的目录;</span></div><div class="line">  <span class="comment">//note: 3.检查每个目录路径是否是可读的。</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAndValidateLogDirs</span></span>(dirs: <span class="type">Seq</span>[<span class="type">File</span>]) &#123;</div><div class="line">    <span class="keyword">if</span>(dirs.map(_.getCanonicalPath).toSet.size &lt; dirs.size)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Duplicate log directory found: "</span> + logDirs.mkString(<span class="string">", "</span>))</div><div class="line">    <span class="keyword">for</span>(dir &lt;- dirs) &#123;</div><div class="line">      <span class="keyword">if</span>(!dir.exists) &#123;</div><div class="line">        info(<span class="string">"Log directory '"</span> + dir.getAbsolutePath + <span class="string">"' not found, creating it."</span>)</div><div class="line">        <span class="keyword">val</span> created = dir.mkdirs()</div><div class="line">        <span class="keyword">if</span>(!created)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Failed to create data directory "</span> + dir.getAbsolutePath)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span>(!dir.isDirectory || !dir.canRead)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(dir.getAbsolutePath + <span class="string">" is not a readable log directory."</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 加载所有的日志,而每个日志也会调用 loadSegments() 方法加载所有的分段,过程比较慢,所有每个日志都会创建一个单独的线程</span></div><div class="line">  <span class="comment">//note: 日志管理器采用线程池提交任务,标识不用的任务可以同时运行</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">loadLogs</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    info(<span class="string">"Loading logs."</span>)</div><div class="line">    <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">    <span class="keyword">val</span> threadPools = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">ExecutorService</span>]</div><div class="line">    <span class="keyword">val</span> jobs = mutable.<span class="type">Map</span>.empty[<span class="type">File</span>, <span class="type">Seq</span>[<span class="type">Future</span>[_]]]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (dir &lt;- <span class="keyword">this</span>.logDirs) &#123; <span class="comment">//note: 处理每一个日志目录</span></div><div class="line">      <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(ioThreads) <span class="comment">//note: 默认为 1</span></div><div class="line">      threadPools.append(pool) <span class="comment">//note: 每个对应的数据目录都有一个线程池</span></div><div class="line"></div><div class="line">      <span class="keyword">val</span> cleanShutdownFile = <span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">Log</span>.<span class="type">CleanShutdownFile</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (cleanShutdownFile.exists) &#123;</div><div class="line">        debug(</div><div class="line">          <span class="string">"Found clean shutdown file. "</span> +</div><div class="line">          <span class="string">"Skipping recovery for all logs in data directory: "</span> +</div><div class="line">          dir.getAbsolutePath)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// log recovery itself is being performed by `Log` class during initialization</span></div><div class="line">        brokerState.newState(<span class="type">RecoveringFromUncleanShutdown</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">var</span> recoveryPoints = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read <span class="comment">//note: 读取检查点文件</span></div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">          warn(<span class="string">"Error occured while reading recovery-point-offset-checkpoint file of directory "</span> + dir, e)</div><div class="line">          warn(<span class="string">"Resetting the recovery checkpoint to 0"</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">val</span> jobsForDir = <span class="keyword">for</span> &#123;</div><div class="line">        dirContent &lt;- <span class="type">Option</span>(dir.listFiles).toList <span class="comment">//note: 数据目录下的所有日志目录</span></div><div class="line">        logDir &lt;- dirContent <span class="keyword">if</span> logDir.isDirectory <span class="comment">//note: 日志目录下每个分区目录</span></div><div class="line">      &#125; <span class="keyword">yield</span> &#123;</div><div class="line">        <span class="type">CoreUtils</span>.runnable &#123; <span class="comment">//note: 每个分区的目录都对应了一个线程</span></div><div class="line">          debug(<span class="string">"Loading log '"</span> + logDir.getName + <span class="string">"'"</span>)</div><div class="line"></div><div class="line">          <span class="keyword">val</span> topicPartition = <span class="type">Log</span>.parseTopicPartitionName(logDir)</div><div class="line">          <span class="keyword">val</span> config = topicConfigs.getOrElse(topicPartition.topic, defaultConfig)</div><div class="line">          <span class="keyword">val</span> logRecoveryPoint = recoveryPoints.getOrElse(topicPartition, <span class="number">0</span>L)</div><div class="line"></div><div class="line">          <span class="keyword">val</span> current = <span class="keyword">new</span> <span class="type">Log</span>(logDir, config, logRecoveryPoint, scheduler, time)<span class="comment">//note: 创建 Log 对象后，初始化时会加载所有的 segment</span></div><div class="line">          <span class="keyword">if</span> (logDir.getName.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) &#123; <span class="comment">//note: 该目录被标记为删除</span></div><div class="line">            <span class="keyword">this</span>.logsToBeDeleted.add(current)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> previous = <span class="keyword">this</span>.logs.put(topicPartition, current) <span class="comment">//note: 创建日志后,加入日志管理的映射表</span></div><div class="line">            <span class="keyword">if</span> (previous != <span class="literal">null</span>) &#123;</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</div><div class="line">                <span class="string">"Duplicate log directories found: %s, %s!"</span>.format(</div><div class="line">                  current.dir.getAbsolutePath, previous.dir.getAbsolutePath))</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      jobs(cleanShutdownFile) = jobsForDir.map(pool.submit).toSeq <span class="comment">//note: 提交任务</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">for</span> ((cleanShutdownFile, dirJobs) &lt;- jobs) &#123;</div><div class="line">        dirJobs.foreach(_.get)</div><div class="line">        cleanShutdownFile.delete()</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">ExecutionException</span> =&gt; &#123;</div><div class="line">        error(<span class="string">"There was an error in one of the threads during logs loading: "</span> + e.getCause)</div><div class="line">        <span class="keyword">throw</span> e.getCause</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      threadPools.foreach(_.shutdown())</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    info(<span class="string">s"Logs loading complete in <span class="subst">$&#123;time.milliseconds - startMs&#125;</span> ms."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>初始化 LogManger 代码有两个主要方法：</p>
<ol>
<li><code>createAndValidateLogDirs()</code>：创建指定的数据目录，并做相应的检查： 1.确保数据目录中没有重复的数据目录、2.数据目录不存在的话就创建相应的目录；3. 检查每个目录路径是否是可读的；</li>
<li><code>loadLogs()</code>：加载所有的日志分区，而每个日志也会调用 <code>loadSegments()</code> 方法加载该分区所有的 segment 文件，过程比较慢，所以 LogManager 使用线程池的方式，为每个日志的加载都会创建一个单独的线程。</li>
</ol>
<p>虽然使用的是线程池提交任务，并发进行 load 分区日志，但这个任务本身是阻塞式的，只有当所有的分区日志加载完成，才能调用 <code>startup()</code> 启动 LogManager 线程。</p>
<h3 id="LogManager-启动"><a href="#LogManager-启动" class="headerlink" title="LogManager 启动"></a>LogManager 启动</h3><p>在日志目录的所有分区日志都加载完成后，KafkaServer 调用 <code>startup()</code> 方法启动 LogManager 线程，LogManager 启动后，后台会运行四个定时任务，代码实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">/* Schedule the cleanup task to delete old logs */</span></div><div class="line">  <span class="keyword">if</span>(scheduler != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">//note: 定时清理过期的日志 segment,并维护日志的大小</span></div><div class="line">    info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</div><div class="line">    scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</div><div class="line">                       cleanupLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = retentionCheckMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时刷新还没有写到磁盘上日志</span></div><div class="line">    info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</div><div class="line">    scheduler.schedule(<span class="string">"kafka-log-flusher"</span>,</div><div class="line">                       flushDirtyLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = flushCheckMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时将所有数据目录所有日志的检查点写到检查点文件中</span></div><div class="line">    scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</div><div class="line">                       checkpointRecoveryPointOffsets,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = flushCheckpointMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时删除标记为 delete 的日志文件</span></div><div class="line">    scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</div><div class="line">                       deleteLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = defaultConfig.fileDeleteDelayMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 如果设置为 true， 自动清理 compaction 类型的 topic</span></div><div class="line">  <span class="keyword">if</span>(cleanerConfig.enableCleaner)</div><div class="line">    cleaner.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>四个后台定时线程的作用：</p>
<ol>
<li><code>cleanupLogs</code>：定时清理过期的日志 segment，并维护日志的大小（默认5min）；</li>
<li><code>flushDirtyLogs</code>：定时刷新将还没有写到磁盘上日志刷新到磁盘（默认 无限大）；</li>
<li><code>checkpointRecoveryPointOffsets</code>：定时将所有数据目录所有日志的检查点写到检查点文件中（默认 60s）；</li>
<li><code>deleteLogs</code>：定时删除标记为 delete 的日志文件（默认 30s）。</li>
</ol>
<h3 id="检查点文件"><a href="#检查点文件" class="headerlink" title="检查点文件"></a>检查点文件</h3><p>在 LogManager 中有一个非常重要的文件——检查点文件：</p>
<ol>
<li>Kafka 启动时创建 LogManager，读取检查点文件，并把每个分区对应的检查点（checkPoint）作为日志的恢复点（recoveryPoint），最后创建分区对应的日志实例；</li>
<li>消息追加到分区对应的日志，在刷新日志时，将最新的偏移量作为日志的检查点（也即是刷新日志时，会更新检查点位置）；</li>
<li>LogManager 会启动一个定时任务，读取所有日志的检查点，并写入全局的检查点文件（定时将检查点的位置更新到检查点文件中）。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note：通常所有数据目录都会一起执行，不会专门操作某一个数据目录的检查点文件</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpointRecoveryPointOffsets</span></span>() &#123;</div><div class="line">  <span class="keyword">this</span>.logDirs.foreach(checkpointLogsInDir)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Make a checkpoint for all logs in provided directory.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 对数据目录下的所有日志（即所有分区），将其检查点写入检查点文件</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkpointLogsInDir</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> recoveryPoints = <span class="keyword">this</span>.logsByDir.get(dir.toString)</div><div class="line">  <span class="keyword">if</span> (recoveryPoints.isDefined) &#123;</div><div class="line">    <span class="keyword">this</span>.recoveryPointCheckpoints(dir).write(recoveryPoints.get.mapValues(_.recoveryPoint))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>这里留一个问题：启动时，如果发现检查点文件的 offset 比 segment 中最大的 offset 小时（最新的检查点在更新到文件前机器宕机了），应该怎么处理？答案将在下一篇文章中讲述。</p>
</blockquote>
<h2 id="日志刷新"><a href="#日志刷新" class="headerlink" title="日志刷新"></a>日志刷新</h2><p>日志管理器会定时调度 <code>flushDirtyLogs()</code> 方法，定期将页面缓存中的数据真正刷新到磁盘文件中。如果缓存中的数据（在 pagecache 中）在 flush 到磁盘之前，Broker 宕机了，那么会导致数据丢失（多副本减少了这个风险）。</p>
<p>在 Kafka 中有两种策略，将日志刷新到磁盘上：</p>
<ul>
<li>时间策略，（<code>log.flush.interval.ms</code> 中配置调度周期，默认为无限大，即选择大小策略）：</li>
<li>大小策略，（<code>log.flush.interval.messages</code> 中配置当未刷新的 msg 数超过这个值后，进行刷新）。</li>
</ul>
<p>LogManager 刷新日志的实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: LogManager 启动时，会启动一个周期性调度任务，调度这个方法，定时刷新日志。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">flushDirtyLogs</span></span>() = &#123;</div><div class="line">  debug(<span class="string">"Checking for dirty logs to flush..."</span>)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> ((topicPartition, log) &lt;- logs) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">//note: 每个日志的刷新时间并不相同</span></div><div class="line">      <span class="keyword">val</span> timeSinceLastFlush = time.milliseconds - log.lastFlushTime</div><div class="line">      debug(<span class="string">"Checking if flush is needed on "</span> + topicPartition.topic + <span class="string">" flush interval  "</span> + log.config.flushMs +</div><div class="line">            <span class="string">" last flushed "</span> + log.lastFlushTime + <span class="string">" time since last flush: "</span> + timeSinceLastFlush)</div><div class="line">      <span class="keyword">if</span>(timeSinceLastFlush &gt;= log.config.flushMs)</div><div class="line">        log.flush</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Error flushing topic "</span> + topicPartition.topic, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>LogManager 这个方法最后的结果还是调用了 <code>log.flush()</code> 进行刷新操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Flush all log segments</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(): <span class="type">Unit</span> = flush(<span class="keyword">this</span>.logEndOffset)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Flush log segments for all offsets up to offset-1</div><div class="line"> *</div><div class="line"> * @param offset The offset to flush up to (non-inclusive); the new recovery point</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(offset: <span class="type">Long</span>) : <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (offset &lt;= <span class="keyword">this</span>.recoveryPoint)</div><div class="line">    <span class="keyword">return</span></div><div class="line">  debug(<span class="string">"Flushing log '"</span> + name + <span class="string">" up to offset "</span> + offset + <span class="string">", last flushed: "</span> + lastFlushTime + <span class="string">" current time: "</span> +</div><div class="line">        time.milliseconds + <span class="string">" unflushed = "</span> + unflushedMessages)</div><div class="line">  <span class="comment">//note: 刷新检查点到最新偏移量之间的所有日志分段</span></div><div class="line">  <span class="keyword">for</span>(segment &lt;- logSegments(<span class="keyword">this</span>.recoveryPoint, offset))</div><div class="line">    segment.flush()<span class="comment">//note: 刷新数据文件和索引文件（调用操作系统的 fsync）</span></div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(offset &gt; <span class="keyword">this</span>.recoveryPoint) &#123;</div><div class="line">      <span class="keyword">this</span>.recoveryPoint = offset</div><div class="line">      lastflushedTime.set(time.milliseconds)<span class="comment">//note: 更新刷新时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的内容实际上只是按 <code>log.flush.interval.ms</code> 设置去 flush 日志到磁盘，那么 <code>log.flush.interval.messages</code> 策略是在什么地方生效的呢？用心想一下，大家应该能猜出来，是在数据追加到 Log 中的时候，这时候会判断没有 flush 的数据大小是否达到阈值，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 其他部分这里暂时忽略了</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</div><div class="line">  <span class="comment">// now append to the log</span></div><div class="line">  segment.append(firstOffset = appendInfo.firstOffset,</div><div class="line">    largestOffset = appendInfo.lastOffset,</div><div class="line">    largestTimestamp = appendInfo.maxTimestamp,</div><div class="line">    shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</div><div class="line">    records = validRecords)</div><div class="line"></div><div class="line">  <span class="comment">// increment the log end offset</span></div><div class="line">  updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</div><div class="line"></div><div class="line">  trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></div><div class="line">    .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</div><div class="line">    flush()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h2><p>为了保证分区的总大小不超过阈值（<code>log.retention.bytes</code>），日志管理器会定时清理旧的数据。</p>
<blockquote>
<p>不过一般情况下，都是通过配置 <code>log.retention.hours</code> 来配置 segment 的保存时间，而不是通过单日志的总大小配置，因为不同的 topic，其 partition 大小相差很大，导致最后的保存时间可能也不一致，不利于管理。</p>
</blockquote>
<p>清理旧日志分段方法，主要有两种：</p>
<ol>
<li>删除：超过时间或大小阈值的旧 segment，直接进行删除；</li>
<li>压缩：不是直接删除日志分段，而是采用合并压缩的方式进行。</li>
</ol>
<p>这里主要讲述第一种方法，第二种将会后续文章介绍。</p>
<p>先看下 LogManager 中日志清除任务的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Delete any eligible logs. Return the number of segments deleted.</div><div class="line"> * Only consider logs that are not compacted.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 日志清除任务</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanupLogs</span></span>() &#123;</div><div class="line">  debug(<span class="string">"Beginning log cleanup..."</span>)</div><div class="line">  <span class="keyword">var</span> total = <span class="number">0</span></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">  <span class="keyword">for</span>(log &lt;- allLogs; <span class="keyword">if</span> !log.config.compact) &#123;</div><div class="line">    debug(<span class="string">"Garbage collecting '"</span> + log.name + <span class="string">"'"</span>)</div><div class="line">    total += log.deleteOldSegments() <span class="comment">//note: 清理过期的 segment</span></div><div class="line">  &#125;</div><div class="line">  debug(<span class="string">"Log cleanup completed. "</span> + total + <span class="string">" files deleted in "</span> +</div><div class="line">                (time.milliseconds - startMs) / <span class="number">1000</span> + <span class="string">" seconds"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>日志清除任务的实现还是在 Log 的 <code>deleteOldSegments()</code> 中实现的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Delete any log segments that have either expired due to time based retention</div><div class="line">  * or because the log size is &gt; retentionSize</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (!config.delete) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  deleteRetenionMsBreachedSegments() + deleteRetentionSizeBreachedSegments()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 清除保存时间满足条件的 segment</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetenionMsBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (config.retentionMs &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">  deleteOldSegments(startMs - _.largestTimestamp &gt; config.retentionMs)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 清除保存大小满足条件的 segment</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetentionSizeBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (config.retentionSize &lt; <span class="number">0</span> || size &lt; config.retentionSize) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">var</span> diff = size - config.retentionSize</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shouldDelete</span></span>(segment: <span class="type">LogSegment</span>) = &#123;</div><div class="line">    <span class="keyword">if</span> (diff - segment.size &gt;= <span class="number">0</span>) &#123;</div><div class="line">      diff -= segment.size</div><div class="line">      <span class="literal">true</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  deleteOldSegments(shouldDelete)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清除日志的两个方法：</p>
<ol>
<li><code>deleteRetenionMsBreachedSegments()</code>：如果 segment 保存时间超过设置的时间，那么进行删除；</li>
<li><code>deleteRetentionSizeBreachedSegments()</code>：如果当前最新的日志大小减少下一个即将删除的 segment 分段的大小超过阈值，那么就允许删除该 segment，否则就不允许。</li>
</ol>
<p>调用 <code>deleteOldSegments()</code> 方法删除日志数据文件及索引文件的具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 清除相应的 segment 及相应的索引文件</span></div><div class="line"><span class="comment">//note: 其中 predicate 是一个高阶函数，只有返回值为 true 该 segment 才会被删除</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(predicate: <span class="type">LogSegment</span> =&gt; <span class="type">Boolean</span>): <span class="type">Int</span> = &#123;</div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> deletable = deletableSegments(predicate)</div><div class="line">    <span class="keyword">val</span> numToDelete = deletable.size</div><div class="line">    <span class="keyword">if</span> (numToDelete &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// we must always have at least one segment, so if we are going to delete all the segments, create a new one first</span></div><div class="line">      <span class="keyword">if</span> (segments.size == numToDelete)</div><div class="line">        roll()</div><div class="line">      <span class="comment">// remove the segments for lookups</span></div><div class="line">      deletable.foreach(deleteSegment) <span class="comment">//note: 删除 segment</span></div><div class="line">    &#125;</div><div class="line">    numToDelete</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</div><div class="line">  info(<span class="string">"Scheduling log segment %d for log %s for deletion."</span>.format(segment.baseOffset, name))</div><div class="line">  lock synchronized &#123;</div><div class="line">    segments.remove(segment.baseOffset) <span class="comment">//note:  从映射关系表中删除数据</span></div><div class="line">    asyncDeleteSegment(segment) <span class="comment">//note: 异步删除日志 segment</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Perform an asynchronous delete on the given file if it exists (otherwise do nothing)</div><div class="line"> *</div><div class="line"> * @throws KafkaStorageException if the file can't be renamed and still exists</div><div class="line"> */</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">asyncDeleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</div><div class="line">  segment.changeFileSuffixes(<span class="string">""</span>, <span class="type">Log</span>.<span class="type">DeletedFileSuffix</span>) <span class="comment">//note: 先将 segment 的数据文件和索引文件后缀添加 `.deleted`</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteSeg</span></span>() &#123;</div><div class="line">    info(<span class="string">"Deleting segment %d from log %s."</span>.format(segment.baseOffset, name))</div><div class="line">    segment.delete()</div><div class="line">  &#125;</div><div class="line">  scheduler.schedule(<span class="string">"delete-file"</span>, deleteSeg, delay = config.fileDeleteDelayMs) <span class="comment">//note: 异步调度进行删除</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的讲解来看，Kafka LogManager 线程工作还是比较清晰简洁的，它的作用就是负责日志的创建、检索、清理，并不负责日志的读写等实际操作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>操作系统之共享对象学习</title>
    <link href="http://matt33.com/2018/02/04/linux-mmap/"/>
    <id>http://matt33.com/2018/02/04/linux-mmap/</id>
    <published>2018-02-04T15:59:16.000Z</published>
    <updated>2018-02-04T16:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 Kafka 的存储层这部分代码时，看到了很多地方使用操作系统的共享内存机制，Kafka 中所有日志文件的索引都是使用了 <code>mmap</code> 做内存映射，<code>mmap</code> 这块刚好也是一个值得深入学习的知识点，于是就就深入地看了一下、做了一下总结，本文的内容主要来自《深入理解操作系统》第三版 9.8 存储器映射部分。</p>
<h2 id="存储器映射"><a href="#存储器映射" class="headerlink" title="存储器映射"></a>存储器映射</h2><p>Linux 通过将一个虚拟存储器区域与一个磁盘上的对象关联起来，以初始化这个虚拟存储器区域的内容，这个过程就被称为 <strong>存储器映射(memory mapping)</strong>，虚拟存储器区域可以映射到下面两种类型的对象中的一种：</p>
<ul>
<li>Unix 文件系统的普通文件：一个区域可以映射到一个普通磁盘文件的连续部分，例如一个可执行的目标文件。文件区会被分成了页大小的片，每一片包含一个虚拟页面的初始内容。因为按需进行页面调度，所以这些虚拟页面没有实际交换进入物理存储器，直到 CPU 第一次引用页面（如果区域比文件区要大，那么就用零来填充这个区域的余下部分）；</li>
<li>匿名文件：一个区域也可以映射到一个匿名文件，匿名文件是由内核创建的，包含的是二进制零。CPU 第一次引用这样一区域内的虚拟页面时，内核就在物理存储器中找到一个合适的牺牲页面，如果该页面被修改过，就将这个页面换出来，用二进制零覆盖牺牲页面并更新页表，将这个页面标记为是驻留在存储器中的，但是要注意的是在磁盘和存储器之间并没有实际的数据传输，因为这个原因，映射到匿名文件区域中的页面有时也叫做 <strong>请求二进制零的页（demand-zero page）</strong>。</li>
</ul>
<p>上面这个是存储器映射的基础内容，理解完这部分之后，我们再来看共享对象（共享内存）和 mmap。</p>
<h2 id="共享对象"><a href="#共享对象" class="headerlink" title="共享对象"></a>共享对象</h2><p>存储器映射的出现，它是为了要解决是什么问题呢？先看一下对于操作系统来说，没有存储器映射的话面临的情况：</p>
<p>在操作系统中，进程这一抽象能够为每个进程提供自己私有的虚拟地址空间，可以免受其他进程的错误读写，但是，对于操作系统的每一个进程，它们都有同样的只读文本区域，如：每个 C 程序都需要调用一些标准的 C 库函数、需要程序需要访问只读运行时库代码的相同拷贝等等。那么如果每个进行都在物理存储器中保持这些常用代码的复制拷贝，那就是极端的浪费了。</p>
<p>而存储器映射机制的出现，就给我们提供了一种清晰的机制，用来 <strong>控制多个进程如何共享对象</strong>。</p>
<p>一个对象可以被映射到虚拟存储器的一个区域，要么作为共享对象，要么作为私有对象：</p>
<ul>
<li>如果是作为共享对象，那么这个进程对这个区域的任何写操作，对于那些也会把这个共享对象映射到它们虚拟存储器的其他进程而言也是可见的，而且这些变化，也会反映在磁盘上的原始对象中；</li>
<li>如果是作为私有对象，这样的改变，对于其他进程来说是不可变的，并且进程对这个区域所做的任何写操作都不会反映在磁盘上的对象中。</li>
</ul>
<p>关于共享对象，举一个例子，如下图所示：</p>
<p><img src="/images/linux/mmap1.png" alt="一个共享对象"></p>
<p>假设进程1将一个共享对象映射到它的虚拟地址存储器的一个区域中，如上图左边所示，现在假设进程2将同一个共享对象映射到它的地址空间（与进程1虚拟地址空间并不一定一样），如右边所示。因为每个对象都有一个唯一的文件名，内核可以迅速地判断进程1已经映射了这个对象，而且可以使进程2中的页表条目指向相应的物理页面。关键点在于即使对象被映射到了多个共享区域，物理存储器中也只需要存放共享对象的一个拷贝。</p>
<h3 id="私有对象的-copy-on-write"><a href="#私有对象的-copy-on-write" class="headerlink" title="私有对象的 copy-on-write"></a>私有对象的 copy-on-write</h3><p>在私有对象中，操作系统是使用了一种叫做 <strong>写时拷贝（copy-on-write）</strong> 的巧妙技术将其映射到虚拟存储器中的。一个私有对象开始生命周期的方式基本上与共享对象一样，在物理存储器中只保存有私有对象的一份拷贝。</p>
<ul>
<li>如下图的左边部分所示，其中两个进程将一个私有对象映射到它们虚拟存储器的不同区域，但是却共享这个对象同一个物理拷贝。这时，对于每个映射私有对象的进程，相应私有区域的页表条目都被标记为只读，并且区域结构被标记为私有的写时拷贝，只要没有进程试图写它自己的私有区域，它们就可以继续共享物理存储器中对象的一个单独拷贝；</li>
<li>如果只有有一个进程试图写私有区域的某个页面，那么这个写操作就会触发一个保护故障：如下图右边所示，该故障处理程序触发的原因是由于进程试图写私有的写时拷贝区域的一个页面引起的，它就会在物理存储器中创建这个页面的一个新拷贝，更新页表条目指向这个新拷贝，然后恢复这个页面的写权限，当故障处理程序返回时，CPU 重新执行这个写操作，现在在新创建的页面上这个写操作就可以正常执行了。</li>
</ul>
<p><img src="/images/linux/mmap2.png" alt="一个写时拷贝对象"></p>
<p>copy-on-write 最充分地使用了稀有的物理存储器。</p>
<h3 id="再看-fork-函数"><a href="#再看-fork-函数" class="headerlink" title="再看 fork 函数"></a>再看 fork 函数</h3><p>学习了虚拟存储器映射进制后，回头再看 Linux 中的 fork 函数，就会明白 fork 函数是如何创建一个带有自己独立虚拟地址控制的新进程。</p>
<ol>
<li>当 fork 函数被当前进程调用时，内核为新进程创建各种数据结构，并分配给它一个唯一的 PID；</li>
<li>为了给新进程创建虚拟存储器，它创建了当前进程的 <code>mm_struct</code>、区域结构和页表的原样拷贝，它将两个进程的每个页面都标记为只读，并将两个进程中的每个区域结构都标记为私有的写时拷贝；</li>
<li>当 fork 函数在新进程中返回时，新进程现在的虚拟存储器刚好和调用 fork 时存在的虚拟存储器相同；</li>
<li>当两个进程中的任一个进行写操作时，写时拷贝机制就会创建新页面，因此，也就为每个进程保持了私有地址空间的抽象概念。</li>
</ol>
<h2 id="mmap-函数"><a href="#mmap-函数" class="headerlink" title="mmap 函数"></a>mmap 函数</h2><p>经过前面的介绍，既然在操作系统中有了存储器映射的机制，那么我们应该怎么使用呢？这就需要引入 UNIX 中另一个重要的函数 —— <code>mmap</code>，它是用来创建新的虚拟存储器区域，并将对象映射到这些区域中。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> *<span class="title">mmap</span><span class="params">(<span class="keyword">void</span> *start, <span class="keyword">size_t</span> length, <span class="keyword">int</span> prot, <span class="keyword">int</span> flags, <span class="keyword">int</span> fd, <span class="keyword">off_t</span> offset)</span></span>;</div></pre></td></tr></table></figure>
<p>成功执行时，<code>mmap()</code> 返回指向映射区的指针，失败时，<code>mmap()</code> 返回 <code>MAP_FAILED</code>(-1)， error被设为以下的某个值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">EACCES：访问出错</div><div class="line">EAGAIN：文件已被锁定，或者太多的内存已被锁定</div><div class="line">EBADF：fd不是有效的文件描述词</div><div class="line">EINVAL：一个或者多个参数无效</div><div class="line">ENFILE：已达到系统对打开文件的限制</div><div class="line">ENODEV：指定文件所在的文件系统不支持内存映射</div><div class="line">ENOMEM：内存不足，或者进程已超出最大内存映射数量</div><div class="line">EPERM：权能不足，操作不允许</div><div class="line">ETXTBSY：已写的方式打开文件，同时指定MAP_DENYWRITE标志</div><div class="line">SIGSEGV：试着向只读区写入</div><div class="line">SIGBUS：试着访问不属于进程的内存区</div></pre></td></tr></table></figure>
<p>mmap 要求内核创建一个新的虚拟存储器区域，最好是从地址 <code>start</code> 开始的一个区域，并将文件描述符 fd 指定的对象的一个连续的片（chunk）映射到这个新的区域，连续的对象片大小为 <code>length</code> 字节，从距文件开始偏移量为 <code>offset</code> 字节的地方开始，<code>start</code> 地址仅仅是一个暗示，通常设置为 NULL，如下图所示。</p>
<p><img src="/images/linux/mmap3.png" alt="mmap 函数解释"></p>
<p>参数 port 包含描述新映射的虚拟存储器区域的访问权限（在相应区域结构中的 <code>vm_port</code> 位）：</p>
<ul>
<li>PORT_EXEC：这个区域内的页面由可以被 CPU 执行的指令组成；</li>
<li>PORT_READ：这个区域内的页面可读；</li>
<li>PORT_WRITE：这个区域内的页面可写；</li>
<li>PORT_NONE：这个区域内的页面不能被访问。</li>
</ul>
<p>参数 <code>flags</code> 指定映射对象的类型，映射选项和映射页是否可以共享（下面列出的只是其中一部分）</p>
<ul>
<li>MAP_ANON：表示被映射的对象就是一个匿名对象，而相应的虚拟页面是请求二进制零的；</li>
<li>MAP_PRIVATE：表示被映射的对象是一个私有的、写时拷贝的对象；</li>
<li>MAP_SHARED：表示是一个共享对象。</li>
</ul>
<p>示例如：<code>bufp = Mmap(-1, size, PORT_READ, MAP_PRIVATE|MAP_ANON, 0, 0);</code>，让内核创建一个新的包含 size 字节的只读、私有、请求二进制零的虚拟存储器区域。</p>
<p>删除虚拟存储器区域，使用 <code>int munmap(void *start, size_t length);</code>，若成功返回0，若出错返回 -1.</p>
<p>下面看一个示例：使用 mmap 实现一个功能：将一个任意大小的磁盘文件拷贝到 stdout，输入文件的名字必须作为一个命令行参数来传递。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">"csapp.h"</span></span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmapcopy</span><span class="params">(<span class="keyword">int</span> fd,<span class="keyword">int</span> size)</span></span>&#123;</div><div class="line">    <span class="keyword">char</span> *bufp;</div><div class="line">    bufp =(<span class="keyword">char</span> *)mmap(<span class="literal">NULL</span>,size,PROT_READ,MAP_PRIVATE,fd,<span class="number">0</span>);<span class="comment">//在进程空间中创建一个新的虚拟存储器区域，将磁盘文件映射到这个区域中</span></div><div class="line">    write(<span class="number">1</span>,bufp,size);<span class="comment">//将信息写入标准输出</span></div><div class="line">    <span class="comment">//POSIX 定义了 STDIN_FILENO、STDOUT_FILENO 和 STDERR_FILENO 来代替 0、1、2。这三个符号常量的定义位于头文件 unistd.h。</span></div><div class="line">        munmap(bufp,size);<span class="comment">//删除虚拟存储器区域</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span>&#123;</div><div class="line">    <span class="keyword">struct</span> stat _stat;  <span class="comment">//文末附上关于这个结构体详细内容的链接</span></div><div class="line">    <span class="keyword">int</span> fd;</div><div class="line">    <span class="keyword">if</span>(argc != <span class="number">2</span>)&#123;</div><div class="line">        <span class="built_in">printf</span>(<span class="string">"usage :%s &lt;filename&gt;"</span>,argv[<span class="number">0</span>]);</div><div class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line">    fd = open(argv[<span class="number">1</span>],O_RDONLY,<span class="number">0</span>);</div><div class="line">    <span class="comment">//fd1 = open(argv[2],O_RDWR|O_APPEND,0);  以“读写+追加”模式打开一个额外的文件，将在函数里尝试向它追加信息。</span></div><div class="line"></div><div class="line">    fstat(fd,&amp;_stat);  <span class="comment">//fstat将文件标识符fd所标识的文件状态，复制到结构体stat中</span></div><div class="line">    mmapcopy(fd,_stat.st_size);</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>参考文献：</p>
<ul>
<li>《深入理解操作系统 第三版》；</li>
<li><a href="http://www.cnblogs.com/huxiao-tee/p/4660352.html" target="_blank" rel="external">认真分析mmap：是什么 为什么 怎么用</a>；</li>
<li><a href="http://blog.csdn.net/qq973177663/article/details/51246267" target="_blank" rel="external">使用mmap实现一个文件输出函数</a>.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Kafka 的存储层这部分代码时，看到了很多地方使用操作系统的共享内存机制，Kafka 中所有日志文件的索引都是使用了 &lt;code&gt;mmap&lt;/code&gt; 做内存映射，&lt;code&gt;mmap&lt;/code&gt; 这块刚好也是一个值得深入学习的知识点，于是就就深入地看了一下、做了
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="linux" scheme="http://matt33.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 GroupCoordinator 详解（十）</title>
    <link href="http://matt33.com/2018/01/28/server-group-coordinator/"/>
    <id>http://matt33.com/2018/01/28/server-group-coordinator/</id>
    <published>2018-01-28T14:23:05.000Z</published>
    <updated>2018-04-30T13:18:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>突然发现距离上一篇文章，已经过去两个多月了，有两个月没有写博客了，之前定的是年前把这个系列写完，现在看来只能往后拖了，后面估计还有五篇文章左右，尽量在春节前完成吧。继续之前的内容开始讲解，这篇文章，主要是想把 GroupCoordinator 的内容总结一下，也算是开始了 Kafka Server 端的讲解，Kafka 的 Server 端主要有三块内容：GroupCoordinator、Controller 和 ReplicaManager，其中，GroupCoordinator 的内容是与 Consumer 端紧密结合在一起的，有一部分内容在前面已经断断续续介绍过，这里会做一个总结。</p>
<p>关于 GroupCoordinator，代码中有一段注释介绍得比较清晰，这里引用一下：</p>
<blockquote>
<p>GroupCoordinator handles general group membership and offset management.</p>
<p>Each Kafka server instantiates a coordinator which is responsible for a set of groups. Groups are assigned to coordinators based on their group names.</p>
</blockquote>
<p>简单来说就是，GroupCoordinator 是负责进行 consumer 的 group 成员与 offset 管理（但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息），那它是怎么管理的呢？这个从 GroupCoordinator 处理的 client 端请求类型可以看出来，它处理的请求类型主要有以下几种：</p>
<ol>
<li>ApiKeys.OFFSET_COMMIT;</li>
<li>ApiKeys.OFFSET_FETCH;</li>
<li>ApiKeys.JOIN_GROUP;</li>
<li>ApiKeys.LEAVE_GROUP;</li>
<li>ApiKeys.SYNC_GROUP;</li>
<li>ApiKeys.DESCRIBE_GROUPS;</li>
<li>ApiKeys.LIST_GROUPS;</li>
<li>ApiKeys.HEARTBEAT;</li>
</ol>
<p>而 Kafka Server 端要处理的请求总共有以下 21 种，其中有 8 种是由 GroupCoordinator 来完成的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="type">ApiKeys</span>.forId(request.requestId) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">PRODUCE</span> =&gt; handleProducerRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">FETCH</span> =&gt; handleFetchRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LIST_OFFSETS</span> =&gt; handleOffsetRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">METADATA</span> =&gt; handleTopicMetadataRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LEADER_AND_ISR</span> =&gt; handleLeaderAndIsrRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span> =&gt; handleStopReplicaRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">UPDATE_METADATA_KEY</span> =&gt; handleUpdateMetadataRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">CONTROLLED_SHUTDOWN_KEY</span> =&gt; handleControlledShutdownRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">OFFSET_COMMIT</span> =&gt; handleOffsetCommitRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">OFFSET_FETCH</span> =&gt; handleOffsetFetchRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">GROUP_COORDINATOR</span> =&gt; handleGroupCoordinatorRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">JOIN_GROUP</span> =&gt; handleJoinGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">HEARTBEAT</span> =&gt; handleHeartbeatRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LEAVE_GROUP</span> =&gt; handleLeaveGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">SYNC_GROUP</span> =&gt; handleSyncGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">DESCRIBE_GROUPS</span> =&gt; handleDescribeGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LIST_GROUPS</span> =&gt; handleListGroupsRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">SASL_HANDSHAKE</span> =&gt; handleSaslHandshakeRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">API_VERSIONS</span> =&gt; handleApiVersionsRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">CREATE_TOPICS</span> =&gt; handleCreateTopicsRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">DELETE_TOPICS</span> =&gt; handleDeleteTopicsRequest(request)</div><div class="line">  <span class="keyword">case</span> requestId =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Unknown api code "</span> + requestId)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="GroupCoordinator-简介"><a href="#GroupCoordinator-简介" class="headerlink" title="GroupCoordinator 简介"></a>GroupCoordinator 简介</h2><p>这里先简单看下 GroupCoordinator 的基本内容。</p>
<h3 id="GroupCoordinator-的启动"><a href="#GroupCoordinator-的启动" class="headerlink" title="GroupCoordinator 的启动"></a>GroupCoordinator 的启动</h3><p>Broker 在启动时，也就是 KafkaServer 在 <code>startup()</code> 方法中会有以下一段内容，它表示每个 Broker 在启动是都会启动 GroupCoordinator 服务。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* start group coordinator */</span></div><div class="line"><span class="comment">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</span></div><div class="line">groupCoordinator = <span class="type">GroupCoordinator</span>(config, zkUtils, replicaManager, <span class="type">Time</span>.<span class="type">SYSTEM</span>)</div><div class="line">groupCoordinator.startup()<span class="comment">//note: 启动 groupCoordinator</span></div></pre></td></tr></table></figure>
<p>GroupCoordinator 服务在调用 <code>setup()</code> 方法启动后，进行的操作如下，实际上只是把一个标志变量值 <code>isActive</code> 设置为 true，并且启动了一个后台线程来删除过期的 group metadata。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">* Startup logic executed at the same time when the server starts up.</div><div class="line">*/</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>(enableMetadataExpiration: <span class="type">Boolean</span> = <span class="literal">true</span>) &#123;</div><div class="line">  info(<span class="string">"Starting up."</span>)</div><div class="line">  <span class="keyword">if</span> (enableMetadataExpiration)</div><div class="line">    groupManager.enableMetadataExpiration()</div><div class="line">  isActive.set(<span class="literal">true</span>)</div><div class="line">  info(<span class="string">"Startup complete."</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="group-如何选择相应的-GroupCoordinator"><a href="#group-如何选择相应的-GroupCoordinator" class="headerlink" title="group 如何选择相应的 GroupCoordinator"></a>group 如何选择相应的 GroupCoordinator</h3><p>要说这个，就必须介绍一下这个 <code>__consumer_offsets</code> topic 了，它是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有50个 partition，每个 partition 默认有三个副本，而具体的一个 group 的消费情况要存储到哪一个 partition 上，是根据 <code>abs(GroupId.hashCode()) % NumPartitions</code> 来计算的（其中，NumPartitions 是 <code>__consumer_offsets</code> 的 partition 数，默认是50个）。</p>
<p>对于 consumer group 而言，是根据其 <code>group.id</code> 进行 hash 并计算得到其具对应的 partition 值，该 partition leader 所在 Broker 即为该 Group 所对应的 GroupCoordinator，GroupCoordinator 会存储与该 group 相关的所有的 Meta 信息。</p>
<h3 id="GroupCoordinator-的-metadata"><a href="#GroupCoordinator-的-metadata" class="headerlink" title="GroupCoordinator 的 metadata"></a>GroupCoordinator 的 metadata</h3><p>对于 consumer group 而言，其对应的 metadata 信息主要包含一下内容：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Group contains the following metadata:</div><div class="line"> *</div><div class="line"> *  Membership metadata:</div><div class="line"> *  1. Members registered in this group</div><div class="line"> *  2. Current protocol assigned to the group (e.g. partition assignment strategy for consumers)</div><div class="line"> *  3. Protocol metadata associated with group members</div><div class="line"> *</div><div class="line"> *  State metadata:</div><div class="line"> *  1. group state</div><div class="line"> *  2. generation id</div><div class="line"> *  3. leader id</div><div class="line"> */</div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> group 的 meta 信息,对 group 级别而言,每个 group 都会有一个实例对象</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="class"><span class="keyword">class</span> <span class="title">GroupMetadata</span>(<span class="params">val groupId: <span class="type">String</span>, initialState: <span class="type">GroupState</span> = <span class="type">Empty</span></span>) </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> state: <span class="type">GroupState</span> = initialState <span class="comment">// group 的状态</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> members = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">MemberMetadata</span>] <span class="comment">// group 的 member 信息</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> offsets = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>] <span class="comment">//对应的 commit offset</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> pendingOffsetCommits = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>] <span class="comment">// commit offset 成功后更新到上面的 map 中</span></div><div class="line"></div><div class="line">  <span class="keyword">var</span> protocolType: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line">  <span class="keyword">var</span> generationId = <span class="number">0</span> <span class="comment">// generation id</span></div><div class="line">  <span class="keyword">var</span> leaderId: <span class="type">String</span> = <span class="literal">null</span> <span class="comment">// leader consumer id</span></div><div class="line">  <span class="keyword">var</span> protocol: <span class="type">String</span> = <span class="literal">null</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>而对于每个 consumer 而言，其 metadata 信息主要包括以下内容：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Member metadata contains the following metadata:</div><div class="line"> *</div><div class="line"> * Heartbeat metadata:</div><div class="line"> * 1. negotiated heartbeat session timeout 心跳超时时间</div><div class="line"> * 2. timestamp of the latest heartbeat 上次发送心跳的时间</div><div class="line"> *</div><div class="line"> * Protocol metadata:</div><div class="line"> * 1. the list of supported protocols (ordered by preference) 支持的 partition reassign 协议</div><div class="line"> * 2. the metadata associated with each protocol</div><div class="line"> *</div><div class="line"> * In addition, it also contains the following state information:</div><div class="line"> *</div><div class="line"> * 1. Awaiting rebalance callback: when the group is in the prepare-rebalance state,</div><div class="line"> *                                 its rebalance callback will be kept in the metadata if the</div><div class="line"> *                                 member has sent the join group request</div><div class="line"> * 2. Awaiting sync callback: when the group is in the awaiting-sync state, its sync callback</div><div class="line"> *                            is kept in metadata until the leader provides the group assignment</div><div class="line"> *                            and the group transitions to stable</div><div class="line"> */</div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 记录 group 中每个成员的状态信息</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="class"><span class="keyword">class</span> <span class="title">MemberMetadata</span>(<span class="params">val memberId: <span class="type">String</span>,</span></span></div><div class="line">                                          val groupId: <span class="type">String</span>,</div><div class="line">                                          val clientId: <span class="type">String</span>,</div><div class="line">                                          val clientHost: <span class="type">String</span>,</div><div class="line">                                          val rebalanceTimeoutMs: <span class="type">Int</span>,</div><div class="line">                                          val sessionTimeoutMs: <span class="type">Int</span>,</div><div class="line">                                          val protocolType: <span class="type">String</span>,</div><div class="line">                                          var supportedProtocols: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Array</span>[<span class="type">Byte</span>])]) &#123;&#125;</div></pre></td></tr></table></figure>
<h2 id="GroupCoordinator-请求处理"><a href="#GroupCoordinator-请求处理" class="headerlink" title="GroupCoordinator 请求处理"></a>GroupCoordinator 请求处理</h2><p>正如前面所述，Kafka Server 端可以介绍的21种请求中，其中有8种是由 GroupCoordinator 来处理的，这里主要介绍一下，GroupCoordinator 如何处理这些请求的。</p>
<h3 id="Offset-请求的处理"><a href="#Offset-请求的处理" class="headerlink" title="Offset 请求的处理"></a>Offset 请求的处理</h3><p>关于 Offset 请求的处理，有两个：</p>
<ul>
<li>OFFSET_FETCH：查询 offset；</li>
<li>OFFSET_COMMIT：提供 offset；</li>
</ul>
<h4 id="OFFSET-FETCH-请求处理"><a href="#OFFSET-FETCH-请求处理" class="headerlink" title="OFFSET_FETCH 请求处理"></a>OFFSET_FETCH 请求处理</h4><p>关于 OFFSET_FETCH 请求，Server 端的处理如下，新版 offset 默认是保存在 Kafka 中，这里也以保存在 Kafka 中为例，从下面的实现中也可以看出，在 fetch commit 是分两种情况：</p>
<ul>
<li>获取 group 所消费的所有 topic-partition 的 offset；</li>
<li>获取指定 topic-partition 的 offset。</li>
</ul>
<p>两种情况都是调用 <code>coordinator.handleFetchOffsets()</code> 方法实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle an offset fetch request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleOffsetFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> header = request.header</div><div class="line">  <span class="keyword">val</span> offsetFetchRequest = request.body.asInstanceOf[<span class="type">OffsetFetchRequest</span>]</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">authorizeTopicDescribe</span></span>(partition: <span class="type">TopicPartition</span>) =</div><div class="line">    authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, partition.topic)) <span class="comment">//note: 验证 Describe 权限</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> offsetFetchResponse =</div><div class="line">    <span class="comment">// reject the request if not authorized to the group</span></div><div class="line">    <span class="keyword">if</span> (!authorize(request.session, <span class="type">Read</span>, <span class="keyword">new</span> <span class="type">Resource</span>(<span class="type">Group</span>, offsetFetchRequest.groupId)))</div><div class="line">      offsetFetchRequest.getErrorResponse(<span class="type">Errors</span>.<span class="type">GROUP_AUTHORIZATION_FAILED</span>)</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">if</span> (header.apiVersion == <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">val</span> (authorizedPartitions, unauthorizedPartitions) = offsetFetchRequest.partitions.asScala</div><div class="line">          .partition(authorizeTopicDescribe)</div><div class="line"></div><div class="line">        <span class="comment">// version 0 reads offsets from ZK</span></div><div class="line">        <span class="keyword">val</span> authorizedPartitionData = authorizedPartitions.map &#123; topicPartition =&gt;</div><div class="line">          <span class="keyword">val</span> topicDirs = <span class="keyword">new</span> <span class="type">ZKGroupTopicDirs</span>(offsetFetchRequest.groupId, topicPartition.topic)</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">if</span> (!metadataCache.contains(topicPartition.topic))</div><div class="line">              (topicPartition, <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>)</div><div class="line">            <span class="keyword">else</span> &#123;</div><div class="line">              <span class="keyword">val</span> payloadOpt = zkUtils.readDataMaybeNull(<span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>/<span class="subst">$&#123;topicPartition.partition&#125;</span>"</span>)._1</div><div class="line">              payloadOpt <span class="keyword">match</span> &#123;</div><div class="line">                <span class="keyword">case</span> <span class="type">Some</span>(payload) =&gt;</div><div class="line">                  (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(</div><div class="line">                      payload.toLong, <span class="type">OffsetFetchResponse</span>.<span class="type">NO_METADATA</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">                <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">                  (topicPartition, <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">              (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(</div><div class="line">                  <span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="type">OffsetFetchResponse</span>.<span class="type">NO_METADATA</span>, <span class="type">Errors</span>.forException(e)))</div><div class="line">          &#125;</div><div class="line">        &#125;.toMap</div><div class="line"></div><div class="line">        <span class="keyword">val</span> unauthorizedPartitionData = unauthorizedPartitions.map(_ -&gt; <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>).toMap</div><div class="line">        <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, (authorizedPartitionData ++ unauthorizedPartitionData).asJava, header.apiVersion)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// versions 1 and above read offsets from Kafka</span></div><div class="line">        <span class="keyword">if</span> (offsetFetchRequest.isAllPartitions) &#123;<span class="comment">//note: 获取这个 group 消费的所有 tp offset</span></div><div class="line">          <span class="keyword">val</span> (error, allPartitionData) = coordinator.handleFetchOffsets(offsetFetchRequest.groupId)</div><div class="line">          <span class="keyword">if</span> (error != <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">            offsetFetchRequest.getErrorResponse(error)</div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// clients are not allowed to see offsets for topics that are not authorized for Describe</span></div><div class="line">            <span class="comment">//note: 如果没有 Describe 权限的话,不能查看相应的 offset</span></div><div class="line">            <span class="keyword">val</span> authorizedPartitionData = allPartitionData.filter &#123; <span class="keyword">case</span> (topicPartition, _) =&gt; authorizeTopicDescribe(topicPartition) &#125;</div><div class="line">            <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, authorizedPartitionData.asJava, header.apiVersion)</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 获取指定列表的 tp offset</span></div><div class="line">          <span class="keyword">val</span> (authorizedPartitions, unauthorizedPartitions) = offsetFetchRequest.partitions.asScala</div><div class="line">            .partition(authorizeTopicDescribe)</div><div class="line">          <span class="keyword">val</span> (error, authorizedPartitionData) = coordinator.handleFetchOffsets(offsetFetchRequest.groupId,</div><div class="line">            <span class="type">Some</span>(authorizedPartitions))</div><div class="line">          <span class="keyword">if</span> (error != <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">            offsetFetchRequest.getErrorResponse(error)</div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> unauthorizedPartitionData = unauthorizedPartitions.map(_ -&gt; <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>).toMap</div><div class="line">            <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, (authorizedPartitionData ++ unauthorizedPartitionData).asJava, header.apiVersion)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  trace(<span class="string">s"Sending offset fetch response <span class="subst">$offsetFetchResponse</span> for correlation id <span class="subst">$&#123;header.correlationId&#125;</span> to client <span class="subst">$&#123;header.clientId&#125;</span>."</span>)</div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, offsetFetchResponse))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>coordinator.handleFetchOffsets()</code> 的实现中，主要是调用了 <code>groupManager.getOffsets()</code> 获取相应的 offset 信息，在查询时加锁的原因应该是为了避免在查询的过程中 offset 不断更新。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOffsets</span></span>(groupId: <span class="type">String</span>, topicPartitionsOpt: <span class="type">Option</span>[<span class="type">Seq</span>[<span class="type">TopicPartition</span>]]): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>] = &#123;</div><div class="line">  trace(<span class="string">"Getting offsets of %s for group %s."</span>.format(topicPartitionsOpt.getOrElse(<span class="string">"all partitions"</span>), groupId))</div><div class="line">  <span class="keyword">val</span> group = groupMetadataCache.get(groupId)</div><div class="line">  <span class="keyword">if</span> (group == <span class="literal">null</span>) &#123;</div><div class="line">    topicPartitionsOpt.getOrElse(<span class="type">Seq</span>.empty[<span class="type">TopicPartition</span>]).map &#123; topicPartition =&gt;</div><div class="line">      (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="string">""</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">    &#125;.toMap</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    group synchronized &#123;</div><div class="line">      <span class="keyword">if</span> (group.is(<span class="type">Dead</span>)) &#123; <span class="comment">//note: group 状态已经变成 dead, offset 返回 -1（INVALID_OFFSET）</span></div><div class="line">        topicPartitionsOpt.getOrElse(<span class="type">Seq</span>.empty[<span class="type">TopicPartition</span>]).map &#123; topicPartition =&gt;</div><div class="line">          (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="string">""</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">        &#125;.toMap</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">          topicPartitionsOpt <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: 返回 group 消费的所有 tp 的 offset 信息（只返回这边已有 offset 的 tp）</span></div><div class="line">              <span class="comment">// Return offsets for all partitions owned by this consumer group. (this only applies to consumers</span></div><div class="line">              <span class="comment">// that commit offsets to Kafka.)</span></div><div class="line">              group.allOffsets.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">                topicPartition -&gt; <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(offsetAndMetadata.offset, offsetAndMetadata.metadata, <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> <span class="type">Some</span>(topicPartitions) =&gt;</div><div class="line">              topicPartitionsOpt.getOrElse(<span class="type">Seq</span>.empty[<span class="type">TopicPartition</span>]).map &#123; topicPartition =&gt;</div><div class="line">                <span class="keyword">val</span> partitionData = group.offset(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">                  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: offset 没有的话就返回-1</span></div><div class="line">                    <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="string">""</span>, <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">                  <span class="keyword">case</span> <span class="type">Some</span>(offsetAndMetadata) =&gt;</div><div class="line">                    <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(offsetAndMetadata.offset, offsetAndMetadata.metadata, <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">                &#125;</div><div class="line">                topicPartition -&gt; partitionData</div><div class="line">              &#125;.toMap</div><div class="line">          &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="OFFSET-COMMIT-请求处理"><a href="#OFFSET-COMMIT-请求处理" class="headerlink" title="OFFSET_COMMIT 请求处理"></a>OFFSET_COMMIT 请求处理</h4><p>对 OFFSET_COMMIT 请求的处理，部分内容已经介绍过，可以参考 <a href="http://matt33.com/2017/11/18/consumer-subscribe/#commit-offset-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86">commit offset 请求处理</a>，处理过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doCommitOffsets</span></span>(group: <span class="type">GroupMetadata</span>,</div><div class="line">                    memberId: <span class="type">String</span>,</div><div class="line">                    generationId: <span class="type">Int</span>,</div><div class="line">                    offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                    responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">var</span> delayedOffsetStore: <span class="type">Option</span>[<span class="type">DelayedStore</span>] = <span class="type">None</span></div><div class="line"></div><div class="line">  group synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (group.is(<span class="type">Dead</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId &lt; <span class="number">0</span> &amp;&amp; group.is(<span class="type">Empty</span>)) &#123;<span class="comment">//note: 来自 assign 的情况</span></div><div class="line">      <span class="comment">// the group is only using Kafka to store offsets</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (group.is(<span class="type">AwaitingSync</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!group.has(memberId)) &#123;<span class="comment">//note: 有可能 simple 与 high level 的冲突了,这里就直接拒绝相应的请求</span></div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">      completeAndScheduleNextHeartbeatExpiration(group, member)<span class="comment">//note: 更新下次需要的心跳时间</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback) <span class="comment">//note: commit offset</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// store the offsets without holding the group lock</span></div><div class="line">  delayedOffsetStore.foreach(groupManager.store)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里主要介绍一下 <code>groupManager.prepareStoreOffsets()</code> 方法，处理逻辑如下，这里简单说一下其 offset 存储的过程：</p>
<ol>
<li>首先过滤掉那些 offset 超过范围的 metadata；</li>
<li>将 offset 信息追加到 replicated log 中；</li>
<li>调用 <code>prepareOffsetCommit()</code> 方法，先将 offset 信息更新到 group 的 pendingOffsetCommits 中（这时还没有真正提交，后面如果失败的话，是可以撤回的）；</li>
<li>在 <code>putCacheCallback</code> 回调函数中，如果 offset 信息追加到 replicated log 成功，那么就更新缓存（将 group 的 pendingOffsetCommits 中的信息更新到 offset 变量中）。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Store offsets by appending it to the replicated log and then inserting to cache</div><div class="line"> */</div><div class="line"><span class="comment">//note: 记录 commit 的 offset</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareStoreOffsets</span></span>(group: <span class="type">GroupMetadata</span>,</div><div class="line">                        consumerId: <span class="type">String</span>,</div><div class="line">                        generationId: <span class="type">Int</span>,</div><div class="line">                        offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                        responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>): <span class="type">Option</span>[<span class="type">DelayedStore</span>] = &#123;</div><div class="line">  <span class="comment">// first filter out partitions with offset metadata size exceeding limit</span></div><div class="line">  <span class="comment">//note: 首先过滤掉 offset 信息超过范围的 metadata</span></div><div class="line">  <span class="keyword">val</span> filteredOffsetMetadata = offsetMetadata.filter &#123; <span class="keyword">case</span> (_, offsetAndMetadata) =&gt;</div><div class="line">    validateOffsetMetadataLength(offsetAndMetadata.metadata)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// construct the message set to append</span></div><div class="line">  <span class="comment">//note: 构造一个 msg set 追加</span></div><div class="line">  getMagicAndTimestamp(partitionFor(group.groupId)) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>((magicValue, timestampType, timestamp)) =&gt;</div><div class="line">      <span class="keyword">val</span> records = filteredOffsetMetadata.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">        <span class="type">Record</span>.create(magicValue, timestampType, timestamp,</div><div class="line">          <span class="type">GroupMetadataManager</span>.offsetCommitKey(group.groupId, topicPartition), <span class="comment">//note: key是一个三元组: group、topic、partition</span></div><div class="line">          <span class="type">GroupMetadataManager</span>.offsetCommitValue(offsetAndMetadata))</div><div class="line">      &#125;.toSeq</div><div class="line"></div><div class="line">      <span class="keyword">val</span> offsetTopicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, partitionFor(group.groupId))</div><div class="line"></div><div class="line">      <span class="comment">//note: 将 offset 信息追加到 replicated log 中</span></div><div class="line">      <span class="keyword">val</span> entries = <span class="type">Map</span>(offsetTopicPartition -&gt; <span class="type">MemoryRecords</span>.withRecords(timestampType, compressionType, records:_*))</div><div class="line"></div><div class="line">      <span class="comment">// set the callback function to insert offsets into cache after log append completed</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">putCacheCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</div><div class="line">        <span class="comment">// the append response should only contain the topics partition</span></div><div class="line">        <span class="keyword">if</span> (responseStatus.size != <span class="number">1</span> || ! responseStatus.contains(offsetTopicPartition))</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Append status %s should only have one partition %s"</span></div><div class="line">            .format(responseStatus, offsetTopicPartition))</div><div class="line"></div><div class="line">        <span class="comment">// construct the commit response status and insert</span></div><div class="line">        <span class="comment">// the offset and metadata to cache if the append status has no error</span></div><div class="line">        <span class="keyword">val</span> status = responseStatus(offsetTopicPartition)</div><div class="line"></div><div class="line">        <span class="keyword">val</span> responseCode =</div><div class="line">          group synchronized &#123;</div><div class="line">            <span class="keyword">if</span> (status.error == <span class="type">Errors</span>.<span class="type">NONE</span>) &#123; <span class="comment">//note: 如果已经追加到了 replicated log 中了,那么就更新其缓存</span></div><div class="line">              <span class="keyword">if</span> (!group.is(<span class="type">Dead</span>)) &#123; <span class="comment">//note: 更新到 group 的 offset 中</span></div><div class="line">                filteredOffsetMetadata.foreach &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">                  group.completePendingOffsetWrite(topicPartition, offsetAndMetadata)</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">              <span class="type">Errors</span>.<span class="type">NONE</span>.code</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">              <span class="keyword">if</span> (!group.is(<span class="type">Dead</span>)) &#123;</div><div class="line">                filteredOffsetMetadata.foreach &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">                  group.failPendingOffsetWrite(topicPartition, offsetAndMetadata)</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line"></div><div class="line">              debug(<span class="string">s"Offset commit <span class="subst">$filteredOffsetMetadata</span> from group <span class="subst">$&#123;group.groupId&#125;</span>, consumer <span class="subst">$consumerId</span> "</span> +</div><div class="line">                <span class="string">s"with generation <span class="subst">$generationId</span> failed when appending to log due to <span class="subst">$&#123;status.error.exceptionName&#125;</span>"</span>)</div><div class="line"></div><div class="line">              <span class="comment">// transform the log append error code to the corresponding the commit status error code</span></div><div class="line">              <span class="keyword">val</span> responseError = status.error <span class="keyword">match</span> &#123;</div><div class="line">                <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">NOT_ENOUGH_REPLICAS</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">NOT_ENOUGH_REPLICAS_AFTER_APPEND</span> =&gt;</div><div class="line">                  <span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span></div><div class="line"></div><div class="line">                <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NOT_LEADER_FOR_PARTITION</span> =&gt;</div><div class="line">                  <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span></div><div class="line"></div><div class="line">                <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">MESSAGE_TOO_LARGE</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">RECORD_LIST_TOO_LARGE</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">INVALID_FETCH_SIZE</span> =&gt;</div><div class="line">                  <span class="type">Errors</span>.<span class="type">INVALID_COMMIT_OFFSET_SIZE</span></div><div class="line"></div><div class="line">                <span class="keyword">case</span> other =&gt; other</div><div class="line">              &#125;</div><div class="line"></div><div class="line">              responseError.code</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line"></div><div class="line">        <span class="comment">// compute the final error codes for the commit response</span></div><div class="line">        <span class="keyword">val</span> commitStatus = offsetMetadata.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">          <span class="keyword">if</span> (validateOffsetMetadataLength(offsetAndMetadata.metadata))</div><div class="line">            (topicPartition, responseCode)</div><div class="line">          <span class="keyword">else</span></div><div class="line">            (topicPartition, <span class="type">Errors</span>.<span class="type">OFFSET_METADATA_TOO_LARGE</span>.code)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// finally trigger the callback logic passed from the API layer</span></div><div class="line">        responseCallback(commitStatus)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      group synchronized &#123;</div><div class="line">        group.prepareOffsetCommit(offsetMetadata) <span class="comment">//note: 添加到 group 的 pendingOffsetCommits 中</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="type">Some</span>(<span class="type">DelayedStore</span>(entries, putCacheCallback)) <span class="comment">//note:</span></div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">val</span> commitStatus = offsetMetadata.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">        (topicPartition, <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">      &#125;</div><div class="line">      responseCallback(commitStatus)</div><div class="line">      <span class="type">None</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="group-相关的处理"><a href="#group-相关的处理" class="headerlink" title="group 相关的处理"></a>group 相关的处理</h3><p>这一小节主要介绍 GroupCoordinator 处理 group 相关的请求。</p>
<h4 id="JOIN-GROUP-和-SYNC-GROUP请求处理"><a href="#JOIN-GROUP-和-SYNC-GROUP请求处理" class="headerlink" title="JOIN_GROUP 和 SYNC_GROUP请求处理"></a>JOIN_GROUP 和 SYNC_GROUP请求处理</h4><p>这两个请求的处理实际上在 <a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a> 中已经详细介绍过，这里就不再陈述。</p>
<h4 id="DESCRIBE-GROUPS-请求处理"><a href="#DESCRIBE-GROUPS-请求处理" class="headerlink" title="DESCRIBE_GROUPS 请求处理"></a>DESCRIBE_GROUPS 请求处理</h4><p>关于 DESCRIBE_GROUPS 请求处理实现如下，主要是返回 group 中各个 member 的详细信息，包含的变量信息为 <code>memberId, clientId, clientHost, metadata(protocol), assignment</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleDescribeGroup</span></span>(groupId: <span class="type">String</span>): (<span class="type">Errors</span>, <span class="type">GroupSummary</span>) = &#123;</div><div class="line">  <span class="keyword">if</span> (!isActive.get) &#123;</div><div class="line">    (<span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>, <span class="type">GroupCoordinator</span>.<span class="type">EmptyGroup</span>)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;</div><div class="line">    (<span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>, <span class="type">GroupCoordinator</span>.<span class="type">EmptyGroup</span>)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;</div><div class="line">    (<span class="type">Errors</span>.<span class="type">GROUP_LOAD_IN_PROGRESS</span>, <span class="type">GroupCoordinator</span>.<span class="type">EmptyGroup</span>)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    groupManager.getGroup(groupId) <span class="keyword">match</span> &#123; <span class="comment">//note: 返回 group 详细信息,主要是 member 的详细信息</span></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; (<span class="type">Errors</span>.<span class="type">NONE</span>, <span class="type">GroupCoordinator</span>.<span class="type">DeadGroup</span>)</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt;</div><div class="line">        group synchronized &#123;</div><div class="line">          (<span class="type">Errors</span>.<span class="type">NONE</span>, group.summary)</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="LEAVE-GROUP-请求处理"><a href="#LEAVE-GROUP-请求处理" class="headerlink" title="LEAVE_GROUP 请求处理"></a>LEAVE_GROUP 请求处理</h4><p>在什么情况下，Server 会收到 LEAVE_GROUP 的请求呢？一般来说是：</p>
<ol>
<li>consumer 调用 <code>unsubscribe()</code> 方法，取消了对所有 topic 的订阅时；</li>
<li>consumer 的心跳线程超时时，这时 consumer 会主动发送 LEAVE_GROUP 请求；</li>
<li>在 server 端，如果在给定的时间没收到 client 的心跳请求，这时候会自动触发 LEAVE_GROUP 操作。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleLeaveGroup</span></span>(groupId: <span class="type">String</span>, memberId: <span class="type">String</span>, responseCallback: <span class="type">Short</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (!isActive.get) &#123;</div><div class="line">    responseCallback(<span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>.code)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;</div><div class="line">    responseCallback(<span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;</div><div class="line">    responseCallback(<span class="type">Errors</span>.<span class="type">GROUP_LOAD_IN_PROGRESS</span>.code)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    groupManager.getGroup(groupId) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">// if the group is marked as dead, it means some other thread has just removed the group</span></div><div class="line">        <span class="comment">// from the coordinator metadata; this is likely that the group has migrated to some other</span></div><div class="line">        <span class="comment">// coordinator OR the group is in a transient unstable phase. Let the consumer to retry</span></div><div class="line">        <span class="comment">// joining without specified consumer id,</span></div><div class="line">        responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt;</div><div class="line">        group synchronized &#123;</div><div class="line">          <span class="keyword">if</span> (group.is(<span class="type">Dead</span>) || !group.has(memberId)) &#123;</div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">            removeHeartbeatForLeavingMember(group, member)<span class="comment">//<span class="doctag">NOTE:</span> 认为心跳完成</span></div><div class="line">            onMemberFailure(group, member)<span class="comment">//<span class="doctag">NOTE:</span> 从 group 移除当前 member,并进行 rebalance</span></div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onMemberFailure</span></span>(group: <span class="type">GroupMetadata</span>, member: <span class="type">MemberMetadata</span>) &#123;</div><div class="line">  trace(<span class="string">"Member %s in group %s has failed"</span>.format(member.memberId, group.groupId))</div><div class="line">  group.remove(member.memberId)<span class="comment">//<span class="doctag">NOTE:</span> 从 Group 移除当前 member 信息</span></div><div class="line">  group.currentState <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Dead</span> | <span class="type">Empty</span> =&gt;</div><div class="line">    <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt; maybePrepareRebalance(group)<span class="comment">//<span class="doctag">NOTE:</span> 进行 rebalance</span></div><div class="line">    <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt; joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))<span class="comment">//<span class="doctag">NOTE:</span> 检查 join-group 是否可以完成</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出，GroupCoordinator 在处理 LEAVE_GROUP 请求时，实际上就是调用了 <code>onMemberFailure()</code> 方法，从 group 移除了失败的 member 的，并且将进行相应的状态转换：</p>
<ol>
<li>如果 group 原来是在 Dead 或 Empty 时，那么由于 group 本来就没有 member，就不再进行任何操作；</li>
<li>如果 group 原来是在 Stable 或 AwaitingSync 时，那么将会执行 <code>maybePrepareRebalance()</code> 方法，进行 rebalance 操作（后面的过程就跟最开始 join-group 时一样，参考源码分析六）；</li>
<li>如果 group 已经在 PreparingRebalance 状态了，那么这里将检查一下 join-group 的延迟操作是否完成了，如果操作完成了，那么 GroupCoordinator 就会向 group 的 member 发送 join-group response，然后将状态更新为 AwaitingSync.</li>
</ol>
<h3 id="HEARTBEAT-心跳请求处理"><a href="#HEARTBEAT-心跳请求处理" class="headerlink" title="HEARTBEAT 心跳请求处理"></a>HEARTBEAT 心跳请求处理</h3><p>心跳请求是非常重要的请求之一：</p>
<ol>
<li>对于 Server 端来说，它是 GroupCoordinator 判断一个 consumer member 是否存活的重要条件，如果其中一个 consumer 在给定的时间没有发送心跳请求，那么就会将这个 consumer 从这个 group 中移除，并执行 rebalance 操作；</li>
<li>对于 Client 端而言，心跳请求是 client 感应 group 状态变化的一个重要中介，比如：此时有一个新的 consumer 加入到 consumer group 中了，这时候会进行 rebalace 操作，group 端的状态会发送变化，当 group 其他 member 发送心跳请求，GroupCoordinator 就会通知 client 此时这个 group 正处于 rebalance 阶段，让它们 rejoin group。</li>
</ol>
<p>GroupCoordinator 处理心跳请求的过程如下所示。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> Server 端处理心跳请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleHeartbeat</span></span>(groupId: <span class="type">String</span>,</div><div class="line">                  memberId: <span class="type">String</span>,</div><div class="line">                  generationId: <span class="type">Int</span>,</div><div class="line">                  responseCallback: <span class="type">Short</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line"><span class="keyword">if</span> (!isActive.get) &#123;<span class="comment">//<span class="doctag">NOTE:</span> GroupCoordinator 已经失败</span></div><div class="line">  responseCallback(<span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>.code)</div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;<span class="comment">//<span class="doctag">NOTE:</span> 当前的 GroupCoordinator 不包含这个 group</span></div><div class="line">  responseCallback(<span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;<span class="comment">//<span class="doctag">NOTE:</span> group 的状态信息正在 loading,直接返回成功结果</span></div><div class="line">  <span class="comment">// the group is still loading, so respond just blindly</span></div><div class="line">  responseCallback(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">  groupManager.getGroup(groupId) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> 当前 GroupCoordinator 不包含这个 group</span></div><div class="line">      responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt; <span class="comment">//<span class="doctag">NOTE:</span> 包含这个 group</span></div><div class="line">      group synchronized &#123;</div><div class="line">        group.currentState <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Dead</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 的状态已经变为 dead,意味着 group 的 meta 已经被清除,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">            <span class="comment">// if the group is marked as dead, it means some other thread has just removed the group</span></div><div class="line">            <span class="comment">// from the coordinator metadata; this is likely that the group has migrated to some other</span></div><div class="line">            <span class="comment">// coordinator OR the group is in a transient unstable phase. Let the member retry</span></div><div class="line">            <span class="comment">// joining without the specified member id,</span></div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">Empty</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 的状态为 Empty, 意味着 group 的成员为空,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">AwaitingSync</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 状态为 AwaitingSync, 意味着 group 刚 rebalance 结束</span></div><div class="line">            <span class="keyword">if</span> (!group.has(memberId)) <span class="comment">//<span class="doctag">NOTE:</span> group 不包含这个 member,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">            <span class="keyword">else</span> <span class="comment">//<span class="doctag">NOTE:</span> 返回当前 group 正在进行 rebalance,要求 client rejoin 这个 group</span></div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code)</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 状态为 PreparingRebalance</span></div><div class="line">            <span class="keyword">if</span> (!group.has(memberId)) &#123; <span class="comment">//<span class="doctag">NOTE:</span> group 不包含这个 member,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//<span class="doctag">NOTE:</span> 正常处理心跳信息,并返回 REBALANCE_IN_PROGRESS 错误</span></div><div class="line">              <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">              <span class="comment">//note: 更新心跳时间,认为心跳完成,并监控下次的调度情况（超时的话,会把这个 member 从 group 中移除）</span></div><div class="line">              completeAndScheduleNextHeartbeatExpiration(group, member)</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code)</div><div class="line">            &#125;</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">Stable</span> =&gt;</div><div class="line">            <span class="keyword">if</span> (!group.has(memberId)) &#123;</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//<span class="doctag">NOTE:</span> 正确处理心跳信息</span></div><div class="line">              <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">              <span class="comment">//note: 更新心跳时间,认为心跳完成,并监控下次的调度情况（超时的话,会把这个 member 从 group 中移除）</span></div><div class="line">              completeAndScheduleNextHeartbeatExpiration(group, member)</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="group-的状态机"><a href="#group-的状态机" class="headerlink" title="group 的状态机"></a>group 的状态机</h2><p>GroupCoordinator 在进行 group 和 offset 相关的管理操作时，有一项重要的工作就是处理和维护 group 状态的变化，一个 Group 状态机如下如所示。</p>
<p><img src="/images/kafka/group.png" alt="Group 状态机"></p>
<p>在这个状态机中，最核心就是 rebalance 操作，简单说一下 rebalance 过程：</p>
<ol>
<li>当一些条件发生时将 group 从 <strong>Stable</strong> 状态变为 <strong>PreparingRebalance</strong>；</li>
<li>然后就是等待 group 中的所有 consumer member 发送 join-group 请求加入 group，如果都已经发送 join-group 请求，此时 GroupCoordinator 会向所有 member 发送 join-group response，那么 group 的状态变为 <strong>AwaitingSync</strong>；</li>
<li>leader consumer 会收到各个 member 订阅的 topic 详细信息，等待其分配好 partition 后，通过 sync-group 请求将结果发给 GroupCoordinator（非 leader consumer 发送的 sync-group 请求的 data 是为空的）；</li>
<li>如果 GroupCoordinator 收到了 leader consumer 发送的 response，获取到了这个 group 各个 member 所分配的 topic-partition 列表，group 的状态就会变成 <strong>Stable</strong>。</li>
</ol>
<p>这就是一次完整的 rebalance 过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;突然发现距离上一篇文章，已经过去两个多月了，有两个月没有写博客了，之前定的是年前把这个系列写完，现在看来只能往后拖了，后面估计还有五篇文章左右，尽量在春节前完成吧。继续之前的内容开始讲解，这篇文章，主要是想把 GroupCoordinator 的内容总结一下，也算是开始了 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Consumer 两种 commit 机制和 partition 分配机制（九）</title>
    <link href="http://matt33.com/2017/11/19/consumer-two-summary/"/>
    <id>http://matt33.com/2017/11/19/consumer-two-summary/</id>
    <published>2017-11-19T07:15:32.000Z</published>
    <updated>2017-11-19T11:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>紧接着上篇文章，这篇文章讲述 Consumer 提供的两种 commit 机制和两种 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。</p>
<h2 id="两种-commit-机制"><a href="#两种-commit-机制" class="headerlink" title="两种 commit 机制"></a>两种 commit 机制</h2><p>先看下两种不同的 commit 机制，一种是同步 commit，一种是异步 commit，既然其作用都是 offset commit，应该不难猜到它们底层使用接口都是一样的，其调用流程如下图所示：</p>
<p><img src="/images/kafka/two-commit.png" alt="两种 commit 机制"></p>
<h3 id="同步-commit"><a href="#同步-commit" class="headerlink" title="同步 commit"></a>同步 commit</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 对 poll() 中返回的所有 topics 和 partition 列表进行 commit</span></div><div class="line"><span class="comment">// 这个方法只能将 offset 提交 Kafka 中，Kafka 将会在每次 rebalance 之后的第一次拉取或启动时使用同步 commit</span></div><div class="line"><span class="comment">// 这是同步 commit，它将会阻塞进程，直到 commit 成功或者遇到一些错误</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">()</span> </span>&#123;&#125;</div><div class="line"><span class="comment">// 只对指定的 topic-partition 列表进行 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span> </span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>其实，从上图中，就已经可以看出，同步 commit 的实现方式，<code>client.poll()</code> 方法会阻塞直到这个request 完成或超时才会返回。</p>
<h3 id="异步-commit"><a href="#异步-commit" class="headerlink" title="异步 commit"></a>异步 commit</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 异步 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">()</span> </span>&#123;&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span> </span>&#123;&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span> </span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>而对于异步的 commit，最后调用的都是 <code>doCommitOffsetsAsync</code> 方法，其具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//org.apache.kafka.clients.consumer.internals.ConsumerCoordinator</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doCommitOffsetsAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, <span class="keyword">final</span> OffsetCommitCallback callback)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.subscriptions.needRefreshCommits();</div><div class="line">    RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets);<span class="comment">//note: 发送 offset-commit 请求</span></div><div class="line">    <span class="keyword">final</span> OffsetCommitCallback cb = callback == <span class="keyword">null</span> ? defaultOffsetCommitCallback : callback;</div><div class="line">    future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;Void&gt;() &#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(Void value)</span> </span>&#123;</div><div class="line">            <span class="keyword">if</span> (interceptors != <span class="keyword">null</span>)</div><div class="line">                interceptors.onCommit(offsets);</div><div class="line"></div><div class="line">            <span class="comment">//note: 添加成功的请求,以唤醒相应的回调函数</span></div><div class="line">            completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(cb, offsets, <span class="keyword">null</span>));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">            Exception commitException = e;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (e <span class="keyword">instanceof</span> RetriableException)</div><div class="line">                commitException = <span class="keyword">new</span> RetriableCommitFailedException(e);</div><div class="line"></div><div class="line">            <span class="comment">//note: 添加失败的请求,以唤醒相应的回调函数</span></div><div class="line">            completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(cb, offsets, commitException));</div><div class="line">        &#125;</div><div class="line">    &#125;);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在异步 commit 中，可以添加相应的回调函数，如果 request 处理成功或处理失败，ConsumerCoordinator 会通过 <code>invokeCompletedOffsetCommitCallbacks()</code> 方法唤醒相应的回调函数。</p>
<p>关于 offset commit 请求的处理见上一篇文章中的<a href="http://matt33.com/2017/11/18/consumer-subscribe/#commit-offset-请求处理">Offset Commit 请求处理</a>，对于提交的 offset，GroupCoordinator 会记录在 GroupMetadata 对象中。</p>
<h2 id="两种-partition-分配机制"><a href="#两种-partition-分配机制" class="headerlink" title="两种 partition 分配机制"></a>两种 partition 分配机制</h2><p>consumer 提供的两种不同 partition 分配策略，可以通过 <code>partition.assignment.strategy</code> 参数进行配置，默认情况下使用的是 <code>org.apache.kafka.clients.consumer.RangeAssignor</code>，Kafka 中提供另一种 partition 的分配策略 <code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>，它们关系如下图所示：</p>
<p><img src="/images/kafka/PartitionAssignor.png" alt="Kafka 系统内置的两种 partition 分配机制"></p>
<p>通过上图可以看出，用户可以自定义相应的 partition 分配机制，只需要继承这个 <code>AbstractPartitionAssignor</code> 抽象类即可。</p>
<h3 id="AbstractPartitionAssignor"><a href="#AbstractPartitionAssignor" class="headerlink" title="AbstractPartitionAssignor"></a>AbstractPartitionAssignor</h3><p>AbstractPartitionAssignor 有一个抽象方法，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Perform the group assignment given the partition counts and member subscriptions</div><div class="line"> * <span class="doctag">@param</span> partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded</div><div class="line"> *                           from this map.</div><div class="line"> * <span class="doctag">@param</span> subscriptions Map from the memberId to their respective topic subscription</div><div class="line"> * <span class="doctag">@return</span> Map from each member to the list of partitions assigned to them.</div><div class="line"> */</div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 根据 partitionsPerTopic 和 subscriptions 进行分配,具体的实现会在子类中实现（不同的子类的实现各异）</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,Map&lt;String, List&lt;String&gt;&gt; subscriptions);</div></pre></td></tr></table></figure>
<p><code>assign()</code> 这个方法，有两个参数：</p>
<ul>
<li><code>partitionsPerTopic</code>：所订阅的每个 topic 与其 partition 数的对应关系，metadata 没有的 topic 将会被移除；</li>
<li><code>subscriptions</code>：每个 consumerId 与其所订阅的 topic 列表的关系。</li>
</ul>
<p><code>RangeAssignor</code> 和 <code>RoundRobinAssignor</code> 通过这个方法 <code>assign()</code> 的实现，来进行相应的 partition 分配。</p>
<h3 id="RangeAssignor-分配模式"><a href="#RangeAssignor-分配模式" class="headerlink" title="RangeAssignor 分配模式"></a>RangeAssignor 分配模式</h3><p>直接看一下这个方法的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,</div><div class="line">                                                Map&lt;String, List&lt;String&gt;&gt; subscriptions) &#123;</div><div class="line">    Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);<span class="comment">//note: (topic, List&lt;consumerId&gt;)</span></div><div class="line">    Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (String memberId : subscriptions.keySet())</div><div class="line">        assignment.put(memberId, <span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());<span class="comment">//note: 初始化</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123;</div><div class="line">        String topic = topicEntry.getKey();</div><div class="line">        List&lt;String&gt; consumersForTopic = topicEntry.getValue();</div><div class="line"></div><div class="line">        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);</div><div class="line">        <span class="keyword">if</span> (numPartitionsForTopic == <span class="keyword">null</span>)</div><div class="line">            <span class="keyword">continue</span>;</div><div class="line"></div><div class="line">        Collections.sort(consumersForTopic);</div><div class="line"></div><div class="line">        <span class="comment">//note: 假设 partition 有 7个,consumer 有5个</span></div><div class="line">        <span class="keyword">int</span> numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();<span class="comment">//note: 1</span></div><div class="line">        <span class="keyword">int</span> consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();<span class="comment">//note: 2</span></div><div class="line"></div><div class="line">        List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, n = consumersForTopic.size(); i &lt; n; i++) &#123;</div><div class="line">            <span class="comment">//note: i=0, start: 0, length: 2, topic-partition: p0,p1</span></div><div class="line">            <span class="comment">//note: i=1, start: 2, length: 2, topic-partition: p2,p3</span></div><div class="line">            <span class="comment">//note: i=2, start: 4, length: 1, topic-partition: p4</span></div><div class="line">            <span class="comment">//note: i=3, start: 5, length: 1, topic-partition: p5</span></div><div class="line">            <span class="comment">//note: i=4, start: 6, length: 1, topic-partition: p6</span></div><div class="line">            <span class="keyword">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</div><div class="line">            <span class="keyword">int</span> length = numPartitionsPerConsumer + (i + <span class="number">1</span> &gt; consumersWithExtraPartition ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">            assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> assignment;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>假设 topic 的 partition 数为 numPartitionsForTopic，group 中订阅这个 topic 的 member 数为 <code>consumersForTopic.size()</code>，首先需要算出两个值：</p>
<ul>
<li><code>numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size()</code>：表示平均每个 consumer 会分配到几个 partition；    </li>
<li><code>consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size()</code>：表示平均分配后还剩下多少个 partition 未分配。</li>
</ul>
<p>分配的规则是：对于剩下的那些 partition 分配到前 consumersWithExtraPartition 个 consumer 上，也就是前 consumersWithExtraPartition 个 consumer 获得 topic-partition 列表会比后面多一个。</p>
<p>在上述的程序中，举了一个例子，假设有一个 topic 有 7 个 partition，group 有5个 consumer，这个5个 consumer 都订阅这个 topic，那么 range 的分配方式如下：</p>
<ul>
<li>consumer 0：start: 0, length: 2, topic-partition: p0,p1；</li>
<li>consumer 1：start: 2, length: 2, topic-partition: p2,p3；</li>
<li>consumer 2：start: 4, length: 1, topic-partition: p4；</li>
<li>consumer 3：start: 5, length: 1, topic-partition: p5；</li>
<li>consumer 4：start: 6, length: 1, topic-partition: p6</li>
</ul>
<p>而如果 group 中有 consumer 没有订阅这个 topic，那么这个 consumer 将不会参与分配。下面再举个例子，将有两个 topic，一个 partition 有5个，一个 partition 有7个，group 有5个 consumer，但是只有前3个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下：</p>
<table>
<thead>
<tr>
<th>consumer</th>
<th>订阅的 topic1 的列表</th>
<th>订阅的 topic2 的列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>consumer 0</td>
<td>t1p0, t1p1</td>
<td>t2p0, t2p1</td>
</tr>
<tr>
<td>consumer 1</td>
<td>t1p2, t1p3</td>
<td>t2p2, t2p3</td>
</tr>
<tr>
<td>consumer 2</td>
<td>t1p4</td>
<td>t2p4</td>
</tr>
<tr>
<td>consumer 3</td>
<td></td>
<td>t2p5</td>
</tr>
<tr>
<td>consumer 4</td>
<td></td>
<td>t2p6</td>
</tr>
</tbody>
</table>
<h3 id="RoundRobinAssignor"><a href="#RoundRobinAssignor" class="headerlink" title="RoundRobinAssignor"></a>RoundRobinAssignor</h3><p>这个是 roundrobin 的实现，其实现方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="keyword">public</span> Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,</div><div class="line">                                                Map&lt;String, List&lt;String&gt;&gt; subscriptions) &#123;</div><div class="line">    Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (String memberId : subscriptions.keySet())</div><div class="line">        assignment.put(memberId, <span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());</div><div class="line"></div><div class="line">    CircularIterator&lt;String&gt; assigner = <span class="keyword">new</span> CircularIterator&lt;&gt;(Utils.sorted(subscriptions.keySet()));<span class="comment">//note: 环行迭代</span></div><div class="line">    <span class="keyword">for</span> (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) &#123;</div><div class="line">        <span class="keyword">final</span> String topic = partition.topic();</div><div class="line">        <span class="keyword">while</span> (!subscriptions.get(assigner.peek()).contains(topic))<span class="comment">//note: 遍历直到找到订阅这个 topic 的 partition</span></div><div class="line">            assigner.next();</div><div class="line">        assignment.get(assigner.next()).add(partition);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> assignment;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> List&lt;TopicPartition&gt; <span class="title">allPartitionsSorted</span><span class="params">(Map&lt;String, Integer&gt; partitionsPerTopic,</span></span></div><div class="line">                                                Map&lt;String, List&lt;String&gt;&gt; subscriptions) &#123;</div><div class="line">    SortedSet&lt;String&gt; topics = <span class="keyword">new</span> TreeSet&lt;&gt;();<span class="comment">//<span class="doctag">NOTE:</span> 所有的 topics（有序）</span></div><div class="line">    <span class="keyword">for</span> (List&lt;String&gt; subscription : subscriptions.values())</div><div class="line">        topics.addAll(subscription);</div><div class="line"></div><div class="line">    List&lt;TopicPartition&gt; allPartitions = <span class="keyword">new</span> ArrayList&lt;&gt;();<span class="comment">//<span class="doctag">NOTE:</span> 订阅的 Topic的所有的 TopicPartition 集合</span></div><div class="line">    <span class="keyword">for</span> (String topic : topics) &#123;</div><div class="line">        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);</div><div class="line">        <span class="keyword">if</span> (numPartitionsForTopic != <span class="keyword">null</span>)</div><div class="line">            <span class="comment">//note: topic 的所有 partition 都添加进去</span></div><div class="line">            allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> allPartitions;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>roundrobin 的实现原则，简单来说就是：列出所有 topic-partition 和列出所有的 consumer member，然后开始分配，一轮之后继续下一轮，假设有有一个 topic，它有7个 partition，group 有3个 consumer 都订阅了这个 topic，那么其分配方式为：</p>
<table>
<thead>
<tr>
<th>consumer</th>
<th>分配列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>consumer 0</td>
<td>tp0, tp3, tp6</td>
</tr>
<tr>
<td>consumer 1</td>
<td>tp1, tp4</td>
</tr>
<tr>
<td>consumer 2</td>
<td>tp2, tp5</td>
</tr>
</tbody>
</table>
<p>对于多个 topic 的订阅，将有两个 topic，一个 partition 有5个，一个 partition 有7个，group 有5个 consumer，但是只有前3个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下：</p>
<table>
<thead>
<tr>
<th>consumer</th>
<th>订阅的 topic1 的列表</th>
<th>订阅的 topic2 的列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>consumer 0</td>
<td>t1p0, t1p3</td>
<td>t2p0, t2p5</td>
</tr>
<tr>
<td>consumer 1</td>
<td>t1p1, t1p4</td>
<td>t2p1, t2p6</td>
</tr>
<tr>
<td>consumer 2</td>
<td>t1p2</td>
<td>t2p2</td>
</tr>
<tr>
<td>consumer 3</td>
<td></td>
<td>t2p3</td>
</tr>
<tr>
<td>consumer 4</td>
<td></td>
<td>t2p4</td>
</tr>
</tbody>
</table>
<p>roundrobin 分配方式与 range 的分配方式还是略有不同。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;紧接着上篇文章，这篇文章讲述 Consumer 提供的两种 commit 机制和两种 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。&lt;/p&gt;
&lt;h2 id=&quot;两种-commit-机制&quot;&gt;&lt;a href=&quot;#两种-comm
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Consumer 两种订阅模式（八）</title>
    <link href="http://matt33.com/2017/11/18/consumer-subscribe/"/>
    <id>http://matt33.com/2017/11/18/consumer-subscribe/</id>
    <published>2017-11-18T06:43:50.000Z</published>
    <updated>2017-12-21T15:31:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>在前面两篇 Kafka Consumer 的文章中，Consumer Poll 模型这部分基本上已经完整结束，Consumer 这块的文章计划是要写五篇，这篇是 Consumer 这块的第三篇，本来计划是要从其中的三个小块细节内容着手，这三个地方有一个相同之处，那就是在 Kafka Consumer 中都提供了两个不同的解决方案，但具体怎么去使用是需要用户根据自己的业务场景去配置，这里会讲述其底层的具体实现（但为了阅读得更为方便，本来计划的这篇文章将拆分为两篇来，第一篇先讲述第一点，后面两点放在一起讲述）。</p>
<p>本篇文章讲述的这三点内容分别是：</p>
<ol>
<li>consumer 的两种订阅模式， <code>subscribe()</code>和<code>assign()</code> 模式，一种是 topic 粒度（使用 group 管理），一种是 topic-partition 粒度（用户自己去管理）；</li>
<li>consumer 的两种 commit 实现，<code>commitAsync()</code>和<code>commitSync()</code>，即同步 commit 和异步 commit；</li>
<li>consumer 提供的两种不同 <code>partition.assignment.strategy</code>，这是关于一个 group 订阅一些 topic 后，group 内各个 consumer 实例的 partition 分配策略。</li>
</ol>
<p>0.9.X 之前 Kafka Consumer 是支持两个不同的订阅模型 —— high level 和 simple level，这两种模型的最大区别是：第一个其 offset 管理是由 Kafka 来做，包括 rebalance 操作，第二个则是由使用者自己去做，自己去管理相关的 offset，以及自己去进行 rebalance。</p>
<p>在新版的 consumer 中对 high level 和 simple level 的接口实现了统一，简化了相应的相应的编程模型。</p>
<h2 id="订阅模式"><a href="#订阅模式" class="headerlink" title="订阅模式"></a>订阅模式</h2><p>在新版的 Consumer 中，high level 模型现在叫做订阅模式，KafkaConsumer 提供了三种 API，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 订阅指定的 topic 列表,并且会自动进行动态 partition 订阅</span></div><div class="line"><span class="comment">// 当发生以下情况时,会进行 rebalance: 1.订阅的 topic 列表改变; 2.topic 被创建或删除; 3.consumer 线程 die; 4. 加一个新的 consumer 线程</span></div><div class="line"><span class="comment">// 当发生 rebalance 时，会唤醒 ConsumerRebalanceListener 线程</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span></span>&#123;&#125;</div><div class="line"><span class="comment">// 同上，但是这里没有设置 listener</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span> </span>&#123;&#125;</div><div class="line"><span class="comment">//note: 订阅那些满足一定规则(pattern)的 topic</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span></span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>以上三种 API 都是按照 topic 级别去订阅，可以动态地获取其分配的 topic-partition，这是使用 <strong>Group 动态管理</strong>，它不能与手动 partition 管理一起使用。当监控到发生下面的事件时，Group 将会触发 rebalance 操作：</p>
<ol>
<li>订阅的 topic 列表变化；</li>
<li>topic 被创建或删除；</li>
<li>consumer group 的某个 consumer 实例挂掉；</li>
<li>一个新的 consumer 实例通过 <code>join</code> 方法加入到一个 group 中。</li>
</ol>
<p>在这种模式下，当 KafkaConsumer 调用 pollOnce 方法时，第一步会首先加入到一个 group 中，并获取其分配的 topic-partition 列表（见<a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a>），前面两篇文章都是以这种情况来讲述的。</p>
<p>这里介绍一下当调用 <code>subscribe()</code> 方法之后，Consumer 所做的事情，分两种情况介绍，一种按 topic 列表订阅，一种是按 pattern 模式订阅：</p>
<ol>
<li>topic 列表订阅<ol>
<li>更新 SubscriptionState 中记录的 <code>subscription</code>（记录的是订阅的 topic 列表），将 SubscriptionType 类型设置为 <strong>AUTO_TOPICS</strong>；</li>
<li>更新 metadata 中的 topic 列表（<code>topics</code> 变量），并请求更新 metadata；</li>
</ol>
</li>
<li>pattern 模式订阅<ol>
<li>更新 SubscriptionState 中记录的 <code>subscribedPattern</code>，设置为 pattern，将 SubscriptionType 类型设置为 <strong>AUTO_PATTERN</strong>；</li>
<li>设置 Metadata 的 needMetadataForAllTopics 为 true，即在请求 metadata 时，需要更新所有 topic 的 metadata 信息，设置后再请求更新 metadata；</li>
<li>调用 <code>coordinator.updatePatternSubscription()</code> 方法，遍历所有 topic 的 metadata，找到所有满足 pattern 的 topic 列表，更新到 SubscriptionState 的 <code>subscriptions</code> 和 Metadata 的 <code>topics</code> 中；</li>
<li>通过在 ConsumerCoordinator 中调用 <code>addMetadataListener()</code> 方法在 Metadata 中添加 listener 当每次 metadata update 时就调用第三步的方法更新，但是只有当本地缓存的 topic 列表与现在要订阅的 topic 列表不同时，才会触发 rebalance 操作。</li>
</ol>
</li>
</ol>
<p>其他部分，两者基本一样，只是 pattern 模型在每次更新 topic-metadata 时，获取全局的 topic 列表，如果发现有新加入的符合条件的 topic，就立马去订阅，其他的地方，包括 Group 管理、topic-partition 的分配都是一样的。</p>
<h2 id="分配模式"><a href="#分配模式" class="headerlink" title="分配模式"></a>分配模式</h2><p>下面来看一下 Consumer 提供的分配模式，熟悉 0.8.X 版本的人，可能会把这种方法称为 simple consumer 的接口，当调用 <code>assign()</code> 方法手动分配 topic-partition 列表时，是不会使用 consumer 的 Group 管理机制，也即是当 consumer group member 变化或 topic 的 metadata 信息变化时是不会触发 rebalance 操作的。比如：当 topic 的 partition 增加时，这里是无法感知，需要用户进行相应的处理，Apache Flink 就是使用的这种方式，后续我会写篇文章介绍 Flink 是如何实现这种机制的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 手动向 consumer 分配一些 topic-partition 列表，并且这个接口不允许增加分配的 topic-partition 列表，将会覆盖之前分配的 topic-partition 列表，如果给定的 topic-partition 列表为空，它的作用将会与 unsubscribe() 方法一样。</span></div><div class="line"><span class="comment">//note: 这种手动 topic 分配是不会使用 consumer 的 group 管理，当 group 的 member 变化或 topic 的 metadata 变化也不会触发 rebalance 操作。</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>这里来看一下 Kafka 提供的 Group 管理到底是什么？</p>
<p>如果有印象的话，在<a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a>中介绍 Poll 模型的第一步中，详细介绍了 <code>ConsumerCoordinator.poll()</code> 方法，我们再来看一下这个方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: 它确保了这个 group 的 coordinator 是已知的,并且这个 consumer 是已经加入到了 group 中,也用于 offset 周期性的 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">poll</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">    invokeCompletedOffsetCommitCallbacks();<span class="comment">// note: 用于测试</span></div><div class="line"></div><div class="line">    <span class="comment">// note: Step1 通过 subscribe() 方法订阅 topic,并且 coordinator 未知,初始化 Consumer Coordinator</span></div><div class="line">    <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned() &amp;&amp; coordinatorUnknown()) &#123;</div><div class="line">        <span class="comment">// note: 获取 GroupCoordinator 地址,并且建立连接</span></div><div class="line">        ensureCoordinatorReady();</div><div class="line">        now = time.milliseconds();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// note: Step2 判断是否需要重新加入 group,如果订阅的 partition 变化或则分配的 partition 变化时,需要 rejoin</span></div><div class="line">    <span class="comment">// note: 如果订阅模式不是 AUTO_TOPICS 或 AUTO_PATTERN,直接跳过</span></div><div class="line">    <span class="keyword">if</span> (needRejoin()) &#123;</div><div class="line">        <span class="comment">// note: rejoin group 之前先刷新一下 metadata（对于 AUTO_PATTERN 而言）</span></div><div class="line">        <span class="keyword">if</span> (subscriptions.hasPatternSubscription())</div><div class="line">            client.ensureFreshMetadata();</div><div class="line"></div><div class="line">        <span class="comment">// note: 确保 group 是 active; 加入 group; 分配订阅的 partition</span></div><div class="line">        ensureActiveGroup();</div><div class="line">        now = time.milliseconds();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// note: Step3 检查心跳线程运行是否正常,如果心跳线程失败,则抛出异常,反之更新 poll 调用的时间</span></div><div class="line">    <span class="comment">// note: 发送心跳请求是在 ensureCoordinatorReady() 中调用的</span></div><div class="line">    pollHeartbeat(now);</div><div class="line">    <span class="comment">// note: Step4 自动 commit 时,当定时达到时,进行自动 commit</span></div><div class="line">    maybeAutoCommitOffsetsAsync(now);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果使用的是 assign 模式，也即是非 AUTO_TOPICS 或 AUTO_PATTERN 模式时，Consumer 实例在调用 poll 方法时，是不会向 GroupCoordinator 发送 join-group/sync-group/heartbeat 请求的，也就是说 GroupCoordinator 是拿不到这个 Consumer 实例的相关信息，也不会去维护这个 member 是否存活，这种情况下就需要用户自己管理自己的处理程序。但是在这种模式是可以进行 offset commit的。</p>
<h3 id="commit-offset-请求处理"><a href="#commit-offset-请求处理" class="headerlink" title="commit offset 请求处理"></a>commit offset 请求处理</h3><p>当 Kafka Serve 端受到来自 client 端的 Offset Commit 请求时，其处理逻辑如下所示，是在 <code>kafka.coordinator.GroupCoordinator</code> 中实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// kafka.coordinator.GroupCoordinator</span></div><div class="line"><span class="comment">//note: GroupCoordinator 处理 Offset Commit 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleCommitOffsets</span></span>(groupId: <span class="type">String</span>,</div><div class="line">                        memberId: <span class="type">String</span>,</div><div class="line">                        generationId: <span class="type">Int</span>,</div><div class="line">                        offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                        responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (!isActive.get) &#123;</div><div class="line">    responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>.code))</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;</div><div class="line">    responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code))</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;</div><div class="line">    responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">GROUP_LOAD_IN_PROGRESS</span>.code))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    groupManager.getGroup(groupId) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">if</span> (generationId &lt; <span class="number">0</span>) &#123;</div><div class="line">          <span class="comment">// the group is not relying on Kafka for group management, so allow the commit</span></div><div class="line">          <span class="comment">//note: 不使用 group-coordinator 管理的情况</span></div><div class="line">          <span class="comment">//note: 如果 groupID不存在,就新建一个 GroupMetadata, 其group 状态为 Empty,否则就返回已有的 groupid</span></div><div class="line">          <span class="comment">//note: 如果 simple 的 groupId 与一个 active 的 group 重复了,这里就有可能被覆盖掉了</span></div><div class="line">          <span class="keyword">val</span> group = groupManager.addGroup(<span class="keyword">new</span> <span class="type">GroupMetadata</span>(groupId))</div><div class="line">          doCommitOffsets(group, memberId, generationId, offsetMetadata, responseCallback)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="comment">// or this is a request coming from an older generation. either way, reject the commit</span></div><div class="line">          <span class="comment">//note: 过期的 offset-commit</span></div><div class="line">          responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code))</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt;</div><div class="line">        doCommitOffsets(group, memberId, generationId, offsetMetadata, responseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 真正的处理逻辑</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doCommitOffsets</span></span>(group: <span class="type">GroupMetadata</span>,</div><div class="line">                    memberId: <span class="type">String</span>,</div><div class="line">                    generationId: <span class="type">Int</span>,</div><div class="line">                    offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                    responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">var</span> delayedOffsetStore: <span class="type">Option</span>[<span class="type">DelayedStore</span>] = <span class="type">None</span></div><div class="line"></div><div class="line">  group synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (group.is(<span class="type">Dead</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId &lt; <span class="number">0</span> &amp;&amp; group.is(<span class="type">Empty</span>)) &#123;<span class="comment">//note: 来自 assign 的情况</span></div><div class="line">      <span class="comment">// the group is only using Kafka to store offsets</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (group.is(<span class="type">AwaitingSync</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!group.has(memberId)) &#123;<span class="comment">//note: 有可能 simple 与 high level 的冲突了,这里就直接拒绝相应的请求</span></div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">      completeAndScheduleNextHeartbeatExpiration(group, member)<span class="comment">//note: 更新下次需要的心跳时间</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// store the offsets without holding the group lock</span></div><div class="line">  delayedOffsetStore.foreach(groupManager.store)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>处理过程如下：</p>
<ol>
<li>如果这个 group 还不存在（groupManager没有这个 group 信息），并且 generation 为 -1（一般情况下应该都是这样），就新建一个 GroupMetadata, 其 Group 状态为 Empty；</li>
<li>现在 group 已经存在，就调用 <code>doCommitOffsets()</code> 提交 offset；</li>
<li>如果是来自 assign 模式的请求，并且其对应的 group 的状态为 Empty（generationId &lt; 0 &amp;&amp; group.is(Empty)），那么就记录这个 offset；</li>
<li>如果是来自 assign 模式的请求，但这个 group 的状态不为 Empty（!group.has(memberId)），也就是说，这个 group 已经处在活跃状态，assign 模式下的 group 是不会处于的活跃状态的，可以认为是 assign 模式使用的 group.id 与 subscribe 模式下使用的 group 相同，这种情况下就会拒绝 assign 模式下的这个 offset commit 请求。</li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>根据上面的讲述，这里做一下小节，如下图所示：</p>
<p><img src="/images/kafka/two-subscribe.png" alt="两种订阅模式"></p>
<p>简单做一下总结：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>不同之处</th>
<th>相同之处</th>
</tr>
</thead>
<tbody>
<tr>
<td>subscribe()</td>
<td>使用 Kafka Group 管理，自动进行 rebalance 操作</td>
<td>可以在 Kafka 保存 offset</td>
</tr>
<tr>
<td>assign()</td>
<td>用户自己进行相关的处理</td>
<td>也可以进行 offset commit，但是尽量保证 group.id 唯一性，如果使用一个与上面模式一样的 group，offset commit 请求将会被拒绝</td>
</tr>
</tbody>
</table>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面两篇 Kafka Consumer 的文章中，Consumer Poll 模型这部分基本上已经完整结束，Consumer 这块的文章计划是要写五篇，这篇是 Consumer 这块的第三篇，本来计划是要从其中的三个小块细节内容着手，这三个地方有一个相同之处，那就是在 K
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
</feed>
