<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matt&#39;s Blog</title>
  <subtitle>王蒙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matt33.com/"/>
  <updated>2018-05-01T04:35:20.000Z</updated>
  <id>http://matt33.com/</id>
  
  <author>
    <name>Matt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka 源码解析之 ReplicaManager 详解（十五）</title>
    <link href="http://matt33.com/2018/05/01/kafka-replica-manager/"/>
    <id>http://matt33.com/2018/05/01/kafka-replica-manager/</id>
    <published>2018-05-01T04:07:01.000Z</published>
    <updated>2018-05-01T04:35:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类为入口，通过对 ReplicaManager 的详解，顺便再把 Kafka 存储层的内容做一个简单的总结。</p>
<h2 id="ReplicaManager-简介"><a href="#ReplicaManager-简介" class="headerlink" title="ReplicaManager 简介"></a>ReplicaManager 简介</h2><p>前面三篇文章，关于 Produce 请求、Fetch 请求以及副本同步流程的启动都是由 ReplicaManager 来控制的，ReplicaManager 可以说是 Server 端重要的组成部分，回头再仔细看下 KafkaApi 这个类，就会发现 Server 端要处理的多种类型的请求都是 ReplicaManager 来处理的，ReplicaManager 需要处理的请求的有以下六种：</p>
<ol>
<li>LeaderAndIsr 请求；</li>
<li>StopReplica 请求；</li>
<li>UpdateMetadata 请求；</li>
<li>Produce 请求；</li>
<li>Fetch 请求；</li>
<li>ListOffset 请求；</li>
</ol>
<p>其中后面三个已经在前面的文章中介绍过，前面三个都是 Controller 发送的请求，虽然是由 ReplicaManager 中处理的，也会在 Controller 部分展开详细的介绍。</p>
<p>这里先看下面这张图，这张图把 ReplicaManager、Partition、Replica、LogManager、Log、logSegment 这几个抽象的类之间的调用关系简单地串了起来，也算是对存储层这几个重要的部分简单总结了一下：</p>
<p><img src="/images/kafka/replica-manager.png" alt="存储层各个类之间关系"></p>
<p>对着上面的图，简单总结一下：</p>
<ol>
<li>ReplicaManager 是最外层暴露的一个实例，前面说的那几种类型的请求都是由这个实例来处理的；</li>
<li>LogManager 负责管理本节点上所有的日志（Log）实例，它作为 ReplicaManager 的变量传入到了 ReplicaManager 中，ReplicaManager 通过 LogManager 可以对相应的日志实例进行操作；</li>
<li>在 ReplicaManager 中有一个变量：allPartitions，它负责管理本节点所有的 Partition 实例（只要本节点有这个 partition 的日志实例，就会有一个对应的 Partition 对对象实例）；</li>
<li>在创建 Partition 实例时，ReplicaManager 也会作为成员变量传入到 Partition 实例中，Partition 通过 ReplicaManager 可以获得 LogManager 实例、brokerId 等；</li>
<li>Partition 会为它的每一个副本创建一个 Replica 对象实例，但只会为那个在本地副本创建 Log 对象实例（LogManager 不存在这个 Log 对象的情况下，有的话直接引用），这样的话，本地的 Replica 也就与 Log 实例建立了一个联系。</li>
</ol>
<p>关于 ReplicaManager 的 allPartitions 变量可以看下面这张图（假设 Partition 设置的是3副本）：</p>
<p><img src="/images/kafka/all-partition.png" alt="ReplicaManager 的 allPartitions 变量"></p>
<p>allPartitions 管理的 Partition 实例，因为是 3 副本，所以每个 Partition 实例又会管理着三个 Replica，其中只有本地副本（对于上图，就是值 replica.id = 1 的副本）才有对应的 Log 实例对象（HW 和 LEO 的介绍参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Offset 那些事</a>）。</p>
<h2 id="ReplicaManager-启动"><a href="#ReplicaManager-启动" class="headerlink" title="ReplicaManager 启动"></a>ReplicaManager 启动</h2><p>KafkaServer 在启动时，就初始化了 ReplicaManager 实例，如下所示，KafkaServer 在初始化 logManager 后，将 logManager 作为参数传递给了 ReplicaManager。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(isShuttingDown.get)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(startupComplete.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">      brokerState.newState(<span class="type">Starting</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* start scheduler */</span></div><div class="line">      kafkaScheduler.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* setup zookeeper */</span></div><div class="line">      zkUtils = initZk()</div><div class="line"></div><div class="line">      <span class="comment">/* Get or create cluster_id */</span></div><div class="line">      _clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">      info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* generate brokerId */</span></div><div class="line">      config.brokerId =  getBrokerId</div><div class="line">      <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">      <span class="comment">/* start log manager */</span></div><div class="line">      <span class="comment">//note: 启动日志管理线程</span></div><div class="line">      logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">      logManager.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* start replica manager */</span></div><div class="line">      <span class="comment">//note: 启动 replica manager</span></div><div class="line">      replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">        isShuttingDown, quotaManagers.follower)</div><div class="line">      replicaManager.startup()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">      isStartingUp.set(<span class="literal">false</span>)</div><div class="line">      shutdown()</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">    &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h3 id="startup"><a href="#startup" class="headerlink" title="startup"></a>startup</h3><p>ReplicaManager <code>startup()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// start ISR expiration thread</span></div><div class="line">  <span class="comment">// A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR</span></div><div class="line">  <span class="comment">//note: 周期性检查 isr 是否有 replica 过期需要从 isr 中移除</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-expiration"</span>, maybeShrinkIsr, period = config.replicaLagTimeMaxMs / <span class="number">2</span>, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  <span class="comment">//note: 周期性检查是不是有 topic-partition 的 isr 需要变动,如果需要,就更新到 zk 上,来触发 controller</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-change-propagation"</span>, maybePropagateIsrChanges, period = <span class="number">2500</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法与 LogManager 的 <code>startup()</code> 方法类似，也是启动了相应的定时任务，这里，ReplicaManger 启动了两个周期性的任务：</p>
<ol>
<li>maybeShrinkIsr: 判断 topic-partition 的 isr 是否有 replica 因为延迟或 hang 住需要从 isr 中移除；</li>
<li>maybePropagateIsrChanges：判断是不是需要对 isr 进行更新，如果有 topic-partition 的 isr 发生了变动需要进行更新，那么这个方法就会被调用，它会触发 zk 的相应节点，进而触发 controller 进行相应的操作。</li>
</ol>
<p>关于 ReplicaManager 这两个方法的处理过程及 topic-partition isr 变动情况的触发，下面这张流程图做了简单的说明，如下所示：</p>
<p><img src="/images/kafka/replica-manager-startup.png" alt="ReplicaManager 的 Startup 方法启动两个周期性任务及 isr 扩充的情况"></p>
<h3 id="maybeShrinkIsr"><a href="#maybeShrinkIsr" class="headerlink" title="maybeShrinkIsr"></a>maybeShrinkIsr</h3><p>如前面流程图所示， ReplicaManager 的 <code>maybeShrinkIsr()</code> 实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code>  会遍历本节点所有的 Partition 实例，来检查它们 isr 中的 replica 是否需要从 isr 中移除，Partition 中这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查这个 isr 中的每个 replcia</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(replicaMaxLagTimeMs: <span class="type">Long</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123; <span class="comment">//note: 只有本地副本是 leader, 才会做这个操作</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="comment">//note: 检查当前 isr 的副本是否需要从 isr 中移除</span></div><div class="line">        <span class="keyword">val</span> outOfSyncReplicas = getOutOfSyncReplicas(leaderReplica, replicaMaxLagTimeMs)</div><div class="line">        <span class="keyword">if</span>(outOfSyncReplicas.nonEmpty) &#123;</div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas <span class="comment">//note: new isr</span></div><div class="line">          assert(newInSyncReplicas.nonEmpty)</div><div class="line">          info(<span class="string">"Shrinking ISR for partition [%s,%d] from %s to %s"</span>.format(topic, partitionId,</div><div class="line">            inSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>), newInSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// update ISR in zk and in cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新 isr 到 zk</span></div><div class="line">          <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line"></div><div class="line">          replicaManager.isrShrinkRate.mark() <span class="comment">//note: 更新 metrics</span></div><div class="line">          maybeIncrementLeaderHW(leaderReplica) <span class="comment">//note: isr 变动了,判断是否需要更新 partition 的 hw</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="literal">false</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// do nothing if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 检查 isr 中的副本是否需要从 isr 中移除</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOutOfSyncReplicas</span></span>(leaderReplica: <span class="type">Replica</span>, maxLagMs: <span class="type">Long</span>): <span class="type">Set</span>[<span class="type">Replica</span>] = &#123;</div><div class="line">  <span class="comment">//note: 获取那些不应该咋 isr 中副本的列表</span></div><div class="line">  <span class="comment">//note: 1. hang 住的 replica: replica 的 LEO 超过 maxLagMs 没有更新, 那么这个 replica 将会被从 isr 中移除;</span></div><div class="line">  <span class="comment">//note: 2. 数据同步慢的 replica: 副本在 maxLagMs 内没有追上 leader 当前的 LEO, 那么这个 replica 讲会从 ist 中移除;</span></div><div class="line">  <span class="comment">//note: 都是通过 lastCaughtUpTimeMs 来判断的</span></div><div class="line">  <span class="keyword">val</span> candidateReplicas = inSyncReplicas - leaderReplica</div><div class="line"></div><div class="line">  <span class="keyword">val</span> laggingReplicas = candidateReplicas.filter(r =&gt; (time.milliseconds - r.lastCaughtUpTimeMs) &gt; maxLagMs)</div><div class="line">  <span class="keyword">if</span> (laggingReplicas.nonEmpty)</div><div class="line">    debug(<span class="string">"Lagging replicas for partition %s are %s"</span>.format(topicPartition, laggingReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">  laggingReplicas</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code> 这个方法的实现可以简单总结为以下几步：</p>
<ol>
<li>先判断本地副本是不是这个 partition 的 leader，<strong>这个操作只会在 leader 上进行</strong>，如果不是 leader 直接跳过；</li>
<li>通过 <code>getOutOfSyncReplicas()</code> 方法遍历除 leader 外 isr 的所有 replica，找到那些满足条件（<strong>落后超过 maxLagMs 时间的副本</strong>）需要从 isr 中移除的 replica；</li>
<li>得到了新的 isr 列表，调用 <code>updateIsr()</code> 将新的 isr 更新到 zk 上，并且这个方法内部又调用了 ReplicaManager 的 <code>recordIsrChange()</code> 方法来告诉 ReplicaManager 当前这个 topic-partition 的 isr 发生了变化（<strong>可以看出，zk 上这个 topic-partition 的 isr 信息虽然变化了，但是实际上 controller 还是无法感知的</strong>）；</li>
<li>因为 isr 发生了变动，所以这里会通过 <code>maybeIncrementLeaderHW()</code> 方法来检查一下这个 partition 的 HW 是否需要更新。</li>
</ol>
<p><code>updateIsr()</code> 和 <code>maybeIncrementLeaderHW()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查是否需要更新 partition 的 HW,这个方法将在两种情况下触发:</span></div><div class="line"><span class="comment">//note: 1.Partition ISR 变动; 2. 任何副本的 LEO 改变;</span></div><div class="line"><span class="comment">//note: 在获取 HW 时,是从 isr 和认为能追得上的副本中选择最小的 LEO,之所以也要从能追得上的副本中选择,是为了等待 follower 追上 HW,否则可能没机会追上了</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeIncrementLeaderHW</span></span>(leaderReplica: <span class="type">Replica</span>, curTime: <span class="type">Long</span> = time.milliseconds): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取 isr 以及能够追上 isr （认为最近一次 fetch 的时间在 replica.lag.time.max.time 之内） 副本的 LEO 信息。</span></div><div class="line">  <span class="keyword">val</span> allLogEndOffsets = assignedReplicas.filter &#123; replica =&gt;</div><div class="line">    curTime - replica.lastCaughtUpTimeMs &lt;= replicaManager.config.replicaLagTimeMaxMs || inSyncReplicas.contains(replica)</div><div class="line">  &#125;.map(_.logEndOffset)</div><div class="line">  <span class="keyword">val</span> newHighWatermark = allLogEndOffsets.min(<span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>.<span class="type">OffsetOrdering</span>) <span class="comment">//note: 新的 HW</span></div><div class="line">  <span class="keyword">val</span> oldHighWatermark = leaderReplica.highWatermark</div><div class="line">  <span class="keyword">if</span> (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset || oldHighWatermark.onOlderSegment(newHighWatermark)) &#123;</div><div class="line">    leaderReplica.highWatermark = newHighWatermark</div><div class="line">    debug(<span class="string">"High watermark for partition [%s,%d] updated to %s"</span>.format(topic, partitionId, newHighWatermark))</div><div class="line">    <span class="literal">true</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    debug(<span class="string">"Skipping update high watermark since Old hw %s is larger than new hw %s for partition [%s,%d]. All leo's are %s"</span></div><div class="line">      .format(oldHighWatermark, newHighWatermark, topic, partitionId, allLogEndOffsets.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="literal">false</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateIsr</span></span>(newIsr: <span class="type">Set</span>[<span class="type">Replica</span>]) &#123;</div><div class="line">  <span class="keyword">val</span> newLeaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(localBrokerId, leaderEpoch, newIsr.map(r =&gt; r.brokerId).toList, zkVersion)</div><div class="line">  <span class="keyword">val</span> (updateSucceeded,newVersion) = <span class="type">ReplicationUtils</span>.updateLeaderAndIsr(zkUtils, topic, partitionId,</div><div class="line">    newLeaderAndIsr, controllerEpoch, zkVersion) <span class="comment">//note: 执行更新操作</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span>(updateSucceeded) &#123; <span class="comment">//note: 成功更新到 zk 上</span></div><div class="line">    replicaManager.recordIsrChange(topicPartition) <span class="comment">//note: 告诉 replicaManager 这个 partition 的 isr 需要更新</span></div><div class="line">    inSyncReplicas = newIsr</div><div class="line">    zkVersion = newVersion</div><div class="line">    trace(<span class="string">"ISR updated to [%s] and zkVersion updated to [%d]"</span>.format(newIsr.mkString(<span class="string">","</span>), zkVersion))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    info(<span class="string">"Cached zkVersion [%d] not equal to that in zookeeper, skip updating ISR"</span>.format(zkVersion))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="maybePropagateIsrChanges"><a href="#maybePropagateIsrChanges" class="headerlink" title="maybePropagateIsrChanges"></a>maybePropagateIsrChanges</h3><p>ReplicaManager <code>maybePropagateIsrChanges()</code> 方法的作用是将那些 isr 变动的 topic-partition 列表（<code>isrChangeSet</code>）通过 ReplicationUtils 的 <code>propagateIsrChanges()</code> 方法更新 zk 上，这时候 Controller 才能知道哪些 topic-partition 的 isr 发生了变动。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是周期性的运行,来判断 partition 的 isr 是否需要更新,</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybePropagateIsrChanges</span></span>() &#123;</div><div class="line">  <span class="keyword">val</span> now = <span class="type">System</span>.currentTimeMillis()</div><div class="line">  isrChangeSet synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (isrChangeSet.nonEmpty &amp;&amp; <span class="comment">//note:  有 topic-partition 的 isr 需要更新</span></div><div class="line">      (lastIsrChangeMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationBlackOut</span> &lt; now || <span class="comment">//note: 5s 内没有触发过</span></div><div class="line">        lastIsrPropagationMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationInterval</span> &lt; now)) &#123; <span class="comment">//note: 距离上次触发有60s</span></div><div class="line">      <span class="type">ReplicationUtils</span>.propagateIsrChanges(zkUtils, isrChangeSet) <span class="comment">//note: 在 zk 创建 isr 变动的提醒</span></div><div class="line">      isrChangeSet.clear() <span class="comment">//note: 清空 isrChangeSet,它记录着 isr 变动的 topic-partition 信息</span></div><div class="line">      lastIsrPropagationMs.set(now) <span class="comment">//note: 最近一次触发这个方法的时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Partition-ISR-变化"><a href="#Partition-ISR-变化" class="headerlink" title="Partition ISR 变化"></a>Partition ISR 变化</h2><p>前面讲述了 ReplicaManager 周期性调度的两个方法：<code>maybeShrinkIsr()</code> 和 <code>maybePropagateIsrChanges()</code> ，其中 <code>maybeShrinkIsr()</code> 是来检查 isr 中是否有 replica 需要从 isr 中移除，也就是说这个方法只会减少 isr 中的副本数，那么 isr 中副本数的增加是在哪里触发的呢？</p>
<p>观察上面流程图的第三部分，ReplicaManager 在处理来自 replica 的 Fetch 请求时，会将 Fetch 的相关信息到更新 Partition 中，Partition 调用 <code>maybeExpandIsr()</code> 方法来判断 isr 是否需要更新。</p>
<p>举一个例子，一个 topic 的 partition 1有三个副本，其中 replica 1 为 leader replica，那么这个副本之间关系图如下所示：</p>
<p><img src="/images/kafka/partition_replica.png" alt="Leader replica 与 follower replica"></p>
<p>简单分析一下上面的图：</p>
<ol>
<li>对于 replica 1 而言，它是 leader，首先 replica 1 有对应的 Log 实例对象，而且它会记录其他远程副本的 LEO，以便更新这个 Partition 的 HW；</li>
<li>对于 replica 2 而言，它是 follower，replica 2 有对应的 Log 实例对象，它只会有本地的 LEO 和 HW 记录，没有其他副本的 LEO 记录。</li>
<li>replica 2 和 replica 3 从 replica 1 上拉取数据，进行数据同步。</li>
</ol>
<p>再来看前面的流程图，ReplicaManager 在 <code>FetchMessages()</code> 方法对来自副本的 Fetch 请求进行处理的，实际上是会更新相应 replica 的 LEO 信息的，这时候 leader 可以根据副本 LEO 信息的变动来判断 这个副本是否满足加入 isr 的条件，下面详细来看下这个过程。</p>
<h3 id="updateFollowerLogReadResults"><a href="#updateFollowerLogReadResults" class="headerlink" title="updateFollowerLogReadResults"></a>updateFollowerLogReadResults</h3><p>在 ReplicaManager 的 <code>FetchMessages()</code> 方法中，如果 Fetch 请求是来自副本，那么会调用 <code>updateFollowerLogReadResults()</code> 更新远程副本的信息，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的作用就是找到本节点这个 Partition 对象，然后调用其 <code>updateReplicaLogReadResult()</code> 方法更新副本的 LEO 信息和拉取时间信息。</p>
<h3 id="updateReplicaLogReadResult"><a href="#updateReplicaLogReadResult" class="headerlink" title="updateReplicaLogReadResult"></a>updateReplicaLogReadResult</h3><p>这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新这个 partition replica 的 the end offset</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateReplicaLogReadResult</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  getReplica(replicaId) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(replica) =&gt;</div><div class="line">      <span class="comment">//note: 更新副本的信息</span></div><div class="line">      replica.updateLogReadResult(logReadResult)</div><div class="line">      <span class="comment">// check if we need to expand ISR to include this replica</span></div><div class="line">      <span class="comment">// if it is not in the ISR yet</span></div><div class="line">      <span class="comment">//note: 如果该副本不在 isr 中,检查是否需要进行更新</span></div><div class="line">      maybeExpandIsr(replicaId, logReadResult)</div><div class="line"></div><div class="line">      debug(<span class="string">"Recorded replica %d log end offset (LEO) position %d for partition %s."</span></div><div class="line">        .format(replicaId, logReadResult.info.fetchOffsetMetadata.messageOffset, topicPartition))</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotAssignedReplicaException</span>((<span class="string">"Leader %d failed to record follower %d's position %d since the replica"</span> +</div><div class="line">        <span class="string">" is not recognized to be one of the assigned replicas %s for partition %s."</span>)</div><div class="line">        .format(localBrokerId,</div><div class="line">                replicaId,</div><div class="line">                logReadResult.info.fetchOffsetMetadata.messageOffset,</div><div class="line">                assignedReplicas.map(_.brokerId).mkString(<span class="string">","</span>),</div><div class="line">                topicPartition))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法分为以下两步：</p>
<ol>
<li><code>updateLogReadResult()</code>：更新副本的相关信息，这里是更新该副本的 LEO、lastFetchLeaderLogEndOffset 和 lastFetchTimeMs；</li>
<li><code>maybeExpandIsr()</code>：判断 isr 是否需要扩充，即是否有不在 isr 内的副本满足进入 isr 的条件。</li>
</ol>
<h3 id="maybeExpandIsr"><a href="#maybeExpandIsr" class="headerlink" title="maybeExpandIsr"></a>maybeExpandIsr</h3><p><code>maybeExpandIsr()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查当前 Partition 是否需要扩充 ISR, 副本的 LEO 大于等于 hw 的副本将会被添加到 isr 中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeExpandIsr</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="comment">// check if this replica needs to be added to the ISR</span></div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> replica = getReplica(replicaId).get</div><div class="line">        <span class="keyword">val</span> leaderHW = leaderReplica.highWatermark</div><div class="line">        <span class="keyword">if</span>(!inSyncReplicas.contains(replica) &amp;&amp;</div><div class="line">           assignedReplicas.map(_.brokerId).contains(replicaId) &amp;&amp;</div><div class="line">           replica.logEndOffset.offsetDiff(leaderHW) &gt;= <span class="number">0</span>) &#123; <span class="comment">//note: replica LEO 大于 HW 的情况下,加入 isr 列表</span></div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas + replica</div><div class="line">          info(<span class="string">s"Expanding ISR for partition <span class="subst">$topicPartition</span> from <span class="subst">$&#123;inSyncReplicas.map(_.brokerId).mkString(",")&#125;</span> "</span> +</div><div class="line">            <span class="string">s"to <span class="subst">$&#123;newInSyncReplicas.map(_.brokerId).mkString(",")&#125;</span>"</span>)</div><div class="line">          <span class="comment">// update ISR in ZK and cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新到 zk</span></div><div class="line">          replicaManager.isrExpandRate.mark()</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// check if the HW of the partition can now be incremented</span></div><div class="line">        <span class="comment">// since the replica may already be in the ISR and its LEO has just incremented</span></div><div class="line">        <span class="comment">//note: 检查 HW 是否需要更新</span></div><div class="line">        maybeIncrementLeaderHW(leaderReplica, logReadResult.fetchTimeMs)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// nothing to do if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法会根据这个 replica 的 LEO 来判断它是否满足进入 ISR 的条件，如果满足的话，就添加到 ISR 中（前提是这个 replica 在 AR：assign replica 中，并且不在 ISR 中），之后再调用 <code>updateIsr()</code> 更新这个 topic-partition 的 isr 信息和更新 HW 信息。</p>
<h2 id="Updata-Metadata-请求的处理"><a href="#Updata-Metadata-请求的处理" class="headerlink" title="Updata-Metadata 请求的处理"></a>Updata-Metadata 请求的处理</h2><p>这里顺便讲述一下 Update-Metadata 请求的处理流程，先看下在 KafkaApis 中对 Update-Metadata 请求的处理流程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 update-metadata 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleUpdateMetadataRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> updateMetadataRequest = request.body.asInstanceOf[<span class="type">UpdateMetadataRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">val</span> updateMetadataResponse =</div><div class="line">    <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata, 并返回需要删除的 Partition</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = replicaManager.maybeUpdateMetadataCache(correlationId, updateMetadataRequest, metadataCache)</div><div class="line">      <span class="keyword">if</span> (deletedPartitions.nonEmpty)</div><div class="line">        coordinator.handleDeletedPartitions(deletedPartitions) <span class="comment">//note: GroupCoordinator 会清除相关 partition 的信息</span></div><div class="line"></div><div class="line">      <span class="keyword">if</span> (adminManager.hasDelayedTopicOperations) &#123;</div><div class="line">        updateMetadataRequest.partitionStates.keySet.asScala.map(_.topic).foreach &#123; topic =&gt;</div><div class="line">          adminManager.tryCompleteDelayedTopicOperations(topic)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, updateMetadataResponse))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个请求的处理还是调用 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法进行处理的，这个方法会先更新相关的 meta 信息，然后返回需要删除的 topic-partition 信息，GroupCoordinator 再从它的 meta 删除这个 topic-partition 的相关信息。</p>
<h3 id="maybeUpdateMetadataCache"><a href="#maybeUpdateMetadataCache" class="headerlink" title="maybeUpdateMetadataCache"></a>maybeUpdateMetadataCache</h3><p>先看下 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 向所有的 Broker 发送请求,让它们去更新各自的 meta 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeUpdateMetadataCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>, metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Seq</span>[<span class="type">TopicPartition</span>] =  &#123;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(updateMetadataRequest.controllerEpoch &lt; controllerEpoch) &#123; <span class="comment">//note: 来自过期的 controller</span></div><div class="line">      <span class="keyword">val</span> stateControllerEpochErrorMessage = (<span class="string">"Broker %d received update metadata request with correlation id %d from an "</span> +</div><div class="line">        <span class="string">"old controller %d with epoch %d. Latest known controller epoch is %d"</span>).format(localBrokerId,</div><div class="line">        correlationId, updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch,</div><div class="line">        controllerEpoch)</div><div class="line">      stateChangeLogger.warn(stateControllerEpochErrorMessage)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(stateControllerEpochErrorMessage)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata 信息,并返回需要删除的 Partition 信息</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = metadataCache.updateCache(correlationId, updateMetadataRequest)</div><div class="line">      controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      deletedPartitions</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法就是：调用 <code>metadataCache.updateCache()</code> 方法更新 meta 缓存，然后返回需要删除的 topic-partition 列表。</p>
<h3 id="updateCache"><a href="#updateCache" class="headerlink" title="updateCache"></a>updateCache</h3><p>MetadataCache 的 <code>updateCache()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新本地的 meta,并返回要删除的 topic-partition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = &#123;</div><div class="line">  inWriteLock(partitionMetadataLock) &#123;</div><div class="line">    controllerId = updateMetadataRequest.controllerId <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> id <span class="keyword">if</span> id &lt; <span class="number">0</span> =&gt; <span class="type">None</span></div><div class="line">        <span class="keyword">case</span> id =&gt; <span class="type">Some</span>(id)</div><div class="line">      &#125;</div><div class="line">    <span class="comment">//note: 清空 aliveNodes 和 aliveBrokers 记录,并更新成最新的记录</span></div><div class="line">    aliveNodes.clear()</div><div class="line">    aliveBrokers.clear()</div><div class="line">    updateMetadataRequest.liveBrokers.asScala.foreach &#123; broker =&gt;</div><div class="line">      <span class="comment">// `aliveNodes` is a hot path for metadata requests for large clusters, so we use java.util.HashMap which</span></div><div class="line">      <span class="comment">// is a bit faster than scala.collection.mutable.HashMap. When we drop support for Scala 2.10, we could</span></div><div class="line">      <span class="comment">// move to `AnyRefMap`, which has comparable performance.</span></div><div class="line">      <span class="keyword">val</span> nodes = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]</div><div class="line">      <span class="keyword">val</span> endPoints = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">EndPoint</span>]</div><div class="line">      broker.endPoints.asScala.foreach &#123; ep =&gt;</div><div class="line">        endPoints += <span class="type">EndPoint</span>(ep.host, ep.port, ep.listenerName, ep.securityProtocol)</div><div class="line">        nodes.put(ep.listenerName, <span class="keyword">new</span> <span class="type">Node</span>(broker.id, ep.host, ep.port))</div><div class="line">      &#125;</div><div class="line">      aliveBrokers(broker.id) = <span class="type">Broker</span>(broker.id, endPoints, <span class="type">Option</span>(broker.rack))</div><div class="line">      aliveNodes(broker.id) = nodes.asScala</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> deletedPartitions = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">TopicPartition</span>] <span class="comment">//note:</span></div><div class="line">    updateMetadataRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (tp, info) =&gt;</div><div class="line">      <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId</div><div class="line">      <span class="keyword">val</span> controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      <span class="keyword">if</span> (info.leader == <span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>) &#123; <span class="comment">//note: partition 被标记为了删除</span></div><div class="line">        removePartitionInfo(tp.topic, tp.partition) <span class="comment">//note: 从 cache 中删除</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> deleted partition <span class="subst">$tp</span> from metadata cache in response to UpdateMetadata "</span> +</div><div class="line">          <span class="string">s"request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">        deletedPartitions += tp</div><div class="line">      &#125; <span class="keyword">else</span> &#123;<span class="comment">//note: 更新</span></div><div class="line">        <span class="keyword">val</span> partitionInfo = partitionStateToPartitionStateInfo(info)</div><div class="line">        addOrUpdatePartitionInfo(tp.topic, tp.partition, partitionInfo) <span class="comment">//note: 更新 topic-partition meta</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> cached leader info <span class="subst">$partitionInfo</span> for partition <span class="subst">$tp</span> in response to "</span> +</div><div class="line">          <span class="string">s"UpdateMetadata request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    deletedPartitions</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它的处理流程如下：</p>
<ol>
<li>清空本节点的 aliveNodes 和 aliveBrokers 记录，并更新为最新的记录；</li>
<li>对于要删除的 topic-partition，从缓存中删除，并记录下来作为这个方法的返回；</li>
<li>对于其他的 topic-partition，执行 updateOrCreate 操作。</li>
</ol>
<p>到这里 ReplicaManager 算是讲述完了，Kafka 存储层的内容基本也介绍完了，后面会开始讲述 Kafka Controller 部分的内容，争取这部分能够在一个半月内总结完。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本同步机制实现（十四）</title>
    <link href="http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/"/>
    <id>http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/</id>
    <published>2018-04-29T10:36:52.000Z</published>
    <updated>2018-04-29T11:15:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。本篇文章我们就来看一下 Kafka 副本同步这块的内容，对于每个 broker 来说，它上面的 replica 对象，除了 leader 就是 follower，只要这台 broker 有 follower replica，broker 就会启动副本同步流程从 leader 同步数据，副本同步机制的实现是 Kafka Server 端非常重要的内容，在这篇文章中，主要会从以下几块来讲解：</p>
<ol>
<li>Kafka 在什么情况下会启动副本同步线程？</li>
<li>Kafka 副本同步线程启动流程及付副本同步流程的处理逻辑；</li>
<li>Kafka 副本同步需要解决的问题以及 Kafka 是如何解决这些问题的？</li>
<li>Kafka 在什么情况下会关闭一个副本同步线程。</li>
</ol>
<blockquote>
<p>小插曲：本来想先介绍一下与 LeaderAndIsr 请求相关的，因为副本同步线程的启动与这部分是息息相关的，但是发现涉及到了很多 controller 端的内容，而 controller 这部分还没开始涉及，所以本篇文章涉及到 LeaderAndIsr 请求的部分先简单讲述一下其处理逻辑，在 controller 这块再详细介绍。</p>
</blockquote>
<h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>Kafka Server 端的副本同步，是由 replica fetcher 线程来负责的，而它又是由 ReplicaManager 来控制的。关于 ReplicaManger，不知道大家还记不记得在 <a href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a> 有一个简单的表格，如下所示。ReplicaManager 通过对 Partition 对象的管理，来控制着 Partition 对应的 Replica 实例，而 Replica 实例又是通过 Log 对象实例来管理着其底层的存储内容。</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 的内容准备专门写一篇文章来介绍，刚好也作为对 Kafka 存储层内容的一个总结。</p>
<p>下面回到这篇文章的主题 —— 副本同步机制，在 ReplicaManager 中有一个实例变量 <code>replicaFetcherManager</code>，它负责管理所有副本同步线程，副本同步线程的启动和关闭都是由这个实例来操作的，关于副本同步相关处理逻辑，下面这张图可以作为一个整体流程，包括了 replica fetcher 线程的启动、工作流程、关闭三个部分，如下图所示：</p>
<p><img src="/images/kafka/fetcher_thread.png" alt="副本同步机制"></p>
<p>后面的讲述会围绕着这张图开始，这里看不懂或不理解也没有关系，后面会一一讲解。</p>
<h2 id="replica-fetcher-线程何时启动"><a href="#replica-fetcher-线程何时启动" class="headerlink" title="replica fetcher 线程何时启动"></a>replica fetcher 线程何时启动</h2><p>Broker 会在什么情况下启动副本同步线程呢？简单想一下这部分的逻辑：首先 broker 分配的任何一个 partition 都是以 Replica 对象实例的形式存在，而 Replica 在 Kafka 上是有两个角色： leader 和 follower，只要这个 Replica 是 follower，它便会向 leader 进行数据同步。</p>
<p>反应在 ReplicaManager 上就是如果 Broker 的本地副本被选举为 follower，那么它将会启动副本同步线程，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于给定的这些副本，将本地副本设置为 follower</span></div><div class="line"><span class="comment">//note: 1. 从 leader partition 集合移除这些 partition；</span></div><div class="line"><span class="comment">//note: 2. 将这些 partition 标记为 follower，之后这些 partition 就不会再接收 produce 的请求了；</span></div><div class="line"><span class="comment">//note: 3. 停止对这些 partition 的副本同步，这样这些副本就不会再有（来自副本请求线程）的数据进行追加了；</span></div><div class="line"><span class="comment">//note: 4. 对这些 partition 的 offset 进行 checkpoint，如果日志需要截断就进行截断操作；</span></div><div class="line"><span class="comment">//note: 5. 清空 purgatory 中的 produce 和 fetch 请求；</span></div><div class="line"><span class="comment">//note: 6. 如果 broker 没有掉线，向这些 partition 的新 leader 启动副本同步线程；</span></div><div class="line"><span class="comment">//note: 上面这些操作的顺序性，保证了这些副本在 offset checkpoint 之前将不会接收新的数据，这样的话，在 checkpoint 之前这些数据都可以保证刷到磁盘</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 删除对这些 partition 的副本同步线程</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 完成那些延迟请求的处理</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>makeFollowers()</code> 的处理过程如下：</p>
<ol>
<li>先从本地记录 leader partition 的集合中将这些 partition 移除，因为这些 partition 已经被选举为了 follower；</li>
<li>将这些 partition 的本地副本设置为 follower，后面就不会接收关于这个 partition 的 Produce 请求了，如果依然有 client 在向这台 broker 发送数据，那么它将会返回相应的错误；</li>
<li>先停止关于这些 partition 的副本同步线程（如果本地副本之前是 follower 现在还是 follower，先关闭的原因是：这个 partition 的 leader 可能发生了变化），这样的话可以保证这些 partition 的本地副本将不会再有新的数据追加；</li>
<li>对这些 partition 本地副本日志文件进行截断操作并进行 checkpoint 操作；</li>
<li>完成那些延迟处理的 Produce 和 Fetch 请求；</li>
<li>如果本地的 broker 没有掉线，那么向这些 partition 新选举出来的 leader 启动副本同步线程。</li>
</ol>
<p>关于第6步，并不一定会为每一个 partition 都启动一个 fetcher 线程，对于一个目的 broker，只会启动 <code>num.replica.fetchers</code> 个线程，具体这个 topic-partition 会分配到哪个 fetcher 线程上，是根据 topic 名和 partition id 进行计算得到，实现所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取分配到这个 topic-partition 的 fetcher 线程 id</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFetcherId</span></span>(topic: <span class="type">String</span>, partitionId: <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="type">Utils</span>.abs(<span class="number">31</span> * topic.hashCode() + partitionId) % numFetchers</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="replica-fetcher-线程参数设置"><a href="#replica-fetcher-线程参数设置" class="headerlink" title="replica fetcher 线程参数设置"></a>replica fetcher 线程参数设置</h3><p>关于副本同步线程有一些参数配置，具体如下表所示：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>num.replica.fetchers</td>
<td>从一个 broker 同步数据的 fetcher 线程数，增加这个值时也会增加该 broker 的 Io 并行度（也就是说：从一台 broker 同步数据，最多能开这么大的线程数）</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.wait.max.ms</td>
<td>对于 follower replica 而言，每个 Fetch 请求的最大等待时间，这个值应该比 <code>replica.lag.time.max.ms</code> 要小，否则对于那些吞吐量特别低的 topic 可能会导致 isr 频繁抖动</td>
<td>500</td>
</tr>
<tr>
<td>replica.high.watermark.checkpoint.interval.ms</td>
<td>hw 刷到磁盘频率</td>
<td>500</td>
</tr>
<tr>
<td>replica.lag.time.max.ms</td>
<td>如果一个 follower 在这个时间内没有发送任何 fetch 请求或者在这个时间内没有追上 leader 当前的 log end offset，那么将会从 isr 中移除</td>
<td>10000</td>
</tr>
<tr>
<td>replica.fetch.min.bytes</td>
<td>每次 fetch 请求最少拉取的数据量，如果不满足这个条件，那么要等待 replicaMaxWaitTimeMs</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.backoff.ms</td>
<td>拉取时，如果遇到错误，下次拉取等待的时间</td>
<td>1000</td>
</tr>
<tr>
<td>replica.fetch.max.bytes</td>
<td>在对每个 partition 拉取时，最大的拉取数量，这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>1048576</td>
</tr>
<tr>
<td>replica.fetch.response.max.bytes</td>
<td>对于一个 fetch 请求，返回的最大数据量（可能会涉及多个 partition），这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>10MB</td>
</tr>
</tbody>
</table>
<h2 id="replica-fetcher-线程启动"><a href="#replica-fetcher-线程启动" class="headerlink" title="replica fetcher 线程启动"></a>replica fetcher 线程启动</h2><p>如上面的图所示，在 ReplicaManager 调用 <code>makeFollowers()</code> 启动 replica fetcher 线程后，它实际上是通过 ReplicaFetcherManager 实例进行相关 topic-partition 同步线程的启动和关闭，其启动过程分为下面两步：</p>
<ol>
<li>ReplicaFetcherManager 调用 <code>addFetcherForPartitions()</code> 添加对这些 topic-partition 的数据同步流程；</li>
<li>ReplicaFetcherManager 调用 <code>createFetcherThread()</code> 初始化相应的 ReplicaFetcherThread 线程。</li>
</ol>
<h3 id="addFetcherForPartitions"><a href="#addFetcherForPartitions" class="headerlink" title="addFetcherForPartitions"></a>addFetcherForPartitions</h3><p><code>addFetcherForPartitions()</code> 的具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 为一个 topic-partition 添加 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addFetcherForPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">BrokerAndInitialOffset</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="comment">//note: 为这些 topic-partition 分配相应的 fetch 线程 id</span></div><div class="line">    <span class="keyword">val</span> partitionsPerFetcher = partitionAndOffsets.groupBy &#123; <span class="keyword">case</span>(topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">      <span class="type">BrokerAndFetcherId</span>(brokerAndInitialOffset.broker, getFetcherId(topicPartition.topic, topicPartition.partition))&#125;</div><div class="line">    <span class="keyword">for</span> ((brokerAndFetcherId, partitionAndOffsets) &lt;- partitionsPerFetcher) &#123;</div><div class="line">      <span class="comment">//note: 为 BrokerAndFetcherId 构造 fetcherThread 线程</span></div><div class="line">      <span class="keyword">var</span> fetcherThread: <span class="type">AbstractFetcherThread</span> = <span class="literal">null</span></div><div class="line">      fetcherThreadMap.get(brokerAndFetcherId) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(f) =&gt; fetcherThread = f</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">//note: 创建 fetcher 线程</span></div><div class="line">          fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)</div><div class="line">          fetcherThreadMap.put(brokerAndFetcherId, fetcherThread)</div><div class="line">          fetcherThread.start</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 添加 topic-partition 列表</span></div><div class="line">      fetcherThreadMap(brokerAndFetcherId).addPartitions(partitionAndOffsets.map &#123; <span class="keyword">case</span> (tp, brokerAndInitOffset) =&gt;</div><div class="line">        tp -&gt; brokerAndInitOffset.initOffset</div><div class="line">      &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  info(<span class="string">"Added fetcher for partitions %s"</span>.format(partitionAndOffsets.map &#123; <span class="keyword">case</span> (topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">    <span class="string">"["</span> + topicPartition + <span class="string">", initOffset "</span> + brokerAndInitialOffset.initOffset + <span class="string">" to broker "</span> + brokerAndInitialOffset.broker + <span class="string">"] "</span>&#125;))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法其实是做了下面这几件事：</p>
<ol>
<li>先计算这个 topic-partition 对应的 fetcher id；</li>
<li>根据 leader 和 fetcher id 获取对应的 replica fetcher 线程，如果没有找到，就调用 <code>createFetcherThread()</code> 创建一个新的 fetcher 线程；</li>
<li>如果是新启动的 replica fetcher 线程，那么就启动这个线程；</li>
<li>将 topic-partition 记录到 <code>fetcherThreadMap</code> 中，这个变量记录每个 replica fetcher 线程要同步的 topic-partition 列表。</li>
</ol>
<h3 id="createFetcherThread"><a href="#createFetcherThread" class="headerlink" title="createFetcherThread"></a>createFetcherThread</h3><p>ReplicaFetcherManager 创建 replica Fetcher 线程的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建 replica-fetch 线程</span></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createFetcherThread</span></span>(fetcherId: <span class="type">Int</span>, sourceBroker: <span class="type">BrokerEndPoint</span>): <span class="type">AbstractFetcherThread</span> = &#123;</div><div class="line">  <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="string">"ReplicaFetcherThread-%d-%d"</span>.format(fetcherId, sourceBroker.id)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt;</div><div class="line">      <span class="string">"%s:ReplicaFetcherThread-%d-%d"</span>.format(p, fetcherId, sourceBroker.id)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">ReplicaFetcherThread</span>(threadName, fetcherId, sourceBroker, brokerConfig,</div><div class="line">    replicaMgr, metrics, time, quotaManager) <span class="comment">//note: replica-fetch 线程</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="replica-fetcher-线程处理过程"><a href="#replica-fetcher-线程处理过程" class="headerlink" title="replica fetcher 线程处理过程"></a>replica fetcher 线程处理过程</h2><p>replica fetcher 线程在启动之后就开始进行正常数据同步流程了，在文章最开始流程图中的第二部分（线程处理过程）已经给出了大概的处理过程，这节会详细介绍一下，这个过程都是在 ReplicaFetcherThread 线程中实现的。</p>
<h3 id="doWoker"><a href="#doWoker" class="headerlink" title="doWoker"></a>doWoker</h3><p>ReplicaFetcherThread 的 <code>doWork()</code> 方法是一直在这个线程中的 <code>run()</code> 中调用的，实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  info(<span class="string">"Starting "</span>)</div><div class="line">  <span class="keyword">try</span>&#123;</div><div class="line">    <span class="keyword">while</span>(isRunning.get())&#123;</div><div class="line">      doWork()</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span>&#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span>(isRunning.get())</div><div class="line">        error(<span class="string">"Error due to "</span>, e)</div><div class="line">  &#125;</div><div class="line">  shutdownLatch.countDown()</div><div class="line">  info(<span class="string">"Stopped "</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">  <span class="comment">//note: 构造 fetch request</span></div><div class="line">  <span class="keyword">val</span> fetchRequest = inLock(partitionMapLock) &#123;</div><div class="line">    <span class="keyword">val</span> fetchRequest = buildFetchRequest(partitionStates.partitionStates.asScala.map &#123; state =&gt;</div><div class="line">      state.topicPartition -&gt; state.value</div><div class="line">    &#125;)</div><div class="line">    <span class="keyword">if</span> (fetchRequest.isEmpty) &#123; <span class="comment">//note: 如果没有活跃的 partition，在下次调用之前，sleep fetchBackOffMs 时间</span></div><div class="line">      trace(<span class="string">"There are no active partitions. Back off for %d ms before sending a fetch request"</span>.format(fetchBackOffMs))</div><div class="line">      partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    &#125;</div><div class="line">    fetchRequest</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!fetchRequest.isEmpty)</div><div class="line">    processFetchRequest(fetchRequest) <span class="comment">//note: 发送 fetch 请求，处理 fetch 的结果</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>doWork()</code> 方法中主要做了两件事：</p>
<ol>
<li>构造相应的 Fetch 请求（<code>buildFetchRequest()</code>）；</li>
<li>通过 <code>processFetchRequest()</code> 方法发送 Fetch 请求，并对其结果进行相应的处理。</li>
</ol>
<h3 id="buildFetchRequest"><a href="#buildFetchRequest" class="headerlink" title="buildFetchRequest"></a>buildFetchRequest</h3><p>通过 <code>buildFetchRequest()</code> 方法构造相应的 Fetcher 请求时，会设置 replicaId，该值会代表了这个 Fetch 请求是来自副本同步，而不是来自 consumer。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, 对于 consumer, 该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="processFetchRequest"><a href="#processFetchRequest" class="headerlink" title="processFetchRequest"></a>processFetchRequest</h3><p><code>processFetchRequest()</code> 这个方法的作用是发送 Fetch 请求，并对返回的结果进行处理，最终写入到本地副本的 Log 实例中，其具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processFetchRequest</span></span>(fetchRequest: <span class="type">REQ</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsWithError = mutable.<span class="type">Set</span>[<span class="type">TopicPartition</span>]()</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updatePartitionsWithError</span></span>(partition: <span class="type">TopicPartition</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    partitionsWithError += partition</div><div class="line">    partitionStates.moveToEnd(partition)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> responseData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PD</span>)] = <span class="type">Seq</span>.empty</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    trace(<span class="string">"Issuing to broker %d of fetch request %s"</span>.format(sourceBroker.id, fetchRequest))</div><div class="line">    responseData = fetch(fetchRequest) <span class="comment">//note: 发送 fetch 请求，获取 fetch 结果</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">        warn(<span class="string">s"Error in fetch <span class="subst">$fetchRequest</span>"</span>, t)</div><div class="line">        inLock(partitionMapLock) &#123; <span class="comment">//note: fetch 时发生错误，sleep 一会</span></div><div class="line">          partitionStates.partitionSet.asScala.foreach(updatePartitionsWithError)</div><div class="line">          <span class="comment">// there is an error occurred while fetching partitions, sleep a while</span></div><div class="line">          <span class="comment">// note that `ReplicaFetcherThread.handlePartitionsWithError` will also introduce the same delay for every</span></div><div class="line">          <span class="comment">// partition with error effectively doubling the delay. It would be good to improve this.</span></div><div class="line">          partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  fetcherStats.requestRate.mark()</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (responseData.nonEmpty) &#123; <span class="comment">//note: fetch 结果不为空</span></div><div class="line">    <span class="comment">// process fetched data</span></div><div class="line">    inLock(partitionMapLock) &#123;</div><div class="line"></div><div class="line">      responseData.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionData) =&gt;</div><div class="line">        <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">        <span class="keyword">val</span> partitionId = topicPartition.partition</div><div class="line">        <span class="type">Option</span>(partitionStates.stateValue(topicPartition)).foreach(currentPartitionFetchState =&gt;</div><div class="line">          <span class="comment">// we append to the log if the current offset is defined and it is the same as the offset requested during fetch</span></div><div class="line">          <span class="comment">//note: 如果 fetch 的 offset 与返回结果的 offset 相同，并且返回没有异常，那么就将拉取的数据追加到对应的 partition 上</span></div><div class="line">          <span class="keyword">if</span> (fetchRequest.offset(topicPartition) == currentPartitionFetchState.offset) &#123;</div><div class="line">            <span class="type">Errors</span>.forCode(partitionData.errorCode) <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NONE</span> =&gt;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line">                  <span class="keyword">val</span> newOffset = records.shallowEntries.asScala.lastOption.map(_.nextOffset).getOrElse(</div><div class="line">                    currentPartitionFetchState.offset)</div><div class="line"></div><div class="line">                  fetcherLagStats.getAndMaybePut(topic, partitionId).lag = <span class="type">Math</span>.max(<span class="number">0</span>L, partitionData.highWatermark - newOffset)</div><div class="line">                  <span class="comment">// Once we hand off the partition data to the subclass, we can't mess with it any more in this thread</span></div><div class="line">                  <span class="comment">//note: 将 fetch 的数据追加到日志文件中</span></div><div class="line">                  processPartitionData(topicPartition, currentPartitionFetchState.offset, partitionData)</div><div class="line"></div><div class="line">                  <span class="keyword">val</span> validBytes = records.validBytes</div><div class="line">                  <span class="keyword">if</span> (validBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">                    <span class="comment">// Update partitionStates only if there is no exception during processPartitionData</span></div><div class="line">                    <span class="comment">//note: 更新 fetch 的 offset 位置</span></div><div class="line">                    partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                    fetcherStats.byteRate.mark(validBytes) <span class="comment">//note: 更新 metrics</span></div><div class="line">                  &#125;</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> ime: <span class="type">CorruptRecordException</span> =&gt;</div><div class="line">                    <span class="comment">// we log the error and continue. This ensures two things</span></div><div class="line">                    <span class="comment">// 1. If there is a corrupt message in a topic partition, it does not bring the fetcher thread down and cause other topic partition to also lag</span></div><div class="line">                    <span class="comment">// 2. If the message is corrupt due to a transient state in the log (truncation, partial writes can cause this), we simply continue and</span></div><div class="line">                    <span class="comment">// should get fixed in the subsequent fetches</span></div><div class="line">                    <span class="comment">//note: CRC 验证失败时，打印日志，并继续进行（这个线程还会有其他的 tp 拉取，防止影响其他副本同步）</span></div><div class="line">                    logger.error(<span class="string">"Found invalid messages during fetch for partition ["</span> + topic + <span class="string">","</span> + partitionId + <span class="string">"] offset "</span> + currentPartitionFetchState.offset  + <span class="string">" error "</span> + ime.getMessage)</div><div class="line">                    updatePartitionsWithError(topicPartition);</div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    <span class="comment">//note: 这里还会抛出异常，是 RUNTimeException</span></div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"error processing data for partition [%s,%d] offset %d"</span></div><div class="line">                      .format(topic, partitionId, currentPartitionFetchState.offset), e)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">OFFSET_OUT_OF_RANGE</span> =&gt; <span class="comment">//note: Out-of-range 的情况处理</span></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> newOffset = handleOffsetOutOfRange(topicPartition)</div><div class="line">                  partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                  error(<span class="string">"Current offset %d for partition [%s,%d] out of range; reset offset to %d"</span></div><div class="line">                    .format(currentPartitionFetchState.offset, topic, partitionId, newOffset))</div><div class="line">                &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理 out-of-range 是抛出的异常</span></div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    error(<span class="string">"Error getting offset for partition [%s,%d] to broker %d"</span>.format(topic, partitionId, sourceBroker.id), e)</div><div class="line">                    updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> _ =&gt; <span class="comment">//note: 其他的异常情况</span></div><div class="line">                <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">                  error(<span class="string">"Error for partition [%s,%d] to broker %d:%s"</span>.format(topic, partitionId, sourceBroker.id,</div><div class="line">                    partitionData.exception.get))</div><div class="line">                  updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理拉取遇到的错误读的 tp</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithError.nonEmpty) &#123;</div><div class="line">    debug(<span class="string">"handling partitions with error for %s"</span>.format(partitionsWithError))</div><div class="line">    handlePartitionsWithErrors(partitionsWithError)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其处理过程简单总结一下：</p>
<ol>
<li>通过 <code>fetch()</code> 方法，发送 Fetch 请求，获取相应的 response（如果遇到异常，那么在下次发送 Fetch 请求之前，会 sleep 一段时间再发）；</li>
<li>如果返回的结果 不为空，并且 Fetch 请求的 offset 信息与返回结果的 offset 信息对得上，那么就会调用 <code>processPartitionData()</code> 方法将拉取到的数据追加本地副本的日志文件中，如果返回结果有错误信息，那么就对相应错误进行相应的处理；</li>
<li>对在 Fetch 过程中遇到异常或返回错误的 topic-partition，会进行 delay 操作，下次 Fetch 请求的发生至少要间隔 <code>replica.fetch.backoff.ms</code> 时间。</li>
</ol>
<h4 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h4><p><code>fetch()</code> 方法作用是发送 Fetch 请求，并返回相应的结果，其具体的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送 fetch 请求，获取拉取结果</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span></span>(fetchRequest: <span class="type">FetchRequest</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)] = &#123;</div><div class="line">  <span class="keyword">val</span> clientResponse = sendRequest(fetchRequest.underlying)</div><div class="line">  <span class="keyword">val</span> fetchResponse = clientResponse.responseBody.asInstanceOf[<span class="type">FetchResponse</span>]</div><div class="line">  fetchResponse.responseData.asScala.toSeq.map &#123; <span class="keyword">case</span> (key, value) =&gt;</div><div class="line">    key -&gt; <span class="keyword">new</span> <span class="type">PartitionData</span>(value)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 发送请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(requestBuilder: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>]): <span class="type">ClientResponse</span> = &#123;</div><div class="line">  <span class="keyword">import</span> kafka.utils.<span class="type">NetworkClientBlockingOps</span>._</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">if</span> (!networkClient.blockingReady(sourceNode, socketTimeout)(time))</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SocketTimeoutException</span>(<span class="string">s"Failed to connect within <span class="subst">$socketTimeout</span> ms"</span>)</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> clientRequest = networkClient.newClientRequest(sourceBroker.id.toString, requestBuilder,</div><div class="line">        time.milliseconds(), <span class="literal">true</span>)</div><div class="line">      networkClient.blockingSendAndReceive(clientRequest)(time) <span class="comment">//note: 阻塞直到获取返回结果</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      networkClient.close(sourceBroker.id.toString)</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processPartitionData"><a href="#processPartitionData" class="headerlink" title="processPartitionData"></a>processPartitionData</h4><p>这个方法的作用是，处理 Fetch 请求的具体数据内容，简单来说就是：检查一下数据大小是否超过限制、将数据追加到本地副本的日志文件中、更新本地副本的 hw 值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// process fetched data</span></div><div class="line"><span class="comment">//note: 处理 fetch 的数据，将 fetch 的数据追加的日志文件中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">processPartitionData</span></span>(topicPartition: <span class="type">TopicPartition</span>, fetchOffset: <span class="type">Long</span>, partitionData: <span class="type">PartitionData</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</div><div class="line">    <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line"></div><div class="line">    <span class="comment">//note: 检查 records</span></div><div class="line">    maybeWarnIfOversizedRecords(records, topicPartition)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (fetchOffset != replica.logEndOffset.messageOffset)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Offset mismatch for partition %s: fetched offset = %d, log end offset = %d."</span>.format(topicPartition, fetchOffset, replica.logEndOffset.messageOffset))</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d for partition %s. Received %d messages and leader hw %d"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))</div><div class="line">    replica.log.get.append(records, assignOffsets = <span class="literal">false</span>) <span class="comment">//note: 将 fetch 的数据追加到 log 中</span></div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d after appending %d bytes of messages for partition %s"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, records.sizeInBytes, topicPartition))</div><div class="line">    <span class="comment">//note: 更新 replica 的 hw（logEndOffset 在追加数据后也会立马进行修改)</span></div><div class="line">    <span class="keyword">val</span> followerHighWatermark = replica.logEndOffset.messageOffset.min(partitionData.highWatermark)</div><div class="line">    <span class="comment">// for the follower replica, we do not need to keep</span></div><div class="line">    <span class="comment">// its segment base offset the physical position,</span></div><div class="line">    <span class="comment">// these values will be computed upon making the leader</span></div><div class="line">    <span class="comment">//note: 这个值主要是用在 leader replica 上的</span></div><div class="line">    replica.highWatermark = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(followerHighWatermark)</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">s"Follower <span class="subst">$&#123;replica.brokerId&#125;</span> set replica high watermark for partition <span class="subst">$topicPartition</span> to <span class="subst">$followerHighWatermark</span>"</span>)</div><div class="line">    <span class="keyword">if</span> (quota.isThrottled(topicPartition))</div><div class="line">      quota.record(records.sizeInBytes)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">s"Disk error while replicating data for <span class="subst">$topicPartition</span>"</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="副本同步异常情况的处理"><a href="#副本同步异常情况的处理" class="headerlink" title="副本同步异常情况的处理"></a>副本同步异常情况的处理</h2><p>在副本同步的过程中，会遇到哪些异常情况呢？</p>
<p>大家一定会想到关于 offset 的问题，在 Kafka 中，关于 offset 的处理，无论是 producer 端、consumer 端还是其他地方，offset 似乎都是一个形影不离的问题。在副本同步时，关于 offset，会遇到什么问题呢？下面举两个异常的场景：</p>
<ol>
<li>假如当前本地（id：1）的副本现在是 leader，其 LEO 假设为1000，而另一个在 isr 中的副本（id：2）其 LEO 为800，此时出现网络抖动，id 为1 的机器掉线后又上线了，但是此时副本的 leader 实际上已经变成了 2，而2的 LEO 为800，这时候1启动副本同步线程去2上拉取数据，希望从 offset=1000 的地方开始拉取，但是2上最大的 offset 才是800，这种情况该如何处理呢？</li>
<li>假设一个 replica （id：1）其 LEO 是10，它已经掉线好几天，这个 partition leader 的 offset 范围是 [100, 800]，那么 1 重启启动时，它希望从 offset=10 的地方开始拉取数据时，这时候发生了 OutOfRange，不过跟上面不同的是这里是小于了 leader offset 的范围，这种情况又该怎么处理？</li>
</ol>
<p>以上两种情况都是 offset OutOfRange 的情况，只不过：一是 Fetch Offset 超过了 leader 的 LEO，二是 Fetch Offset 小于 leader 最小的 offset，在介绍 Kafka 解决方案之前，我们先来自己思考一下这两种情况应该怎么处理？</p>
<ol>
<li>如果 fetch offset 超过 leader 的 offset，这时候副本应该是回溯到 leader 的 LEO 位置（超过这个值的数据删除），然后再去进行副本同步，当然这种解决方案其实是无法保证 leader 与 follower 数据的完全一致，再次发生 leader 切换时，可能会导致数据的可见性不一致，但既然用户允许了脏选举的发生，其实我们是可以认为用户是可以接收这种情况发生的；</li>
<li>这种就比较容易处理，首先清空本地的数据，因为本地的数据都已经过期了，然后从 leader 的最小 offset 位置开始拉取数据。</li>
</ol>
<p>上面是我们比较容易想出的解决方案，而在 Kafka 中，其解决方案也很类似，不过遇到情况比上面我们列出的两种情况多了一些复杂，其解决方案如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">   * Unclean leader election: A follower goes down, in the meanwhile the leader keeps appending messages. The follower comes back up</div><div class="line">   * and before it has completely caught up with the leader's logs, all replicas in the ISR go down. The follower is now uncleanly</div><div class="line">   * elected as the new leader, and it starts appending messages from the client. The old leader comes back up, becomes a follower</div><div class="line">   * and it may discover that the current leader's end offset is behind its own end offset.</div><div class="line">   *</div><div class="line">   * In such a case, truncate the current follower's log to the current leader's end offset and continue fetching.</div><div class="line">   *</div><div class="line">   * There is a potential for a mismatch between the logs of the two replicas here. We don't fix this mismatch as of now.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 脏选举的发生</span></div><div class="line">  <span class="comment">//note: 获取最新的 offset</span></div><div class="line">  <span class="keyword">val</span> leaderEndOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">LATEST_TIMESTAMP</span>,</div><div class="line">    brokerConfig.brokerId)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (leaderEndOffset &lt; replica.logEndOffset.messageOffset) &#123; <span class="comment">//note: leaderEndOffset 小于 副本 LEO 的情况</span></div><div class="line">    <span class="comment">// Prior to truncating the follower's log, ensure that doing so is not disallowed by the configuration for unclean leader election.</span></div><div class="line">    <span class="comment">// This situation could only happen if the unclean election configuration for a topic changes while a replica is down. Otherwise,</span></div><div class="line">    <span class="comment">// we should never encounter this situation since a non-ISR leader cannot be elected if disallowed by the broker configuration.</span></div><div class="line">    <span class="comment">//note: 这种情况只是发生在 unclear election 的情况下</span></div><div class="line">    <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(brokerConfig.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(replicaMgr.zkUtils,</div><div class="line">      <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许 unclear elect 时,直接退出进程</span></div><div class="line">      <span class="comment">// Log a fatal error and shutdown the broker to ensure that data loss does not unexpectedly occur.</span></div><div class="line">      fatal(<span class="string">"Exiting because log truncation is not allowed for partition %s,"</span>.format(topicPartition) +</div><div class="line">        <span class="string">" Current leader %d's latest offset %d is less than replica %d's latest offset %d"</span></div><div class="line">        .format(sourceBroker.id, leaderEndOffset, brokerConfig.brokerId, replica.logEndOffset.messageOffset))</div><div class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: warn 日志信息</span></div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's latest offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderEndOffset))</div><div class="line">    <span class="comment">//note: 进行截断操作,将offset 大于等于targetOffset 的数据和索引删除</span></div><div class="line">    replicaMgr.logManager.truncateTo(<span class="type">Map</span>(topicPartition -&gt; leaderEndOffset))</div><div class="line">    leaderEndOffset</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: leader 的 LEO 大于 follower 的 LEO 的情况下,还发生了 OutOfRange</span></div><div class="line">    <span class="comment">//note: 1. follower 下线了很久,其 LEO 已经小于了 leader 的 StartOffset;</span></div><div class="line">    <span class="comment">//note: 2. 脏选举发生时, 如果 old leader 的 HW 大于 new leader 的 LEO,此时 old leader 回溯到 HW,并且这个位置开始拉取数据发生了 Out of range</span></div><div class="line">    <span class="comment">//note:    当这个方法调用时,随着 produce 持续产生数据,可能出现 leader LEO 大于 Follower LEO 的情况（不做任何处理,重试即可解决,但</span></div><div class="line">    <span class="comment">//note:    无法保证数据的一致性）。</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * If the leader's log end offset is greater than the follower's log end offset, there are two possibilities:</div><div class="line">     * 1. The follower could have been down for a long time and when it starts up, its end offset could be smaller than the leader's</div><div class="line">     * start offset because the leader has deleted old logs (log.logEndOffset &lt; leaderStartOffset).</div><div class="line">     * 2. When unclean leader election occurs, it is possible that the old leader's high watermark is greater than</div><div class="line">     * the new leader's log end offset. So when the old leader truncates its offset to its high watermark and starts</div><div class="line">     * to fetch from the new leader, an OffsetOutOfRangeException will be thrown. After that some more messages are</div><div class="line">     * produced to the new leader. While the old leader is trying to handle the OffsetOutOfRangeException and query</div><div class="line">     * the log end offset of the new leader, the new leader's log end offset becomes higher than the follower's log end offset.</div><div class="line">     *</div><div class="line">     * In the first case, the follower's current log end offset is smaller than the leader's log start offset. So the</div><div class="line">     * follower should truncate all its logs, roll out a new segment and start to fetch from the current leader's log</div><div class="line">     * start offset.</div><div class="line">     * In the second case, the follower should just keep the current log segments and retry the fetch. In the second</div><div class="line">     * case, there will be some inconsistency of data between old and new leader. We are not solving it here.</div><div class="line">     * If users want to have strong consistency guarantees, appropriate configurations needs to be set for both</div><div class="line">     * brokers and producers.</div><div class="line">     *</div><div class="line">     * Putting the two cases together, the follower should fetch from the higher one of its replica log end offset</div><div class="line">     * and the current leader's log start offset.</div><div class="line">     *</div><div class="line">     */</div><div class="line">    <span class="keyword">val</span> leaderStartOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">EARLIEST_TIMESTAMP</span>,</div><div class="line">      brokerConfig.brokerId)</div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's start offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderStartOffset))</div><div class="line">    <span class="keyword">val</span> offsetToFetch = <span class="type">Math</span>.max(leaderStartOffset, replica.logEndOffset.messageOffset)</div><div class="line">    <span class="comment">// Only truncate log when current leader's log start offset is greater than follower's log end offset.</span></div><div class="line">    <span class="keyword">if</span> (leaderStartOffset &gt; replica.logEndOffset.messageOffset) <span class="comment">//note: 如果 leader 的 startOffset 大于副本的最大 offset</span></div><div class="line">      <span class="comment">//note: 将这个 log 的数据全部清空,并且从 leaderStartOffset 开始拉取数据</span></div><div class="line">      replicaMgr.logManager.truncateFullyAndStartAt(topicPartition, leaderStartOffset)</div><div class="line">    offsetToFetch</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>针对第一种情况，在 Kafka 中，实际上还会发生这样一种情况，1 在收到 OutOfRange 错误时，这时去 leader 上获取的 LEO 值与最小的 offset 值，这时候却发现 leader 的 LEO 已经从 800 变成了 1100（这个 topic-partition 的数据量增长得比较快），再按照上面的解决方案就不太合理，Kafka 这边的解决方案是：遇到这种情况，进行重试就可以了，下次同步时就会正常了，但是依然会有上面说的那个问题。</p>
<h2 id="replica-fetcher-线程的关闭"><a href="#replica-fetcher-线程的关闭" class="headerlink" title="replica fetcher 线程的关闭"></a>replica fetcher 线程的关闭</h2><p>最后我们再来介绍一下 replica fetcher 线程在什么情况下会关闭，同样，看一下最开始那张图的第三部分，图中已经比较清晰地列出了 replica fetcher 线程关闭的条件，在三种情况下会关闭对这个 topic-partition 的拉取操作（<code>becomeLeaderOrFollower()</code> 这个方法会在对 LeaderAndIsr 请求处理的文章中讲解，这里先忽略）：</p>
<ol>
<li><code>stopReplica()</code>：broker 收到了 controller 发来的 StopReplica 请求，这时会开始关闭对指定 topic-partition 的同步线程；</li>
<li><code>makeLeaders</code>：这些 partition 的本地副本被选举成了 leader，这时候就会先停止对这些 topic-partition 副本同步线程；</li>
<li><code>makeFollowers()</code>：前面已经介绍过，这里实际上停止副本同步，然后再开启副本同步线程，因为这些 topic-partition 的 leader 可能发生了切换。</li>
</ol>
<blockquote>
<p>这里直接说线程关闭，其实不是很准确，因为每个 replica fetcher 线程操作的是多个 topic-partition，而在关闭的粒度是 partition 级别，只有这个线程分配的 partition 全部关闭后，这个线程才会真正被关闭。</p>
</blockquote>
<h3 id="关闭副本同步"><a href="#关闭副本同步" class="headerlink" title="关闭副本同步"></a>关闭副本同步</h3><p>看下 ReplicaManager 中触发 replica fetcher 线程关闭的三个方法。</p>
<h4 id="stopReplica"><a href="#stopReplica" class="headerlink" title="stopReplica"></a>stopReplica</h4><p>StopReplica 的请求实际上是 Controller 发送过来的，这个在 controller 部分会讲述，它触发的条件有多种，比如：broker 下线、partition replica 迁移等等，ReplicaManager 这里的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 tp 的 leader replica</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLeaderReplicaIfLocal</span></span>(topicPartition: <span class="type">TopicPartition</span>): <span class="type">Replica</span> =  &#123;</div><div class="line">  <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取对应的 Partiion 对象</span></div><div class="line">  partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">s"Partition <span class="subst">$topicPartition</span> doesn't exist on <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">      partition.leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt; leaderReplica <span class="comment">//note: 返回 leader 对应的副本</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">s"Leader not local for partition <span class="subst">$topicPartition</span> on broker <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h4><p><code>makeLeaders()</code> 方法的调用是在 broker 上这个 partition 的副本被设置为 leader 时触发的，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> * Make the current broker to become leader for a given set of partitions by:</div><div class="line"> *</div><div class="line"> * 1. Stop fetchers for these partitions</div><div class="line"> * 2. Update the partition metadata in cache</div><div class="line"> * 3. Add these partitions to the leader partitions set</div><div class="line"> *</div><div class="line"> * If an unexpected error is thrown in this function, it will be propagated to KafkaApis where</div><div class="line"> * the error message will be set on each partition since we do not know which partition caused it. Otherwise,</div><div class="line"> * return the set of partitions that are made leader due to this method</div><div class="line"> *</div><div class="line"> *  <span class="doctag">TODO:</span> the above may need to be fixed later</div><div class="line"> */</div><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，这个方法的过程逻辑如下：</p>
<ol>
<li>先停止对这些 partition 的副本同步流程，因为这些 partition 的本地副本已经被选举成为了 leader；</li>
<li>将这些 partition 的本地副本设置为 leader，并且开始更新相应 meta 信息（主要是记录其他 follower 副本的相关信息）；</li>
<li>将这些 partition 添加到本地记录的 leader partition 集合中。</li>
</ol>
<h4 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h4><p>这个在前面已经讲述过了，参考前面的讲述。</p>
<h3 id="removeFetcherForPartitions"><a href="#removeFetcherForPartitions" class="headerlink" title="removeFetcherForPartitions"></a>removeFetcherForPartitions</h3><p>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 删除对这些 topic-partition 的副本同步设置，这里在实现时，会遍历所有的 replica fetcher 线程，都执行 <code>removePartitions()</code> 方法来移除对应的 topic-partition 集合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除一个 partition 的 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeFetcherForPartitions</span></span>(partitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">for</span> (fetcher &lt;- fetcherThreadMap.values) <span class="comment">//note: 遍历所有的 fetchThread 去移除这个 topic-partition 集合</span></div><div class="line">      fetcher.removePartitions(partitions)</div><div class="line">  &#125;</div><div class="line">  info(<span class="string">"Removed fetcher for partitions %s"</span>.format(partitions.mkString(<span class="string">","</span>)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="removePartitions"><a href="#removePartitions" class="headerlink" title="removePartitions"></a>removePartitions</h3><p>这个方法的作用是：ReplicaFetcherThread 将这些 topic-partition 从自己要拉取的 partition 列表中移除。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removePartitions</span></span>(topicPartitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  partitionMapLock.lockInterruptibly()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    topicPartitions.foreach &#123; topicPartition =&gt;</div><div class="line">      partitionStates.remove(topicPartition)</div><div class="line">      fetcherLagStats.unregister(topicPartition.topic, topicPartition.partition)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> partitionMapLock.unlock()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ReplicaFetcherThread-的关闭"><a href="#ReplicaFetcherThread-的关闭" class="headerlink" title="ReplicaFetcherThread 的关闭"></a>ReplicaFetcherThread 的关闭</h3><p>前面介绍那么多，似乎还是没有真正去关闭，那么 ReplicaFetcherThread 真正关闭是哪里操作的呢？</p>
<p>实际上 ReplicaManager 每次处理完 LeaderAndIsr 请求后，都会调用 ReplicaFetcherManager 的 <code>shutdownIdleFetcherThreads()</code> 方法，如果 fetcher 线程要拉取的 topic-partition 集合为空，那么就会关闭掉对应的 fetcher 线程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 关闭没有拉取 topic-partition 任务的拉取线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownIdleFetcherThreads</span></span>() &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> keysToBeRemoved = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">BrokerAndFetcherId</span>]</div><div class="line">    <span class="keyword">for</span> ((key, fetcher) &lt;- fetcherThreadMap) &#123;</div><div class="line">      <span class="keyword">if</span> (fetcher.partitionCount &lt;= <span class="number">0</span>) &#123; <span class="comment">//note: 如果该线程拉取的 partition 数小于 0</span></div><div class="line">        fetcher.shutdown()</div><div class="line">        keysToBeRemoved += key</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    fetcherThreadMap --= keysToBeRemoved</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Replica Fetcher 线程这部分的内容终于讲解完了，希望能对大家有所帮助，有问题欢迎通过留言、微博或邮件进行交流。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</title>
    <link href="http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/"/>
    <id>http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/</id>
    <published>2018-04-15T15:21:16.000Z</published>
    <updated>2018-04-17T12:40:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底层的 Log 实例部分。与 Produce 请求不一样的地方是，对于 Fetch 请求，是有两种不同的来源：consumer 和 follower，consumer 读取数据与副本同步数据都是通过向 leader 发送 Fetch 请求来实现的，在对这两种不同情况处理过程中，其底层的实现是统一的，只是实现方法的参数不同而已，在本文中会详细讲述对这两种不同情况的处理。</p>
<h2 id="Fetch-请求处理的整体流程"><a href="#Fetch-请求处理的整体流程" class="headerlink" title="Fetch 请求处理的整体流程"></a>Fetch 请求处理的整体流程</h2><p>Fetch 请求（读请求）的处理与 Produce 请求（写请求）的整体流程非常类似，读和写由最上面的抽象层做入口，最终还是在存储层的 Log 对象实例进行真正的读写操作，在这一点上，Kafka 封装的非常清晰，这样的系统设计是非常值得学习的，甚至可以作为分布式系统的模范系统来学习。</p>
<p>Fetch 请求处理的整体流程如下图所示，与 Produce 请求的处理流程非常相似。</p>
<p><img src="/images/kafka/kafka_fetch_request.png" alt="Server 端处理 Fetch 请求的总体过程"></p>
<h3 id="Fetch-请求的来源"><a href="#Fetch-请求的来源" class="headerlink" title="Fetch 请求的来源"></a>Fetch 请求的来源</h3><p>那 Server 要处理的 Fetch 请求有几种类型呢？来自于哪里呢？第一个来源肯定是 Consumer，Consumer 在消费数据时会向 Server 端发送 Fetch 请求，那么是不是还没有其他的类型，对 Kafka 比较熟悉的同学大概会猜到，还有一种就是：副本同步，follower 在从 leader 同步数据时，也是发送的 Fetch 请求，下面看下这两种情况的具体实现（代码会进行简化，并不完全与源码一致，便于理解）。</p>
<h4 id="Consumer-Fetch-请求"><a href="#Consumer-Fetch-请求" class="headerlink" title="Consumer Fetch 请求"></a>Consumer Fetch 请求</h4><p>Consumer 的 Fetch 请求是在 poll 方法中调用的，Fetcher 请求的构造过程及发送如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Set-up a fetch request for any node that we have assigned partitions for which doesn't already have</div><div class="line"> * an in-flight fetch or pending fetch data.</div><div class="line"> * <span class="doctag">@return</span> number of fetches sent</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向订阅的所有 partition （只要该 leader 暂时没有拉取请求）所在 leader 发送 fetch请求</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">//note: 1 创建 Fetch Request</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; fetchRequestMap = createFetchRequests();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchRequest.Builder&gt; fetchEntry : fetchRequestMap.entrySet()) &#123;</div><div class="line">        <span class="keyword">final</span> FetchRequest.Builder request = fetchEntry.getValue();</div><div class="line">        <span class="keyword">final</span> Node fetchTarget = fetchEntry.getKey();</div><div class="line"></div><div class="line">        log.debug(<span class="string">"Sending fetch for partitions &#123;&#125; to broker &#123;&#125;"</span>, request.fetchData().keySet(), fetchTarget);</div><div class="line">        <span class="comment">//note: 2 发送 Fetch Request</span></div><div class="line">        client.send(fetchTarget, request)</div><div class="line">                .addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</div><div class="line">                        ...</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">                        ...</div><div class="line">                    &#125;</div><div class="line">                &#125;);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> fetchRequestMap.size();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Create fetch requests for all nodes for which we have assigned partitions</div><div class="line"> * that have no existing requests in flight.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 为所有 node 创建 fetch request</span></div><div class="line"><span class="keyword">private</span> Map&lt;Node, FetchRequest.Builder&gt; createFetchRequests() &#123;</div><div class="line">    <span class="comment">// create the fetch info</span></div><div class="line">    Cluster cluster = metadata.fetch();</div><div class="line">    Map&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; fetchable = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (TopicPartition partition : fetchablePartitions()) &#123;</div><div class="line">        Node node = cluster.leaderFor(partition);</div><div class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</div><div class="line">            metadata.requestUpdate();</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.client.pendingRequestCount(node) == <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">// if there is a leader and no in-flight requests, issue a new fetch</span></div><div class="line">            LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt; fetch = fetchable.get(node);</div><div class="line">            <span class="keyword">if</span> (fetch == <span class="keyword">null</span>) &#123;</div><div class="line">                fetch = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</div><div class="line">                fetchable.put(node, fetch);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">long</span> position = <span class="keyword">this</span>.subscriptions.position(partition);</div><div class="line">            <span class="comment">//note: 要 fetch 的 position 以及 fetch 的大小</span></div><div class="line">            fetch.put(partition, <span class="keyword">new</span> FetchRequest.PartitionData(position, <span class="keyword">this</span>.fetchSize));</div><div class="line">            log.trace(<span class="string">"Added fetch request for partition &#123;&#125; at offset &#123;&#125; to node &#123;&#125;"</span>, partition, position, node);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            log.trace(<span class="string">"Skipping fetch for partition &#123;&#125; because there is an in-flight request to &#123;&#125;"</span>, partition, node);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// create the fetches</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; requests = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; entry : fetchable.entrySet()) &#123;</div><div class="line">        Node node = entry.getKey();</div><div class="line">        <span class="comment">// 构造 Fetch 请求</span></div><div class="line">        FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</div><div class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></div><div class="line">        requests.put(node, fetch);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> requests;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出，Consumer 的 Fetcher 请求构造为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</div><div class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></div></pre></td></tr></table></figure>
<h4 id="Replica-同步-Fetch-请求"><a href="#Replica-同步-Fetch-请求" class="headerlink" title="Replica 同步 Fetch 请求"></a>Replica 同步 Fetch 请求</h4><p>在 Replica 同步（Replica 同步流程的讲解将会在下篇文章中详细展开）的 Fetch 请求中，其 Fetch 请求的构造如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, consumer 的该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>与 Consumer Fetch 请求进行对比，这里区别仅在于在构造 FetchRequest 时，调用了 <code>setReplicaId()</code> 方法设置了对应的 replicaId，而 Consumer 在构造时则没有进行设置，该值默认为 <code>CONSUMER_REPLICA_ID</code>，即 <strong>-1</strong>，这个值是作为 Consumer 的 Fetch 请求与 Replica 同步的 Fetch 请求的区分。</p>
<h2 id="Server-端的处理"><a href="#Server-端的处理" class="headerlink" title="Server 端的处理"></a>Server 端的处理</h2><p>这里开始真正讲解 Fetch 请求的处理过程，会按照前面图中的处理流程开始讲解，本节主要是 Server 端抽象层的内容。</p>
<h3 id="KafkaApis-如何处理-Fetch-请求"><a href="#KafkaApis-如何处理-Fetch-请求" class="headerlink" title="KafkaApis 如何处理 Fetch 请求"></a>KafkaApis 如何处理 Fetch 请求</h3><p>关于 Fetch 请求的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle a fetch request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> fetchRequest = request.body.asInstanceOf[<span class="type">FetchRequest</span>]</div><div class="line">  <span class="keyword">val</span> versionId = request.header.apiVersion</div><div class="line">  <span class="keyword">val</span> clientId = request.header.clientId</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断 tp 是否存在以及是否有 Describe 权限</span></div><div class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = fetchRequest.fetchData.asScala.toSeq.partition &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic)) &amp;&amp; metadataCache.contains(tp.topic)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断 tp 是否有 Read 权限</span></div><div class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForReadRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Read</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 不存在或没有 Describe 权限的 topic 返回 UNKNOWN_TOPIC_OR_PARTITION 错误</span></div><div class="line">  <span class="keyword">val</span> nonExistingOrUnauthorizedForDescribePartitionData = nonExistingOrUnauthorizedForDescribeTopics.map &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 没有 Read 权限的 topic 返回 TOPIC_AUTHORIZATION_FAILED 错误</span></div><div class="line">  <span class="keyword">val</span> unauthorizedForReadPartitionData = unauthorizedForReadRequestInfo.map &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// the callback for sending a fetch response</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responsePartitionData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)]) &#123;</div><div class="line">    ....</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetchResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</div><div class="line">      trace(<span class="string">s"Sending fetch response to client <span class="subst">$clientId</span> of "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;convertedPartitionData.map &#123; case (_, v) =&gt; v.records.sizeInBytes &#125;</span>.sum&#125; bytes"</span>)</div><div class="line">      <span class="keyword">val</span> fetchResponse = <span class="keyword">if</span> (delayTimeMs &gt; <span class="number">0</span>) <span class="keyword">new</span> <span class="type">FetchResponse</span>(versionId, fetchedPartitionData, delayTimeMs) <span class="keyword">else</span> response</div><div class="line">      requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, fetchResponse))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></div><div class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</div><div class="line"></div><div class="line">    <span class="comment">//note: 配额情况的处理</span></div><div class="line">    <span class="keyword">if</span> (fetchRequest.isFromFollower) &#123;</div><div class="line">      <span class="comment">// We've already evaluated against the quota and are good to go. Just need to record it now.</span></div><div class="line">      <span class="keyword">val</span> responseSize = sizeOfThrottledPartitions(versionId, fetchRequest, mergedPartitionData, quotas.leader)</div><div class="line">      quotas.leader.record(responseSize)</div><div class="line">      fetchResponseCallback(<span class="number">0</span>)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      quotas.fetch.recordAndMaybeThrottle(request.session.sanitizedUser, clientId, response.sizeOf, fetchResponseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</div><div class="line">    sendResponseCallback(<span class="type">Seq</span>.empty)</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// call the replica manager to fetch messages from the local replica</span></div><div class="line">    <span class="comment">//note: 从 replica 上拉取数据,满足条件后调用回调函数进行返回</span></div><div class="line">    replicaManager.fetchMessages(</div><div class="line">      fetchRequest.maxWait.toLong, <span class="comment">//note: 拉取请求最长的等待时间</span></div><div class="line">      fetchRequest.replicaId, <span class="comment">//note: Replica 编号，Consumer 的为 -1</span></div><div class="line">      fetchRequest.minBytes, <span class="comment">//note: 拉取请求设置的最小拉取字节</span></div><div class="line">      fetchRequest.maxBytes, <span class="comment">//note: 拉取请求设置的最大拉取字节</span></div><div class="line">      versionId &lt;= <span class="number">2</span>,</div><div class="line">      authorizedRequestInfo,</div><div class="line">      replicationQuota(fetchRequest),</div><div class="line">      sendResponseCallback)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Fetch 请求处理的真正实现是在 replicaManager 的 <code>fetchMessages()</code> 方法中，在这里，可以看出，无论是 Fetch 请求还是 Produce 请求，都是通过副本管理器来实现的，副本管理器（ReplicaManager）管理的对象是分区实例（Partition），而每个分区都会与相应的副本实例对应（Replica），在这个节点上的副本又会与唯一的 Log 实例对应，正如流程图的上半部分一样，Server 就是通过这几部分抽象概念来管理真正存储层的内容。</p>
<h3 id="ReplicaManager-如何处理-Fetch-请求"><a href="#ReplicaManager-如何处理-Fetch-请求" class="headerlink" title="ReplicaManager 如何处理 Fetch 请求"></a>ReplicaManager 如何处理 Fetch 请求</h3><p>ReplicaManger 处理 Fetch 请求的入口在 <code>fetchMessages()</code> 方法。</p>
<h4 id="fetchMessages"><a href="#fetchMessages" class="headerlink" title="fetchMessages"></a>fetchMessages</h4><p><code>fetchMessages()</code> 方法的具体如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Fetch messages from the leader replica, and wait until enough data can be fetched and return;</div><div class="line"> * the callback function will be triggered either when timeout or required fetch info is satisfied</div><div class="line"> */</div><div class="line"><span class="comment">//note: 从 leader 拉取数据,等待拉取到足够的数据或者达到 timeout 时间后返回拉取的结果</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetchMessages</span></span>(timeout: <span class="type">Long</span>,</div><div class="line">                  replicaId: <span class="type">Int</span>,</div><div class="line">                  fetchMinBytes: <span class="type">Int</span>,</div><div class="line">                  fetchMaxBytes: <span class="type">Int</span>,</div><div class="line">                  hardMaxBytesLimit: <span class="type">Boolean</span>,</div><div class="line">                  fetchInfos: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</div><div class="line">                  quota: <span class="type">ReplicaQuota</span> = <span class="type">UnboundedQuota</span>,</div><div class="line">                  responseCallback: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> isFromFollower = replicaId &gt;= <span class="number">0</span> <span class="comment">//note: 判断请求是来自 consumer （这个值为 -1）还是副本同步</span></div><div class="line">  <span class="comment">//note: 默认都是从 leader 拉取，推测这个值只是为了后续能从 follower 消费数据而设置的</span></div><div class="line">  <span class="keyword">val</span> fetchOnlyFromLeader: <span class="type">Boolean</span> = replicaId != <span class="type">Request</span>.<span class="type">DebuggingConsumerId</span></div><div class="line">  <span class="comment">//note: 如果拉取请求来自 consumer（true）,只拉取 HW 以内的数据,如果是来自 Replica 同步,则没有该限制（false）。</span></div><div class="line">  <span class="keyword">val</span> fetchOnlyCommitted: <span class="type">Boolean</span> = ! <span class="type">Request</span>.isValidBrokerId(replicaId)</div><div class="line"></div><div class="line">  <span class="comment">// read from local logs</span></div><div class="line">  <span class="comment">//note：获取本地日志</span></div><div class="line">  <span class="keyword">val</span> logReadResults = readFromLocalLog(</div><div class="line">    replicaId = replicaId,</div><div class="line">    fetchOnlyFromLeader = fetchOnlyFromLeader,</div><div class="line">    readOnlyCommitted = fetchOnlyCommitted,</div><div class="line">    fetchMaxBytes = fetchMaxBytes,</div><div class="line">    hardMaxBytesLimit = hardMaxBytesLimit,</div><div class="line">    readPartitionInfo = fetchInfos,</div><div class="line">    quota = quota)</div><div class="line"></div><div class="line">  <span class="comment">// if the fetch comes from the follower,</span></div><div class="line">  <span class="comment">// update its corresponding log end offset</span></div><div class="line">  <span class="comment">//note: 如果 fetch 来自 broker 的副本同步,那么就更新相关的 log end offset</span></div><div class="line">  <span class="keyword">if</span>(<span class="type">Request</span>.isValidBrokerId(replicaId))</div><div class="line">    updateFollowerLogReadResults(replicaId, logReadResults)</div><div class="line"></div><div class="line">  <span class="comment">// check if this fetch request can be satisfied right away</span></div><div class="line">  <span class="keyword">val</span> logReadResultValues = logReadResults.map &#123; <span class="keyword">case</span> (_, v) =&gt; v &#125;</div><div class="line">  <span class="keyword">val</span> bytesReadable = logReadResultValues.map(_.info.records.sizeInBytes).sum</div><div class="line">  <span class="keyword">val</span> errorReadingData = logReadResultValues.foldLeft(<span class="literal">false</span>) ((errorIncurred, readResult) =&gt;</div><div class="line">    errorIncurred || (readResult.error != <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line"></div><div class="line">  <span class="comment">// respond immediately if 1) fetch request does not want to wait</span></div><div class="line">  <span class="comment">//                        2) fetch request does not require any data</span></div><div class="line">  <span class="comment">//                        3) has enough data to respond</span></div><div class="line">  <span class="comment">//                        4) some error happens while reading data</span></div><div class="line">  <span class="comment">//note: 如果满足以下条件的其中一个,将会立马返回结果:</span></div><div class="line">  <span class="comment">//note: 1. timeout 达到; 2. 拉取结果为空; 3. 拉取到足够的数据; 4. 拉取是遇到 error</span></div><div class="line">  <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span> || fetchInfos.isEmpty || bytesReadable &gt;= fetchMinBytes || errorReadingData) &#123;</div><div class="line">    <span class="keyword">val</span> fetchPartitionData = logReadResults.map &#123; <span class="keyword">case</span> (tp, result) =&gt;</div><div class="line">      tp -&gt; <span class="type">FetchPartitionData</span>(result.error, result.hw, result.info.records)</div><div class="line">    &#125;</div><div class="line">    responseCallback(fetchPartitionData)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">//note： 其他情况下,延迟发送结果</span></div><div class="line">    <span class="comment">// construct the fetch results from the read results</span></div><div class="line">    <span class="keyword">val</span> fetchPartitionStatus = logReadResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</div><div class="line">      <span class="keyword">val</span> fetchInfo = fetchInfos.collectFirst &#123;</div><div class="line">        <span class="keyword">case</span> (tp, v) <span class="keyword">if</span> tp == topicPartition =&gt; v</div><div class="line">      &#125;.getOrElse(sys.error(<span class="string">s"Partition <span class="subst">$topicPartition</span> not found in fetchInfos"</span>))</div><div class="line">      (topicPartition, <span class="type">FetchPartitionStatus</span>(result.info.fetchOffsetMetadata, fetchInfo))</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> fetchMetadata = <span class="type">FetchMetadata</span>(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, fetchOnlyFromLeader,</div><div class="line">      fetchOnlyCommitted, isFromFollower, replicaId, fetchPartitionStatus)</div><div class="line">    <span class="keyword">val</span> delayedFetch = <span class="keyword">new</span> <span class="type">DelayedFetch</span>(timeout, fetchMetadata, <span class="keyword">this</span>, quota, responseCallback)</div><div class="line"></div><div class="line">    <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed fetch operation</span></div><div class="line">    <span class="keyword">val</span> delayedFetchKeys = fetchPartitionStatus.map &#123; <span class="keyword">case</span> (tp, _) =&gt; <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(tp) &#125;</div><div class="line"></div><div class="line">    <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory;</span></div><div class="line">    <span class="comment">// this is because while the delayed fetch operation is being created, new requests</span></div><div class="line">    <span class="comment">// may arrive and hence make this operation completable.</span></div><div class="line">    delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>整体来说，分为以下几步：</p>
<ol>
<li><code>readFromLocalLog()</code>：调用该方法，从本地日志拉取相应的数据；</li>
<li>判断 Fetch 请求来源，如果来自副本同步，那么更新该副本的 the end offset 记录，如果该副本不在 isr 中，并判断是否需要更新 isr；</li>
<li>返回结果，满足条件的话立马返回，否则的话，通过延迟操作，延迟返回结果。</li>
</ol>
<h4 id="readFromLocalLog"><a href="#readFromLocalLog" class="headerlink" title="readFromLocalLog"></a>readFromLocalLog</h4><p><code>readFromLocalLog()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Read from multiple topic partitions at the given offset up to maxSize bytes</div><div class="line"> */</div><div class="line"><span class="comment">//note: 按 offset 从 tp 列表中读取相应的数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readFromLocalLog</span></span>(replicaId: <span class="type">Int</span>,</div><div class="line">                     fetchOnlyFromLeader: <span class="type">Boolean</span>,</div><div class="line">                     readOnlyCommitted: <span class="type">Boolean</span>,</div><div class="line">                     fetchMaxBytes: <span class="type">Int</span>,</div><div class="line">                     hardMaxBytesLimit: <span class="type">Boolean</span>,</div><div class="line">                     readPartitionInfo: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</div><div class="line">                     quota: <span class="type">ReplicaQuota</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = &#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(tp: <span class="type">TopicPartition</span>, fetchInfo: <span class="type">PartitionData</span>, limitBytes: <span class="type">Int</span>, minOneMessage: <span class="type">Boolean</span>): <span class="type">LogReadResult</span> = &#123;</div><div class="line">    <span class="keyword">val</span> offset = fetchInfo.offset</div><div class="line">    <span class="keyword">val</span> partitionFetchSize = fetchInfo.maxBytes</div><div class="line"></div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).totalFetchRequestRate.mark()</div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalFetchRequestRate.mark()</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      trace(<span class="string">s"Fetching log segment for partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>, partition fetch size <span class="subst">$partitionFetchSize</span>, "</span> +</div><div class="line">        <span class="string">s"remaining response limit <span class="subst">$limitBytes</span>"</span> +</div><div class="line">        (<span class="keyword">if</span> (minOneMessage) <span class="string">s", ignoring response/partition size limits"</span> <span class="keyword">else</span> <span class="string">""</span>))</div><div class="line"></div><div class="line">      <span class="comment">// decide whether to only fetch from leader</span></div><div class="line">      <span class="comment">//note: 根据决定 [是否只从 leader 读取数据] 来获取相应的副本</span></div><div class="line">      <span class="comment">//note: 根据 tp 获取 Partition 对象, 在获取相应的 Replica 对象</span></div><div class="line">      <span class="keyword">val</span> localReplica = <span class="keyword">if</span> (fetchOnlyFromLeader)</div><div class="line">        getLeaderReplicaIfLocal(tp)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        getReplicaOrException(tp)</div><div class="line"></div><div class="line">      <span class="comment">// decide whether to only fetch committed data (i.e. messages below high watermark)</span></div><div class="line">      <span class="comment">//note: 获取 hw 位置，副本同步不设置这个值</span></div><div class="line">      <span class="keyword">val</span> maxOffsetOpt = <span class="keyword">if</span> (readOnlyCommitted)</div><div class="line">        <span class="type">Some</span>(localReplica.highWatermark.messageOffset)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">None</span></div><div class="line"></div><div class="line">      <span class="comment">/* Read the LogOffsetMetadata prior to performing the read from the log.</span></div><div class="line">       * We use the LogOffsetMetadata to determine if a particular replica is in-sync or not.</div><div class="line">       * Using the log end offset after performing the read can lead to a race condition</div><div class="line">       * where data gets appended to the log immediately after the replica has consumed from it</div><div class="line">       * This can cause a replica to always be out of sync.</div><div class="line">       */</div><div class="line">      <span class="keyword">val</span> initialLogEndOffset = localReplica.logEndOffset.messageOffset <span class="comment">//note: the end offset</span></div><div class="line">      <span class="keyword">val</span> initialHighWatermark = localReplica.highWatermark.messageOffset <span class="comment">//note: hw</span></div><div class="line">      <span class="keyword">val</span> fetchTimeMs = time.milliseconds</div><div class="line">      <span class="keyword">val</span> logReadInfo = localReplica.log <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</div><div class="line">          <span class="keyword">val</span> adjustedFetchSize = math.min(partitionFetchSize, limitBytes)</div><div class="line"></div><div class="line">          <span class="comment">// Try the read first, this tells us whether we need all of adjustedFetchSize for this partition</span></div><div class="line">          <span class="comment">//note: 从指定的 offset 位置开始读取数据，副本同步不需要 maxOffsetOpt</span></div><div class="line">          <span class="keyword">val</span> fetch = log.read(offset, adjustedFetchSize, maxOffsetOpt, minOneMessage)</div><div class="line"></div><div class="line">          <span class="comment">// If the partition is being throttled, simply return an empty set.</span></div><div class="line">          <span class="keyword">if</span> (shouldLeaderThrottle(quota, tp, replicaId)) <span class="comment">//note: 如果被限速了,那么返回 空 集合</span></div><div class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">          <span class="comment">// For FetchRequest version 3, we replace incomplete message sets with an empty one as consumers can make</span></div><div class="line">          <span class="comment">// progress in such cases and don't need to report a `RecordTooLargeException`</span></div><div class="line">          <span class="keyword">else</span> <span class="keyword">if</span> (!hardMaxBytesLimit &amp;&amp; fetch.firstEntryIncomplete)</div><div class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">          <span class="keyword">else</span> fetch</div><div class="line"></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          error(<span class="string">s"Leader for partition <span class="subst">$tp</span> does not have a local log"</span>)</div><div class="line">          <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 返回最后的结果,返回的都是 LogReadResult 对象</span></div><div class="line">      <span class="type">LogReadResult</span>(info = logReadInfo,</div><div class="line">                    hw = initialHighWatermark,</div><div class="line">                    leaderLogEndOffset = initialLogEndOffset,</div><div class="line">                    fetchTimeMs = fetchTimeMs,</div><div class="line">                    readSize = partitionFetchSize,</div><div class="line">                    exception = <span class="type">None</span>)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// <span class="doctag">NOTE:</span> Failed fetch requests metric is not incremented for known exceptions since it</span></div><div class="line">      <span class="comment">// is supposed to indicate un-expected failure of a broker in handling a fetch request</span></div><div class="line">      <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</div><div class="line">               _: <span class="type">NotLeaderForPartitionException</span> |</div><div class="line">               _: <span class="type">ReplicaNotAvailableException</span> |</div><div class="line">               _: <span class="type">OffsetOutOfRangeException</span>) =&gt;</div><div class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</div><div class="line">                      hw = <span class="number">-1</span>L,</div><div class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</div><div class="line">                      fetchTimeMs = <span class="number">-1</span>L,</div><div class="line">                      readSize = partitionFetchSize,</div><div class="line">                      exception = <span class="type">Some</span>(e))</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).failedFetchRequestRate.mark()</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().failedFetchRequestRate.mark()</div><div class="line">        error(<span class="string">s"Error processing fetch operation on partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>"</span>, e)</div><div class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</div><div class="line">                      hw = <span class="number">-1</span>L,</div><div class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</div><div class="line">                      fetchTimeMs = <span class="number">-1</span>L,</div><div class="line">                      readSize = partitionFetchSize,</div><div class="line">                      exception = <span class="type">Some</span>(e))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> limitBytes = fetchMaxBytes</div><div class="line">  <span class="keyword">val</span> result = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]</div><div class="line">  <span class="keyword">var</span> minOneMessage = !hardMaxBytesLimit</div><div class="line">  readPartitionInfo.foreach &#123; <span class="keyword">case</span> (tp, fetchInfo) =&gt;</div><div class="line">    <span class="keyword">val</span> readResult = read(tp, fetchInfo, limitBytes, minOneMessage) <span class="comment">//note: 读取该 tp 的数据</span></div><div class="line">    <span class="keyword">val</span> messageSetSize = readResult.info.records.sizeInBytes</div><div class="line">    <span class="comment">// Once we read from a non-empty partition, we stop ignoring request and partition level size limits</span></div><div class="line">    <span class="keyword">if</span> (messageSetSize &gt; <span class="number">0</span>)</div><div class="line">      minOneMessage = <span class="literal">false</span></div><div class="line">    limitBytes = math.max(<span class="number">0</span>, limitBytes - messageSetSize)</div><div class="line">    result += (tp -&gt; readResult)</div><div class="line">  &#125;</div><div class="line">  result</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>readFromLocalLog()</code> 方法的处理过程：</p>
<ol>
<li>先根据要拉取的 topic-partition 获取对应的 Partition 对象，根据 Partition 对象获取对应的 Replica 对象；</li>
<li>根据 Replica 对象找到对应的 Log 对象，然后调用其 <code>read()</code> 方法从指定的位置读取数据。</li>
</ol>
<h2 id="存储层对-Fetch-请求的处理"><a href="#存储层对-Fetch-请求的处理" class="headerlink" title="存储层对 Fetch 请求的处理"></a>存储层对 Fetch 请求的处理</h2><p>接着前面的流程开始往下走。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>每个 Replica 会对应一个 log 对象，而每个 log 对象会管理相应的 LogSegment 实例。</p>
<h4 id="read"><a href="#read" class="headerlink" title="read()"></a>read()</h4><p>Log 对象的 <code>read()</code> 方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 从指定 offset 开始读取数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxLength: <span class="type">Int</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>] = <span class="type">None</span>, minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</div><div class="line">  trace(<span class="string">"Reading %d bytes from offset %d in log %s of length %d bytes"</span>.format(maxLength, startOffset, name, size))</div><div class="line"></div><div class="line">  <span class="comment">// Because we don't use lock for reading, the synchronization is a little bit tricky.</span></div><div class="line">  <span class="comment">// We create the local variables to avoid race conditions with updates to the log.</span></div><div class="line">  <span class="keyword">val</span> currentNextOffsetMetadata = nextOffsetMetadata</div><div class="line">  <span class="keyword">val</span> next = currentNextOffsetMetadata.messageOffset</div><div class="line">  <span class="keyword">if</span>(startOffset == next)</div><div class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(currentNextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line"></div><div class="line">  <span class="comment">//note: 先查找对应的日志分段（segment）</span></div><div class="line">  <span class="keyword">var</span> entry = segments.floorEntry(startOffset)</div><div class="line"></div><div class="line">  <span class="comment">// attempt to read beyond the log end offset is an error</span></div><div class="line">  <span class="keyword">if</span>(startOffset &gt; next || entry == <span class="literal">null</span>)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(<span class="string">"Request for offset %d but we only have log segments in the range %d to %d."</span>.format(startOffset, segments.firstKey, next))</div><div class="line"></div><div class="line">  <span class="comment">// Do the read on the segment with a base offset less than the target offset</span></div><div class="line">  <span class="comment">// but if that segment doesn't contain any messages with an offset greater than that</span></div><div class="line">  <span class="comment">// continue to read from successive segments until we get some messages or we reach the end of the log</span></div><div class="line">  <span class="keyword">while</span>(entry != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">// If the fetch occurs on the active segment, there might be a race condition where two fetch requests occur after</span></div><div class="line">    <span class="comment">// the message is appended but before the nextOffsetMetadata is updated. In that case the second fetch may</span></div><div class="line">    <span class="comment">// cause OffsetOutOfRangeException. To solve that, we cap the reading up to exposed position instead of the log</span></div><div class="line">    <span class="comment">// end of the active segment.</span></div><div class="line">    <span class="comment">//note: 如果 Fetch 请求刚好发生在 the active segment 上,当多个 Fetch 请求同时处理,如果 nextOffsetMetadata 更新不及时,可能会导致</span></div><div class="line">    <span class="comment">//note: 发送 OffsetOutOfRangeException 异常; 为了解决这个问题, 这里能读取的最大位置是对应的物理位置（exposedPos）</span></div><div class="line">    <span class="comment">//note: 而不是 the log end of the active segment.</span></div><div class="line">    <span class="keyword">val</span> maxPosition = &#123;</div><div class="line">      <span class="keyword">if</span> (entry == segments.lastEntry) &#123;</div><div class="line">        <span class="comment">//note: nextOffsetMetadata 对应的实际物理位置</span></div><div class="line">        <span class="keyword">val</span> exposedPos = nextOffsetMetadata.relativePositionInSegment.toLong</div><div class="line">        <span class="comment">// Check the segment again in case a new segment has just rolled out.</span></div><div class="line">        <span class="keyword">if</span> (entry != segments.lastEntry) <span class="comment">//note: 可能会有新的 segment 产生,所以需要再次判断</span></div><div class="line">          <span class="comment">// New log segment has rolled out, we can read up to the file end.</span></div><div class="line">          entry.getValue.size</div><div class="line">        <span class="keyword">else</span></div><div class="line">          exposedPos</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        entry.getValue.size</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 从 segment 中读取相应的数据</span></div><div class="line">    <span class="keyword">val</span> fetchInfo = entry.getValue.read(startOffset, maxOffset, maxLength, maxPosition, minOneMessage)</div><div class="line">    <span class="keyword">if</span>(fetchInfo == <span class="literal">null</span>) &#123; <span class="comment">//note: 如果该日志分段没有读取到数据,则读取更高的日志分段</span></div><div class="line">      entry = segments.higherEntry(entry.getKey)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">return</span> fetchInfo</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// okay we are beyond the end of the last segment with no data fetched although the start offset is in range,</span></div><div class="line">  <span class="comment">// this can happen when all messages with offset larger than start offsets have been deleted.</span></div><div class="line">  <span class="comment">// In this case, we will return the empty set with log end offset metadata</span></div><div class="line">  <span class="type">FetchDataInfo</span>(nextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从实现可以看出，该方法会先查找对应的 Segment 对象（日志分段），然后循环直到读取到数据结束，如果当前的日志分段没有读取到相应的数据，那么会更新日志分段及对应的最大位置。</p>
<p>日志分段实际上是逻辑概念，它管理了物理概念的一个数据文件、一个时间索引文件和一个 offset 索引文件，读取日志分段时，会先读取 offset 索引文件再读取数据文件，具体步骤如下：</p>
<ol>
<li>根据要读取的起始偏移量（startOffset）读取 offset 索引文件中对应的物理位置；</li>
<li>查找 offset 索引文件最后返回：起始偏移量对应的最近物理位置（startPosition）；</li>
<li>根据 startPosition 直接定位到数据文件，然后读取数据文件内容；</li>
<li>最多能读到数据文件的结束位置（maxPosition）。</li>
</ol>
<h3 id="LogSegment"><a href="#LogSegment" class="headerlink" title="LogSegment"></a>LogSegment</h3><p>关乎 数据文件、offset 索引文件和时间索引文件真正的操作都是在 LogSegment 对象中的，日志读取也与这个方法息息相关。</p>
<h4 id="read-1"><a href="#read-1" class="headerlink" title="read()"></a>read()</h4><p><code>read()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 读取日志分段（副本同步不会设置 maxSize）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>], maxSize: <span class="type">Int</span>, maxPosition: <span class="type">Long</span> = size,</div><div class="line">         minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (maxSize &lt; <span class="number">0</span>)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Invalid max size for log read (%d)"</span>.format(maxSize))</div><div class="line"></div><div class="line">  <span class="comment">//note: log 文件物理长度</span></div><div class="line">  <span class="keyword">val</span> logSize = log.sizeInBytes <span class="comment">// this may change, need to save a consistent copy</span></div><div class="line">  <span class="comment">//note: 将起始的 offset 转换为起始的实际物理位置</span></div><div class="line">  <span class="keyword">val</span> startOffsetAndSize = translateOffset(startOffset)</div><div class="line"></div><div class="line">  <span class="comment">// if the start position is already off the end of the log, return null</span></div><div class="line">  <span class="keyword">if</span> (startOffsetAndSize == <span class="literal">null</span>)</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> startPosition = startOffsetAndSize.position.toInt</div><div class="line">  <span class="keyword">val</span> offsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(startOffset, <span class="keyword">this</span>.baseOffset, startPosition)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> adjustedMaxSize =</div><div class="line">    <span class="keyword">if</span> (minOneMessage) math.max(maxSize, startOffsetAndSize.size)</div><div class="line">    <span class="keyword">else</span> maxSize</div><div class="line"></div><div class="line">  <span class="comment">// return a log segment but with zero size in the case below</span></div><div class="line">  <span class="keyword">if</span> (adjustedMaxSize == <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line"></div><div class="line">  <span class="comment">// calculate the length of the message set to read based on whether or not they gave us a maxOffset</span></div><div class="line">  <span class="comment">//note: 计算读取的长度</span></div><div class="line">  <span class="keyword">val</span> length = maxOffset <span class="keyword">match</span> &#123;</div><div class="line">    <span class="comment">//note: 副本同步时的计算方式</span></div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="comment">// no max offset, just read until the max position</span></div><div class="line">      min((maxPosition - startPosition).toInt, adjustedMaxSize) <span class="comment">//note: 直接读取到最大的位置</span></div><div class="line">    <span class="comment">//note: consumer 拉取时,计算方式</span></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt;</div><div class="line">      <span class="comment">// there is a max offset, translate it to a file position and use that to calculate the max read size;</span></div><div class="line">      <span class="comment">// when the leader of a partition changes, it's possible for the new leader's high watermark to be less than the</span></div><div class="line">      <span class="comment">// true high watermark in the previous leader for a short window. In this window, if a consumer fetches on an</span></div><div class="line">      <span class="comment">// offset between new leader's high watermark and the log end offset, we want to return an empty response.</span></div><div class="line">      <span class="keyword">if</span> (offset &lt; startOffset)</div><div class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>, firstEntryIncomplete = <span class="literal">false</span>)</div><div class="line">      <span class="keyword">val</span> mapping = translateOffset(offset, startPosition)</div><div class="line">      <span class="keyword">val</span> endPosition =</div><div class="line">        <span class="keyword">if</span> (mapping == <span class="literal">null</span>)</div><div class="line">          logSize <span class="comment">// the max offset is off the end of the log, use the end of the file</span></div><div class="line">        <span class="keyword">else</span></div><div class="line">          mapping.position</div><div class="line">      min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 根据起始的物理位置和读取长度读取数据文件</span></div><div class="line">  <span class="type">FetchDataInfo</span>(offsetMetadata, log.read(startPosition, length),</div><div class="line">    firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的实现来看，上述过程分为以下三部分：</p>
<ol>
<li>根据 startOffset 得到实际的物理位置（<code>translateOffset()</code>）；</li>
<li>计算要读取的实际物理长度；</li>
<li>根据实际起始物理位置和要读取实际物理长度读取数据文件。</li>
</ol>
<h4 id="translateOffset"><a href="#translateOffset" class="headerlink" title="translateOffset()"></a>translateOffset()</h4><p><code>translateOffset()</code> 方法的实现过程主要分为两部分：</p>
<ol>
<li>查找 offset 索引文件：调用 offset 索引文件的 <code>lookup()</code> 查找方法，获取离 startOffset 最接近的物理位置；</li>
<li>调用数据文件的 <code>searchFor()</code> 方法，从指定的物理位置开始读取每条数据，知道找到对应 offset 的物理位置。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">translateOffset</span></span>(offset: <span class="type">Long</span>, startingFilePosition: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogEntryPosition</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取离 offset 最新的物理位置,返回包括 offset 和物理位置（不是准确值）</span></div><div class="line">  <span class="keyword">val</span> mapping = index.lookup(offset)</div><div class="line">  <span class="comment">//note: 从指定的位置开始消费,直到找到 offset 对应的实际物理位置,返回包括 offset 和物理位置（准确值）</span></div><div class="line">  log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="查找-offset-索引文件"><a href="#查找-offset-索引文件" class="headerlink" title="查找 offset 索引文件"></a>查找 offset 索引文件</h5><p>offset 索引文件是使用内存映射（不了解的，可以阅读 <a href="http://matt33.com/2018/02/04/linux-mmap/">操作系统之共享对象学习</a>）的方式加载到内存中的，在查询的过程中，内存映射是会发生变化，所以在 <code>lookup()</code> 中先拷贝出来了一个（idx），然后再进行查询，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 查找小于等于指定 offset 的最大 offset,并且返回对应的 offset 和实际物理位置</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(targetOffset: <span class="type">Long</span>): <span class="type">OffsetPosition</span> = &#123;</div><div class="line">  maybeLock(lock) &#123;</div><div class="line">    <span class="keyword">val</span> idx = mmap.duplicate <span class="comment">//note: 查询时,mmap 会发生变化,先复制出来一个</span></div><div class="line">    <span class="keyword">val</span> slot = indexSlotFor(idx, targetOffset, <span class="type">IndexSearchType</span>.<span class="type">KEY</span>) <span class="comment">//note: 二分查找</span></div><div class="line">    <span class="keyword">if</span>(slot == <span class="number">-1</span>)</div><div class="line">      <span class="type">OffsetPosition</span>(baseOffset, <span class="number">0</span>)</div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="comment">//note: 先计算绝对偏移量,再计算物理位置</span></div><div class="line">      parseEntry(idx, slot).asInstanceOf[<span class="type">OffsetPosition</span>]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">parseEntry</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">IndexEntry</span> = &#123;</div><div class="line">    <span class="type">OffsetPosition</span>(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">relativeOffset</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize)</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">physical</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize + <span class="number">4</span>)</div></pre></td></tr></table></figure>
<p>关于 relativeOffset 和 physical 的计算方法，可以参考下面这张图（来自《Kafka 计算内幕》）：</p>
<p><img src="/images/kafka/offset-physical.png" alt="根据索引条目编号查找偏移量的值和物理位置的值"></p>
<h5 id="搜索数据文件获取准确的物理位置"><a href="#搜索数据文件获取准确的物理位置" class="headerlink" title="搜索数据文件获取准确的物理位置"></a>搜索数据文件获取准确的物理位置</h5><p>前面通过 offset 索引文件获取的物理位置是一个接近值，下面通过实际读取数据文件将会得到一个真正的准确值，它是通过遍历数据文件实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Search forward for the file position of the last offset that is greater than or equal to the target offset</div><div class="line"> * and return its physical position and the size of the message (including log overhead) at the returned offset. If</div><div class="line"> * no such offsets are found, return null.</div><div class="line"> *</div><div class="line"> * @param targetOffset The offset to search for.</div><div class="line"> * @param startingPosition The starting position in the file to begin searching from.</div><div class="line"> */</div><div class="line">public <span class="type">LogEntryPosition</span> searchForOffsetWithSize(long targetOffset, int startingPosition) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="type">FileChannelLogEntry</span> entry : shallowEntriesFrom(startingPosition)) &#123;</div><div class="line">        long offset = entry.offset();</div><div class="line">        <span class="keyword">if</span> (offset &gt;= targetOffset)</div><div class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">LogEntryPosition</span>(offset, entry.position(), entry.sizeInBytes());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里，一个 Fetch 请求的处理过程算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</title>
    <link href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/"/>
    <id>http://matt33.com/2018/03/18/kafka-server-handle-produce-request/</id>
    <published>2018-03-18T08:32:01.000Z</published>
    <updated>2018-03-18T08:45:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer Client 的 Produce 请求为入口。但是 Server 端的内容较多，本篇文章并不能全部涵盖，涉及到其他内容，在本篇文章暂时先不详细讲述，后面会再分析，本篇文章会以 Server 处理 produce 为主线，主要详细讲解 Kafka 存储层的内容。</p>
<h2 id="produce-请求处理整体流程"><a href="#produce-请求处理整体流程" class="headerlink" title="produce 请求处理整体流程"></a>produce 请求处理整体流程</h2><p>根据在这篇 <a href="http://matt33.com/2017/06/25/kafka-producer-send-module/">Kafka 源码解析之 Producer 发送模型（一）</a> 中的讲解，在 Producer Client 端，Producer 会维护一个 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches</code> 的变量，然后会根据 topic-partition 的 leader 信息，将 leader 在同一台机器上的 batch 放在一个 request 中，发送到 server，这样可以节省很多网络开销，提高发送效率。</p>
<p>Producer Client 发送请求的方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 produce 请求</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;RecordBatch&gt; batches)</span> </span>&#123;</div><div class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</div><div class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</div><div class="line">    <span class="keyword">for</span> (RecordBatch batch : batches) &#123;</div><div class="line">        TopicPartition tp = batch.topicPartition;</div><div class="line">        produceRecordsByPartition.put(tp, batch.records());</div><div class="line">        recordsByPartition.put(tp, batch);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    ProduceRequest.Builder requestBuilder =</div><div class="line">            <span class="keyword">new</span> ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</div><div class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</div><div class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</div><div class="line">        &#125;</div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    String nodeId = Integer.toString(destination);</div><div class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, callback);</div><div class="line">    client.send(clientRequest, now);</div><div class="line">    log.trace(<span class="string">"Sent produce request to &#123;&#125;: &#123;&#125;"</span>, nodeId, requestBuilder);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在发送 Produce 的请求里，Client 是把一个 <code>Map&lt;TopicPartition, MemoryRecords&gt;</code> 类型的 <code>produceRecordsByPartition</code> 作为内容发送给了 Server 端，那么 Server 端是如何处理这个请求的呢？这就是本篇文章要讲述的内容，Server 处理这个请求的总体逻辑如下图所示：</p>
<p><img src="/images/kafka/kafka_produce_process.png" alt="Server 端处理 produce 请求的总体过程"></p>
<p>Broker 在收到 Produce 请求后，会有一个 KafkaApis 进行处理，KafkaApis 是 Server 端处理所有请求的入口，它会负责将请求的具体处理交给相应的组件进行处理，从上图可以看到 Produce 请求是交给了 ReplicaManager 对象进行处理了。</p>
<h2 id="Server-端处理"><a href="#Server-端处理" class="headerlink" title="Server 端处理"></a>Server 端处理</h2><p>Server 端的处理过程会按照上图的流程一块一块去介绍。</p>
<h3 id="KafkaApis-处理-Produce-请求"><a href="#KafkaApis-处理-Produce-请求" class="headerlink" title="KafkaApis 处理 Produce 请求"></a>KafkaApis 处理 Produce 请求</h3><p>KafkaApis 处理 produce 请求是在 <code>handleProducerRequest()</code> 方法中完成，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle a produce request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleProducerRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> produceRequest = request.body.asInstanceOf[<span class="type">ProduceRequest</span>]</div><div class="line">  <span class="keyword">val</span> numBytesAppended = request.header.sizeOf + produceRequest.sizeOf</div><div class="line"></div><div class="line">  <span class="comment">//note: 按 exist 和有 Describe 权限进行筛选</span></div><div class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = produceRequest.partitionRecords.asScala.partition &#123;</div><div class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic)) &amp;&amp; metadataCache.contains(topicPartition.topic)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断有没有 Write 权限</span></div><div class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForWriteRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</div><div class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Write</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// the callback for sending a produce response</span></div><div class="line">  <span class="comment">//note: 回调函数</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> mergedResponseStatus = responseStatus ++</div><div class="line">      unauthorizedForWriteRequestInfo.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)) ++</div><div class="line">      nonExistingOrUnauthorizedForDescribeTopics.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>))</div><div class="line"></div><div class="line">    <span class="keyword">var</span> errorInResponse = <span class="literal">false</span></div><div class="line"></div><div class="line">    mergedResponseStatus.foreach &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</div><div class="line">      <span class="keyword">if</span> (status.error != <span class="type">Errors</span>.<span class="type">NONE</span>) &#123;</div><div class="line">        errorInResponse = <span class="literal">true</span></div><div class="line">        debug(<span class="string">"Produce request with correlation id %d from client %s on partition %s failed due to %s"</span>.format(</div><div class="line">          request.header.correlationId,</div><div class="line">          request.header.clientId,</div><div class="line">          topicPartition,</div><div class="line">          status.error.exceptionName))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">produceResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</div><div class="line">      <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">// no operation needed if producer request.required.acks = 0; however, if there is any error in handling</span></div><div class="line">        <span class="comment">// the request, since no response is expected by the producer, the server will close socket server so that</span></div><div class="line">        <span class="comment">// the producer client will know that some error has happened and will refresh its metadata</span></div><div class="line">        <span class="comment">//note: 因为设置的 ack=0, 相当于 client 会默认发送成功了,如果 server 在处理的过程出现了错误,那么就会关闭 socket 连接来间接地通知 client</span></div><div class="line">        <span class="comment">//note: client 会重新刷新 meta,重新建立相应的连接</span></div><div class="line">        <span class="keyword">if</span> (errorInResponse) &#123;</div><div class="line">          <span class="keyword">val</span> exceptionsSummary = mergedResponseStatus.map &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</div><div class="line">            topicPartition -&gt; status.error.exceptionName</div><div class="line">          &#125;.mkString(<span class="string">", "</span>)</div><div class="line">          info(</div><div class="line">            <span class="string">s"Closing connection due to error during produce request with correlation id <span class="subst">$&#123;request.header.correlationId&#125;</span> "</span> +</div><div class="line">              <span class="string">s"from client id <span class="subst">$&#123;request.header.clientId&#125;</span> with ack=0\n"</span> +</div><div class="line">              <span class="string">s"Topic and partition to exceptions: <span class="subst">$exceptionsSummary</span>"</span></div><div class="line">          )</div><div class="line">          requestChannel.closeConnection(request.processor, request)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          requestChannel.noOperation(request.processor, request)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> respBody = request.header.apiVersion <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava)</div><div class="line">          <span class="keyword">case</span> version@(<span class="number">1</span> | <span class="number">2</span>) =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava, delayTimeMs, version)</div><div class="line">          <span class="comment">// This case shouldn't happen unless a new version of ProducerRequest is added without</span></div><div class="line">          <span class="comment">// updating this part of the code to handle it properly.</span></div><div class="line">          <span class="keyword">case</span> version =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Version `<span class="subst">$version</span>` of ProduceRequest is not handled. Code must be updated."</span>)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, respBody))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></div><div class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</div><div class="line"></div><div class="line">    quotas.produce.recordAndMaybeThrottle(</div><div class="line">      request.session.sanitizedUser,</div><div class="line">      request.header.clientId,</div><div class="line">      numBytesAppended,</div><div class="line">      produceResponseCallback)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</div><div class="line">    sendResponseCallback(<span class="type">Map</span>.empty)</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">val</span> internalTopicsAllowed = request.header.clientId == <span class="type">AdminUtils</span>.<span class="type">AdminClientId</span></div><div class="line"></div><div class="line">    <span class="comment">// call the replica manager to append messages to the replicas</span></div><div class="line">    <span class="comment">//note: 追加 Record</span></div><div class="line">    replicaManager.appendRecords(</div><div class="line">      produceRequest.timeout.toLong,</div><div class="line">      produceRequest.acks,</div><div class="line">      internalTopicsAllowed,</div><div class="line">      authorizedRequestInfo,</div><div class="line">      sendResponseCallback)</div><div class="line"></div><div class="line">    <span class="comment">// if the request is put into the purgatory, it will have a held reference</span></div><div class="line">    <span class="comment">// and hence cannot be garbage collected; hence we clear its data here in</span></div><div class="line">    <span class="comment">// order to let GC re-claim its memory since it is already appended to log</span></div><div class="line">    produceRequest.clearPartitionRecords()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>总体来说，处理过程是（在权限系统的情况下）：</p>
<ol>
<li>查看 topic 是否存在，以及 client 是否有相应的 Desribe 权限；</li>
<li>对于已经有 Describe 权限的 topic 查看是否有 Write 权限；</li>
<li>调用 <code>replicaManager.appendRecords()</code> 方法向有 Write 权限的 topic-partition 追加相应的 record。</li>
</ol>
<h3 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h3><p>ReplicaManager 顾名思义，它就是副本管理器，副本管理器的作用是管理这台 broker 上的所有副本（replica）。在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。</p>
<p>Kafka Server 在启动的时候，会创建 ReplicaManager 对象，如下所示。在 ReplicaManager 的构造方法中，它需要 LogManager 作为成员变量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//kafka.server.KafkaServer</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line">    <span class="comment">/* start replica manager */</span></div><div class="line">    replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)</div><div class="line">    replicaManager.startup()</div><div class="line">  &#125;<span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">    isStartingUp.set(<span class="literal">false</span>)</div><div class="line">    shutdown()</div><div class="line">    <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ReplicaManager 的<strong>并不负责具体的日志创建，它只是管理 Broker 上的所有分区</strong>（也就是图中下一步的那个 Partition 对象）。在创建 Partition 对象时，它需要 ReplicaManager 的 logManager 对象，Partition 会通过这个 logManager 对象为每个 replica 创建对应的日志。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR</div><div class="line"> */</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Partition</span>(<span class="params">val topic: <span class="type">String</span>,</span></span></div><div class="line">                val partitionId: <span class="type">Int</span>,</div><div class="line">                time: <span class="type">Time</span>,</div><div class="line">                replicaManager: <span class="type">ReplicaManager</span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partitionId)</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> localBrokerId = replicaManager.config.brokerId</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logManager = replicaManager.logManager <span class="comment">//note: 日志管理器</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ReplicaManager 与 LogManger 对比如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 后面还会介绍，这篇文章先不详细展开。</p>
<h4 id="appendRecords-实现"><a href="#appendRecords-实现" class="headerlink" title="appendRecords() 实现"></a><code>appendRecords()</code> 实现</h4><p>下面我们来看 <code>appendRecords()</code> 方法的具体实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向 partition 的 leader 写入数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecords</span></span>(timeout: <span class="type">Long</span>,</div><div class="line">                  requiredAcks: <span class="type">Short</span>,</div><div class="line">                  internalTopicsAllowed: <span class="type">Boolean</span>,</div><div class="line">                  entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</div><div class="line">                  responseCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (isValidRequiredAcks(requiredAcks)) &#123; <span class="comment">//note: acks 设置有效</span></div><div class="line">    <span class="keyword">val</span> sTime = time.milliseconds</div><div class="line">    <span class="comment">//note: 向本地的副本 log 追加数据</span></div><div class="line">    <span class="keyword">val</span> localProduceResults = appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)</div><div class="line">    debug(<span class="string">"Produce to local log in %d ms"</span>.format(time.milliseconds - sTime))</div><div class="line"></div><div class="line">    <span class="keyword">val</span> produceStatus = localProduceResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</div><div class="line">      topicPartition -&gt;</div><div class="line">              <span class="type">ProducePartitionStatus</span>(</div><div class="line">                result.info.lastOffset + <span class="number">1</span>, <span class="comment">// required offset</span></div><div class="line">                <span class="keyword">new</span> <span class="type">PartitionResponse</span>(result.error, result.info.firstOffset, result.info.logAppendTime)) <span class="comment">// response status</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</div><div class="line">      <span class="comment">//note: 处理 ack=-1 的情况,需要等到 isr 的 follower 都写入成功的话,才能返回最后结果</span></div><div class="line">      <span class="comment">// create delayed produce operation</span></div><div class="line">      <span class="keyword">val</span> produceMetadata = <span class="type">ProduceMetadata</span>(requiredAcks, produceStatus)</div><div class="line">      <span class="comment">//note: 延迟 produce 请求</span></div><div class="line">      <span class="keyword">val</span> delayedProduce = <span class="keyword">new</span> <span class="type">DelayedProduce</span>(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback)</div><div class="line"></div><div class="line">      <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed produce operation</span></div><div class="line">      <span class="keyword">val</span> producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(_)).toSeq</div><div class="line"></div><div class="line">      <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory</span></div><div class="line">      <span class="comment">// this is because while the delayed produce operation is being created, new</span></div><div class="line">      <span class="comment">// requests may arrive and hence make this operation completable.</span></div><div class="line">      delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</div><div class="line"></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we can respond immediately</span></div><div class="line">      <span class="comment">//note: 通过回调函数直接返回结果</span></div><div class="line">      <span class="keyword">val</span> produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</div><div class="line">      responseCallback(produceResponseStatus)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// If required.acks is outside accepted range, something is wrong with the client</span></div><div class="line">    <span class="comment">// Just return an error and don't handle the request at all</span></div><div class="line">    <span class="comment">//note: 返回 INVALID_REQUIRED_ACKS 错误</span></div><div class="line">    <span class="keyword">val</span> responseStatus = entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, _) =&gt;</div><div class="line">      topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">INVALID_REQUIRED_ACKS</span>,</div><div class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.firstOffset, <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span>)</div><div class="line">    &#125;</div><div class="line">    responseCallback(responseStatus)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的实现来看，<code>appendRecords()</code> 的实现主要分为以下几步：</p>
<ol>
<li>首先判断 acks 设置是否有效（-1，0，1三个值有效），无效的话直接返回异常，不再处理；</li>
<li>acks 设置有效的话，调用 <code>appendToLocalLog()</code> 方法将 records 追加到本地对应的 log 对象中；</li>
<li><code>appendToLocalLog()</code> 处理完后，如果发现 clients 设置的 acks=-1，即需要 isr 的其他的副本同步完成才能返回 response，那么就会创建一个 DelayedProduce 对象，等待 isr 的其他副本进行同步，否则的话直接返回追加的结果。</li>
</ol>
<h4 id="appendToLocalLog-的实现"><a href="#appendToLocalLog-的实现" class="headerlink" title="appendToLocalLog() 的实现"></a><code>appendToLocalLog()</code> 的实现</h4><p>追加日志的实际操作是在 <code>appendToLocalLog()</code>  中完成的，这里看下它的具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Append the messages to the local replica logs</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向本地的 replica 写入数据</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">appendToLocalLog</span></span>(internalTopicsAllowed: <span class="type">Boolean</span>,</div><div class="line">                             entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</div><div class="line">                             requiredAcks: <span class="type">Short</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">LogAppendResult</span>] = &#123;</div><div class="line">  trace(<span class="string">"Append [%s] to local log "</span>.format(entriesPerPartition))</div><div class="line">  entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, records) =&gt; <span class="comment">//note: 遍历要写的所有 topic-partition</span></div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).totalProduceRequestRate.mark()</div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalProduceRequestRate.mark()</div><div class="line"></div><div class="line">    <span class="comment">// reject appending to internal topics if it is not allowed</span></div><div class="line">    <span class="comment">//note: 不能向 kafka 内部使用的 topic 追加数据</span></div><div class="line">    <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topicPartition.topic) &amp;&amp; !internalTopicsAllowed) &#123;</div><div class="line">      (topicPartition, <span class="type">LogAppendResult</span>(</div><div class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>,</div><div class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">InvalidTopicException</span>(<span class="string">s"Cannot append to internal topic <span class="subst">$&#123;topicPartition.topic&#125;</span>"</span>))))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">//note: 查找对应的 Partition,并向分区对应的副本写入数据文件</span></div><div class="line">        <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取 topic-partition 的 Partition 对象</span></div><div class="line">        <span class="keyword">val</span> info = partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">            partition.appendRecordsToLeader(records, requiredAcks) <span class="comment">//note: 如果找到了这个对象,就开始追加日志</span></div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">"Partition %s doesn't exist on %d"</span></div><div class="line">            .format(topicPartition, localBrokerId)) <span class="comment">//note: 没有找到的话,返回异常</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 追加的 msg 数</span></div><div class="line">        <span class="keyword">val</span> numAppendedMessages =</div><div class="line">          <span class="keyword">if</span> (info.firstOffset == <span class="number">-1</span>L || info.lastOffset == <span class="number">-1</span>L)</div><div class="line">            <span class="number">0</span></div><div class="line">          <span class="keyword">else</span></div><div class="line">            info.lastOffset - info.firstOffset + <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment">// update stats for successfully appended bytes and messages as bytesInRate and messageInRate</span></div><div class="line">        <span class="comment">//note:  更新 metrics</span></div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesInRate.mark(records.sizeInBytes)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesInRate.mark(records.sizeInBytes)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).messagesInRate.mark(numAppendedMessages)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.messagesInRate.mark(numAppendedMessages)</div><div class="line"></div><div class="line">        trace(<span class="string">"%d bytes written to log %s-%d beginning at offset %d and ending at offset %d"</span></div><div class="line">          .format(records.sizeInBytes, topicPartition.topic, topicPartition.partition, info.firstOffset, info.lastOffset))</div><div class="line">        (topicPartition, <span class="type">LogAppendResult</span>(info))</div><div class="line">      &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理追加过程中出现的异常</span></div><div class="line">        <span class="comment">// <span class="doctag">NOTE:</span> Failed produce requests metric is not incremented for known exceptions</span></div><div class="line">        <span class="comment">// it is supposed to indicate un-expected failures of a broker in handling a produce request</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">          fatal(<span class="string">"Halting due to unrecoverable I/O error while handling produce request: "</span>, e)</div><div class="line">          <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">          (topicPartition, <span class="literal">null</span>)</div><div class="line">        <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</div><div class="line">                 _: <span class="type">NotLeaderForPartitionException</span> |</div><div class="line">                 _: <span class="type">RecordTooLargeException</span> |</div><div class="line">                 _: <span class="type">RecordBatchTooLargeException</span> |</div><div class="line">                 _: <span class="type">CorruptRecordException</span> |</div><div class="line">                 _: <span class="type">InvalidTimestampException</span>) =&gt;</div><div class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(e)))</div><div class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">          <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).failedProduceRequestRate.mark()</div><div class="line">          <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.failedProduceRequestRate.mark()</div><div class="line">          error(<span class="string">"Error processing append operation on partition %s"</span>.format(topicPartition), t)</div><div class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(t)))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看到 <code>appendToLocalLog()</code> 的实现如下：</p>
<ol>
<li>首先判断要写的 topic 是不是 Kafka 内置的 topic，内置的 topic 是不允许 Producer 写入的；</li>
<li>先查找 topic-partition 对应的 Partition 对象，如果在 <code>allPartitions</code> 中查找到了对应的 partition，那么直接调用 <code>partition.appendRecordsToLeader()</code> 方法追加相应的 records，否则会向 client 抛出异常。</li>
</ol>
<h3 id="Partition-appendRecordsToLeader-方法"><a href="#Partition-appendRecordsToLeader-方法" class="headerlink" title="Partition.appendRecordsToLeader() 方法"></a>Partition.appendRecordsToLeader() 方法</h3><p>ReplicaManager 在追加 records 时，调用的是 Partition 的 <code>appendRecordsToLeader()</code> 方法，其具体的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecordsToLeader</span></span>(records: <span class="type">MemoryRecords</span>, requiredAcks: <span class="type">Int</span> = <span class="number">0</span>) = &#123;</div><div class="line">  <span class="keyword">val</span> (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> log = leaderReplica.log.get <span class="comment">//note: 获取对应的 Log 对象</span></div><div class="line">        <span class="keyword">val</span> minIsr = log.config.minInSyncReplicas</div><div class="line">        <span class="keyword">val</span> inSyncSize = inSyncReplicas.size</div><div class="line"></div><div class="line">        <span class="comment">// Avoid writing to leader if there are not enough insync replicas to make it safe</span></div><div class="line">        <span class="comment">//note: 如果 ack 设置为-1, isr 数小于设置的 min.isr 时,就会向 producer 抛出相应的异常</span></div><div class="line">        <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == <span class="number">-1</span>) &#123;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotEnoughReplicasException</span>(<span class="string">"Number of insync replicas for partition %s is [%d], below required minimum [%d]"</span></div><div class="line">            .format(topicPartition, inSyncSize, minIsr))</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 向副本对应的 log 追加响应的数据</span></div><div class="line">        <span class="keyword">val</span> info = log.append(records, assignOffsets = <span class="literal">true</span>)</div><div class="line">        <span class="comment">// probably unblock some follower fetch requests since log end offset has been updated</span></div><div class="line">        replicaManager.tryCompleteDelayedFetch(<span class="type">TopicPartitionOperationKey</span>(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</div><div class="line">        <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line">        <span class="comment">//note: 判断是否需要增加 HHW（追加日志后会进行一次判断）</span></div><div class="line">        (info, maybeIncrementLeaderHW(leaderReplica))</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">//note: leader 不在本台机器上</span></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">"Leader not local for partition %s on broker %d"</span></div><div class="line">          .format(topicPartition, localBrokerId))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line"></div><div class="line">  info</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法里，会根据 topic 的 <code>min.isrs</code> 配置以及当前这个 partition 的 isr 情况判断是否可以写入，如果不满足条件，就会抛出 <code>NotEnoughReplicasException</code> 的异常，如果满足条件，就会调用 <code>log.append()</code> 向 replica 追加日志。</p>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><p>跟着最开始图中的流程及代码分析，走到这里，才算是到了 Kafka 的存储层部分，在这里会详细讲述在存储层 Kafka 如何写入日志。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>在上面有过一些介绍，每个 replica 会对应一个 log 对象，log 对象是管理当前分区的一个单位，它会包含这个分区的所有 segment 文件（包括对应的 offset 索引和时间戳索引文件），它会提供一些增删查的方法。</p>
<p>在 Log 对象的初始化时，有三个变量是比较重要的：</p>
<ol>
<li><code>nextOffsetMetadata</code>：可以叫做下一个偏移量元数据，它包括 activeSegment 的下一条消息的偏移量，该 activeSegment 的基准偏移量及日志分段的大小；</li>
<li><code>activeSegment</code>：指的是该 Log 管理的 segments 中那个最新的 segment（这里叫做活跃的 segment），一个 Log 中只会有一个活跃的 segment，其他的 segment 都已经被持久化到磁盘了；</li>
<li><code>logEndOffset</code>：表示下一条消息的 offset，它取自 <code>nextOffsetMetadata</code> 的 offset，实际上就是活动日志分段的下一个偏移量。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: nextOffsetMetadata 声明为 volatile，如果该值被修改，其他使用此变量的线程就可以立刻见到变化后的值，在生产和消费都会使用到这个值</span></div><div class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> nextOffsetMetadata: <span class="type">LogOffsetMetadata</span> = _</div><div class="line"></div><div class="line"><span class="comment">/* Calculate the offset of the next message */</span></div><div class="line"><span class="comment">//note: 下一个偏移量元数据</span></div><div class="line"><span class="comment">//note: 第一个参数：下一条消息的偏移量；第二个参数：日志分段的基准偏移量；第三个参数：日志分段大小</span></div><div class="line">nextOffsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(activeSegment.nextOffset(), activeSegment.baseOffset, activeSegment.size.toInt)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* The active segment that is currently taking appends</div><div class="line">*/</div><div class="line"><span class="comment">//note: 任何时刻，只会有一个活动的日志分段</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">activeSegment</span> </span>= segments.lastEntry.getValue</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">*  The offset of the next message that will be appended to the log</div><div class="line">*/</div><div class="line"><span class="comment">//note: 下一条消息的 offset，从 nextOffsetMetadata 中获取的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logEndOffset</span></span>: <span class="type">Long</span> = nextOffsetMetadata.messageOffset</div></pre></td></tr></table></figure>
<h4 id="日志写入"><a href="#日志写入" class="headerlink" title="日志写入"></a>日志写入</h4><p>在 Log 中一个重要的方法就是日志的写入方法，下面来看下这个方法的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Append this message set to the active segment of the log, rolling over to a fresh segment if necessary.</div><div class="line"> *</div><div class="line"> * This method will generally be responsible for assigning offsets to the messages,</div><div class="line"> * however if the assignOffsets=false flag is passed we will only check that the existing offsets are valid.</div><div class="line"> *</div><div class="line"> * @param records The log records to append</div><div class="line"> * @param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given</div><div class="line"> * @throws KafkaStorageException If the append fails due to an I/O error.</div><div class="line"> * @return Information about the appended messages including the first and last offset.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向 active segment 追加 log,必要的情况下,滚动创建新的 segment</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</div><div class="line">  <span class="keyword">val</span> appendInfo = analyzeAndValidateRecords(records) <span class="comment">//note: 返回这批消息的该要信息,并对这批 msg 进行校验</span></div><div class="line"></div><div class="line">  <span class="comment">// if we have any valid messages, append them to the log</span></div><div class="line">  <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> appendInfo</div><div class="line"></div><div class="line">  <span class="comment">// trim any invalid bytes or partial messages before appending it to the on-disk log</span></div><div class="line">  <span class="comment">//note: 删除这批消息中无效的消息</span></div><div class="line">  <span class="keyword">var</span> validRecords = trimInvalidBytes(records, appendInfo)</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// they are valid, insert them in the log</span></div><div class="line">    lock synchronized &#123;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (assignOffsets) &#123;</div><div class="line">        <span class="comment">// assign offsets to the message set</span></div><div class="line">        <span class="comment">//note: 计算这个消息集起始 offset，对 offset 的操作是一个原子操作</span></div><div class="line">        <span class="keyword">val</span> offset = <span class="keyword">new</span> <span class="type">LongRef</span>(nextOffsetMetadata.messageOffset)</div><div class="line">        appendInfo.firstOffset = offset.value <span class="comment">//note: 作为消息集的第一个 offset</span></div><div class="line">        <span class="keyword">val</span> now = time.milliseconds <span class="comment">//note: 设置的时间错以 server 收到的时间戳为准</span></div><div class="line">        <span class="comment">//note: 验证消息,并为没条 record 设置相应的 offset 和 timestrap</span></div><div class="line">        <span class="keyword">val</span> validateAndOffsetAssignResult = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(validRecords,</div><div class="line">                                                        offset,</div><div class="line">                                                        now,</div><div class="line">                                                        appendInfo.sourceCodec,</div><div class="line">                                                        appendInfo.targetCodec,</div><div class="line">                                                        config.compact,</div><div class="line">                                                        config.messageFormatVersion.messageFormatVersion,</div><div class="line">                                                        config.messageTimestampType,</div><div class="line">                                                        config.messageTimestampDifferenceMaxMs)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Error in validating messages while appending to log '%s'"</span>.format(name), e)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: 返回已经计算好 offset 和 timestrap 的 MemoryRecords</span></div><div class="line">        validRecords = validateAndOffsetAssignResult.validatedRecords</div><div class="line">        appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</div><div class="line">        appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</div><div class="line">        appendInfo.lastOffset = offset.value - <span class="number">1</span> <span class="comment">//note: 最后一条消息的 offset</span></div><div class="line">        <span class="keyword">if</span> (config.messageTimestampType == <span class="type">TimestampType</span>.<span class="type">LOG_APPEND_TIME</span>)</div><div class="line">          appendInfo.logAppendTime = now</div><div class="line"></div><div class="line">        <span class="comment">// re-validate message sizes if there's a possibility that they have changed (due to re-compression or message</span></div><div class="line">        <span class="comment">// format conversion)</span></div><div class="line">        <span class="comment">//note: 更新 metrics 的记录</span></div><div class="line">        <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) &#123;</div><div class="line">          <span class="keyword">for</span> (logEntry &lt;- validRecords.shallowEntries.asScala) &#123;</div><div class="line">            <span class="keyword">if</span> (logEntry.sizeInBytes &gt; config.maxMessageSize) &#123;</div><div class="line">              <span class="comment">// we record the original message set size instead of the trimmed size</span></div><div class="line">              <span class="comment">// to be consistent with pre-compression bytesRejectedRate recording</span></div><div class="line">              <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</div><div class="line">              <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(<span class="string">"Message size is %d bytes which exceeds the maximum configured message size of %d."</span></div><div class="line">                .format(logEntry.sizeInBytes, config.maxMessageSize))</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// we are taking the offsets we are given</span></div><div class="line">        <span class="keyword">if</span> (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &lt; nextOffsetMetadata.messageOffset)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Out of order offsets found in "</span> + records.deepEntries.asScala.map(_.offset))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// check messages set size may be exceed config.segmentSize</span></div><div class="line">      <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordBatchTooLargeException</span>(<span class="string">"Message set size is %d bytes which exceeds the maximum configured segment size of %d."</span></div><div class="line">          .format(validRecords.sizeInBytes, config.segmentSize))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// maybe roll the log if this segment is full</span></div><div class="line">      <span class="comment">//note: 如果当前 segment 满了，就需要重新新建一个 segment</span></div><div class="line">      <span class="keyword">val</span> segment = maybeRoll(messagesSize = validRecords.sizeInBytes,</div><div class="line">        maxTimestampInMessages = appendInfo.maxTimestamp,</div><div class="line">        maxOffsetInMessages = appendInfo.lastOffset)</div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// now append to the log</span></div><div class="line">      <span class="comment">//note: 追加消息到当前 segment</span></div><div class="line">      segment.append(firstOffset = appendInfo.firstOffset,</div><div class="line">        largestOffset = appendInfo.lastOffset,</div><div class="line">        largestTimestamp = appendInfo.maxTimestamp,</div><div class="line">        shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</div><div class="line">        records = validRecords)</div><div class="line"></div><div class="line">      <span class="comment">// increment the log end offset</span></div><div class="line">      <span class="comment">//note: 修改最新的 next_offset</span></div><div class="line">      updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</div><div class="line"></div><div class="line">      trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></div><div class="line">        .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)<span class="comment">//note: 满足条件的话，刷新磁盘</span></div><div class="line">        flush()</div><div class="line"></div><div class="line">      appendInfo</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaStorageException</span>(<span class="string">"I/O exception in append to log '%s'"</span>.format(name), e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Server 将每个分区的消息追加到日志中时，是以 segment 为单位的，当 segment 的大小到达阈值大小之后，会滚动新建一个日志分段（segment）保存新的消息，而分区的消息总是追加到最新的日志分段（也就是 activeSegment）中。每个日志分段都会有一个基准偏移量（segmentBaseOffset，或者叫做 baseOffset），这个基准偏移量就是分区级别的绝对偏移量，而且这个值在日志分段是固定的。有了这个基准偏移量，就可以计算出来每条消息在分区中的绝对偏移量，最后把数据以及对应的绝对偏移量写到日志文件中。<code>append()</code> 方法的过程可以总结如下：</p>
<ol>
<li><code>analyzeAndValidateRecords()</code>：对这批要写入的消息进行检测，主要是检查消息的大小及 crc 校验；</li>
<li><code>trimInvalidBytes()</code>：会将这批消息中无效的消息删除，返回一个都是有效消息的 MemoryRecords；</li>
<li><code>LogValidator.validateMessagesAndAssignOffsets()</code>：为每条消息设置相应的 offset（绝对偏移量） 和 timestrap；</li>
<li><code>maybeRoll()</code>：判断是否需要新建一个 segment 的，如果当前的 segment 放不下这批消息的话，需要新建一个 segment；</li>
<li><code>segment.append()</code>：向 segment 中添加消息；</li>
<li>更新 logEndOffset 和判断是否需要刷新磁盘（如果需要的话，调用 <code>flush()</code> 方法刷到磁盘）。</li>
</ol>
<p>关于 timestrap 的设置，这里也顺便介绍一下，在新版的 Kafka 中，每条 msg 都会有一个对应的时间戳记录，producer 端可以设置这个字段 <code>message.timestamp.type</code> 来选择 timestrap 的类型，默认是按照创建时间，只能选择从下面的选择中二选一：</p>
<ol>
<li><code>CreateTime</code>，默认值；</li>
<li><code>LogAppendTime</code>。</li>
</ol>
<h4 id="日志分段"><a href="#日志分段" class="headerlink" title="日志分段"></a>日志分段</h4><p>在 Log 的 <code>append()</code> 方法中，会调用 <code>maybeRoll()</code> 方法来判断是否需要进行相应日志分段操作，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Roll the log over to a new empty log segment if necessary.</div><div class="line"> *</div><div class="line"> * @param messagesSize The messages set size in bytes</div><div class="line"> * @param maxTimestampInMessages The maximum timestamp in the messages.</div><div class="line"> * logSegment will be rolled if one of the following conditions met</div><div class="line"> * &lt;ol&gt;</div><div class="line"> * &lt;li&gt; The logSegment is full</div><div class="line"> * &lt;li&gt; The maxTime has elapsed since the timestamp of first message in the segment (or since the create time if</div><div class="line"> * the first message does not have a timestamp)</div><div class="line"> * &lt;li&gt; The index is full</div><div class="line"> * &lt;/ol&gt;</div><div class="line"> * @return The currently active segment after (perhaps) rolling to a new segment</div><div class="line"> */</div><div class="line"><span class="comment">//note: 判断是否需要创建日志分段,如果不需要返回当前分段,需要的话,返回新创建的日志分段</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeRoll</span></span>(messagesSize: <span class="type">Int</span>, maxTimestampInMessages: <span class="type">Long</span>, maxOffsetInMessages: <span class="type">Long</span>): <span class="type">LogSegment</span> = &#123;</div><div class="line">  <span class="keyword">val</span> segment = activeSegment <span class="comment">//note: 对活跃的日志分段进行判断,它也是最新的一个日志分段</span></div><div class="line">  <span class="keyword">val</span> now = time.milliseconds</div><div class="line">  <span class="comment">//note: 距离上次日志分段的时间是否达到了设置的阈值（log.roll.hours）</span></div><div class="line">  <span class="keyword">val</span> reachedRollMs = segment.timeWaitedForRoll(now, maxTimestampInMessages) &gt; config.segmentMs - segment.rollJitterMs</div><div class="line">  <span class="comment">//note: 这是五个条件: 1. 文件满了,不足以放心这么大的 messageSet; 2. 文件有数据,并且到分段的时间阈值; 3. 索引文件满了;</span></div><div class="line">  <span class="comment">//note: 4. 时间索引文件满了; 5. 最大的 offset，其相对偏移量超过了正整数的阈值</span></div><div class="line">  <span class="keyword">if</span> (segment.size &gt; config.segmentSize - messagesSize ||</div><div class="line">      (segment.size &gt; <span class="number">0</span> &amp;&amp; reachedRollMs) ||</div><div class="line">      segment.index.isFull || segment.timeIndex.isFull || !segment.canConvertToRelativeOffset(maxOffsetInMessages)) &#123;</div><div class="line">    debug(<span class="string">s"Rolling new log segment in <span class="subst">$name</span> (log_size = <span class="subst">$&#123;segment.size&#125;</span>/<span class="subst">$&#123;config.segmentSize&#125;</span>&#125;, "</span> +</div><div class="line">        <span class="string">s"index_size = <span class="subst">$&#123;segment.index.entries&#125;</span>/<span class="subst">$&#123;segment.index.maxEntries&#125;</span>, "</span> +</div><div class="line">        <span class="string">s"time_index_size = <span class="subst">$&#123;segment.timeIndex.entries&#125;</span>/<span class="subst">$&#123;segment.timeIndex.maxEntries&#125;</span>, "</span> +</div><div class="line">        <span class="string">s"inactive_time_ms = <span class="subst">$&#123;segment.timeWaitedForRoll(now, maxTimestampInMessages)&#125;</span>/<span class="subst">$&#123;config.segmentMs - segment.rollJitterMs&#125;</span>)."</span>)</div><div class="line">    roll(maxOffsetInMessages - <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>) <span class="comment">//note: 创建新的日志分段</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    segment <span class="comment">//note: 使用当前的日志分段</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从 <code>maybeRoll()</code> 的实现可以看到，是否需要创建新的日志分段，有下面几种情况：</p>
<ol>
<li>当前日志分段的大小加上消息的大小超过了日志分段的阈值（<code>log.segment.bytes</code>）；</li>
<li>距离上次创建日志分段的时间达到了一定的阈值（<code>log.roll.hours</code>），并且数据文件有数据；</li>
<li>索引文件满了；</li>
<li>时间索引文件满了；</li>
<li>最大的 offset，其相对偏移量超过了正整数的阈值。</li>
</ol>
<p>如果上面的其中一个条件，就会创建新的 segment 文件，见 <code>roll()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Roll the log over to a new active segment starting with the current logEndOffset.</div><div class="line"> * This will trim the index to the exact size of the number of entries it currently contains.</div><div class="line"> *</div><div class="line"> * @return The newly rolled segment</div><div class="line"> */</div><div class="line"><span class="comment">//note: 滚动创建日志,并添加到日志管理的映射表中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">roll</span></span>(expectedNextOffset: <span class="type">Long</span> = <span class="number">0</span>): <span class="type">LogSegment</span> = &#123;</div><div class="line">  <span class="keyword">val</span> start = time.nanoseconds</div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> newOffset = <span class="type">Math</span>.max(expectedNextOffset, logEndOffset) <span class="comment">//note: 选择最新的 offset 作为基准偏移量</span></div><div class="line">    <span class="keyword">val</span> logFile = logFilename(dir, newOffset) <span class="comment">//note: 创建数据文件</span></div><div class="line">    <span class="keyword">val</span> indexFile = indexFilename(dir, newOffset) <span class="comment">//note: 创建 offset 索引文件</span></div><div class="line">    <span class="keyword">val</span> timeIndexFile = timeIndexFilename(dir, newOffset) <span class="comment">//note: 创建 time 索引文件</span></div><div class="line">    <span class="keyword">for</span>(file &lt;- <span class="type">List</span>(logFile, indexFile, timeIndexFile); <span class="keyword">if</span> file.exists) &#123;</div><div class="line">      warn(<span class="string">"Newly rolled segment file "</span> + file.getName + <span class="string">" already exists; deleting it first"</span>)</div><div class="line">      file.delete()</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    segments.lastEntry() <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="literal">null</span> =&gt;</div><div class="line">      <span class="keyword">case</span> entry =&gt; &#123;</div><div class="line">        <span class="keyword">val</span> seg = entry.getValue</div><div class="line">        seg.onBecomeInactiveSegment()</div><div class="line">        seg.index.trimToValidSize()</div><div class="line">        seg.timeIndex.trimToValidSize()</div><div class="line">        seg.log.trim()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 创建一个 segment 对象</span></div><div class="line">    <span class="keyword">val</span> segment = <span class="keyword">new</span> <span class="type">LogSegment</span>(dir,</div><div class="line">                                 startOffset = newOffset,</div><div class="line">                                 indexIntervalBytes = config.indexInterval,</div><div class="line">                                 maxIndexSize = config.maxIndexSize,</div><div class="line">                                 rollJitterMs = config.randomSegmentJitter,</div><div class="line">                                 time = time,</div><div class="line">                                 fileAlreadyExists = <span class="literal">false</span>,</div><div class="line">                                 initFileSize = initFileSize,</div><div class="line">                                 preallocate = config.preallocate)</div><div class="line">    <span class="keyword">val</span> prev = addSegment(segment) <span class="comment">//note: 添加到日志管理中</span></div><div class="line">    <span class="keyword">if</span>(prev != <span class="literal">null</span>)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Trying to roll a new log segment for topic partition %s with start offset %d while it already exists."</span>.format(name, newOffset))</div><div class="line">    <span class="comment">// We need to update the segment base offset and append position data of the metadata when log rolls.</span></div><div class="line">    <span class="comment">// The next offset should not change.</span></div><div class="line">    updateLogEndOffset(nextOffsetMetadata.messageOffset) <span class="comment">//note: 更新 offset</span></div><div class="line">    <span class="comment">// schedule an asynchronous flush of the old segment</span></div><div class="line">    scheduler.schedule(<span class="string">"flush-log"</span>, () =&gt; flush(newOffset), delay = <span class="number">0</span>L)</div><div class="line"></div><div class="line">    info(<span class="string">"Rolled new log segment for '"</span> + name + <span class="string">"' in %.0f ms."</span>.format((<span class="type">System</span>.nanoTime - start) / (<span class="number">1000.0</span>*<span class="number">1000.0</span>)))</div><div class="line"></div><div class="line">    segment</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>创建一个 segment 对象，真正的实现是在 Log 的 <code>roll()</code> 方法中，也就是上面的方法中，创建 segment 对象，主要包括三部分：数据文件、offset 索引文件和 time 索引文件。</p>
<h4 id="offset-索引文件"><a href="#offset-索引文件" class="headerlink" title="offset 索引文件"></a>offset 索引文件</h4><p>这里顺便讲述一下 offset 索引文件，Kafka 的索引文件有下面一个特点：</p>
<ol>
<li>采用 <strong>绝对偏移量+相对偏移量</strong> 的方式进行存储的，每个 segment 最开始绝对偏移量也是其基准偏移量；</li>
<li>数据文件每隔一定的大小创建一个索引条目，而不是每条消息会创建索引条目，通过 <code>index.interval.bytes</code> 来配置，默认是 4096，也就是4KB；</li>
</ol>
<p>这样做的好处也非常明显：</p>
<ol>
<li>因为不是每条消息都创建相应的索引条目，所以索引条目是稀疏的；</li>
<li>索引的相对偏移量占据4个字节，而绝对偏移量占据8个字节，加上物理位置的4个字节，使用相对索引可以将每条索引条目的大小从12字节减少到8个字节；</li>
<li>因为偏移量有序的，再读取数据时，可以按照二分查找的方式去快速定位偏移量的位置；</li>
<li>这样的稀疏索引是可以完全放到内存中，加快偏移量的查找。</li>
</ol>
<h3 id="LogSegment-写入"><a href="#LogSegment-写入" class="headerlink" title="LogSegment 写入"></a>LogSegment 写入</h3><p>真正的日志写入，还是在 LogSegment 的 <code>append()</code> 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line"> * Append the given messages starting with the given offset. Add</div><div class="line"> * an entry to the index if needed.</div><div class="line"> *</div><div class="line"> * It is assumed this method is being called from within a lock.</div><div class="line"> *</div><div class="line"> * @param firstOffset The first offset in the message set.</div><div class="line"> * @param largestTimestamp The largest timestamp in the message set.</div><div class="line"> * @param shallowOffsetOfMaxTimestamp The offset of the message that has the largest timestamp in the messages to append.</div><div class="line"> * @param records The log entries to append.</div><div class="line"> */</div><div class="line"> <span class="comment">//note: 在指定的 offset 处追加指定的 msgs, 需要的情况下追加相应的索引</span></div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(firstOffset: <span class="type">Long</span>, largestOffset: <span class="type">Long</span>, largestTimestamp: <span class="type">Long</span>, shallowOffsetOfMaxTimestamp: <span class="type">Long</span>, records: <span class="type">MemoryRecords</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (records.sizeInBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">    trace(<span class="string">"Inserting %d bytes at offset %d at position %d with largest timestamp %d at shallow offset %d"</span></div><div class="line">        .format(records.sizeInBytes, firstOffset, log.sizeInBytes(), largestTimestamp, shallowOffsetOfMaxTimestamp))</div><div class="line">    <span class="keyword">val</span> physicalPosition = log.sizeInBytes()</div><div class="line">    <span class="keyword">if</span> (physicalPosition == <span class="number">0</span>)</div><div class="line">      rollingBasedTimestamp = <span class="type">Some</span>(largestTimestamp)</div><div class="line">    <span class="comment">// append the messages</span></div><div class="line">    require(canConvertToRelativeOffset(largestOffset), <span class="string">"largest offset in message set can not be safely converted to relative offset."</span>)</div><div class="line">    <span class="keyword">val</span> appendedBytes = log.append(records) <span class="comment">//note: 追加到数据文件中</span></div><div class="line">    trace(<span class="string">s"Appended <span class="subst">$appendedBytes</span> to <span class="subst">$&#123;log.file()&#125;</span> at offset <span class="subst">$firstOffset</span>"</span>)</div><div class="line">    <span class="comment">// Update the in memory max timestamp and corresponding offset.</span></div><div class="line">    <span class="keyword">if</span> (largestTimestamp &gt; maxTimestampSoFar) &#123;</div><div class="line">      maxTimestampSoFar = largestTimestamp</div><div class="line">      offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// append an entry to the index (if needed)</span></div><div class="line">    <span class="comment">//note: 判断是否需要追加索引（数据每次都会添加到数据文件中,但不是每次都会添加索引的,间隔 indexIntervalBytes 大小才会写入一个索引文件）</span></div><div class="line">    <span class="keyword">if</span>(bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123;</div><div class="line">      index.append(firstOffset, physicalPosition) <span class="comment">//note: 添加索引</span></div><div class="line">      timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</div><div class="line">      bytesSinceLastIndexEntry = <span class="number">0</span> <span class="comment">//note: 重置为0</span></div><div class="line">    &#125;</div><div class="line">    bytesSinceLastIndexEntry += records.sizeInBytes</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>经过上面的分析，一个消息集（MemoryRecords）在 Kafka 存储层的调用情况如下图所示：</p>
<p><img src="/images/kafka/log_append.png" alt="MemoryRecords 追加过程"></p>
<p>最后还是利用底层的 Java NIO 实现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之日志管理（十一）</title>
    <link href="http://matt33.com/2018/03/12/kafka-log-manager/"/>
    <id>http://matt33.com/2018/03/12/kafka-log-manager/</id>
    <published>2018-03-11T16:48:13.000Z</published>
    <updated>2018-03-11T16:26:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是比较多的，会分为三篇左右的文章介绍，本篇先介绍最简单的部分，主要是日志的基本概念、日志管理、日志刷新和日志清理四部分（后两个其实也属于日志管理，为便于讲解，这里分开讲述），日志的读写和分段将在下一篇讲述。</p>
<p>本篇主要的内容如下：</p>
<ol>
<li>Kafka 中 Log 的基本概念；</li>
<li>日志管理；</li>
<li>日志刷新；</li>
<li>日志清理；</li>
</ol>
<h2 id="日志的基本概念"><a href="#日志的基本概念" class="headerlink" title="日志的基本概念"></a>日志的基本概念</h2><p>在 Kafka 的官方文档中，最开始介绍 Kafka 的一句话是：</p>
<blockquote>
<p>Kafka is a distributed, partitioned, replicated commit log service. （0.10.0 之前）</p>
<p>Apache Kafka is a distributed streaming platform. （0.10.0 及之后）</p>
</blockquote>
<p>可以说在 KafkaStream 之前，Kafka 最开始的应用场景就是日志场景或 mq 场景，更多的扮演着一个存储系统，这是 Kafka 立家之本。</p>
<p>Kafka 是一个分布式的（distributed）、可分区的（partitioned）、支持多副本（replicated）的日志提交系统，分布式这个概念很好理解，Kafka 本身就是一个分布式系统，那另外两个概念什么意思呢？</p>
<ul>
<li>可分区的：一个 topic 是可以设置多个分区的，可分区解决了单 topic 线性扩展的问题（也解决了负载均衡的问题）；</li>
<li>支持多副本的：使得 topic 可以做到更多容错性，牺牲性能与空间去换取更高的可靠性。</li>
</ul>
<p>一个 Topic 基本结果如下：</p>
<p><img src="/images/2016-03-07-KafkaMessage/topic.png" alt="Topic"></p>
<p>图中的 topic 由三个 partition 组成，topic 在创建开始，每个 partition 在写入时，其 offset 值是从0开始逐渐增加。topic 的 partition 是可以分配到 Kafka 集群的任何节点上，在实际存储时，每个 partition 是按 segment 文件去存储的（segment 的大小是在 server 端配置的，这就是日志的分段），如下图所示：</p>
<p><img src="/images/2016-03-07-KafkaMessage/segment.png" alt="Segment"></p>
<blockquote>
<p>注：上图是 0.8.2.1 版的 segment 的结构，0.10.2.0 版每个 segment 还会有一个对应的 timestrap 文件。</p>
</blockquote>
<p>再简单介绍一下 topic 的副本的概念，kafka 中为了保证一定可靠性，一般会为设置多个副本，假设一个 topic 设置了三个副本：</p>
<ul>
<li>每个 partition 都会有三个副本，这个三个副本需要分配在不同的 broker 上，在同一台 broker 上的话，就没有什么意义了；</li>
<li>这个三个副本中，会有选举出来了一个 leader，另外两个就是 follower，topic 的读写都是在 leader 上进行的，follower 从 leader 同步 partition 的数据。</li>
</ul>
<blockquote>
<p>follower 不支持读的原因，个人感觉是对于流式系统而言，如果允许 follower 也可以读的话，数据一致性、可见性将会很难保证，对最初 Kafka 的设计将会带来很大的复杂性。</p>
</blockquote>
<p>有了对 topic、partition、副本（replica）、segment、leader、follower 概念的理解之后，下面再看 Kafka 存储层的内容，就不会那么云里雾里了。 </p>
<h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><p>Kafka 的日志管理（LogManager）主要的作用是负责日志的创建、检索、清理，日志相关的读写操作实际上是由日志实例对象（Log）来处理的。</p>
<h3 id="KafkaServer-启动-LogManager-线程"><a href="#KafkaServer-启动-LogManager-线程" class="headerlink" title="KafkaServer 启动 LogManager 线程"></a>KafkaServer 启动 LogManager 线程</h3><p>LogManager 线程是在节点的 Kafka 服务启动时启动的，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//kafka.server.KafkaServer</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line">    <span class="comment">/* start log manager */</span></div><div class="line">    <span class="comment">//note: 启动日志管理线程</span></div><div class="line">    logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">    logManager.startup()</div><div class="line">    &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">    isStartingUp.set(<span class="literal">false</span>)</div><div class="line">    shutdown()</div><div class="line">    <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createLogManager</span></span>(zkClient: <span class="type">ZkClient</span>, brokerState: <span class="type">BrokerState</span>): <span class="type">LogManager</span> = &#123;</div><div class="line">  <span class="keyword">val</span> defaultProps = <span class="type">KafkaServer</span>.copyKafkaConfigToLog(config)</div><div class="line">  <span class="keyword">val</span> defaultLogConfig = <span class="type">LogConfig</span>(defaultProps)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchAllTopicConfigs(zkUtils).map &#123; <span class="keyword">case</span> (topic, configs) =&gt;</div><div class="line">    topic -&gt; <span class="type">LogConfig</span>.fromProps(defaultProps, configs)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// read the log configurations from zookeeper</span></div><div class="line">  <span class="keyword">val</span> cleanerConfig = <span class="type">CleanerConfig</span>(numThreads = config.logCleanerThreads, <span class="comment">//note: 日志清理线程数,默认是1</span></div><div class="line">                                    dedupeBufferSize = config.logCleanerDedupeBufferSize, <span class="comment">//note: 日志清理使用的总内容,默认128MB</span></div><div class="line">                                    dedupeBufferLoadFactor = config.logCleanerDedupeBufferLoadFactor, <span class="comment">//note:  buffer load factor</span></div><div class="line">                                    ioBufferSize = config.logCleanerIoBufferSize, <span class="comment">//note:</span></div><div class="line">                                    maxMessageSize = config.messageMaxBytes, <span class="comment">//note:</span></div><div class="line">                                    maxIoBytesPerSecond = config.logCleanerIoMaxBytesPerSecond, <span class="comment">//note:</span></div><div class="line">                                    backOffMs = config.logCleanerBackoffMs, <span class="comment">//note: 没有日志清理时的 sleep 时间,默认 15s</span></div><div class="line">                                    enableCleaner = config.logCleanerEnable) <span class="comment">//note: 是否允许对 compact 日志进行清理</span></div><div class="line">  <span class="keyword">new</span> <span class="type">LogManager</span>(logDirs = config.logDirs.map(<span class="keyword">new</span> <span class="type">File</span>(_)).toArray, <span class="comment">//note: 日志目录列表</span></div><div class="line">                 topicConfigs = configs,</div><div class="line">                 defaultConfig = defaultLogConfig,</div><div class="line">                 cleanerConfig = cleanerConfig,</div><div class="line">                 ioThreads = config.numRecoveryThreadsPerDataDir,<span class="comment">//note: 每个日志目录在开始时用日志恢复以及关闭时日志flush的线程数,默认1</span></div><div class="line">                 flushCheckMs = config.logFlushSchedulerIntervalMs,</div><div class="line">                 flushCheckpointMs = config.logFlushOffsetCheckpointIntervalMs, <span class="comment">//note: 更新 check-point 的频率,默认是60s</span></div><div class="line">                 retentionCheckMs = config.logCleanupIntervalMs, <span class="comment">//note: log-cleaner 检查 topic 是否需要删除的频率,默认是5min</span></div><div class="line">                 scheduler = kafkaScheduler,</div><div class="line">                 brokerState = brokerState,</div><div class="line">                 time = time)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LogManager-初始化"><a href="#LogManager-初始化" class="headerlink" title="LogManager 初始化"></a>LogManager 初始化</h3><p>LogManager 在初始化时，首先会检查 server 端配置的日志目录信息，然后会加载日志目录下的所有分区日志，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogManager</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="comment">//note: 检查点表示日志已经刷新到磁盘的位置，主要是用于数据恢复</span></div><div class="line">  <span class="keyword">val</span> <span class="type">RecoveryPointCheckpointFile</span> = <span class="string">"recovery-point-offset-checkpoint"</span> <span class="comment">//note: 检查点文件</span></div><div class="line">  </div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logs = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">TopicPartition</span>, <span class="type">Log</span>]() <span class="comment">//note: 分区与日志实例的对应关系</span></div><div class="line"></div><div class="line">  createAndValidateLogDirs(logDirs) <span class="comment">//note: 检查日志目录</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dirLocks = lockLogDirs(logDirs)</div><div class="line">  <span class="comment">//note: 每个数据目录都有一个检查点文件,存储这个数据目录下所有分区的检查点信息</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> recoveryPointCheckpoints = logDirs.map(dir =&gt; (dir, <span class="keyword">new</span> <span class="type">OffsetCheckpoint</span>(<span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">RecoveryPointCheckpointFile</span>)))).toMap</div><div class="line">  loadLogs()</div><div class="line">  </div><div class="line">  <span class="comment">//note: 创建指定的数据目录,并做相应的检查:</span></div><div class="line">  <span class="comment">//note: 1.确保数据目录中没有重复的数据目录;</span></div><div class="line">  <span class="comment">//note: 2.数据不存在的话就创建相应的目录;</span></div><div class="line">  <span class="comment">//note: 3.检查每个目录路径是否是可读的。</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAndValidateLogDirs</span></span>(dirs: <span class="type">Seq</span>[<span class="type">File</span>]) &#123;</div><div class="line">    <span class="keyword">if</span>(dirs.map(_.getCanonicalPath).toSet.size &lt; dirs.size)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Duplicate log directory found: "</span> + logDirs.mkString(<span class="string">", "</span>))</div><div class="line">    <span class="keyword">for</span>(dir &lt;- dirs) &#123;</div><div class="line">      <span class="keyword">if</span>(!dir.exists) &#123;</div><div class="line">        info(<span class="string">"Log directory '"</span> + dir.getAbsolutePath + <span class="string">"' not found, creating it."</span>)</div><div class="line">        <span class="keyword">val</span> created = dir.mkdirs()</div><div class="line">        <span class="keyword">if</span>(!created)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Failed to create data directory "</span> + dir.getAbsolutePath)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span>(!dir.isDirectory || !dir.canRead)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(dir.getAbsolutePath + <span class="string">" is not a readable log directory."</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 加载所有的日志,而每个日志也会调用 loadSegments() 方法加载所有的分段,过程比较慢,所有每个日志都会创建一个单独的线程</span></div><div class="line">  <span class="comment">//note: 日志管理器采用线程池提交任务,标识不用的任务可以同时运行</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">loadLogs</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    info(<span class="string">"Loading logs."</span>)</div><div class="line">    <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">    <span class="keyword">val</span> threadPools = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">ExecutorService</span>]</div><div class="line">    <span class="keyword">val</span> jobs = mutable.<span class="type">Map</span>.empty[<span class="type">File</span>, <span class="type">Seq</span>[<span class="type">Future</span>[_]]]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (dir &lt;- <span class="keyword">this</span>.logDirs) &#123; <span class="comment">//note: 处理每一个日志目录</span></div><div class="line">      <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(ioThreads) <span class="comment">//note: 默认为 1</span></div><div class="line">      threadPools.append(pool) <span class="comment">//note: 每个对应的数据目录都有一个线程池</span></div><div class="line"></div><div class="line">      <span class="keyword">val</span> cleanShutdownFile = <span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">Log</span>.<span class="type">CleanShutdownFile</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (cleanShutdownFile.exists) &#123;</div><div class="line">        debug(</div><div class="line">          <span class="string">"Found clean shutdown file. "</span> +</div><div class="line">          <span class="string">"Skipping recovery for all logs in data directory: "</span> +</div><div class="line">          dir.getAbsolutePath)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// log recovery itself is being performed by `Log` class during initialization</span></div><div class="line">        brokerState.newState(<span class="type">RecoveringFromUncleanShutdown</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">var</span> recoveryPoints = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read <span class="comment">//note: 读取检查点文件</span></div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">          warn(<span class="string">"Error occured while reading recovery-point-offset-checkpoint file of directory "</span> + dir, e)</div><div class="line">          warn(<span class="string">"Resetting the recovery checkpoint to 0"</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">val</span> jobsForDir = <span class="keyword">for</span> &#123;</div><div class="line">        dirContent &lt;- <span class="type">Option</span>(dir.listFiles).toList <span class="comment">//note: 数据目录下的所有日志目录</span></div><div class="line">        logDir &lt;- dirContent <span class="keyword">if</span> logDir.isDirectory <span class="comment">//note: 日志目录下每个分区目录</span></div><div class="line">      &#125; <span class="keyword">yield</span> &#123;</div><div class="line">        <span class="type">CoreUtils</span>.runnable &#123; <span class="comment">//note: 每个分区的目录都对应了一个线程</span></div><div class="line">          debug(<span class="string">"Loading log '"</span> + logDir.getName + <span class="string">"'"</span>)</div><div class="line"></div><div class="line">          <span class="keyword">val</span> topicPartition = <span class="type">Log</span>.parseTopicPartitionName(logDir)</div><div class="line">          <span class="keyword">val</span> config = topicConfigs.getOrElse(topicPartition.topic, defaultConfig)</div><div class="line">          <span class="keyword">val</span> logRecoveryPoint = recoveryPoints.getOrElse(topicPartition, <span class="number">0</span>L)</div><div class="line"></div><div class="line">          <span class="keyword">val</span> current = <span class="keyword">new</span> <span class="type">Log</span>(logDir, config, logRecoveryPoint, scheduler, time)<span class="comment">//note: 创建 Log 对象后，初始化时会加载所有的 segment</span></div><div class="line">          <span class="keyword">if</span> (logDir.getName.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) &#123; <span class="comment">//note: 该目录被标记为删除</span></div><div class="line">            <span class="keyword">this</span>.logsToBeDeleted.add(current)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> previous = <span class="keyword">this</span>.logs.put(topicPartition, current) <span class="comment">//note: 创建日志后,加入日志管理的映射表</span></div><div class="line">            <span class="keyword">if</span> (previous != <span class="literal">null</span>) &#123;</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</div><div class="line">                <span class="string">"Duplicate log directories found: %s, %s!"</span>.format(</div><div class="line">                  current.dir.getAbsolutePath, previous.dir.getAbsolutePath))</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      jobs(cleanShutdownFile) = jobsForDir.map(pool.submit).toSeq <span class="comment">//note: 提交任务</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">for</span> ((cleanShutdownFile, dirJobs) &lt;- jobs) &#123;</div><div class="line">        dirJobs.foreach(_.get)</div><div class="line">        cleanShutdownFile.delete()</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">ExecutionException</span> =&gt; &#123;</div><div class="line">        error(<span class="string">"There was an error in one of the threads during logs loading: "</span> + e.getCause)</div><div class="line">        <span class="keyword">throw</span> e.getCause</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      threadPools.foreach(_.shutdown())</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    info(<span class="string">s"Logs loading complete in <span class="subst">$&#123;time.milliseconds - startMs&#125;</span> ms."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>初始化 LogManger 代码有两个主要方法：</p>
<ol>
<li><code>createAndValidateLogDirs()</code>：创建指定的数据目录，并做相应的检查： 1.确保数据目录中没有重复的数据目录、2.数据目录不存在的话就创建相应的目录；3. 检查每个目录路径是否是可读的；</li>
<li><code>loadLogs()</code>：加载所有的日志分区，而每个日志也会调用 <code>loadSegments()</code> 方法加载该分区所有的 segment 文件，过程比较慢，所以 LogManager 使用线程池的方式，为每个日志的加载都会创建一个单独的线程。</li>
</ol>
<p>虽然使用的是线程池提交任务，并发进行 load 分区日志，但这个任务本身是阻塞式的，只有当所有的分区日志加载完成，才能调用 <code>startup()</code> 启动 LogManager 线程。</p>
<h3 id="LogManager-启动"><a href="#LogManager-启动" class="headerlink" title="LogManager 启动"></a>LogManager 启动</h3><p>在日志目录的所有分区日志都加载完成后，KafkaServer 调用 <code>startup()</code> 方法启动 LogManager 线程，LogManager 启动后，后台会运行四个定时任务，代码实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">/* Schedule the cleanup task to delete old logs */</span></div><div class="line">  <span class="keyword">if</span>(scheduler != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">//note: 定时清理过期的日志 segment,并维护日志的大小</span></div><div class="line">    info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</div><div class="line">    scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</div><div class="line">                       cleanupLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = retentionCheckMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时刷新还没有写到磁盘上日志</span></div><div class="line">    info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</div><div class="line">    scheduler.schedule(<span class="string">"kafka-log-flusher"</span>,</div><div class="line">                       flushDirtyLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = flushCheckMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时将所有数据目录所有日志的检查点写到检查点文件中</span></div><div class="line">    scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</div><div class="line">                       checkpointRecoveryPointOffsets,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = flushCheckpointMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时删除标记为 delete 的日志文件</span></div><div class="line">    scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</div><div class="line">                       deleteLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = defaultConfig.fileDeleteDelayMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 如果设置为 true， 自动清理 compaction 类型的 topic</span></div><div class="line">  <span class="keyword">if</span>(cleanerConfig.enableCleaner)</div><div class="line">    cleaner.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>四个后台定时线程的作用：</p>
<ol>
<li><code>cleanupLogs</code>：定时清理过期的日志 segment，并维护日志的大小（默认5min）；</li>
<li><code>flushDirtyLogs</code>：定时刷新将还没有写到磁盘上日志刷新到磁盘（默认 无限大）；</li>
<li><code>checkpointRecoveryPointOffsets</code>：定时将所有数据目录所有日志的检查点写到检查点文件中（默认 60s）；</li>
<li><code>deleteLogs</code>：定时删除标记为 delete 的日志文件（默认 30s）。</li>
</ol>
<h3 id="检查点文件"><a href="#检查点文件" class="headerlink" title="检查点文件"></a>检查点文件</h3><p>在 LogManager 中有一个非常重要的文件——检查点文件：</p>
<ol>
<li>Kafka 启动时创建 LogManager，读取检查点文件，并把每个分区对应的检查点（checkPoint）作为日志的恢复点（recoveryPoint），最后创建分区对应的日志实例；</li>
<li>消息追加到分区对应的日志，在刷新日志时，将最新的偏移量作为日志的检查点（也即是刷新日志时，会更新检查点位置）；</li>
<li>LogManager 会启动一个定时任务，读取所有日志的检查点，并写入全局的检查点文件（定时将检查点的位置更新到检查点文件中）。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note：通常所有数据目录都会一起执行，不会专门操作某一个数据目录的检查点文件</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpointRecoveryPointOffsets</span></span>() &#123;</div><div class="line">  <span class="keyword">this</span>.logDirs.foreach(checkpointLogsInDir)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Make a checkpoint for all logs in provided directory.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 对数据目录下的所有日志（即所有分区），将其检查点写入检查点文件</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkpointLogsInDir</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> recoveryPoints = <span class="keyword">this</span>.logsByDir.get(dir.toString)</div><div class="line">  <span class="keyword">if</span> (recoveryPoints.isDefined) &#123;</div><div class="line">    <span class="keyword">this</span>.recoveryPointCheckpoints(dir).write(recoveryPoints.get.mapValues(_.recoveryPoint))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>这里留一个问题：启动时，如果发现检查点文件的 offset 比 segment 中最大的 offset 小时（最新的检查点在更新到文件前机器宕机了），应该怎么处理？答案将在下一篇文章中讲述。</p>
</blockquote>
<h2 id="日志刷新"><a href="#日志刷新" class="headerlink" title="日志刷新"></a>日志刷新</h2><p>日志管理器会定时调度 <code>flushDirtyLogs()</code> 方法，定期将页面缓存中的数据真正刷新到磁盘文件中。如果缓存中的数据（在 pagecache 中）在 flush 到磁盘之前，Broker 宕机了，那么会导致数据丢失（多副本减少了这个风险）。</p>
<p>在 Kafka 中有两种策略，将日志刷新到磁盘上：</p>
<ul>
<li>时间策略，（<code>log.flush.interval.ms</code> 中配置调度周期，默认为无限大，即选择大小策略）：</li>
<li>大小策略，（<code>log.flush.interval.messages</code> 中配置当未刷新的 msg 数超过这个值后，进行刷新）。</li>
</ul>
<p>LogManager 刷新日志的实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: LogManager 启动时，会启动一个周期性调度任务，调度这个方法，定时刷新日志。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">flushDirtyLogs</span></span>() = &#123;</div><div class="line">  debug(<span class="string">"Checking for dirty logs to flush..."</span>)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> ((topicPartition, log) &lt;- logs) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">//note: 每个日志的刷新时间并不相同</span></div><div class="line">      <span class="keyword">val</span> timeSinceLastFlush = time.milliseconds - log.lastFlushTime</div><div class="line">      debug(<span class="string">"Checking if flush is needed on "</span> + topicPartition.topic + <span class="string">" flush interval  "</span> + log.config.flushMs +</div><div class="line">            <span class="string">" last flushed "</span> + log.lastFlushTime + <span class="string">" time since last flush: "</span> + timeSinceLastFlush)</div><div class="line">      <span class="keyword">if</span>(timeSinceLastFlush &gt;= log.config.flushMs)</div><div class="line">        log.flush</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Error flushing topic "</span> + topicPartition.topic, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>LogManager 这个方法最后的结果还是调用了 <code>log.flush()</code> 进行刷新操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Flush all log segments</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(): <span class="type">Unit</span> = flush(<span class="keyword">this</span>.logEndOffset)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Flush log segments for all offsets up to offset-1</div><div class="line"> *</div><div class="line"> * @param offset The offset to flush up to (non-inclusive); the new recovery point</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(offset: <span class="type">Long</span>) : <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (offset &lt;= <span class="keyword">this</span>.recoveryPoint)</div><div class="line">    <span class="keyword">return</span></div><div class="line">  debug(<span class="string">"Flushing log '"</span> + name + <span class="string">" up to offset "</span> + offset + <span class="string">", last flushed: "</span> + lastFlushTime + <span class="string">" current time: "</span> +</div><div class="line">        time.milliseconds + <span class="string">" unflushed = "</span> + unflushedMessages)</div><div class="line">  <span class="comment">//note: 刷新检查点到最新偏移量之间的所有日志分段</span></div><div class="line">  <span class="keyword">for</span>(segment &lt;- logSegments(<span class="keyword">this</span>.recoveryPoint, offset))</div><div class="line">    segment.flush()<span class="comment">//note: 刷新数据文件和索引文件（调用操作系统的 fsync）</span></div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(offset &gt; <span class="keyword">this</span>.recoveryPoint) &#123;</div><div class="line">      <span class="keyword">this</span>.recoveryPoint = offset</div><div class="line">      lastflushedTime.set(time.milliseconds)<span class="comment">//note: 更新刷新时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的内容实际上只是按 <code>log.flush.interval.ms</code> 设置去 flush 日志到磁盘，那么 <code>log.flush.interval.messages</code> 策略是在什么地方生效的呢？用心想一下，大家应该能猜出来，是在数据追加到 Log 中的时候，这时候会判断没有 flush 的数据大小是否达到阈值，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 其他部分这里暂时忽略了</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</div><div class="line">  <span class="comment">// now append to the log</span></div><div class="line">  segment.append(firstOffset = appendInfo.firstOffset,</div><div class="line">    largestOffset = appendInfo.lastOffset,</div><div class="line">    largestTimestamp = appendInfo.maxTimestamp,</div><div class="line">    shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</div><div class="line">    records = validRecords)</div><div class="line"></div><div class="line">  <span class="comment">// increment the log end offset</span></div><div class="line">  updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</div><div class="line"></div><div class="line">  trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></div><div class="line">    .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</div><div class="line">    flush()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h2><p>为了保证分区的总大小不超过阈值（<code>log.retention.bytes</code>），日志管理器会定时清理旧的数据。</p>
<blockquote>
<p>不过一般情况下，都是通过配置 <code>log.retention.hours</code> 来配置 segment 的保存时间，而不是通过单日志的总大小配置，因为不同的 topic，其 partition 大小相差很大，导致最后的保存时间可能也不一致，不利于管理。</p>
</blockquote>
<p>清理旧日志分段方法，主要有两种：</p>
<ol>
<li>删除：超过时间或大小阈值的旧 segment，直接进行删除；</li>
<li>压缩：不是直接删除日志分段，而是采用合并压缩的方式进行。</li>
</ol>
<p>这里主要讲述第一种方法，第二种将会后续文章介绍。</p>
<p>先看下 LogManager 中日志清除任务的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Delete any eligible logs. Return the number of segments deleted.</div><div class="line"> * Only consider logs that are not compacted.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 日志清除任务</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanupLogs</span></span>() &#123;</div><div class="line">  debug(<span class="string">"Beginning log cleanup..."</span>)</div><div class="line">  <span class="keyword">var</span> total = <span class="number">0</span></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">  <span class="keyword">for</span>(log &lt;- allLogs; <span class="keyword">if</span> !log.config.compact) &#123;</div><div class="line">    debug(<span class="string">"Garbage collecting '"</span> + log.name + <span class="string">"'"</span>)</div><div class="line">    total += log.deleteOldSegments() <span class="comment">//note: 清理过期的 segment</span></div><div class="line">  &#125;</div><div class="line">  debug(<span class="string">"Log cleanup completed. "</span> + total + <span class="string">" files deleted in "</span> +</div><div class="line">                (time.milliseconds - startMs) / <span class="number">1000</span> + <span class="string">" seconds"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>日志清除任务的实现还是在 Log 的 <code>deleteOldSegments()</code> 中实现的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Delete any log segments that have either expired due to time based retention</div><div class="line">  * or because the log size is &gt; retentionSize</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (!config.delete) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  deleteRetenionMsBreachedSegments() + deleteRetentionSizeBreachedSegments()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 清除保存时间满足条件的 segment</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetenionMsBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (config.retentionMs &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">  deleteOldSegments(startMs - _.largestTimestamp &gt; config.retentionMs)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 清除保存大小满足条件的 segment</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetentionSizeBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (config.retentionSize &lt; <span class="number">0</span> || size &lt; config.retentionSize) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">var</span> diff = size - config.retentionSize</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shouldDelete</span></span>(segment: <span class="type">LogSegment</span>) = &#123;</div><div class="line">    <span class="keyword">if</span> (diff - segment.size &gt;= <span class="number">0</span>) &#123;</div><div class="line">      diff -= segment.size</div><div class="line">      <span class="literal">true</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  deleteOldSegments(shouldDelete)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清除日志的两个方法：</p>
<ol>
<li><code>deleteRetenionMsBreachedSegments()</code>：如果 segment 保存时间超过设置的时间，那么进行删除；</li>
<li><code>deleteRetentionSizeBreachedSegments()</code>：如果当前最新的日志大小减少下一个即将删除的 segment 分段的大小超过阈值，那么就允许删除该 segment，否则就不允许。</li>
</ol>
<p>调用 <code>deleteOldSegments()</code> 方法删除日志数据文件及索引文件的具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 清除相应的 segment 及相应的索引文件</span></div><div class="line"><span class="comment">//note: 其中 predicate 是一个高阶函数，只有返回值为 true 该 segment 才会被删除</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(predicate: <span class="type">LogSegment</span> =&gt; <span class="type">Boolean</span>): <span class="type">Int</span> = &#123;</div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> deletable = deletableSegments(predicate)</div><div class="line">    <span class="keyword">val</span> numToDelete = deletable.size</div><div class="line">    <span class="keyword">if</span> (numToDelete &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// we must always have at least one segment, so if we are going to delete all the segments, create a new one first</span></div><div class="line">      <span class="keyword">if</span> (segments.size == numToDelete)</div><div class="line">        roll()</div><div class="line">      <span class="comment">// remove the segments for lookups</span></div><div class="line">      deletable.foreach(deleteSegment) <span class="comment">//note: 删除 segment</span></div><div class="line">    &#125;</div><div class="line">    numToDelete</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</div><div class="line">  info(<span class="string">"Scheduling log segment %d for log %s for deletion."</span>.format(segment.baseOffset, name))</div><div class="line">  lock synchronized &#123;</div><div class="line">    segments.remove(segment.baseOffset) <span class="comment">//note:  从映射关系表中删除数据</span></div><div class="line">    asyncDeleteSegment(segment) <span class="comment">//note: 异步删除日志 segment</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Perform an asynchronous delete on the given file if it exists (otherwise do nothing)</div><div class="line"> *</div><div class="line"> * @throws KafkaStorageException if the file can't be renamed and still exists</div><div class="line"> */</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">asyncDeleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</div><div class="line">  segment.changeFileSuffixes(<span class="string">""</span>, <span class="type">Log</span>.<span class="type">DeletedFileSuffix</span>) <span class="comment">//note: 先将 segment 的数据文件和索引文件后缀添加 `.deleted`</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteSeg</span></span>() &#123;</div><div class="line">    info(<span class="string">"Deleting segment %d from log %s."</span>.format(segment.baseOffset, name))</div><div class="line">    segment.delete()</div><div class="line">  &#125;</div><div class="line">  scheduler.schedule(<span class="string">"delete-file"</span>, deleteSeg, delay = config.fileDeleteDelayMs) <span class="comment">//note: 异步调度进行删除</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的讲解来看，Kafka LogManager 线程工作还是比较清晰简洁的，它的作用就是负责日志的创建、检索、清理，并不负责日志的读写等实际操作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>操作系统之共享对象学习</title>
    <link href="http://matt33.com/2018/02/04/linux-mmap/"/>
    <id>http://matt33.com/2018/02/04/linux-mmap/</id>
    <published>2018-02-04T15:59:16.000Z</published>
    <updated>2018-02-04T16:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 Kafka 的存储层这部分代码时，看到了很多地方使用操作系统的共享内存机制，Kafka 中所有日志文件的索引都是使用了 <code>mmap</code> 做内存映射，<code>mmap</code> 这块刚好也是一个值得深入学习的知识点，于是就就深入地看了一下、做了一下总结，本文的内容主要来自《深入理解操作系统》第三版 9.8 存储器映射部分。</p>
<h2 id="存储器映射"><a href="#存储器映射" class="headerlink" title="存储器映射"></a>存储器映射</h2><p>Linux 通过将一个虚拟存储器区域与一个磁盘上的对象关联起来，以初始化这个虚拟存储器区域的内容，这个过程就被称为 <strong>存储器映射(memory mapping)</strong>，虚拟存储器区域可以映射到下面两种类型的对象中的一种：</p>
<ul>
<li>Unix 文件系统的普通文件：一个区域可以映射到一个普通磁盘文件的连续部分，例如一个可执行的目标文件。文件区会被分成了页大小的片，每一片包含一个虚拟页面的初始内容。因为按需进行页面调度，所以这些虚拟页面没有实际交换进入物理存储器，直到 CPU 第一次引用页面（如果区域比文件区要大，那么就用零来填充这个区域的余下部分）；</li>
<li>匿名文件：一个区域也可以映射到一个匿名文件，匿名文件是由内核创建的，包含的是二进制零。CPU 第一次引用这样一区域内的虚拟页面时，内核就在物理存储器中找到一个合适的牺牲页面，如果该页面被修改过，就将这个页面换出来，用二进制零覆盖牺牲页面并更新页表，将这个页面标记为是驻留在存储器中的，但是要注意的是在磁盘和存储器之间并没有实际的数据传输，因为这个原因，映射到匿名文件区域中的页面有时也叫做 <strong>请求二进制零的页（demand-zero page）</strong>。</li>
</ul>
<p>上面这个是存储器映射的基础内容，理解完这部分之后，我们再来看共享对象（共享内存）和 mmap。</p>
<h2 id="共享对象"><a href="#共享对象" class="headerlink" title="共享对象"></a>共享对象</h2><p>存储器映射的出现，它是为了要解决是什么问题呢？先看一下对于操作系统来说，没有存储器映射的话面临的情况：</p>
<p>在操作系统中，进程这一抽象能够为每个进程提供自己私有的虚拟地址空间，可以免受其他进程的错误读写，但是，对于操作系统的每一个进程，它们都有同样的只读文本区域，如：每个 C 程序都需要调用一些标准的 C 库函数、需要程序需要访问只读运行时库代码的相同拷贝等等。那么如果每个进行都在物理存储器中保持这些常用代码的复制拷贝，那就是极端的浪费了。</p>
<p>而存储器映射机制的出现，就给我们提供了一种清晰的机制，用来 <strong>控制多个进程如何共享对象</strong>。</p>
<p>一个对象可以被映射到虚拟存储器的一个区域，要么作为共享对象，要么作为私有对象：</p>
<ul>
<li>如果是作为共享对象，那么这个进程对这个区域的任何写操作，对于那些也会把这个共享对象映射到它们虚拟存储器的其他进程而言也是可见的，而且这些变化，也会反映在磁盘上的原始对象中；</li>
<li>如果是作为私有对象，这样的改变，对于其他进程来说是不可变的，并且进程对这个区域所做的任何写操作都不会反映在磁盘上的对象中。</li>
</ul>
<p>关于共享对象，举一个例子，如下图所示：</p>
<p><img src="/images/linux/mmap1.png" alt="一个共享对象"></p>
<p>假设进程1将一个共享对象映射到它的虚拟地址存储器的一个区域中，如上图左边所示，现在假设进程2将同一个共享对象映射到它的地址空间（与进程1虚拟地址空间并不一定一样），如右边所示。因为每个对象都有一个唯一的文件名，内核可以迅速地判断进程1已经映射了这个对象，而且可以使进程2中的页表条目指向相应的物理页面。关键点在于即使对象被映射到了多个共享区域，物理存储器中也只需要存放共享对象的一个拷贝。</p>
<h3 id="私有对象的-copy-on-write"><a href="#私有对象的-copy-on-write" class="headerlink" title="私有对象的 copy-on-write"></a>私有对象的 copy-on-write</h3><p>在私有对象中，操作系统是使用了一种叫做 <strong>写时拷贝（copy-on-write）</strong> 的巧妙技术将其映射到虚拟存储器中的。一个私有对象开始生命周期的方式基本上与共享对象一样，在物理存储器中只保存有私有对象的一份拷贝。</p>
<ul>
<li>如下图的左边部分所示，其中两个进程将一个私有对象映射到它们虚拟存储器的不同区域，但是却共享这个对象同一个物理拷贝。这时，对于每个映射私有对象的进程，相应私有区域的页表条目都被标记为只读，并且区域结构被标记为私有的写时拷贝，只要没有进程试图写它自己的私有区域，它们就可以继续共享物理存储器中对象的一个单独拷贝；</li>
<li>如果只有有一个进程试图写私有区域的某个页面，那么这个写操作就会触发一个保护故障：如下图右边所示，该故障处理程序触发的原因是由于进程试图写私有的写时拷贝区域的一个页面引起的，它就会在物理存储器中创建这个页面的一个新拷贝，更新页表条目指向这个新拷贝，然后恢复这个页面的写权限，当故障处理程序返回时，CPU 重新执行这个写操作，现在在新创建的页面上这个写操作就可以正常执行了。</li>
</ul>
<p><img src="/images/linux/mmap2.png" alt="一个写时拷贝对象"></p>
<p>copy-on-write 最充分地使用了稀有的物理存储器。</p>
<h3 id="再看-fork-函数"><a href="#再看-fork-函数" class="headerlink" title="再看 fork 函数"></a>再看 fork 函数</h3><p>学习了虚拟存储器映射进制后，回头再看 Linux 中的 fork 函数，就会明白 fork 函数是如何创建一个带有自己独立虚拟地址控制的新进程。</p>
<ol>
<li>当 fork 函数被当前进程调用时，内核为新进程创建各种数据结构，并分配给它一个唯一的 PID；</li>
<li>为了给新进程创建虚拟存储器，它创建了当前进程的 <code>mm_struct</code>、区域结构和页表的原样拷贝，它将两个进程的每个页面都标记为只读，并将两个进程中的每个区域结构都标记为私有的写时拷贝；</li>
<li>当 fork 函数在新进程中返回时，新进程现在的虚拟存储器刚好和调用 fork 时存在的虚拟存储器相同；</li>
<li>当两个进程中的任一个进行写操作时，写时拷贝机制就会创建新页面，因此，也就为每个进程保持了私有地址空间的抽象概念。</li>
</ol>
<h2 id="mmap-函数"><a href="#mmap-函数" class="headerlink" title="mmap 函数"></a>mmap 函数</h2><p>经过前面的介绍，既然在操作系统中有了存储器映射的机制，那么我们应该怎么使用呢？这就需要引入 UNIX 中另一个重要的函数 —— <code>mmap</code>，它是用来创建新的虚拟存储器区域，并将对象映射到这些区域中。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> *<span class="title">mmap</span><span class="params">(<span class="keyword">void</span> *start, <span class="keyword">size_t</span> length, <span class="keyword">int</span> prot, <span class="keyword">int</span> flags, <span class="keyword">int</span> fd, <span class="keyword">off_t</span> offset)</span></span>;</div></pre></td></tr></table></figure>
<p>成功执行时，<code>mmap()</code> 返回指向映射区的指针，失败时，<code>mmap()</code> 返回 <code>MAP_FAILED</code>(-1)， error被设为以下的某个值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">EACCES：访问出错</div><div class="line">EAGAIN：文件已被锁定，或者太多的内存已被锁定</div><div class="line">EBADF：fd不是有效的文件描述词</div><div class="line">EINVAL：一个或者多个参数无效</div><div class="line">ENFILE：已达到系统对打开文件的限制</div><div class="line">ENODEV：指定文件所在的文件系统不支持内存映射</div><div class="line">ENOMEM：内存不足，或者进程已超出最大内存映射数量</div><div class="line">EPERM：权能不足，操作不允许</div><div class="line">ETXTBSY：已写的方式打开文件，同时指定MAP_DENYWRITE标志</div><div class="line">SIGSEGV：试着向只读区写入</div><div class="line">SIGBUS：试着访问不属于进程的内存区</div></pre></td></tr></table></figure>
<p>mmap 要求内核创建一个新的虚拟存储器区域，最好是从地址 <code>start</code> 开始的一个区域，并将文件描述符 fd 指定的对象的一个连续的片（chunk）映射到这个新的区域，连续的对象片大小为 <code>length</code> 字节，从距文件开始偏移量为 <code>offset</code> 字节的地方开始，<code>start</code> 地址仅仅是一个暗示，通常设置为 NULL，如下图所示。</p>
<p><img src="/images/linux/mmap3.png" alt="mmap 函数解释"></p>
<p>参数 port 包含描述新映射的虚拟存储器区域的访问权限（在相应区域结构中的 <code>vm_port</code> 位）：</p>
<ul>
<li>PORT_EXEC：这个区域内的页面由可以被 CPU 执行的指令组成；</li>
<li>PORT_READ：这个区域内的页面可读；</li>
<li>PORT_WRITE：这个区域内的页面可写；</li>
<li>PORT_NONE：这个区域内的页面不能被访问。</li>
</ul>
<p>参数 <code>flags</code> 指定映射对象的类型，映射选项和映射页是否可以共享（下面列出的只是其中一部分）</p>
<ul>
<li>MAP_ANON：表示被映射的对象就是一个匿名对象，而相应的虚拟页面是请求二进制零的；</li>
<li>MAP_PRIVATE：表示被映射的对象是一个私有的、写时拷贝的对象；</li>
<li>MAP_SHARED：表示是一个共享对象。</li>
</ul>
<p>示例如：<code>bufp = Mmap(-1, size, PORT_READ, MAP_PRIVATE|MAP_ANON, 0, 0);</code>，让内核创建一个新的包含 size 字节的只读、私有、请求二进制零的虚拟存储器区域。</p>
<p>删除虚拟存储器区域，使用 <code>int munmap(void *start, size_t length);</code>，若成功返回0，若出错返回 -1.</p>
<p>下面看一个示例：使用 mmap 实现一个功能：将一个任意大小的磁盘文件拷贝到 stdout，输入文件的名字必须作为一个命令行参数来传递。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">"csapp.h"</span></span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmapcopy</span><span class="params">(<span class="keyword">int</span> fd,<span class="keyword">int</span> size)</span></span>&#123;</div><div class="line">    <span class="keyword">char</span> *bufp;</div><div class="line">    bufp =(<span class="keyword">char</span> *)mmap(<span class="literal">NULL</span>,size,PROT_READ,MAP_PRIVATE,fd,<span class="number">0</span>);<span class="comment">//在进程空间中创建一个新的虚拟存储器区域，将磁盘文件映射到这个区域中</span></div><div class="line">    write(<span class="number">1</span>,bufp,size);<span class="comment">//将信息写入标准输出</span></div><div class="line">    <span class="comment">//POSIX 定义了 STDIN_FILENO、STDOUT_FILENO 和 STDERR_FILENO 来代替 0、1、2。这三个符号常量的定义位于头文件 unistd.h。</span></div><div class="line">        munmap(bufp,size);<span class="comment">//删除虚拟存储器区域</span></div><div class="line">    <span class="keyword">return</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span>&#123;</div><div class="line">    <span class="keyword">struct</span> stat _stat;  <span class="comment">//文末附上关于这个结构体详细内容的链接</span></div><div class="line">    <span class="keyword">int</span> fd;</div><div class="line">    <span class="keyword">if</span>(argc != <span class="number">2</span>)&#123;</div><div class="line">        <span class="built_in">printf</span>(<span class="string">"usage :%s &lt;filename&gt;"</span>,argv[<span class="number">0</span>]);</div><div class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line">    fd = open(argv[<span class="number">1</span>],O_RDONLY,<span class="number">0</span>);</div><div class="line">    <span class="comment">//fd1 = open(argv[2],O_RDWR|O_APPEND,0);  以“读写+追加”模式打开一个额外的文件，将在函数里尝试向它追加信息。</span></div><div class="line"></div><div class="line">    fstat(fd,&amp;_stat);  <span class="comment">//fstat将文件标识符fd所标识的文件状态，复制到结构体stat中</span></div><div class="line">    mmapcopy(fd,_stat.st_size);</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>参考文献：</p>
<ul>
<li>《深入理解操作系统 第三版》；</li>
<li><a href="http://www.cnblogs.com/huxiao-tee/p/4660352.html" target="_blank" rel="external">认真分析mmap：是什么 为什么 怎么用</a>；</li>
<li><a href="http://blog.csdn.net/qq973177663/article/details/51246267" target="_blank" rel="external">使用mmap实现一个文件输出函数</a>.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Kafka 的存储层这部分代码时，看到了很多地方使用操作系统的共享内存机制，Kafka 中所有日志文件的索引都是使用了 &lt;code&gt;mmap&lt;/code&gt; 做内存映射，&lt;code&gt;mmap&lt;/code&gt; 这块刚好也是一个值得深入学习的知识点，于是就就深入地看了一下、做了
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="linux" scheme="http://matt33.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 GroupCoordinator 详解（十）</title>
    <link href="http://matt33.com/2018/01/28/server-group-coordinator/"/>
    <id>http://matt33.com/2018/01/28/server-group-coordinator/</id>
    <published>2018-01-28T14:23:05.000Z</published>
    <updated>2018-04-30T13:18:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>突然发现距离上一篇文章，已经过去两个多月了，有两个月没有写博客了，之前定的是年前把这个系列写完，现在看来只能往后拖了，后面估计还有五篇文章左右，尽量在春节前完成吧。继续之前的内容开始讲解，这篇文章，主要是想把 GroupCoordinator 的内容总结一下，也算是开始了 Kafka Server 端的讲解，Kafka 的 Server 端主要有三块内容：GroupCoordinator、Controller 和 ReplicaManager，其中，GroupCoordinator 的内容是与 Consumer 端紧密结合在一起的，有一部分内容在前面已经断断续续介绍过，这里会做一个总结。</p>
<p>关于 GroupCoordinator，代码中有一段注释介绍得比较清晰，这里引用一下：</p>
<blockquote>
<p>GroupCoordinator handles general group membership and offset management.</p>
<p>Each Kafka server instantiates a coordinator which is responsible for a set of groups. Groups are assigned to coordinators based on their group names.</p>
</blockquote>
<p>简单来说就是，GroupCoordinator 是负责进行 consumer 的 group 成员与 offset 管理（但每个 GroupCoordinator 只是管理一部分的 consumer group member 和 offset 信息），那它是怎么管理的呢？这个从 GroupCoordinator 处理的 client 端请求类型可以看出来，它处理的请求类型主要有以下几种：</p>
<ol>
<li>ApiKeys.OFFSET_COMMIT;</li>
<li>ApiKeys.OFFSET_FETCH;</li>
<li>ApiKeys.JOIN_GROUP;</li>
<li>ApiKeys.LEAVE_GROUP;</li>
<li>ApiKeys.SYNC_GROUP;</li>
<li>ApiKeys.DESCRIBE_GROUPS;</li>
<li>ApiKeys.LIST_GROUPS;</li>
<li>ApiKeys.HEARTBEAT;</li>
</ol>
<p>而 Kafka Server 端要处理的请求总共有以下 21 种，其中有 8 种是由 GroupCoordinator 来完成的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="type">ApiKeys</span>.forId(request.requestId) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">PRODUCE</span> =&gt; handleProducerRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">FETCH</span> =&gt; handleFetchRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LIST_OFFSETS</span> =&gt; handleOffsetRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">METADATA</span> =&gt; handleTopicMetadataRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LEADER_AND_ISR</span> =&gt; handleLeaderAndIsrRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span> =&gt; handleStopReplicaRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">UPDATE_METADATA_KEY</span> =&gt; handleUpdateMetadataRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">CONTROLLED_SHUTDOWN_KEY</span> =&gt; handleControlledShutdownRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">OFFSET_COMMIT</span> =&gt; handleOffsetCommitRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">OFFSET_FETCH</span> =&gt; handleOffsetFetchRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">GROUP_COORDINATOR</span> =&gt; handleGroupCoordinatorRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">JOIN_GROUP</span> =&gt; handleJoinGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">HEARTBEAT</span> =&gt; handleHeartbeatRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LEAVE_GROUP</span> =&gt; handleLeaveGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">SYNC_GROUP</span> =&gt; handleSyncGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">DESCRIBE_GROUPS</span> =&gt; handleDescribeGroupRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">LIST_GROUPS</span> =&gt; handleListGroupsRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">SASL_HANDSHAKE</span> =&gt; handleSaslHandshakeRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">API_VERSIONS</span> =&gt; handleApiVersionsRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">CREATE_TOPICS</span> =&gt; handleCreateTopicsRequest(request)</div><div class="line">  <span class="keyword">case</span> <span class="type">ApiKeys</span>.<span class="type">DELETE_TOPICS</span> =&gt; handleDeleteTopicsRequest(request)</div><div class="line">  <span class="keyword">case</span> requestId =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Unknown api code "</span> + requestId)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="GroupCoordinator-简介"><a href="#GroupCoordinator-简介" class="headerlink" title="GroupCoordinator 简介"></a>GroupCoordinator 简介</h2><p>这里先简单看下 GroupCoordinator 的基本内容。</p>
<h3 id="GroupCoordinator-的启动"><a href="#GroupCoordinator-的启动" class="headerlink" title="GroupCoordinator 的启动"></a>GroupCoordinator 的启动</h3><p>Broker 在启动时，也就是 KafkaServer 在 <code>startup()</code> 方法中会有以下一段内容，它表示每个 Broker 在启动是都会启动 GroupCoordinator 服务。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* start group coordinator */</span></div><div class="line"><span class="comment">// Hardcode Time.SYSTEM for now as some Streams tests fail otherwise, it would be good to fix the underlying issue</span></div><div class="line">groupCoordinator = <span class="type">GroupCoordinator</span>(config, zkUtils, replicaManager, <span class="type">Time</span>.<span class="type">SYSTEM</span>)</div><div class="line">groupCoordinator.startup()<span class="comment">//note: 启动 groupCoordinator</span></div></pre></td></tr></table></figure>
<p>GroupCoordinator 服务在调用 <code>setup()</code> 方法启动后，进行的操作如下，实际上只是把一个标志变量值 <code>isActive</code> 设置为 true，并且启动了一个后台线程来删除过期的 group metadata。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">* Startup logic executed at the same time when the server starts up.</div><div class="line">*/</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>(enableMetadataExpiration: <span class="type">Boolean</span> = <span class="literal">true</span>) &#123;</div><div class="line">  info(<span class="string">"Starting up."</span>)</div><div class="line">  <span class="keyword">if</span> (enableMetadataExpiration)</div><div class="line">    groupManager.enableMetadataExpiration()</div><div class="line">  isActive.set(<span class="literal">true</span>)</div><div class="line">  info(<span class="string">"Startup complete."</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="group-如何选择相应的-GroupCoordinator"><a href="#group-如何选择相应的-GroupCoordinator" class="headerlink" title="group 如何选择相应的 GroupCoordinator"></a>group 如何选择相应的 GroupCoordinator</h3><p>要说这个，就必须介绍一下这个 <code>__consumer_offsets</code> topic 了，它是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有50个 partition，每个 partition 默认有三个副本，而具体的一个 group 的消费情况要存储到哪一个 partition 上，是根据 <code>abs(GroupId.hashCode()) % NumPartitions</code> 来计算的（其中，NumPartitions 是 <code>__consumer_offsets</code> 的 partition 数，默认是50个）。</p>
<p>对于 consumer group 而言，是根据其 <code>group.id</code> 进行 hash 并计算得到其具对应的 partition 值，该 partition leader 所在 Broker 即为该 Group 所对应的 GroupCoordinator，GroupCoordinator 会存储与该 group 相关的所有的 Meta 信息。</p>
<h3 id="GroupCoordinator-的-metadata"><a href="#GroupCoordinator-的-metadata" class="headerlink" title="GroupCoordinator 的 metadata"></a>GroupCoordinator 的 metadata</h3><p>对于 consumer group 而言，其对应的 metadata 信息主要包含一下内容：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Group contains the following metadata:</div><div class="line"> *</div><div class="line"> *  Membership metadata:</div><div class="line"> *  1. Members registered in this group</div><div class="line"> *  2. Current protocol assigned to the group (e.g. partition assignment strategy for consumers)</div><div class="line"> *  3. Protocol metadata associated with group members</div><div class="line"> *</div><div class="line"> *  State metadata:</div><div class="line"> *  1. group state</div><div class="line"> *  2. generation id</div><div class="line"> *  3. leader id</div><div class="line"> */</div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> group 的 meta 信息,对 group 级别而言,每个 group 都会有一个实例对象</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="class"><span class="keyword">class</span> <span class="title">GroupMetadata</span>(<span class="params">val groupId: <span class="type">String</span>, initialState: <span class="type">GroupState</span> = <span class="type">Empty</span></span>) </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> state: <span class="type">GroupState</span> = initialState <span class="comment">// group 的状态</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> members = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">MemberMetadata</span>] <span class="comment">// group 的 member 信息</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> offsets = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>] <span class="comment">//对应的 commit offset</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> pendingOffsetCommits = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>] <span class="comment">// commit offset 成功后更新到上面的 map 中</span></div><div class="line"></div><div class="line">  <span class="keyword">var</span> protocolType: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></div><div class="line">  <span class="keyword">var</span> generationId = <span class="number">0</span> <span class="comment">// generation id</span></div><div class="line">  <span class="keyword">var</span> leaderId: <span class="type">String</span> = <span class="literal">null</span> <span class="comment">// leader consumer id</span></div><div class="line">  <span class="keyword">var</span> protocol: <span class="type">String</span> = <span class="literal">null</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>而对于每个 consumer 而言，其 metadata 信息主要包括以下内容：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Member metadata contains the following metadata:</div><div class="line"> *</div><div class="line"> * Heartbeat metadata:</div><div class="line"> * 1. negotiated heartbeat session timeout 心跳超时时间</div><div class="line"> * 2. timestamp of the latest heartbeat 上次发送心跳的时间</div><div class="line"> *</div><div class="line"> * Protocol metadata:</div><div class="line"> * 1. the list of supported protocols (ordered by preference) 支持的 partition reassign 协议</div><div class="line"> * 2. the metadata associated with each protocol</div><div class="line"> *</div><div class="line"> * In addition, it also contains the following state information:</div><div class="line"> *</div><div class="line"> * 1. Awaiting rebalance callback: when the group is in the prepare-rebalance state,</div><div class="line"> *                                 its rebalance callback will be kept in the metadata if the</div><div class="line"> *                                 member has sent the join group request</div><div class="line"> * 2. Awaiting sync callback: when the group is in the awaiting-sync state, its sync callback</div><div class="line"> *                            is kept in metadata until the leader provides the group assignment</div><div class="line"> *                            and the group transitions to stable</div><div class="line"> */</div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 记录 group 中每个成员的状态信息</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="class"><span class="keyword">class</span> <span class="title">MemberMetadata</span>(<span class="params">val memberId: <span class="type">String</span>,</span></span></div><div class="line">                                          val groupId: <span class="type">String</span>,</div><div class="line">                                          val clientId: <span class="type">String</span>,</div><div class="line">                                          val clientHost: <span class="type">String</span>,</div><div class="line">                                          val rebalanceTimeoutMs: <span class="type">Int</span>,</div><div class="line">                                          val sessionTimeoutMs: <span class="type">Int</span>,</div><div class="line">                                          val protocolType: <span class="type">String</span>,</div><div class="line">                                          var supportedProtocols: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Array</span>[<span class="type">Byte</span>])]) &#123;&#125;</div></pre></td></tr></table></figure>
<h2 id="GroupCoordinator-请求处理"><a href="#GroupCoordinator-请求处理" class="headerlink" title="GroupCoordinator 请求处理"></a>GroupCoordinator 请求处理</h2><p>正如前面所述，Kafka Server 端可以介绍的21种请求中，其中有8种是由 GroupCoordinator 来处理的，这里主要介绍一下，GroupCoordinator 如何处理这些请求的。</p>
<h3 id="Offset-请求的处理"><a href="#Offset-请求的处理" class="headerlink" title="Offset 请求的处理"></a>Offset 请求的处理</h3><p>关于 Offset 请求的处理，有两个：</p>
<ul>
<li>OFFSET_FETCH：查询 offset；</li>
<li>OFFSET_COMMIT：提供 offset；</li>
</ul>
<h4 id="OFFSET-FETCH-请求处理"><a href="#OFFSET-FETCH-请求处理" class="headerlink" title="OFFSET_FETCH 请求处理"></a>OFFSET_FETCH 请求处理</h4><p>关于 OFFSET_FETCH 请求，Server 端的处理如下，新版 offset 默认是保存在 Kafka 中，这里也以保存在 Kafka 中为例，从下面的实现中也可以看出，在 fetch commit 是分两种情况：</p>
<ul>
<li>获取 group 所消费的所有 topic-partition 的 offset；</li>
<li>获取指定 topic-partition 的 offset。</li>
</ul>
<p>两种情况都是调用 <code>coordinator.handleFetchOffsets()</code> 方法实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle an offset fetch request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleOffsetFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> header = request.header</div><div class="line">  <span class="keyword">val</span> offsetFetchRequest = request.body.asInstanceOf[<span class="type">OffsetFetchRequest</span>]</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">authorizeTopicDescribe</span></span>(partition: <span class="type">TopicPartition</span>) =</div><div class="line">    authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, partition.topic)) <span class="comment">//note: 验证 Describe 权限</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> offsetFetchResponse =</div><div class="line">    <span class="comment">// reject the request if not authorized to the group</span></div><div class="line">    <span class="keyword">if</span> (!authorize(request.session, <span class="type">Read</span>, <span class="keyword">new</span> <span class="type">Resource</span>(<span class="type">Group</span>, offsetFetchRequest.groupId)))</div><div class="line">      offsetFetchRequest.getErrorResponse(<span class="type">Errors</span>.<span class="type">GROUP_AUTHORIZATION_FAILED</span>)</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">if</span> (header.apiVersion == <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">val</span> (authorizedPartitions, unauthorizedPartitions) = offsetFetchRequest.partitions.asScala</div><div class="line">          .partition(authorizeTopicDescribe)</div><div class="line"></div><div class="line">        <span class="comment">// version 0 reads offsets from ZK</span></div><div class="line">        <span class="keyword">val</span> authorizedPartitionData = authorizedPartitions.map &#123; topicPartition =&gt;</div><div class="line">          <span class="keyword">val</span> topicDirs = <span class="keyword">new</span> <span class="type">ZKGroupTopicDirs</span>(offsetFetchRequest.groupId, topicPartition.topic)</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">if</span> (!metadataCache.contains(topicPartition.topic))</div><div class="line">              (topicPartition, <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>)</div><div class="line">            <span class="keyword">else</span> &#123;</div><div class="line">              <span class="keyword">val</span> payloadOpt = zkUtils.readDataMaybeNull(<span class="string">s"<span class="subst">$&#123;topicDirs.consumerOffsetDir&#125;</span>/<span class="subst">$&#123;topicPartition.partition&#125;</span>"</span>)._1</div><div class="line">              payloadOpt <span class="keyword">match</span> &#123;</div><div class="line">                <span class="keyword">case</span> <span class="type">Some</span>(payload) =&gt;</div><div class="line">                  (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(</div><div class="line">                      payload.toLong, <span class="type">OffsetFetchResponse</span>.<span class="type">NO_METADATA</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">                <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">                  (topicPartition, <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">              (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(</div><div class="line">                  <span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="type">OffsetFetchResponse</span>.<span class="type">NO_METADATA</span>, <span class="type">Errors</span>.forException(e)))</div><div class="line">          &#125;</div><div class="line">        &#125;.toMap</div><div class="line"></div><div class="line">        <span class="keyword">val</span> unauthorizedPartitionData = unauthorizedPartitions.map(_ -&gt; <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>).toMap</div><div class="line">        <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, (authorizedPartitionData ++ unauthorizedPartitionData).asJava, header.apiVersion)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// versions 1 and above read offsets from Kafka</span></div><div class="line">        <span class="keyword">if</span> (offsetFetchRequest.isAllPartitions) &#123;<span class="comment">//note: 获取这个 group 消费的所有 tp offset</span></div><div class="line">          <span class="keyword">val</span> (error, allPartitionData) = coordinator.handleFetchOffsets(offsetFetchRequest.groupId)</div><div class="line">          <span class="keyword">if</span> (error != <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">            offsetFetchRequest.getErrorResponse(error)</div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// clients are not allowed to see offsets for topics that are not authorized for Describe</span></div><div class="line">            <span class="comment">//note: 如果没有 Describe 权限的话,不能查看相应的 offset</span></div><div class="line">            <span class="keyword">val</span> authorizedPartitionData = allPartitionData.filter &#123; <span class="keyword">case</span> (topicPartition, _) =&gt; authorizeTopicDescribe(topicPartition) &#125;</div><div class="line">            <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, authorizedPartitionData.asJava, header.apiVersion)</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 获取指定列表的 tp offset</span></div><div class="line">          <span class="keyword">val</span> (authorizedPartitions, unauthorizedPartitions) = offsetFetchRequest.partitions.asScala</div><div class="line">            .partition(authorizeTopicDescribe)</div><div class="line">          <span class="keyword">val</span> (error, authorizedPartitionData) = coordinator.handleFetchOffsets(offsetFetchRequest.groupId,</div><div class="line">            <span class="type">Some</span>(authorizedPartitions))</div><div class="line">          <span class="keyword">if</span> (error != <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">            offsetFetchRequest.getErrorResponse(error)</div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> unauthorizedPartitionData = unauthorizedPartitions.map(_ -&gt; <span class="type">OffsetFetchResponse</span>.<span class="type">UNKNOWN_PARTITION</span>).toMap</div><div class="line">            <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, (authorizedPartitionData ++ unauthorizedPartitionData).asJava, header.apiVersion)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  trace(<span class="string">s"Sending offset fetch response <span class="subst">$offsetFetchResponse</span> for correlation id <span class="subst">$&#123;header.correlationId&#125;</span> to client <span class="subst">$&#123;header.clientId&#125;</span>."</span>)</div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, offsetFetchResponse))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>coordinator.handleFetchOffsets()</code> 的实现中，主要是调用了 <code>groupManager.getOffsets()</code> 获取相应的 offset 信息，在查询时加锁的原因应该是为了避免在查询的过程中 offset 不断更新。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOffsets</span></span>(groupId: <span class="type">String</span>, topicPartitionsOpt: <span class="type">Option</span>[<span class="type">Seq</span>[<span class="type">TopicPartition</span>]]): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>] = &#123;</div><div class="line">  trace(<span class="string">"Getting offsets of %s for group %s."</span>.format(topicPartitionsOpt.getOrElse(<span class="string">"all partitions"</span>), groupId))</div><div class="line">  <span class="keyword">val</span> group = groupMetadataCache.get(groupId)</div><div class="line">  <span class="keyword">if</span> (group == <span class="literal">null</span>) &#123;</div><div class="line">    topicPartitionsOpt.getOrElse(<span class="type">Seq</span>.empty[<span class="type">TopicPartition</span>]).map &#123; topicPartition =&gt;</div><div class="line">      (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="string">""</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">    &#125;.toMap</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    group synchronized &#123;</div><div class="line">      <span class="keyword">if</span> (group.is(<span class="type">Dead</span>)) &#123; <span class="comment">//note: group 状态已经变成 dead, offset 返回 -1（INVALID_OFFSET）</span></div><div class="line">        topicPartitionsOpt.getOrElse(<span class="type">Seq</span>.empty[<span class="type">TopicPartition</span>]).map &#123; topicPartition =&gt;</div><div class="line">          (topicPartition, <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="string">""</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">        &#125;.toMap</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">          topicPartitionsOpt <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: 返回 group 消费的所有 tp 的 offset 信息（只返回这边已有 offset 的 tp）</span></div><div class="line">              <span class="comment">// Return offsets for all partitions owned by this consumer group. (this only applies to consumers</span></div><div class="line">              <span class="comment">// that commit offsets to Kafka.)</span></div><div class="line">              group.allOffsets.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">                topicPartition -&gt; <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(offsetAndMetadata.offset, offsetAndMetadata.metadata, <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> <span class="type">Some</span>(topicPartitions) =&gt;</div><div class="line">              topicPartitionsOpt.getOrElse(<span class="type">Seq</span>.empty[<span class="type">TopicPartition</span>]).map &#123; topicPartition =&gt;</div><div class="line">                <span class="keyword">val</span> partitionData = group.offset(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">                  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: offset 没有的话就返回-1</span></div><div class="line">                    <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">OffsetFetchResponse</span>.<span class="type">INVALID_OFFSET</span>, <span class="string">""</span>, <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">                  <span class="keyword">case</span> <span class="type">Some</span>(offsetAndMetadata) =&gt;</div><div class="line">                    <span class="keyword">new</span> <span class="type">OffsetFetchResponse</span>.<span class="type">PartitionData</span>(offsetAndMetadata.offset, offsetAndMetadata.metadata, <span class="type">Errors</span>.<span class="type">NONE</span>)</div><div class="line">                &#125;</div><div class="line">                topicPartition -&gt; partitionData</div><div class="line">              &#125;.toMap</div><div class="line">          &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="OFFSET-COMMIT-请求处理"><a href="#OFFSET-COMMIT-请求处理" class="headerlink" title="OFFSET_COMMIT 请求处理"></a>OFFSET_COMMIT 请求处理</h4><p>对 OFFSET_COMMIT 请求的处理，部分内容已经介绍过，可以参考 <a href="http://matt33.com/2017/11/18/consumer-subscribe/#commit-offset-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86">commit offset 请求处理</a>，处理过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doCommitOffsets</span></span>(group: <span class="type">GroupMetadata</span>,</div><div class="line">                    memberId: <span class="type">String</span>,</div><div class="line">                    generationId: <span class="type">Int</span>,</div><div class="line">                    offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                    responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">var</span> delayedOffsetStore: <span class="type">Option</span>[<span class="type">DelayedStore</span>] = <span class="type">None</span></div><div class="line"></div><div class="line">  group synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (group.is(<span class="type">Dead</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId &lt; <span class="number">0</span> &amp;&amp; group.is(<span class="type">Empty</span>)) &#123;<span class="comment">//note: 来自 assign 的情况</span></div><div class="line">      <span class="comment">// the group is only using Kafka to store offsets</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (group.is(<span class="type">AwaitingSync</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!group.has(memberId)) &#123;<span class="comment">//note: 有可能 simple 与 high level 的冲突了,这里就直接拒绝相应的请求</span></div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">      completeAndScheduleNextHeartbeatExpiration(group, member)<span class="comment">//note: 更新下次需要的心跳时间</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback) <span class="comment">//note: commit offset</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// store the offsets without holding the group lock</span></div><div class="line">  delayedOffsetStore.foreach(groupManager.store)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里主要介绍一下 <code>groupManager.prepareStoreOffsets()</code> 方法，处理逻辑如下，这里简单说一下其 offset 存储的过程：</p>
<ol>
<li>首先过滤掉那些 offset 超过范围的 metadata；</li>
<li>将 offset 信息追加到 replicated log 中；</li>
<li>调用 <code>prepareOffsetCommit()</code> 方法，先将 offset 信息更新到 group 的 pendingOffsetCommits 中（这时还没有真正提交，后面如果失败的话，是可以撤回的）；</li>
<li>在 <code>putCacheCallback</code> 回调函数中，如果 offset 信息追加到 replicated log 成功，那么就更新缓存（将 group 的 pendingOffsetCommits 中的信息更新到 offset 变量中）。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Store offsets by appending it to the replicated log and then inserting to cache</div><div class="line"> */</div><div class="line"><span class="comment">//note: 记录 commit 的 offset</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareStoreOffsets</span></span>(group: <span class="type">GroupMetadata</span>,</div><div class="line">                        consumerId: <span class="type">String</span>,</div><div class="line">                        generationId: <span class="type">Int</span>,</div><div class="line">                        offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                        responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>): <span class="type">Option</span>[<span class="type">DelayedStore</span>] = &#123;</div><div class="line">  <span class="comment">// first filter out partitions with offset metadata size exceeding limit</span></div><div class="line">  <span class="comment">//note: 首先过滤掉 offset 信息超过范围的 metadata</span></div><div class="line">  <span class="keyword">val</span> filteredOffsetMetadata = offsetMetadata.filter &#123; <span class="keyword">case</span> (_, offsetAndMetadata) =&gt;</div><div class="line">    validateOffsetMetadataLength(offsetAndMetadata.metadata)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// construct the message set to append</span></div><div class="line">  <span class="comment">//note: 构造一个 msg set 追加</span></div><div class="line">  getMagicAndTimestamp(partitionFor(group.groupId)) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>((magicValue, timestampType, timestamp)) =&gt;</div><div class="line">      <span class="keyword">val</span> records = filteredOffsetMetadata.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">        <span class="type">Record</span>.create(magicValue, timestampType, timestamp,</div><div class="line">          <span class="type">GroupMetadataManager</span>.offsetCommitKey(group.groupId, topicPartition), <span class="comment">//note: key是一个三元组: group、topic、partition</span></div><div class="line">          <span class="type">GroupMetadataManager</span>.offsetCommitValue(offsetAndMetadata))</div><div class="line">      &#125;.toSeq</div><div class="line"></div><div class="line">      <span class="keyword">val</span> offsetTopicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, partitionFor(group.groupId))</div><div class="line"></div><div class="line">      <span class="comment">//note: 将 offset 信息追加到 replicated log 中</span></div><div class="line">      <span class="keyword">val</span> entries = <span class="type">Map</span>(offsetTopicPartition -&gt; <span class="type">MemoryRecords</span>.withRecords(timestampType, compressionType, records:_*))</div><div class="line"></div><div class="line">      <span class="comment">// set the callback function to insert offsets into cache after log append completed</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">putCacheCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</div><div class="line">        <span class="comment">// the append response should only contain the topics partition</span></div><div class="line">        <span class="keyword">if</span> (responseStatus.size != <span class="number">1</span> || ! responseStatus.contains(offsetTopicPartition))</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Append status %s should only have one partition %s"</span></div><div class="line">            .format(responseStatus, offsetTopicPartition))</div><div class="line"></div><div class="line">        <span class="comment">// construct the commit response status and insert</span></div><div class="line">        <span class="comment">// the offset and metadata to cache if the append status has no error</span></div><div class="line">        <span class="keyword">val</span> status = responseStatus(offsetTopicPartition)</div><div class="line"></div><div class="line">        <span class="keyword">val</span> responseCode =</div><div class="line">          group synchronized &#123;</div><div class="line">            <span class="keyword">if</span> (status.error == <span class="type">Errors</span>.<span class="type">NONE</span>) &#123; <span class="comment">//note: 如果已经追加到了 replicated log 中了,那么就更新其缓存</span></div><div class="line">              <span class="keyword">if</span> (!group.is(<span class="type">Dead</span>)) &#123; <span class="comment">//note: 更新到 group 的 offset 中</span></div><div class="line">                filteredOffsetMetadata.foreach &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">                  group.completePendingOffsetWrite(topicPartition, offsetAndMetadata)</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line">              <span class="type">Errors</span>.<span class="type">NONE</span>.code</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">              <span class="keyword">if</span> (!group.is(<span class="type">Dead</span>)) &#123;</div><div class="line">                filteredOffsetMetadata.foreach &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">                  group.failPendingOffsetWrite(topicPartition, offsetAndMetadata)</div><div class="line">                &#125;</div><div class="line">              &#125;</div><div class="line"></div><div class="line">              debug(<span class="string">s"Offset commit <span class="subst">$filteredOffsetMetadata</span> from group <span class="subst">$&#123;group.groupId&#125;</span>, consumer <span class="subst">$consumerId</span> "</span> +</div><div class="line">                <span class="string">s"with generation <span class="subst">$generationId</span> failed when appending to log due to <span class="subst">$&#123;status.error.exceptionName&#125;</span>"</span>)</div><div class="line"></div><div class="line">              <span class="comment">// transform the log append error code to the corresponding the commit status error code</span></div><div class="line">              <span class="keyword">val</span> responseError = status.error <span class="keyword">match</span> &#123;</div><div class="line">                <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">NOT_ENOUGH_REPLICAS</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">NOT_ENOUGH_REPLICAS_AFTER_APPEND</span> =&gt;</div><div class="line">                  <span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span></div><div class="line"></div><div class="line">                <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NOT_LEADER_FOR_PARTITION</span> =&gt;</div><div class="line">                  <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span></div><div class="line"></div><div class="line">                <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">MESSAGE_TOO_LARGE</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">RECORD_LIST_TOO_LARGE</span></div><div class="line">                     | <span class="type">Errors</span>.<span class="type">INVALID_FETCH_SIZE</span> =&gt;</div><div class="line">                  <span class="type">Errors</span>.<span class="type">INVALID_COMMIT_OFFSET_SIZE</span></div><div class="line"></div><div class="line">                <span class="keyword">case</span> other =&gt; other</div><div class="line">              &#125;</div><div class="line"></div><div class="line">              responseError.code</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line"></div><div class="line">        <span class="comment">// compute the final error codes for the commit response</span></div><div class="line">        <span class="keyword">val</span> commitStatus = offsetMetadata.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">          <span class="keyword">if</span> (validateOffsetMetadataLength(offsetAndMetadata.metadata))</div><div class="line">            (topicPartition, responseCode)</div><div class="line">          <span class="keyword">else</span></div><div class="line">            (topicPartition, <span class="type">Errors</span>.<span class="type">OFFSET_METADATA_TOO_LARGE</span>.code)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// finally trigger the callback logic passed from the API layer</span></div><div class="line">        responseCallback(commitStatus)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      group synchronized &#123;</div><div class="line">        group.prepareOffsetCommit(offsetMetadata) <span class="comment">//note: 添加到 group 的 pendingOffsetCommits 中</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="type">Some</span>(<span class="type">DelayedStore</span>(entries, putCacheCallback)) <span class="comment">//note:</span></div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">val</span> commitStatus = offsetMetadata.map &#123; <span class="keyword">case</span> (topicPartition, offsetAndMetadata) =&gt;</div><div class="line">        (topicPartition, <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">      &#125;</div><div class="line">      responseCallback(commitStatus)</div><div class="line">      <span class="type">None</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="group-相关的处理"><a href="#group-相关的处理" class="headerlink" title="group 相关的处理"></a>group 相关的处理</h3><p>这一小节主要介绍 GroupCoordinator 处理 group 相关的请求。</p>
<h4 id="JOIN-GROUP-和-SYNC-GROUP请求处理"><a href="#JOIN-GROUP-和-SYNC-GROUP请求处理" class="headerlink" title="JOIN_GROUP 和 SYNC_GROUP请求处理"></a>JOIN_GROUP 和 SYNC_GROUP请求处理</h4><p>这两个请求的处理实际上在 <a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a> 中已经详细介绍过，这里就不再陈述。</p>
<h4 id="DESCRIBE-GROUPS-请求处理"><a href="#DESCRIBE-GROUPS-请求处理" class="headerlink" title="DESCRIBE_GROUPS 请求处理"></a>DESCRIBE_GROUPS 请求处理</h4><p>关于 DESCRIBE_GROUPS 请求处理实现如下，主要是返回 group 中各个 member 的详细信息，包含的变量信息为 <code>memberId, clientId, clientHost, metadata(protocol), assignment</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleDescribeGroup</span></span>(groupId: <span class="type">String</span>): (<span class="type">Errors</span>, <span class="type">GroupSummary</span>) = &#123;</div><div class="line">  <span class="keyword">if</span> (!isActive.get) &#123;</div><div class="line">    (<span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>, <span class="type">GroupCoordinator</span>.<span class="type">EmptyGroup</span>)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;</div><div class="line">    (<span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>, <span class="type">GroupCoordinator</span>.<span class="type">EmptyGroup</span>)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;</div><div class="line">    (<span class="type">Errors</span>.<span class="type">GROUP_LOAD_IN_PROGRESS</span>, <span class="type">GroupCoordinator</span>.<span class="type">EmptyGroup</span>)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    groupManager.getGroup(groupId) <span class="keyword">match</span> &#123; <span class="comment">//note: 返回 group 详细信息,主要是 member 的详细信息</span></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; (<span class="type">Errors</span>.<span class="type">NONE</span>, <span class="type">GroupCoordinator</span>.<span class="type">DeadGroup</span>)</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt;</div><div class="line">        group synchronized &#123;</div><div class="line">          (<span class="type">Errors</span>.<span class="type">NONE</span>, group.summary)</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="LEAVE-GROUP-请求处理"><a href="#LEAVE-GROUP-请求处理" class="headerlink" title="LEAVE_GROUP 请求处理"></a>LEAVE_GROUP 请求处理</h4><p>在什么情况下，Server 会收到 LEAVE_GROUP 的请求呢？一般来说是：</p>
<ol>
<li>consumer 调用 <code>unsubscribe()</code> 方法，取消了对所有 topic 的订阅时；</li>
<li>consumer 的心跳线程超时时，这时 consumer 会主动发送 LEAVE_GROUP 请求；</li>
<li>在 server 端，如果在给定的时间没收到 client 的心跳请求，这时候会自动触发 LEAVE_GROUP 操作。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleLeaveGroup</span></span>(groupId: <span class="type">String</span>, memberId: <span class="type">String</span>, responseCallback: <span class="type">Short</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (!isActive.get) &#123;</div><div class="line">    responseCallback(<span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>.code)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;</div><div class="line">    responseCallback(<span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;</div><div class="line">    responseCallback(<span class="type">Errors</span>.<span class="type">GROUP_LOAD_IN_PROGRESS</span>.code)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    groupManager.getGroup(groupId) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">// if the group is marked as dead, it means some other thread has just removed the group</span></div><div class="line">        <span class="comment">// from the coordinator metadata; this is likely that the group has migrated to some other</span></div><div class="line">        <span class="comment">// coordinator OR the group is in a transient unstable phase. Let the consumer to retry</span></div><div class="line">        <span class="comment">// joining without specified consumer id,</span></div><div class="line">        responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt;</div><div class="line">        group synchronized &#123;</div><div class="line">          <span class="keyword">if</span> (group.is(<span class="type">Dead</span>) || !group.has(memberId)) &#123;</div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">            removeHeartbeatForLeavingMember(group, member)<span class="comment">//<span class="doctag">NOTE:</span> 认为心跳完成</span></div><div class="line">            onMemberFailure(group, member)<span class="comment">//<span class="doctag">NOTE:</span> 从 group 移除当前 member,并进行 rebalance</span></div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onMemberFailure</span></span>(group: <span class="type">GroupMetadata</span>, member: <span class="type">MemberMetadata</span>) &#123;</div><div class="line">  trace(<span class="string">"Member %s in group %s has failed"</span>.format(member.memberId, group.groupId))</div><div class="line">  group.remove(member.memberId)<span class="comment">//<span class="doctag">NOTE:</span> 从 Group 移除当前 member 信息</span></div><div class="line">  group.currentState <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Dead</span> | <span class="type">Empty</span> =&gt;</div><div class="line">    <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt; maybePrepareRebalance(group)<span class="comment">//<span class="doctag">NOTE:</span> 进行 rebalance</span></div><div class="line">    <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt; joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))<span class="comment">//<span class="doctag">NOTE:</span> 检查 join-group 是否可以完成</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出，GroupCoordinator 在处理 LEAVE_GROUP 请求时，实际上就是调用了 <code>onMemberFailure()</code> 方法，从 group 移除了失败的 member 的，并且将进行相应的状态转换：</p>
<ol>
<li>如果 group 原来是在 Dead 或 Empty 时，那么由于 group 本来就没有 member，就不再进行任何操作；</li>
<li>如果 group 原来是在 Stable 或 AwaitingSync 时，那么将会执行 <code>maybePrepareRebalance()</code> 方法，进行 rebalance 操作（后面的过程就跟最开始 join-group 时一样，参考源码分析六）；</li>
<li>如果 group 已经在 PreparingRebalance 状态了，那么这里将检查一下 join-group 的延迟操作是否完成了，如果操作完成了，那么 GroupCoordinator 就会向 group 的 member 发送 join-group response，然后将状态更新为 AwaitingSync.</li>
</ol>
<h3 id="HEARTBEAT-心跳请求处理"><a href="#HEARTBEAT-心跳请求处理" class="headerlink" title="HEARTBEAT 心跳请求处理"></a>HEARTBEAT 心跳请求处理</h3><p>心跳请求是非常重要的请求之一：</p>
<ol>
<li>对于 Server 端来说，它是 GroupCoordinator 判断一个 consumer member 是否存活的重要条件，如果其中一个 consumer 在给定的时间没有发送心跳请求，那么就会将这个 consumer 从这个 group 中移除，并执行 rebalance 操作；</li>
<li>对于 Client 端而言，心跳请求是 client 感应 group 状态变化的一个重要中介，比如：此时有一个新的 consumer 加入到 consumer group 中了，这时候会进行 rebalace 操作，group 端的状态会发送变化，当 group 其他 member 发送心跳请求，GroupCoordinator 就会通知 client 此时这个 group 正处于 rebalance 阶段，让它们 rejoin group。</li>
</ol>
<p>GroupCoordinator 处理心跳请求的过程如下所示。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> Server 端处理心跳请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleHeartbeat</span></span>(groupId: <span class="type">String</span>,</div><div class="line">                  memberId: <span class="type">String</span>,</div><div class="line">                  generationId: <span class="type">Int</span>,</div><div class="line">                  responseCallback: <span class="type">Short</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line"><span class="keyword">if</span> (!isActive.get) &#123;<span class="comment">//<span class="doctag">NOTE:</span> GroupCoordinator 已经失败</span></div><div class="line">  responseCallback(<span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>.code)</div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;<span class="comment">//<span class="doctag">NOTE:</span> 当前的 GroupCoordinator 不包含这个 group</span></div><div class="line">  responseCallback(<span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;<span class="comment">//<span class="doctag">NOTE:</span> group 的状态信息正在 loading,直接返回成功结果</span></div><div class="line">  <span class="comment">// the group is still loading, so respond just blindly</span></div><div class="line">  responseCallback(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line">  groupManager.getGroup(groupId) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> 当前 GroupCoordinator 不包含这个 group</span></div><div class="line">      responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt; <span class="comment">//<span class="doctag">NOTE:</span> 包含这个 group</span></div><div class="line">      group synchronized &#123;</div><div class="line">        group.currentState <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Dead</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 的状态已经变为 dead,意味着 group 的 meta 已经被清除,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">            <span class="comment">// if the group is marked as dead, it means some other thread has just removed the group</span></div><div class="line">            <span class="comment">// from the coordinator metadata; this is likely that the group has migrated to some other</span></div><div class="line">            <span class="comment">// coordinator OR the group is in a transient unstable phase. Let the member retry</span></div><div class="line">            <span class="comment">// joining without the specified member id,</span></div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">Empty</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 的状态为 Empty, 意味着 group 的成员为空,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">            responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">AwaitingSync</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 状态为 AwaitingSync, 意味着 group 刚 rebalance 结束</span></div><div class="line">            <span class="keyword">if</span> (!group.has(memberId)) <span class="comment">//<span class="doctag">NOTE:</span> group 不包含这个 member,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">            <span class="keyword">else</span> <span class="comment">//<span class="doctag">NOTE:</span> 返回当前 group 正在进行 rebalance,要求 client rejoin 这个 group</span></div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code)</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt; <span class="comment">//<span class="doctag">NOTE:</span> group 状态为 PreparingRebalance</span></div><div class="line">            <span class="keyword">if</span> (!group.has(memberId)) &#123; <span class="comment">//<span class="doctag">NOTE:</span> group 不包含这个 member,返回 UNKNOWN_MEMBER_ID 错误</span></div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//<span class="doctag">NOTE:</span> 正常处理心跳信息,并返回 REBALANCE_IN_PROGRESS 错误</span></div><div class="line">              <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">              <span class="comment">//note: 更新心跳时间,认为心跳完成,并监控下次的调度情况（超时的话,会把这个 member 从 group 中移除）</span></div><div class="line">              completeAndScheduleNextHeartbeatExpiration(group, member)</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code)</div><div class="line">            &#125;</div><div class="line"></div><div class="line">          <span class="keyword">case</span> <span class="type">Stable</span> =&gt;</div><div class="line">            <span class="keyword">if</span> (!group.has(memberId)) &#123;</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code)</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//<span class="doctag">NOTE:</span> 正确处理心跳信息</span></div><div class="line">              <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">              <span class="comment">//note: 更新心跳时间,认为心跳完成,并监控下次的调度情况（超时的话,会把这个 member 从 group 中移除）</span></div><div class="line">              completeAndScheduleNextHeartbeatExpiration(group, member)</div><div class="line">              responseCallback(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="group-的状态机"><a href="#group-的状态机" class="headerlink" title="group 的状态机"></a>group 的状态机</h2><p>GroupCoordinator 在进行 group 和 offset 相关的管理操作时，有一项重要的工作就是处理和维护 group 状态的变化，一个 Group 状态机如下如所示。</p>
<p><img src="/images/kafka/group.png" alt="Group 状态机"></p>
<p>在这个状态机中，最核心就是 rebalance 操作，简单说一下 rebalance 过程：</p>
<ol>
<li>当一些条件发生时将 group 从 <strong>Stable</strong> 状态变为 <strong>PreparingRebalance</strong>；</li>
<li>然后就是等待 group 中的所有 consumer member 发送 join-group 请求加入 group，如果都已经发送 join-group 请求，此时 GroupCoordinator 会向所有 member 发送 join-group response，那么 group 的状态变为 <strong>AwaitingSync</strong>；</li>
<li>leader consumer 会收到各个 member 订阅的 topic 详细信息，等待其分配好 partition 后，通过 sync-group 请求将结果发给 GroupCoordinator（非 leader consumer 发送的 sync-group 请求的 data 是为空的）；</li>
<li>如果 GroupCoordinator 收到了 leader consumer 发送的 response，获取到了这个 group 各个 member 所分配的 topic-partition 列表，group 的状态就会变成 <strong>Stable</strong>。</li>
</ol>
<p>这就是一次完整的 rebalance 过程。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;突然发现距离上一篇文章，已经过去两个多月了，有两个月没有写博客了，之前定的是年前把这个系列写完，现在看来只能往后拖了，后面估计还有五篇文章左右，尽量在春节前完成吧。继续之前的内容开始讲解，这篇文章，主要是想把 GroupCoordinator 的内容总结一下，也算是开始了 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Consumer 两种 commit 机制和 partition 分配机制（九）</title>
    <link href="http://matt33.com/2017/11/19/consumer-two-summary/"/>
    <id>http://matt33.com/2017/11/19/consumer-two-summary/</id>
    <published>2017-11-19T07:15:32.000Z</published>
    <updated>2017-11-19T11:44:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>紧接着上篇文章，这篇文章讲述 Consumer 提供的两种 commit 机制和两种 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。</p>
<h2 id="两种-commit-机制"><a href="#两种-commit-机制" class="headerlink" title="两种 commit 机制"></a>两种 commit 机制</h2><p>先看下两种不同的 commit 机制，一种是同步 commit，一种是异步 commit，既然其作用都是 offset commit，应该不难猜到它们底层使用接口都是一样的，其调用流程如下图所示：</p>
<p><img src="/images/kafka/two-commit.png" alt="两种 commit 机制"></p>
<h3 id="同步-commit"><a href="#同步-commit" class="headerlink" title="同步 commit"></a>同步 commit</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 对 poll() 中返回的所有 topics 和 partition 列表进行 commit</span></div><div class="line"><span class="comment">// 这个方法只能将 offset 提交 Kafka 中，Kafka 将会在每次 rebalance 之后的第一次拉取或启动时使用同步 commit</span></div><div class="line"><span class="comment">// 这是同步 commit，它将会阻塞进程，直到 commit 成功或者遇到一些错误</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">()</span> </span>&#123;&#125;</div><div class="line"><span class="comment">// 只对指定的 topic-partition 列表进行 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span> </span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>其实，从上图中，就已经可以看出，同步 commit 的实现方式，<code>client.poll()</code> 方法会阻塞直到这个request 完成或超时才会返回。</p>
<h3 id="异步-commit"><a href="#异步-commit" class="headerlink" title="异步 commit"></a>异步 commit</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 异步 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">()</span> </span>&#123;&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span> </span>&#123;&#125;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span> </span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>而对于异步的 commit，最后调用的都是 <code>doCommitOffsetsAsync</code> 方法，其具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//org.apache.kafka.clients.consumer.internals.ConsumerCoordinator</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doCommitOffsetsAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, <span class="keyword">final</span> OffsetCommitCallback callback)</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.subscriptions.needRefreshCommits();</div><div class="line">    RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets);<span class="comment">//note: 发送 offset-commit 请求</span></div><div class="line">    <span class="keyword">final</span> OffsetCommitCallback cb = callback == <span class="keyword">null</span> ? defaultOffsetCommitCallback : callback;</div><div class="line">    future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;Void&gt;() &#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(Void value)</span> </span>&#123;</div><div class="line">            <span class="keyword">if</span> (interceptors != <span class="keyword">null</span>)</div><div class="line">                interceptors.onCommit(offsets);</div><div class="line"></div><div class="line">            <span class="comment">//note: 添加成功的请求,以唤醒相应的回调函数</span></div><div class="line">            completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(cb, offsets, <span class="keyword">null</span>));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">            Exception commitException = e;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (e <span class="keyword">instanceof</span> RetriableException)</div><div class="line">                commitException = <span class="keyword">new</span> RetriableCommitFailedException(e);</div><div class="line"></div><div class="line">            <span class="comment">//note: 添加失败的请求,以唤醒相应的回调函数</span></div><div class="line">            completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(cb, offsets, commitException));</div><div class="line">        &#125;</div><div class="line">    &#125;);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在异步 commit 中，可以添加相应的回调函数，如果 request 处理成功或处理失败，ConsumerCoordinator 会通过 <code>invokeCompletedOffsetCommitCallbacks()</code> 方法唤醒相应的回调函数。</p>
<p>关于 offset commit 请求的处理见上一篇文章中的<a href="http://matt33.com/2017/11/18/consumer-subscribe/#commit-offset-请求处理">Offset Commit 请求处理</a>，对于提交的 offset，GroupCoordinator 会记录在 GroupMetadata 对象中。</p>
<h2 id="两种-partition-分配机制"><a href="#两种-partition-分配机制" class="headerlink" title="两种 partition 分配机制"></a>两种 partition 分配机制</h2><p>consumer 提供的两种不同 partition 分配策略，可以通过 <code>partition.assignment.strategy</code> 参数进行配置，默认情况下使用的是 <code>org.apache.kafka.clients.consumer.RangeAssignor</code>，Kafka 中提供另一种 partition 的分配策略 <code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>，它们关系如下图所示：</p>
<p><img src="/images/kafka/PartitionAssignor.png" alt="Kafka 系统内置的两种 partition 分配机制"></p>
<p>通过上图可以看出，用户可以自定义相应的 partition 分配机制，只需要继承这个 <code>AbstractPartitionAssignor</code> 抽象类即可。</p>
<h3 id="AbstractPartitionAssignor"><a href="#AbstractPartitionAssignor" class="headerlink" title="AbstractPartitionAssignor"></a>AbstractPartitionAssignor</h3><p>AbstractPartitionAssignor 有一个抽象方法，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Perform the group assignment given the partition counts and member subscriptions</div><div class="line"> * <span class="doctag">@param</span> partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded</div><div class="line"> *                           from this map.</div><div class="line"> * <span class="doctag">@param</span> subscriptions Map from the memberId to their respective topic subscription</div><div class="line"> * <span class="doctag">@return</span> Map from each member to the list of partitions assigned to them.</div><div class="line"> */</div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 根据 partitionsPerTopic 和 subscriptions 进行分配,具体的实现会在子类中实现（不同的子类的实现各异）</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,Map&lt;String, List&lt;String&gt;&gt; subscriptions);</div></pre></td></tr></table></figure>
<p><code>assign()</code> 这个方法，有两个参数：</p>
<ul>
<li><code>partitionsPerTopic</code>：所订阅的每个 topic 与其 partition 数的对应关系，metadata 没有的 topic 将会被移除；</li>
<li><code>subscriptions</code>：每个 consumerId 与其所订阅的 topic 列表的关系。</li>
</ul>
<p><code>RangeAssignor</code> 和 <code>RoundRobinAssignor</code> 通过这个方法 <code>assign()</code> 的实现，来进行相应的 partition 分配。</p>
<h3 id="RangeAssignor-分配模式"><a href="#RangeAssignor-分配模式" class="headerlink" title="RangeAssignor 分配模式"></a>RangeAssignor 分配模式</h3><p>直接看一下这个方法的实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,</div><div class="line">                                                Map&lt;String, List&lt;String&gt;&gt; subscriptions) &#123;</div><div class="line">    Map&lt;String, List&lt;String&gt;&gt; consumersPerTopic = consumersPerTopic(subscriptions);<span class="comment">//note: (topic, List&lt;consumerId&gt;)</span></div><div class="line">    Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (String memberId : subscriptions.keySet())</div><div class="line">        assignment.put(memberId, <span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());<span class="comment">//note: 初始化</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;String&gt;&gt; topicEntry : consumersPerTopic.entrySet()) &#123;</div><div class="line">        String topic = topicEntry.getKey();</div><div class="line">        List&lt;String&gt; consumersForTopic = topicEntry.getValue();</div><div class="line"></div><div class="line">        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);</div><div class="line">        <span class="keyword">if</span> (numPartitionsForTopic == <span class="keyword">null</span>)</div><div class="line">            <span class="keyword">continue</span>;</div><div class="line"></div><div class="line">        Collections.sort(consumersForTopic);</div><div class="line"></div><div class="line">        <span class="comment">//note: 假设 partition 有 7个,consumer 有5个</span></div><div class="line">        <span class="keyword">int</span> numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();<span class="comment">//note: 1</span></div><div class="line">        <span class="keyword">int</span> consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();<span class="comment">//note: 2</span></div><div class="line"></div><div class="line">        List&lt;TopicPartition&gt; partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, n = consumersForTopic.size(); i &lt; n; i++) &#123;</div><div class="line">            <span class="comment">//note: i=0, start: 0, length: 2, topic-partition: p0,p1</span></div><div class="line">            <span class="comment">//note: i=1, start: 2, length: 2, topic-partition: p2,p3</span></div><div class="line">            <span class="comment">//note: i=2, start: 4, length: 1, topic-partition: p4</span></div><div class="line">            <span class="comment">//note: i=3, start: 5, length: 1, topic-partition: p5</span></div><div class="line">            <span class="comment">//note: i=4, start: 6, length: 1, topic-partition: p6</span></div><div class="line">            <span class="keyword">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</div><div class="line">            <span class="keyword">int</span> length = numPartitionsPerConsumer + (i + <span class="number">1</span> &gt; consumersWithExtraPartition ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">            assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> assignment;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>假设 topic 的 partition 数为 numPartitionsForTopic，group 中订阅这个 topic 的 member 数为 <code>consumersForTopic.size()</code>，首先需要算出两个值：</p>
<ul>
<li><code>numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size()</code>：表示平均每个 consumer 会分配到几个 partition；    </li>
<li><code>consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size()</code>：表示平均分配后还剩下多少个 partition 未分配。</li>
</ul>
<p>分配的规则是：对于剩下的那些 partition 分配到前 consumersWithExtraPartition 个 consumer 上，也就是前 consumersWithExtraPartition 个 consumer 获得 topic-partition 列表会比后面多一个。</p>
<p>在上述的程序中，举了一个例子，假设有一个 topic 有 7 个 partition，group 有5个 consumer，这个5个 consumer 都订阅这个 topic，那么 range 的分配方式如下：</p>
<ul>
<li>consumer 0：start: 0, length: 2, topic-partition: p0,p1；</li>
<li>consumer 1：start: 2, length: 2, topic-partition: p2,p3；</li>
<li>consumer 2：start: 4, length: 1, topic-partition: p4；</li>
<li>consumer 3：start: 5, length: 1, topic-partition: p5；</li>
<li>consumer 4：start: 6, length: 1, topic-partition: p6</li>
</ul>
<p>而如果 group 中有 consumer 没有订阅这个 topic，那么这个 consumer 将不会参与分配。下面再举个例子，将有两个 topic，一个 partition 有5个，一个 partition 有7个，group 有5个 consumer，但是只有前3个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下：</p>
<table>
<thead>
<tr>
<th>consumer</th>
<th>订阅的 topic1 的列表</th>
<th>订阅的 topic2 的列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>consumer 0</td>
<td>t1p0, t1p1</td>
<td>t2p0, t2p1</td>
</tr>
<tr>
<td>consumer 1</td>
<td>t1p2, t1p3</td>
<td>t2p2, t2p3</td>
</tr>
<tr>
<td>consumer 2</td>
<td>t1p4</td>
<td>t2p4</td>
</tr>
<tr>
<td>consumer 3</td>
<td></td>
<td>t2p5</td>
</tr>
<tr>
<td>consumer 4</td>
<td></td>
<td>t2p6</td>
</tr>
</tbody>
</table>
<h3 id="RoundRobinAssignor"><a href="#RoundRobinAssignor" class="headerlink" title="RoundRobinAssignor"></a>RoundRobinAssignor</h3><p>这个是 roundrobin 的实现，其实现方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="keyword">public</span> Map&lt;String, List&lt;TopicPartition&gt;&gt; assign(Map&lt;String, Integer&gt; partitionsPerTopic,</div><div class="line">                                                Map&lt;String, List&lt;String&gt;&gt; subscriptions) &#123;</div><div class="line">    Map&lt;String, List&lt;TopicPartition&gt;&gt; assignment = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (String memberId : subscriptions.keySet())</div><div class="line">        assignment.put(memberId, <span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());</div><div class="line"></div><div class="line">    CircularIterator&lt;String&gt; assigner = <span class="keyword">new</span> CircularIterator&lt;&gt;(Utils.sorted(subscriptions.keySet()));<span class="comment">//note: 环行迭代</span></div><div class="line">    <span class="keyword">for</span> (TopicPartition partition : allPartitionsSorted(partitionsPerTopic, subscriptions)) &#123;</div><div class="line">        <span class="keyword">final</span> String topic = partition.topic();</div><div class="line">        <span class="keyword">while</span> (!subscriptions.get(assigner.peek()).contains(topic))<span class="comment">//note: 遍历直到找到订阅这个 topic 的 partition</span></div><div class="line">            assigner.next();</div><div class="line">        assignment.get(assigner.next()).add(partition);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> assignment;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> List&lt;TopicPartition&gt; <span class="title">allPartitionsSorted</span><span class="params">(Map&lt;String, Integer&gt; partitionsPerTopic,</span></span></div><div class="line">                                                Map&lt;String, List&lt;String&gt;&gt; subscriptions) &#123;</div><div class="line">    SortedSet&lt;String&gt; topics = <span class="keyword">new</span> TreeSet&lt;&gt;();<span class="comment">//<span class="doctag">NOTE:</span> 所有的 topics（有序）</span></div><div class="line">    <span class="keyword">for</span> (List&lt;String&gt; subscription : subscriptions.values())</div><div class="line">        topics.addAll(subscription);</div><div class="line"></div><div class="line">    List&lt;TopicPartition&gt; allPartitions = <span class="keyword">new</span> ArrayList&lt;&gt;();<span class="comment">//<span class="doctag">NOTE:</span> 订阅的 Topic的所有的 TopicPartition 集合</span></div><div class="line">    <span class="keyword">for</span> (String topic : topics) &#123;</div><div class="line">        Integer numPartitionsForTopic = partitionsPerTopic.get(topic);</div><div class="line">        <span class="keyword">if</span> (numPartitionsForTopic != <span class="keyword">null</span>)</div><div class="line">            <span class="comment">//note: topic 的所有 partition 都添加进去</span></div><div class="line">            allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> allPartitions;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>roundrobin 的实现原则，简单来说就是：列出所有 topic-partition 和列出所有的 consumer member，然后开始分配，一轮之后继续下一轮，假设有有一个 topic，它有7个 partition，group 有3个 consumer 都订阅了这个 topic，那么其分配方式为：</p>
<table>
<thead>
<tr>
<th>consumer</th>
<th>分配列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>consumer 0</td>
<td>tp0, tp3, tp6</td>
</tr>
<tr>
<td>consumer 1</td>
<td>tp1, tp4</td>
</tr>
<tr>
<td>consumer 2</td>
<td>tp2, tp5</td>
</tr>
</tbody>
</table>
<p>对于多个 topic 的订阅，将有两个 topic，一个 partition 有5个，一个 partition 有7个，group 有5个 consumer，但是只有前3个订阅第一个 topic，而另一个 topic 是所有 consumer 都订阅了，那么其分配结果如下：</p>
<table>
<thead>
<tr>
<th>consumer</th>
<th>订阅的 topic1 的列表</th>
<th>订阅的 topic2 的列表</th>
</tr>
</thead>
<tbody>
<tr>
<td>consumer 0</td>
<td>t1p0, t1p3</td>
<td>t2p0, t2p5</td>
</tr>
<tr>
<td>consumer 1</td>
<td>t1p1, t1p4</td>
<td>t2p1, t2p6</td>
</tr>
<tr>
<td>consumer 2</td>
<td>t1p2</td>
<td>t2p2</td>
</tr>
<tr>
<td>consumer 3</td>
<td></td>
<td>t2p3</td>
</tr>
<tr>
<td>consumer 4</td>
<td></td>
<td>t2p4</td>
</tr>
</tbody>
</table>
<p>roundrobin 分配方式与 range 的分配方式还是略有不同。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;紧接着上篇文章，这篇文章讲述 Consumer 提供的两种 commit 机制和两种 partition 分配机制，具体如何使用是需要用户结合具体的场景进行选择，本文讲述一下其底层实现。&lt;/p&gt;
&lt;h2 id=&quot;两种-commit-机制&quot;&gt;&lt;a href=&quot;#两种-comm
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Consumer 两种订阅模式（八）</title>
    <link href="http://matt33.com/2017/11/18/consumer-subscribe/"/>
    <id>http://matt33.com/2017/11/18/consumer-subscribe/</id>
    <published>2017-11-18T06:43:50.000Z</published>
    <updated>2017-12-21T15:31:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>在前面两篇 Kafka Consumer 的文章中，Consumer Poll 模型这部分基本上已经完整结束，Consumer 这块的文章计划是要写五篇，这篇是 Consumer 这块的第三篇，本来计划是要从其中的三个小块细节内容着手，这三个地方有一个相同之处，那就是在 Kafka Consumer 中都提供了两个不同的解决方案，但具体怎么去使用是需要用户根据自己的业务场景去配置，这里会讲述其底层的具体实现（但为了阅读得更为方便，本来计划的这篇文章将拆分为两篇来，第一篇先讲述第一点，后面两点放在一起讲述）。</p>
<p>本篇文章讲述的这三点内容分别是：</p>
<ol>
<li>consumer 的两种订阅模式， <code>subscribe()</code>和<code>assign()</code> 模式，一种是 topic 粒度（使用 group 管理），一种是 topic-partition 粒度（用户自己去管理）；</li>
<li>consumer 的两种 commit 实现，<code>commitAsync()</code>和<code>commitSync()</code>，即同步 commit 和异步 commit；</li>
<li>consumer 提供的两种不同 <code>partition.assignment.strategy</code>，这是关于一个 group 订阅一些 topic 后，group 内各个 consumer 实例的 partition 分配策略。</li>
</ol>
<p>0.9.X 之前 Kafka Consumer 是支持两个不同的订阅模型 —— high level 和 simple level，这两种模型的最大区别是：第一个其 offset 管理是由 Kafka 来做，包括 rebalance 操作，第二个则是由使用者自己去做，自己去管理相关的 offset，以及自己去进行 rebalance。</p>
<p>在新版的 consumer 中对 high level 和 simple level 的接口实现了统一，简化了相应的相应的编程模型。</p>
<h2 id="订阅模式"><a href="#订阅模式" class="headerlink" title="订阅模式"></a>订阅模式</h2><p>在新版的 Consumer 中，high level 模型现在叫做订阅模式，KafkaConsumer 提供了三种 API，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 订阅指定的 topic 列表,并且会自动进行动态 partition 订阅</span></div><div class="line"><span class="comment">// 当发生以下情况时,会进行 rebalance: 1.订阅的 topic 列表改变; 2.topic 被创建或删除; 3.consumer 线程 die; 4. 加一个新的 consumer 线程</span></div><div class="line"><span class="comment">// 当发生 rebalance 时，会唤醒 ConsumerRebalanceListener 线程</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span></span>&#123;&#125;</div><div class="line"><span class="comment">// 同上，但是这里没有设置 listener</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span> </span>&#123;&#125;</div><div class="line"><span class="comment">//note: 订阅那些满足一定规则(pattern)的 topic</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span></span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>以上三种 API 都是按照 topic 级别去订阅，可以动态地获取其分配的 topic-partition，这是使用 <strong>Group 动态管理</strong>，它不能与手动 partition 管理一起使用。当监控到发生下面的事件时，Group 将会触发 rebalance 操作：</p>
<ol>
<li>订阅的 topic 列表变化；</li>
<li>topic 被创建或删除；</li>
<li>consumer group 的某个 consumer 实例挂掉；</li>
<li>一个新的 consumer 实例通过 <code>join</code> 方法加入到一个 group 中。</li>
</ol>
<p>在这种模式下，当 KafkaConsumer 调用 pollOnce 方法时，第一步会首先加入到一个 group 中，并获取其分配的 topic-partition 列表（见<a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a>），前面两篇文章都是以这种情况来讲述的。</p>
<p>这里介绍一下当调用 <code>subscribe()</code> 方法之后，Consumer 所做的事情，分两种情况介绍，一种按 topic 列表订阅，一种是按 pattern 模式订阅：</p>
<ol>
<li>topic 列表订阅<ol>
<li>更新 SubscriptionState 中记录的 <code>subscription</code>（记录的是订阅的 topic 列表），将 SubscriptionType 类型设置为 <strong>AUTO_TOPICS</strong>；</li>
<li>更新 metadata 中的 topic 列表（<code>topics</code> 变量），并请求更新 metadata；</li>
</ol>
</li>
<li>pattern 模式订阅<ol>
<li>更新 SubscriptionState 中记录的 <code>subscribedPattern</code>，设置为 pattern，将 SubscriptionType 类型设置为 <strong>AUTO_PATTERN</strong>；</li>
<li>设置 Metadata 的 needMetadataForAllTopics 为 true，即在请求 metadata 时，需要更新所有 topic 的 metadata 信息，设置后再请求更新 metadata；</li>
<li>调用 <code>coordinator.updatePatternSubscription()</code> 方法，遍历所有 topic 的 metadata，找到所有满足 pattern 的 topic 列表，更新到 SubscriptionState 的 <code>subscriptions</code> 和 Metadata 的 <code>topics</code> 中；</li>
<li>通过在 ConsumerCoordinator 中调用 <code>addMetadataListener()</code> 方法在 Metadata 中添加 listener 当每次 metadata update 时就调用第三步的方法更新，但是只有当本地缓存的 topic 列表与现在要订阅的 topic 列表不同时，才会触发 rebalance 操作。</li>
</ol>
</li>
</ol>
<p>其他部分，两者基本一样，只是 pattern 模型在每次更新 topic-metadata 时，获取全局的 topic 列表，如果发现有新加入的符合条件的 topic，就立马去订阅，其他的地方，包括 Group 管理、topic-partition 的分配都是一样的。</p>
<h2 id="分配模式"><a href="#分配模式" class="headerlink" title="分配模式"></a>分配模式</h2><p>下面来看一下 Consumer 提供的分配模式，熟悉 0.8.X 版本的人，可能会把这种方法称为 simple consumer 的接口，当调用 <code>assign()</code> 方法手动分配 topic-partition 列表时，是不会使用 consumer 的 Group 管理机制，也即是当 consumer group member 变化或 topic 的 metadata 信息变化时是不会触发 rebalance 操作的。比如：当 topic 的 partition 增加时，这里是无法感知，需要用户进行相应的处理，Apache Flink 就是使用的这种方式，后续我会写篇文章介绍 Flink 是如何实现这种机制的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 手动向 consumer 分配一些 topic-partition 列表，并且这个接口不允许增加分配的 topic-partition 列表，将会覆盖之前分配的 topic-partition 列表，如果给定的 topic-partition 列表为空，它的作用将会与 unsubscribe() 方法一样。</span></div><div class="line"><span class="comment">//note: 这种手动 topic 分配是不会使用 consumer 的 group 管理，当 group 的 member 变化或 topic 的 metadata 变化也不会触发 rebalance 操作。</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;&#125;</div></pre></td></tr></table></figure>
<p>这里来看一下 Kafka 提供的 Group 管理到底是什么？</p>
<p>如果有印象的话，在<a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a>中介绍 Poll 模型的第一步中，详细介绍了 <code>ConsumerCoordinator.poll()</code> 方法，我们再来看一下这个方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: 它确保了这个 group 的 coordinator 是已知的,并且这个 consumer 是已经加入到了 group 中,也用于 offset 周期性的 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">poll</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">    invokeCompletedOffsetCommitCallbacks();<span class="comment">// note: 用于测试</span></div><div class="line"></div><div class="line">    <span class="comment">// note: Step1 通过 subscribe() 方法订阅 topic,并且 coordinator 未知,初始化 Consumer Coordinator</span></div><div class="line">    <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned() &amp;&amp; coordinatorUnknown()) &#123;</div><div class="line">        <span class="comment">// note: 获取 GroupCoordinator 地址,并且建立连接</span></div><div class="line">        ensureCoordinatorReady();</div><div class="line">        now = time.milliseconds();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// note: Step2 判断是否需要重新加入 group,如果订阅的 partition 变化或则分配的 partition 变化时,需要 rejoin</span></div><div class="line">    <span class="comment">// note: 如果订阅模式不是 AUTO_TOPICS 或 AUTO_PATTERN,直接跳过</span></div><div class="line">    <span class="keyword">if</span> (needRejoin()) &#123;</div><div class="line">        <span class="comment">// note: rejoin group 之前先刷新一下 metadata（对于 AUTO_PATTERN 而言）</span></div><div class="line">        <span class="keyword">if</span> (subscriptions.hasPatternSubscription())</div><div class="line">            client.ensureFreshMetadata();</div><div class="line"></div><div class="line">        <span class="comment">// note: 确保 group 是 active; 加入 group; 分配订阅的 partition</span></div><div class="line">        ensureActiveGroup();</div><div class="line">        now = time.milliseconds();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// note: Step3 检查心跳线程运行是否正常,如果心跳线程失败,则抛出异常,反之更新 poll 调用的时间</span></div><div class="line">    <span class="comment">// note: 发送心跳请求是在 ensureCoordinatorReady() 中调用的</span></div><div class="line">    pollHeartbeat(now);</div><div class="line">    <span class="comment">// note: Step4 自动 commit 时,当定时达到时,进行自动 commit</span></div><div class="line">    maybeAutoCommitOffsetsAsync(now);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果使用的是 assign 模式，也即是非 AUTO_TOPICS 或 AUTO_PATTERN 模式时，Consumer 实例在调用 poll 方法时，是不会向 GroupCoordinator 发送 join-group/sync-group/heartbeat 请求的，也就是说 GroupCoordinator 是拿不到这个 Consumer 实例的相关信息，也不会去维护这个 member 是否存活，这种情况下就需要用户自己管理自己的处理程序。但是在这种模式是可以进行 offset commit的。</p>
<h3 id="commit-offset-请求处理"><a href="#commit-offset-请求处理" class="headerlink" title="commit offset 请求处理"></a>commit offset 请求处理</h3><p>当 Kafka Serve 端受到来自 client 端的 Offset Commit 请求时，其处理逻辑如下所示，是在 <code>kafka.coordinator.GroupCoordinator</code> 中实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// kafka.coordinator.GroupCoordinator</span></div><div class="line"><span class="comment">//note: GroupCoordinator 处理 Offset Commit 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleCommitOffsets</span></span>(groupId: <span class="type">String</span>,</div><div class="line">                        memberId: <span class="type">String</span>,</div><div class="line">                        generationId: <span class="type">Int</span>,</div><div class="line">                        offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                        responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (!isActive.get) &#123;</div><div class="line">    responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">GROUP_COORDINATOR_NOT_AVAILABLE</span>.code))</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!isCoordinatorForGroup(groupId)) &#123;</div><div class="line">    responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code))</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isCoordinatorLoadingInProgress(groupId)) &#123;</div><div class="line">    responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">GROUP_LOAD_IN_PROGRESS</span>.code))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    groupManager.getGroup(groupId) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">if</span> (generationId &lt; <span class="number">0</span>) &#123;</div><div class="line">          <span class="comment">// the group is not relying on Kafka for group management, so allow the commit</span></div><div class="line">          <span class="comment">//note: 不使用 group-coordinator 管理的情况</span></div><div class="line">          <span class="comment">//note: 如果 groupID不存在,就新建一个 GroupMetadata, 其group 状态为 Empty,否则就返回已有的 groupid</span></div><div class="line">          <span class="comment">//note: 如果 simple 的 groupId 与一个 active 的 group 重复了,这里就有可能被覆盖掉了</span></div><div class="line">          <span class="keyword">val</span> group = groupManager.addGroup(<span class="keyword">new</span> <span class="type">GroupMetadata</span>(groupId))</div><div class="line">          doCommitOffsets(group, memberId, generationId, offsetMetadata, responseCallback)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="comment">// or this is a request coming from an older generation. either way, reject the commit</span></div><div class="line">          <span class="comment">//note: 过期的 offset-commit</span></div><div class="line">          responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code))</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(group) =&gt;</div><div class="line">        doCommitOffsets(group, memberId, generationId, offsetMetadata, responseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 真正的处理逻辑</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doCommitOffsets</span></span>(group: <span class="type">GroupMetadata</span>,</div><div class="line">                    memberId: <span class="type">String</span>,</div><div class="line">                    generationId: <span class="type">Int</span>,</div><div class="line">                    offsetMetadata: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>],</div><div class="line">                    responseCallback: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">var</span> delayedOffsetStore: <span class="type">Option</span>[<span class="type">DelayedStore</span>] = <span class="type">None</span></div><div class="line"></div><div class="line">  group synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (group.is(<span class="type">Dead</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId &lt; <span class="number">0</span> &amp;&amp; group.is(<span class="type">Empty</span>)) &#123;<span class="comment">//note: 来自 assign 的情况</span></div><div class="line">      <span class="comment">// the group is only using Kafka to store offsets</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (group.is(<span class="type">AwaitingSync</span>)) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">REBALANCE_IN_PROGRESS</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!group.has(memberId)) &#123;<span class="comment">//note: 有可能 simple 与 high level 的冲突了,这里就直接拒绝相应的请求</span></div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">UNKNOWN_MEMBER_ID</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (generationId != group.generationId) &#123;</div><div class="line">      responseCallback(offsetMetadata.mapValues(_ =&gt; <span class="type">Errors</span>.<span class="type">ILLEGAL_GENERATION</span>.code))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> member = group.get(memberId)</div><div class="line">      completeAndScheduleNextHeartbeatExpiration(group, member)<span class="comment">//note: 更新下次需要的心跳时间</span></div><div class="line">      delayedOffsetStore = groupManager.prepareStoreOffsets(group, memberId, generationId,</div><div class="line">        offsetMetadata, responseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// store the offsets without holding the group lock</span></div><div class="line">  delayedOffsetStore.foreach(groupManager.store)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>处理过程如下：</p>
<ol>
<li>如果这个 group 还不存在（groupManager没有这个 group 信息），并且 generation 为 -1（一般情况下应该都是这样），就新建一个 GroupMetadata, 其 Group 状态为 Empty；</li>
<li>现在 group 已经存在，就调用 <code>doCommitOffsets()</code> 提交 offset；</li>
<li>如果是来自 assign 模式的请求，并且其对应的 group 的状态为 Empty（generationId &lt; 0 &amp;&amp; group.is(Empty)），那么就记录这个 offset；</li>
<li>如果是来自 assign 模式的请求，但这个 group 的状态不为 Empty（!group.has(memberId)），也就是说，这个 group 已经处在活跃状态，assign 模式下的 group 是不会处于的活跃状态的，可以认为是 assign 模式使用的 group.id 与 subscribe 模式下使用的 group 相同，这种情况下就会拒绝 assign 模式下的这个 offset commit 请求。</li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>根据上面的讲述，这里做一下小节，如下图所示：</p>
<p><img src="/images/kafka/two-subscribe.png" alt="两种订阅模式"></p>
<p>简单做一下总结：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>不同之处</th>
<th>相同之处</th>
</tr>
</thead>
<tbody>
<tr>
<td>subscribe()</td>
<td>使用 Kafka Group 管理，自动进行 rebalance 操作</td>
<td>可以在 Kafka 保存 offset</td>
</tr>
<tr>
<td>assign()</td>
<td>用户自己进行相关的处理</td>
<td>也可以进行 offset commit，但是尽量保证 group.id 唯一性，如果使用一个与上面模式一样的 group，offset commit 请求将会被拒绝</td>
</tr>
</tbody>
</table>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面两篇 Kafka Consumer 的文章中，Consumer Poll 模型这部分基本上已经完整结束，Consumer 这块的文章计划是要写五篇，这篇是 Consumer 这块的第三篇，本来计划是要从其中的三个小块细节内容着手，这三个地方有一个相同之处，那就是在 K
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Consumer Poll 模型（七）</title>
    <link href="http://matt33.com/2017/11/11/consumer-pollonce/"/>
    <id>http://matt33.com/2017/11/11/consumer-pollonce/</id>
    <published>2017-11-11T03:00:45.000Z</published>
    <updated>2017-11-19T06:51:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇问文章中已经介绍一个 Consumer 实例如何加入到一个 group 中，它是 Consumer Poll 模型第一步要做的事件，本文会完整讲述一个 Consumer 实例在 poll 模型过程中会做哪些事情，只有理解了 poll 模型才能更好地理解 Consumer 端的处理逻辑。</p>
<h2 id="Consumer-示例"><a href="#Consumer-示例" class="headerlink" title="Consumer 示例"></a>Consumer 示例</h2><p>这里以一个 Consumer 的实例代码作为开始，一个比较常见的 Consumer 示例代码如下所示，其主要包含一下几个步骤：</p>
<ol>
<li>构造 Propertity，进行 consumer 相关的配置；</li>
<li>创建 KafkaConsumer 的对象 consumer；</li>
<li>订阅相应的 topic 列表；</li>
<li>调用 consumer 的 poll 方法拉取订阅的消息。</li>
</ol>
<p>前面两步在 Consumer 底层上只是创建了一个 consumer 对象，第三步只有记录一下订阅的 topic 信息，consumer 实际的操作都是第四步，也就是在 <code>poll</code> 方法中实现的，这也是 poll 模型对于理解 consumer 设计非常重要的原因。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * 自动 commit 的情况</div><div class="line"> * Created by matt on 16/7/14.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerAutoOffsetCommit</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String topic;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String group;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        Properties props = <span class="keyword">new</span> Properties();</div><div class="line">        topic=args[<span class="number">0</span>];</div><div class="line">        group=args[<span class="number">1</span>]; <span class="comment">// auto-offset-commit</span></div><div class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"XXX:9092,XXX:9092"</span>);</div><div class="line">        props.put(<span class="string">"group.id"</span>, group);</div><div class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</div><div class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>); <span class="comment">// 自动commit</span></div><div class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>); <span class="comment">// 自动commit的间隔</span></div><div class="line">        props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</div><div class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</div><div class="line">        consumer.subscribe(Arrays.asList(topic)); <span class="comment">// 可消费多个topic,组成一个list</span></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</div><div class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</div><div class="line">                System.out.printf(<span class="string">"offset = %d, key = %s, value = %s \n"</span>, record.offset(), record.key(), record.value());</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    Thread.sleep(<span class="number">100</span>);</div><div class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Poll-模型综述"><a href="#Poll-模型综述" class="headerlink" title="Poll 模型综述"></a>Poll 模型综述</h2><p>当一个 consumer 对象创建之后，只有 poll 方法调用时，consumer 才会真正去连接 kafka 集群，进行相关的操作，其 poll 方法具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//timeout(ms): buffer 中的数据未就绪情况下，等待的最长时间，如果设置为0，立即返回 buffer 中已经就绪的数据</span></div><div class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout)</span> </span>&#123;</div><div class="line">    acquire();</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">if</span> (timeout &lt; <span class="number">0</span>)</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Timeout must not be negative"</span>);</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.hasNoSubscriptionOrUserAssignment())</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Consumer is not subscribed to any topics or assigned any partitions"</span>);</div><div class="line"></div><div class="line">        <span class="comment">// poll for new data until the timeout expires</span></div><div class="line">        <span class="keyword">long</span> start = time.milliseconds();</div><div class="line">        <span class="keyword">long</span> remaining = timeout;</div><div class="line">        <span class="keyword">do</span> &#123;</div><div class="line">            Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollOnce(remaining);</div><div class="line">            <span class="comment">//note: 从订阅的 partition 中拉取数据,pollOnce() 才是对 Consumer 客户端拉取数据的核心实现</span></div><div class="line">            <span class="keyword">if</span> (!records.isEmpty()) &#123;</div><div class="line">                <span class="comment">// 在返回数据之前，发送下次的 fetch 请求，避免用户在下次获取数据时线程 block</span></div><div class="line">                <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.pendingRequestCount() &gt; <span class="number">0</span>)</div><div class="line">                    client.pollNoWakeup();</div><div class="line"></div><div class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors == <span class="keyword">null</span>)</div><div class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> ConsumerRecords&lt;&gt;(records);</div><div class="line">                <span class="keyword">else</span></div><div class="line">                    <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">long</span> elapsed = time.milliseconds() - start;</div><div class="line">            remaining = timeout - elapsed;</div><div class="line">        &#125; <span class="keyword">while</span> (remaining &gt; <span class="number">0</span>);</div><div class="line"></div><div class="line">        <span class="keyword">return</span> ConsumerRecords.empty();</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        release();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>consumer <code>poll</code> 方法主要做了以下几件事情：</p>
<ol>
<li>检查这个 consumer 是否订阅的相应的 topic-partition；</li>
<li>调用 <code>pollOnce()</code> 方法获取相应的 records；</li>
<li>在返回获取的 records 前，发送下一次的 fetch 请求，避免用户在下次请求时线程 block 在 <code>pollOnce()</code> 方法中；</li>
<li>如果在给定的时间（timeout）内获取不到可用的 records，返回空数据。</li>
</ol>
<p>这里可以看出，poll 方法的真正实现是在 pollOnce 方法中，poll 方法通过 pollOnce 方法获取可用的数据。</p>
<h3 id="pollOnce-方法"><a href="#pollOnce-方法" class="headerlink" title="pollOnce 方法"></a>pollOnce 方法</h3><p>这里看下 pollOnce 方法主要做了哪些事情，其具体实现如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: 一次 poll 过程，除了获取新数据外，还会做一些必要的 offset-commit 核 reset-offset  的操作</span></div><div class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollOnce(<span class="keyword">long</span> timeout) &#123;</div><div class="line">    <span class="comment">// note： 1. 获取 GroupCoordinator 地址并连接、加入 Group、sync Group、自动 commit, join 及 sync 期间 group 会进行 rebalance</span></div><div class="line">    coordinator.poll(time.milliseconds());</div><div class="line">    <span class="comment">// note: 2. 更新订阅的 topic-partition 的 offset（如果订阅的 topic-partition list 没有有效的 offset 的情况下）</span></div><div class="line">    <span class="keyword">if</span> (!subscriptions.hasAllFetchPositions())</div><div class="line">        updateFetchPositions(<span class="keyword">this</span>.subscriptions.missingFetchPositions());</div><div class="line"></div><div class="line">    <span class="comment">// note: 3. 获取 fetcher 已经拉取到的数据</span></div><div class="line">    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</div><div class="line">    <span class="keyword">if</span> (!records.isEmpty())</div><div class="line">        <span class="keyword">return</span> records;</div><div class="line">    <span class="comment">// note: 说明上次 fetch 到是的数据已经全部拉取了,需要再次发送 fetch 请求,从 broker 拉取数据</span></div><div class="line"></div><div class="line">    <span class="comment">// note: 4. 发送 fetch 请求,会从多个 topic-partition 拉取数据（只要对应的 topic-partition 没有未完成的请求）</span></div><div class="line">    fetcher.sendFetches();</div><div class="line"></div><div class="line">    <span class="keyword">long</span> now = time.milliseconds();</div><div class="line">    <span class="keyword">long</span> pollTimeout = Math.min(coordinator.timeToNextPoll(now), timeout);</div><div class="line"></div><div class="line">    <span class="comment">//note: 5. 调用 poll 方法发送请求（底层发送请求的接口）</span></div><div class="line">    client.poll(pollTimeout, now, <span class="keyword">new</span> PollCondition() &#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">shouldBlock</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">return</span> !fetcher.hasCompletedFetches();<span class="comment">//note: 有完成的 fetcher 请求的话,这里就不会 block,但是 block 也是有最大时间限制</span></div><div class="line">        &#125;</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    <span class="comment">//note: 6. 如果 group 需要 rebalance,直接返回空数据,这样更快地让 group 进行稳定状态</span></div><div class="line">    <span class="keyword">if</span> (coordinator.needRejoin())</div><div class="line">        <span class="keyword">return</span> Collections.emptyMap();</div><div class="line"></div><div class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>pollOnce 可以简单分为6步来看，其作用分别如下：</p>
<ol>
<li><code>coordinator.poll()</code>：获取 GroupCoordinator 的地址，并建立相应 tcp 连接，发送 join-group、sync-group，之后才真正加入到了一个 group 中，这时会获取其要消费的 topic-partition 列表，如果设置了自动 commit，也会在这一步进行 commit，具体可见 <a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a>，总之，对于一个新建的 group，group 状态将会从 <strong>Empty –&gt; PreparingRebalance –&gt; AwaiSync –&gt; Stable</strong>；</li>
<li><code>updateFetchPositions()</code>： 在上一步中已经获取到了这个 consumer 实例要订阅的 topic-partition list，这一步更新其 fetch-position offset，以便进行拉取；</li>
<li><code>fetcher.sendFetches()</code>：返回其 fetched records，并更新其 fetch-position offset，只有在 offset-commit 时（自动 commit 时，是在第一步实现的），才会更新其 committed offset；</li>
<li><code>fetcher.sendFetches()</code>：只要订阅的 topic-partition list 没有未处理的 fetch 请求，就发送对这个 topic-partition 的 fetch 请求，在真正发送时，还是会按 node 级别去发送，leader 是同一个 node 的 topic-partition 会合成一个请求去发送；</li>
<li><code>client.poll()</code>：调用底层 NetworkClient 提供的接口去发送相应的请求；</li>
<li><code>coordinator.needRejoin()</code>：如果当前实例分配的 topic-partition 列表发送了变化，那么这个 consumer group 就需要进行 rebalance。</li>
</ol>
<h3 id="PollOnce-整体流程"><a href="#PollOnce-整体流程" class="headerlink" title="PollOnce 整体流程"></a>PollOnce 整体流程</h3><p>通过上面一节的介绍，pollOnce 方法做的事情现在已经有了一个比较清晰的认识，PollOnce 其详细流程图如下所示：</p>
<p><img src="/images/kafka/pollonce_only.png" alt="pollOnce 总体流程"></p>
<p>从上图可以看出，Consumer 在实现上，其调用还是比较复杂，不过复杂的地方都封装在底层了，Consumer 的网络模型如下图所示：</p>
<p><img src="/images/kafka/consumer-network.png" alt="Consumer 网络模型"></p>
<p>上面这张图，主要介绍了 KafkaConsumer 的封装模型。</p>
<h2 id="PollOnce-模型详解"><a href="#PollOnce-模型详解" class="headerlink" title="PollOnce 模型详解"></a>PollOnce 模型详解</h2><p>这一节详细讲述一下 PollOnce 模型的实现，主要讲述其前4步，最后的两步比较简单（跟之前也有重复），这里就不再细讲了。</p>
<h3 id="coordinator-poll"><a href="#coordinator-poll" class="headerlink" title="coordinator.poll()"></a><code>coordinator.poll()</code></h3><p>这部分的内容还是挺多的，其详细内部见：<a href="http://matt33.com/2017/10/22/consumer-join-group/">Kafka 源码解析之 Consumer 如何加入一个 Group（六）</a>，一个 consumer 实例在这一步实现的内容是：</p>
<ol>
<li>获取 GroupCoordinator 的地址，并建立相应 tcp 连接；</li>
<li>发送 join-group 请求，然后 group 将会进行 rebalance；</li>
<li>发送 sync-group 请求，之后才正在加入到了一个 group 中，这时会通过请求获取其要消费的 topic-partition 列表；</li>
<li>如果设置了自动 commit，也会在这一步进行 commit offset。</li>
</ol>
<p>通过前面的 pollOnce 流程图也能清楚地看到各个部分是在哪个方法中实现的。</p>
<h3 id="updateFetchPositions"><a href="#updateFetchPositions" class="headerlink" title="updateFetchPositions()"></a><code>updateFetchPositions()</code></h3><p>这个方法主要是用来更新这个 consumer 实例订阅的 topic-partition 列表的 fetch-offset 信息。</p>
<p>在 Fetcher 中，这个 consumer 实例订阅的每个 topic-partition 都会有一个对应的 TopicPartitionState 对象，在这个对象中会记录以下这些内容：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 记录 tp 的一些 offset 信息</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TopicPartitionState</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> Long position; <span class="comment">// last consumed position</span></div><div class="line">    <span class="keyword">private</span> Long highWatermark; <span class="comment">// the high watermark from last fetch</span></div><div class="line">    <span class="keyword">private</span> OffsetAndMetadata committed;  <span class="comment">// last committed position</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> paused;  <span class="comment">// whether this partition has been paused by the user</span></div><div class="line">    <span class="keyword">private</span> OffsetResetStrategy resetStrategy;  <span class="comment">// the strategy to use if the offset needs resetting</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中需要关注的几个属性是：</p>
<ol>
<li>position：Fetcher 下次去拉取时的 offset，Fecher 在拉取时需要知道这个值；</li>
<li>committed：consumer 已经处理完的最新一条消息的 offset，consumer 主动调用 offset-commit 时会更新这个值；</li>
<li>resetStrategy：这 topic-partition offset 重置的策略，重置之后，这个策略就会改为 null，防止再次操作。</li>
</ol>
<p><code>updateFetchPositions()</code> 这个方法的目的就是为了获取其订阅的每个 topic-partition 对应的 position，这样 Fetcher 才知道从哪个 offset 开始去拉取这个 topic-partition 的数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//将 the fetch position 设置为 the committed position（如果有 committed offset 的话），否则就使用配置的重置策略去设置 offset</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">updateFetchPositions</span><span class="params">(Set&lt;TopicPartition&gt; partitions)</span> </span>&#123;</div><div class="line">    <span class="comment">//note: 先重置那些调用 seekToBegin 和 seekToEnd 的 offset 的 tp,设置其  the fetch position 的 offset</span></div><div class="line">    fetcher.resetOffsetsIfNeeded(partitions);</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (!subscriptions.hasAllFetchPositions(partitions)) &#123;</div><div class="line">        <span class="comment">//note: 获取所有分配 tp 的 offset, 即 committed offset, 更新到 TopicPartitionState 中的 committed offset 中</span></div><div class="line">        coordinator.refreshCommittedOffsetsIfNeeded();</div><div class="line"></div><div class="line">        <span class="comment">//note: 如果 the fetch position 值无效,则将上步获取的 committed offset 设置为 the fetch position</span></div><div class="line">        fetcher.updateFetchPositions(partitions);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述过程主要分为三步，可以结合前面的流程来看，这里就不再详细去介绍其下面几层调用的实现了：</p>
<ol>
<li><code>fetcher.resetOffsetsIfNeeded()</code>：处理那些 resetStrategy 不为 null 的 topic-partition（一般是使用了 <code>seekToBegin()</code> 和 <code>seekToEnd()</code> 方法的 topic-partition），Fetcher 会发送 list-offset 请求去获取相应的 offset，实际上在获取时，是根据时间戳（earliest：-2， latest：-1）去查找的相应的 offset，因为从 0.10.2 之后系统在保存 topic 数据时，会保存相应的 timestrap 信息；</li>
<li><code>coordinator.refreshCommittedOffsetsIfNeeded()</code>：发送 offset-fetch 请求获取其所订阅的所有 topic-partition 的 commited offset，如果这个 group 没有关于这个 topic-partition 的 offset 就会根据其默认的 <code>auto.offset.reset</code> 信息返回 -1或-2，并将获取到的信息更新到 committed offset 中；</li>
<li><code>fetcher.updateFetchPositions()</code>：如果 the fetch position 还没有有效值（第一步处理的那些 topic-partition 已经有了有效值），那么就将 the fetch position 设置为 committed offset。</li>
</ol>
<p>到这一步，这个 consumer 订阅的 topic-partition list 都有了相应的 the fetch position，Fetcher 在发送 fetch 请求就知道应该从哪个 offset 开始去拉取这个 topic-partition，自此，发送 fetch 请求前的准备都已经完成。</p>
<h3 id="fetcher-sendFetches"><a href="#fetcher-sendFetches" class="headerlink" title="fetcher.sendFetches()"></a><code>fetcher.sendFetches()</code></h3><p>这个虽然是 pollOnce 的第四步，但我们这里放在第三步来讲，只有在发送 fetch 请求后，才能调用 <code>fetcher.fetchedRecords()</code> 获取到其拉取的数据，所以这里先介绍这个方法，其具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向订阅的所有 partition （只要该 leader 暂时没有拉取请求）所在 leader 发送 fetch 请求</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">//note: 1 创建 Fetch Request</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; fetchRequestMap = createFetchRequests();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchRequest.Builder&gt; fetchEntry : fetchRequestMap.entrySet()) &#123;</div><div class="line">        <span class="keyword">final</span> FetchRequest.Builder request = fetchEntry.getValue();</div><div class="line">        <span class="keyword">final</span> Node fetchTarget = fetchEntry.getKey();</div><div class="line"></div><div class="line">        log.debug(<span class="string">"Sending fetch for partitions &#123;&#125; to broker &#123;&#125;"</span>, request.fetchData().keySet(), fetchTarget);</div><div class="line">        <span class="comment">//note: 2 发送 Fetch Request</span></div><div class="line">        client.send(fetchTarget, request)</div><div class="line">                .addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</div><div class="line">                        FetchResponse response = (FetchResponse) resp.responseBody();</div><div class="line">                        <span class="keyword">if</span> (!matchesRequestedPartitions(request, response)) &#123;</div><div class="line">                            <span class="comment">// obviously we expect the broker to always send us valid responses, so this check</span></div><div class="line">                            <span class="comment">// is mainly for test cases where mock fetch responses must be manually crafted.</span></div><div class="line">                            log.warn(<span class="string">"Ignoring fetch response containing partitions &#123;&#125; since it does not match "</span> +</div><div class="line">                                    <span class="string">"the requested partitions &#123;&#125;"</span>, response.responseData().keySet(),</div><div class="line">                                    request.fetchData().keySet());</div><div class="line">                            <span class="keyword">return</span>;</div><div class="line">                        &#125;</div><div class="line"></div><div class="line">                        Set&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> HashSet&lt;&gt;(response.responseData().keySet());</div><div class="line">                        FetchResponseMetricAggregator metricAggregator = <span class="keyword">new</span> FetchResponseMetricAggregator(sensors, partitions);</div><div class="line"></div><div class="line">                        <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&gt; entry : response.responseData().entrySet()) &#123;</div><div class="line">                            TopicPartition partition = entry.getKey();</div><div class="line">                            <span class="keyword">long</span> fetchOffset = request.fetchData().get(partition).offset;</div><div class="line">                            FetchResponse.PartitionData fetchData = entry.getValue();</div><div class="line">                            completedFetches.add(<span class="keyword">new</span> CompletedFetch(partition, fetchOffset, fetchData, metricAggregator,</div><div class="line">                                    request.version()));<span class="comment">//note: 成功后加入 CompletedFetch</span></div><div class="line">                        &#125;</div><div class="line"></div><div class="line">                        sensors.fetchLatency.record(resp.requestLatencyMs());</div><div class="line">                        sensors.fetchThrottleTimeSensor.record(response.getThrottleTime());</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">                        log.debug(<span class="string">"Fetch request to &#123;&#125; for partitions &#123;&#125; failed"</span>, fetchTarget, request.fetchData().keySet(), e);</div><div class="line">                    &#125;</div><div class="line">                &#125;);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> fetchRequestMap.size();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在发送的 fetch 的过程中，总共分为以下两步：</p>
<ol>
<li><code>createFetchRequests()</code>：为订阅的所有 topic-partition list 创建 fetch 请求（只要该topic-partition 没有还在处理的请求），创建的 fetch 请求依然是按照 node 级别创建的；</li>
<li><code>client.send()</code>：发送 fetch 请求，并设置相应的 Listener，请求处理成功的话，就加入到 completedFetches 中，在加入这个 completedFetches 集合时，是按照 topic-partition 级别去加入，这样也就方便了后续的处理。</li>
</ol>
<p>从这里可以看出，在每次发送 fetch 请求时，都会向所有可发送的 topic-partition 发送 fetch 请求，调用一次 <code>fetcher.sendFetches</code>，拉取到的数据，可需要多次 pollOnce 循环才能处理完，因为 Fetcher 线程是在后台运行，这也保证了尽可能少地阻塞用户的处理线程，因为如果 Fetcher 中没有可处理的数据，用户的线程是会阻塞在 poll 方法中的。</p>
<h3 id="fetcher-fetchedRecords"><a href="#fetcher-fetchedRecords" class="headerlink" title="fetcher.fetchedRecords()"></a><code>fetcher.fetchedRecords()</code></h3><p>这个方法的作用就获取已经从 Server 拉取到的 Records，其源码实现如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 返回获取到的 the fetched records， 并更新 the consumed position</span></div><div class="line"><span class="keyword">public</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() &#123;</div><div class="line">    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; drained = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">int</span> recordsRemaining = maxPollRecords;<span class="comment">//<span class="doctag">NOTE:</span> 在 max.poll.records 中设置单词最大的拉取条数</span></div><div class="line"></div><div class="line">    <span class="keyword">while</span> (recordsRemaining &gt; <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">if</span> (nextInLineRecords == <span class="keyword">null</span> || nextInLineRecords.isDrained()) &#123; <span class="comment">//note: nextInLineRecords 为空时</span></div><div class="line">            CompletedFetch completedFetch = completedFetches.poll();<span class="comment">//note: 当一个 nextInLineRecords 处理完,就从 completedFetches 处理下一个完成的 Fetch 请求</span></div><div class="line">            <span class="keyword">if</span> (completedFetch == <span class="keyword">null</span>)</div><div class="line">                <span class="keyword">break</span>;</div><div class="line"></div><div class="line">            nextInLineRecords = parseCompletedFetch(completedFetch);<span class="comment">//note: 获取下一个要处理的 nextInLineRecords</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            TopicPartition partition = nextInLineRecords.partition;</div><div class="line">            List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = drainRecords(nextInLineRecords, recordsRemaining);<span class="comment">//note:拉取records,更新 position</span></div><div class="line">            <span class="keyword">if</span> (!records.isEmpty()) &#123;</div><div class="line">                List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = drained.get(partition);</div><div class="line">                <span class="keyword">if</span> (currentRecords == <span class="keyword">null</span>) &#123; <span class="comment">//note: 正常情况下,一个 node 只会发送一个 request,一般只会有一个</span></div><div class="line">                    drained.put(partition, records);</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = <span class="keyword">new</span> ArrayList&lt;&gt;(records.size() + currentRecords.size());</div><div class="line">                    newRecords.addAll(currentRecords);</div><div class="line">                    newRecords.addAll(records);</div><div class="line">                    drained.put(partition, newRecords);</div><div class="line">                &#125;</div><div class="line">                recordsRemaining -= records.size();</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> drained;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">private</span> List&lt;ConsumerRecord&lt;K, V&gt;&gt; drainRecords(PartitionRecords&lt;K, V&gt; partitionRecords, <span class="keyword">int</span> maxRecords) &#123;</div><div class="line">    <span class="keyword">if</span> (!subscriptions.isAssigned(partitionRecords.partition)) &#123;</div><div class="line">        <span class="comment">// this can happen when a rebalance happened before fetched records are returned to the consumer's poll call</span></div><div class="line">        log.debug(<span class="string">"Not returning fetched records for partition &#123;&#125; since it is no longer assigned"</span>, partitionRecords.partition);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// note that the consumed position should always be available as long as the partition is still assigned</span></div><div class="line">        <span class="keyword">long</span> position = subscriptions.position(partitionRecords.partition);</div><div class="line">        <span class="keyword">if</span> (!subscriptions.isFetchable(partitionRecords.partition)) &#123;<span class="comment">//note: 这个 tp 不能来消费了,比如调用 pause</span></div><div class="line">            log.debug(<span class="string">"Not returning fetched records for assigned partition &#123;&#125; since it is no longer fetchable"</span>, partitionRecords.partition);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitionRecords.fetchOffset == position) &#123;<span class="comment">//note: offset 对的上,也就是拉取是按顺序拉的</span></div><div class="line">            <span class="comment">//note: 获取该 tp 对应的records,并更新 partitionRecords 的 fetchOffset（用于判断是否顺序）</span></div><div class="line">            List&lt;ConsumerRecord&lt;K, V&gt;&gt; partRecords = partitionRecords.drainRecords(maxRecords);</div><div class="line">            <span class="keyword">if</span> (!partRecords.isEmpty()) &#123;</div><div class="line">                <span class="keyword">long</span> nextOffset = partRecords.get(partRecords.size() - <span class="number">1</span>).offset() + <span class="number">1</span>;</div><div class="line">                log.trace(<span class="string">"Returning fetched records at offset &#123;&#125; for assigned partition &#123;&#125; and update "</span> +</div><div class="line">                        <span class="string">"position to &#123;&#125;"</span>, position, partitionRecords.partition, nextOffset);</div><div class="line"></div><div class="line">                subscriptions.position(partitionRecords.partition, nextOffset);<span class="comment">//note: 更新消费的到 offset（ the fetch position）</span></div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="comment">//note: 获取 Lag（即 position与 hw 之间差值）,hw 为 null 时,才返回 null</span></div><div class="line">            Long partitionLag = subscriptions.partitionLag(partitionRecords.partition);</div><div class="line">            <span class="keyword">if</span> (partitionLag != <span class="keyword">null</span>)</div><div class="line">                <span class="keyword">this</span>.sensors.recordPartitionLag(partitionRecords.partition, partitionLag);</div><div class="line"></div><div class="line">            <span class="keyword">return</span> partRecords;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// these records aren't next in line based on the last consumed position, ignore them</span></div><div class="line">            <span class="comment">// they must be from an obsolete request</span></div><div class="line">            log.debug(<span class="string">"Ignoring fetched records for &#123;&#125; at offset &#123;&#125; since the current position is &#123;&#125;"</span>,</div><div class="line">                    partitionRecords.partition, partitionRecords.fetchOffset, position);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionRecords.drain();</div><div class="line">    <span class="keyword">return</span> Collections.emptyList();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>PartitionRecords 是 <code>parseCompletedFetch()</code> 处理后的类型，其成员变量如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionRecords</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> fetchOffset;</div><div class="line">    <span class="keyword">private</span> TopicPartition partition;</div><div class="line">    <span class="keyword">private</span> List&lt;ConsumerRecord&lt;K, V&gt;&gt; records;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> position = <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>consumer 的 Fetcher 处理从 server 获取的 fetch response 大致分为以下几个过程：</p>
<ol>
<li>通过 <code>completedFetches.poll()</code> 获取已经成功的 fetch response（在 <code>sendFetches()</code> 方法中会把成功的结果放在这个集合中，是拆分为 topic-partition 的粒度放进去的）；</li>
<li><code>parseCompletedFetch()</code> 处理上面获取的 completedFetch，构造成 PartitionRecords 类型；</li>
<li>通过 <code>drainRecords()</code> 方法处理 PartitionRecords 对象，在这个里面会去验证 fetchOffset 是否能对得上，只有 fetchOffset 是一致的情况下才会去处理相应的数据，并更新 the fetch offset 的信息，如果 fetchOffset 不一致，这里就不会处理，the fetch offset 就不会更新，下次 fetch 请求时是会接着 the fetch offset 的位置去请求相应的数据。</li>
<li>返回相应的 Records 数据。</li>
</ol>
<p>自此，consumer 的 poll 模型处理的逻辑就已经基本上讲完了，下篇博客会讲述下面三点内容：</p>
<ol>
<li>consumer 的两种订阅模型；</li>
<li>consumer 的同步 commit 和异步 commit；</li>
<li>consumer 提供的两种 <code>partition.assignment.strategy</code>。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇问文章中已经介绍一个 Consumer 实例如何加入到一个 group 中，它是 Consumer Poll 模型第一步要做的事件，本文会完整讲述一个 Consumer 实例在 poll 模型过程中会做哪些事情，只有理解了 poll 模型才能更好地理解 Consum
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Consumer 如何加入一个 Group（六）</title>
    <link href="http://matt33.com/2017/10/22/consumer-join-group/"/>
    <id>http://matt33.com/2017/10/22/consumer-join-group/</id>
    <published>2017-10-22T13:19:23.000Z</published>
    <updated>2017-10-22T16:11:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>距离上一篇博客（2017-09-10），到现在已经过去一个多月了，理论上这篇文章在上个月就应该写完，无奈拖延症又犯了，一直以这部分过于复杂为借口拖了好久，这两天逼了自己一把，先整理出其中的一篇，后续要加把劲，要不然今年的年度计划（年底前把这个系列写完）就完不成了，废话到此为止，下面步入正文。在 Kafka 中，Consumer 的复杂度要比 producer 高出很多，对于 Producer 而言，没有 producer 组的概念的、也不需要 care offset 等问题，而 Consumer 就不一样了，它需要关注的内容很多，需要考虑分布式消费（Consumer Group），为了防止重复消费或者部分数据未消费需要考虑 offset，这些都对 Consumer 的设计以及 Server 对其处理提出了很高的要求。本来计划是先进行综述，然后再分别介绍各个模块，现在打算反过来，先介绍各个模块，最后再进行综述，本篇为 Consumer 源码分析开篇，先从一个 Consumer 实例如何加入一个 Consumer Group 讲起。</p>
<p>这里的分析是以 0.10.2 为准，在 0.10.2 版的 KafkaConsumer 中，相比于老版的 KafkaConsumer（0.9以前的），新版从0.9开始做了很大改进，总结起来，其优势有以下两点：</p>
<ul>
<li>实现了 High Level 与 Simple Level Consumer API 的统一，极大地简化了实现的复杂度；</li>
<li>增加了 GroupCoordinator 角色，它作用是：<code>GroupCoordinator handles general group membership and offset management</code>；</li>
</ul>
<p>接下来会按照下面这个流程来讲述：</p>
<ol>
<li>GroupCoordinator 简单介绍；</li>
<li>Consumer poll 模型：join-group 是 poll 模型的第一步，其他部分后续再讲；</li>
<li>Consumer join-group 的详细过程以及在这个过程中 group 状态的变化。</li>
</ol>
<h2 id="GroupCoordinator-简介"><a href="#GroupCoordinator-简介" class="headerlink" title="GroupCoordinator 简介"></a>GroupCoordinator 简介</h2><p>这里先简单介绍一下 GroupCoordinator 这个角色，后续有一篇文章进行专门讲述，GroupCoordinator 是运行在 Kafka Broker 上的一个服务，每台 Broker 在运行时都会启动一个这样的服务，但一个 consumer 具体与哪个 Broker 上这个服务交互，就需要先介绍一下 <code>__consumer_offsets</code> 这个 topic。</p>
<h3 id="consumer-offsets-topic"><a href="#consumer-offsets-topic" class="headerlink" title="__consumer_offsets topic"></a><code>__consumer_offsets</code> topic</h3><p><code>__consumer_offsets</code> 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有50个 partition，每个 partition 三副本，如下图所示（只列出了30 个 partition）：</p>
<p><img src="/images/kafka/consumer_offsets.png" alt="__consumer_offsets topic"></p>
<h3 id="GroupCoordinator"><a href="#GroupCoordinator" class="headerlink" title="GroupCoordinator"></a>GroupCoordinator</h3><p>GroupCoordinator 是负责 consumer group member 管理以及 offset 管理。</p>
<p>每个 Consumer Group 都有其对应的 GroupCoordinator，但具体是由哪个 GroupCoordinator 负责与 group.id 的 hash 值有关，通过这个 <strong>abs(GroupId.hashCode()) % NumPartitions</strong> 来计算出一个值（其中，NumPartitions 是 <code>__consumer_offsets</code> 的 partition 数，默认是50个），这个值代表了 <code>__consumer_offsets</code> 的一个 partition，而这个 partition 的 leader 即为这个 Group 要交互的 GroupCoordinator 所在的节点。</p>
<h2 id="Consumer-poll-模型"><a href="#Consumer-poll-模型" class="headerlink" title="Consumer poll 模型"></a>Consumer poll 模型</h2><p>Consumer poll 方法的真正实现是在 <code>pollOnce()</code> 方法中，这里直接看下其源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Do one round of polling. In addition to checking for new data, this does any needed offset commits</div><div class="line"> * (if auto-commit is enabled), and offset resets (if an offset reset policy is defined).</div><div class="line"> * <span class="doctag">@param</span> timeout The maximum time to block in the underlying call to &#123;<span class="doctag">@link</span> ConsumerNetworkClient#poll(long)&#125;.</div><div class="line"> * <span class="doctag">@return</span> The fetched records (may be empty)</div><div class="line"> */</div><div class="line"><span class="comment">// note: 一次 poll 过程,包括检查新的数据、做一些必要的 commit 以及 offset  重置操作</span></div><div class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollOnce(<span class="keyword">long</span> timeout) &#123;</div><div class="line">    <span class="comment">// note： 1. 获取 GroupCoordinator 并连接、加入 Group、sync Group, 期间 group 会进行 rebalance 并获取</span></div><div class="line">    coordinator.poll(time.milliseconds());</div><div class="line">    <span class="comment">// assignment</span></div><div class="line"></div><div class="line">    <span class="comment">// fetch positions if we have partitions we're subscribed to that we</span></div><div class="line">    <span class="comment">// don't know the offset for</span></div><div class="line">    <span class="comment">// note: 2. 更新要拉取 partition 的 offset（如果需要更新的话）</span></div><div class="line">    <span class="keyword">if</span> (!subscriptions.hasAllFetchPositions())</div><div class="line">        updateFetchPositions(<span class="keyword">this</span>.subscriptions.missingFetchPositions());</div><div class="line"></div><div class="line">    <span class="comment">// if data is available already, return it immediately</span></div><div class="line">    <span class="comment">// note: 3. 获取 fetcher 已经拉取到的数据</span></div><div class="line">    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</div><div class="line">    <span class="keyword">if</span> (!records.isEmpty())</div><div class="line">        <span class="keyword">return</span> records;</div><div class="line">    <span class="comment">// note: 说明上次 fetch 到是的数据已经全部拉取了,需要再次发送 fetch 请求,从 broker 拉取数据</span></div><div class="line"></div><div class="line">    <span class="comment">// send any new fetches (won't resend pending fetches)</span></div><div class="line">    <span class="comment">// note: 4. 向订阅的所有 partition 发送 fetch 请求,会从多个 partition 拉取数据</span></div><div class="line">    fetcher.sendFetches();</div><div class="line"></div><div class="line">    <span class="keyword">long</span> now = time.milliseconds();</div><div class="line">    <span class="keyword">long</span> pollTimeout = Math.min(coordinator.timeToNextPoll(now), timeout);</div><div class="line"></div><div class="line">    <span class="comment">//note: 5. 调用 poll 方法发送数据</span></div><div class="line">    client.poll(pollTimeout, now, <span class="keyword">new</span> PollCondition() &#123;</div><div class="line">        <span class="meta">@Override</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">shouldBlock</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="comment">// since a fetch might be completed by the background thread, we need this poll condition</span></div><div class="line">            <span class="comment">// to ensure that we do not block unnecessarily in poll()</span></div><div class="line">            <span class="keyword">return</span> !fetcher.hasCompletedFetches();</div><div class="line">        &#125;</div><div class="line">    &#125;);</div><div class="line"></div><div class="line">    <span class="comment">// after the long poll, we should check whether the group needs to rebalance</span></div><div class="line">    <span class="comment">// prior to returning data so that the group can stabilize faster</span></div><div class="line">    <span class="comment">//note: 6. 如果 group 需要 rebalance, 直接返回空数据,这样更快地让 group 进行稳定状态</span></div><div class="line">    <span class="keyword">if</span> (coordinator.needRejoin())</div><div class="line">        <span class="keyword">return</span> Collections.emptyMap();</div><div class="line"></div><div class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这里，我们把一个 pollOnce 模型分为6个部分，这里简单介绍一下：</p>
<ol>
<li>连接 GroupCoordinator，并发送 join-group、sync-group 请求，加入 group 成功，并获取其分配的 tp 列表；</li>
<li>更新这些分配的 tp 列表的 the last committed offset（没有的话，根据其设置进行获取 offset）；</li>
<li>调用 Fetcher 获取拉取的数据，如果有数据，立马返回，没有的话就进行下面的操作；</li>
<li>调用 Fetcher 发送 fetch 请求（只是加入队列，并未真正发送）；</li>
<li>调用 poll() 方法发送请求；</li>
<li>如果 group 之前是需要 rebalacne 的，直接返回空集合，这样可以便于 group 尽快达到一个稳定的状态。</li>
</ol>
<p>一个 Consumer 实例消费数据的前提是能够加入一个 group 成功，并获取其要订阅的 tp（topic-partition）列表，这都是在第一步中完成的，如果这个 group 是一个新的 group，那么 group 的状态将会由 <strong>Empty –&gt; PreparingRebalance –&gt; AwaitSync –&gt; Stable</strong> 的变化过程，下面将会详细介绍。</p>
<h2 id="Consumer-join-group-详解"><a href="#Consumer-join-group-详解" class="headerlink" title="Consumer join-group 详解"></a>Consumer join-group 详解</h2><p>通过上面，我们知道，poll 模型的第一步是在 <code>ConsumerCoordinator.poll()</code> 中实现的，其整体过程如下所示。</p>
<p><img src="/images/kafka/join-group.png" alt="Consumer 加入一个 group 的整体流程"></p>
<blockquote>
<p>其实，主要观察图中左边的部分即可，也就是 ConsumerCoordinator 和 AbstractCoordinator 中的方法。</p>
</blockquote>
<p>对于一个 Consumer Group，其状态变化图下图所示（后面会讲到）。</p>
<p><img src="/images/kafka/group.png" alt="Group 状态变化图"></p>
<h3 id="ConsumerCoordinator-poll"><a href="#ConsumerCoordinator-poll" class="headerlink" title="ConsumerCoordinator.poll()"></a><code>ConsumerCoordinator.poll()</code></h3><p>先看一下 <code>ConsumerCoordinator.poll()</code> 的具体实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: 它确保了这个 group 的 coordinator 是已知的,并且这个 consumer 是已经加入到了 group 中,也用于 offset 周期性的 commit</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">poll</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">    invokeCompletedOffsetCommitCallbacks();<span class="comment">// note: 用于测试</span></div><div class="line"></div><div class="line">    <span class="comment">// note: Step1 通过 subscribe() 方法订阅 topic,并且 coordinator 未知,初始化 Consumer Coordinator</span></div><div class="line">    <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned() &amp;&amp; coordinatorUnknown()) &#123;</div><div class="line">        <span class="comment">// note: 获取 GroupCoordinator 地址,并且建立连接</span></div><div class="line">        ensureCoordinatorReady();</div><div class="line">        now = time.milliseconds();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// note: Step2 判断是否需要重新加入 group,如果订阅的 partition 变化或则分配的 partition 变化时,需要 rejoin</span></div><div class="line">    <span class="keyword">if</span> (needRejoin()) &#123;</div><div class="line">        <span class="comment">// due to a race condition between the initial metadata fetch and the initial rebalance,</span></div><div class="line">        <span class="comment">// we need to ensure that the metadata is fresh before joining initially. This ensures</span></div><div class="line">        <span class="comment">// that we have matched the pattern against the cluster's topics at least once before joining.</span></div><div class="line">        <span class="comment">// note: rejoin group 之前先刷新一下 metadata（对于 AUTO_PATTERN 而言）</span></div><div class="line">        <span class="keyword">if</span> (subscriptions.hasPatternSubscription())</div><div class="line">            client.ensureFreshMetadata();</div><div class="line"></div><div class="line">        <span class="comment">// note: 确保 group 是 active; 加入 group; 分配订阅的 partition</span></div><div class="line">        ensureActiveGroup();</div><div class="line">        now = time.milliseconds();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// note: Step3 检查心跳线程运行是否正常,如果心跳线程失败,则抛出异常,反之更新 poll 调用的时间</span></div><div class="line">    pollHeartbeat(now);</div><div class="line">    <span class="comment">// note: Step4 自动 commit 时,当定时达到时,进行自动 commit</span></div><div class="line">    maybeAutoCommitOffsetsAsync(now);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 poll 方法中，具体实现，可以分为以下三步：</p>
<ol>
<li>通过 <code>subscribe()</code> 方法订阅 topic, 并且 coordinator 未知，就初始化 Consumer Coordinator（在 <code>ensureCoordinatorReady()</code> 中实现，主要的作用是发送 GroupCoordinator 请求，并建立连接）；</li>
<li>判断是否需要重新加入 group，如果订阅的 partition 变化或则分配的 partition 变化时，需要 rejoin，通过 <code>ensureActiveGroup()</code> 发送 join-group、sync-group 请求，加入 group 并获取其 assign 的 tp list；</li>
<li>检测心跳线程运行是否正常（需要定时向 GroupCoordinator 发送心跳线程，长时间未发送的话 group就会认为该实例已经挂了）；</li>
<li>如果设置的是自动 commit，如果定时达到自动 commit。</li>
</ol>
<p>这其中，有两个地方需要详细介绍，那就是第一步中的 <code>ensureCoordinatorReady()</code> 方法和第二步中的 <code>ensureActiveGroup()</code> 方法。</p>
<h3 id="ensureCoordinatorReady"><a href="#ensureCoordinatorReady" class="headerlink" title="ensureCoordinatorReady()"></a><code>ensureCoordinatorReady()</code></h3><p>这个方法的作用是：选择一个连接数最小的 broker，向其发送 GroupCoordinator 请求，并建立相应的 TCP 连接。</p>
<ul>
<li>其方法的调用如前面的流程图所示：ensureCoordinatorReady() –&gt; lookupCoordinator() –&gt; sendGroupCoordinatorRequest()。</li>
<li>如果 client 获取到 Server response，那么就会与 GroupCoordinator 建立连接；</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: 确保 coordinator 已经 ready（已经连接,并可以发送请求）</span></div><div class="line"><span class="comment">// note: 如果 coordinator 已经 ready 返回 true,否则返回 flase。</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">ensureCoordinatorReady</span><span class="params">(<span class="keyword">long</span> startTimeMs, <span class="keyword">long</span> timeoutMs)</span> </span>&#123;</div><div class="line">    <span class="keyword">long</span> remainingMs = timeoutMs;</div><div class="line"></div><div class="line">    <span class="keyword">while</span> (coordinatorUnknown()) &#123;</div><div class="line">        <span class="comment">// note:  获取 GroupCoordinator,并建立连接</span></div><div class="line">        RequestFuture&lt;Void&gt; future = lookupCoordinator();</div><div class="line">        client.poll(future, remainingMs);</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (future.failed()) &#123;<span class="comment">// note: 如果获取的过程中失败了</span></div><div class="line">            <span class="keyword">if</span> (future.isRetriable()) &#123;</div><div class="line">                remainingMs = timeoutMs - (time.milliseconds() - startTimeMs);</div><div class="line">                <span class="keyword">if</span> (remainingMs &lt;= <span class="number">0</span>)</div><div class="line">                    <span class="keyword">break</span>;</div><div class="line"></div><div class="line">                log.debug(<span class="string">"Coordinator discovery failed for group &#123;&#125;, refreshing metadata"</span>, groupId);</div><div class="line">                client.awaitMetadataUpdate(remainingMs);</div><div class="line">            &#125; <span class="keyword">else</span></div><div class="line">                <span class="keyword">throw</span> future.exception();</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; client.connectionFailed(coordinator)) &#123;</div><div class="line">            <span class="comment">// we found the coordinator, but the connection has failed, so mark</span></div><div class="line">            <span class="comment">// it dead and backoff before retrying discovery</span></div><div class="line">            coordinatorDead();</div><div class="line">            time.sleep(retryBackoffMs);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        remainingMs = timeoutMs - (time.milliseconds() - startTimeMs);</div><div class="line">        <span class="keyword">if</span> (remainingMs &lt;= <span class="number">0</span>)</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> !coordinatorUnknown();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// note: 选择一个连接最小的节点,发送 groupCoordinator 请求</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> RequestFuture&lt;Void&gt; <span class="title">lookupCoordinator</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (findCoordinatorFuture == <span class="keyword">null</span>) &#123;</div><div class="line">        <span class="comment">// find a node to ask about the coordinator</span></div><div class="line">        Node node = <span class="keyword">this</span>.client.leastLoadedNode();<span class="comment">//<span class="doctag">NOTE:</span> 找一个节点,发送 groupCoordinator 的请求</span></div><div class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="comment">// <span class="doctag">TODO:</span> If there are no brokers left, perhaps we should use the bootstrap set</span></div><div class="line">            <span class="comment">// from configuration?</span></div><div class="line">            log.debug(<span class="string">"No broker available to send GroupCoordinator request for group &#123;&#125;"</span>, groupId);</div><div class="line">            <span class="keyword">return</span> RequestFuture.noBrokersAvailable();</div><div class="line">        &#125; <span class="keyword">else</span></div><div class="line">            findCoordinatorFuture = sendGroupCoordinatorRequest(node);<span class="comment">//<span class="doctag">NOTE:</span> 发送请求，并对 response 进行处理</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> findCoordinatorFuture;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 GroupCoordinator 的请求</span></div><div class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;Void&gt; <span class="title">sendGroupCoordinatorRequest</span><span class="params">(Node node)</span> </span>&#123;</div><div class="line">    <span class="comment">// initiate the group metadata request</span></div><div class="line">    log.debug(<span class="string">"Sending GroupCoordinator request for group &#123;&#125; to broker &#123;&#125;"</span>, groupId, node);</div><div class="line">    GroupCoordinatorRequest.Builder requestBuilder =</div><div class="line">            <span class="keyword">new</span> GroupCoordinatorRequest.Builder(<span class="keyword">this</span>.groupId);</div><div class="line">    <span class="keyword">return</span> client.send(node, requestBuilder)</div><div class="line">                 .compose(<span class="keyword">new</span> GroupCoordinatorResponseHandler());</div><div class="line">    <span class="comment">//<span class="doctag">NOTE:</span> compose 的作用是将 GroupCoordinatorResponseHandler 类转换为 RequestFuture</span></div><div class="line">    <span class="comment">//<span class="doctag">NOTE:</span> 实际上就是为返回的 Future 类重置 onSuccess() 和 onFailure() 方法</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 对 GroupCoordinator 的 response 进行处理,回调</span></div><div class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">GroupCoordinatorResponseHandler</span> <span class="keyword">extends</span> <span class="title">RequestFutureAdapter</span>&lt;<span class="title">ClientResponse</span>, <span class="title">Void</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp, RequestFuture&lt;Void&gt; future)</span> </span>&#123;</div><div class="line">        log.debug(<span class="string">"Received GroupCoordinator response &#123;&#125; for group &#123;&#125;"</span>, resp, groupId);</div><div class="line"></div><div class="line">        GroupCoordinatorResponse groupCoordinatorResponse = (GroupCoordinatorResponse) resp.responseBody();</div><div class="line">        <span class="comment">// use MAX_VALUE - node.id as the coordinator id to mimic separate connections</span></div><div class="line">        <span class="comment">// for the coordinator in the underlying network client layer</span></div><div class="line">        <span class="comment">// <span class="doctag">TODO:</span> this needs to be better handled in KAFKA-1935</span></div><div class="line">        Errors error = Errors.forCode(groupCoordinatorResponse.errorCode());</div><div class="line">        clearFindCoordinatorFuture();</div><div class="line">        <span class="keyword">if</span> (error == Errors.NONE) &#123;</div><div class="line">            <span class="comment">// note: 如果正确获取 GroupCoordinator 时, 建立连接,并更新心跳时间</span></div><div class="line">            <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</div><div class="line">                AbstractCoordinator.<span class="keyword">this</span>.coordinator = <span class="keyword">new</span> Node(</div><div class="line">                        Integer.MAX_VALUE - groupCoordinatorResponse.node().id(),</div><div class="line">                        groupCoordinatorResponse.node().host(),</div><div class="line">                        groupCoordinatorResponse.node().port());</div><div class="line">                log.info(<span class="string">"Discovered coordinator &#123;&#125; for group &#123;&#125;."</span>, coordinator, groupId);</div><div class="line">                client.tryConnect(coordinator);<span class="comment">//note: 初始化 tcp 连接</span></div><div class="line">                heartbeat.resetTimeouts(time.milliseconds());<span class="comment">//note: 更新心跳时间</span></div><div class="line">            &#125;</div><div class="line">            future.complete(<span class="keyword">null</span>);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</div><div class="line">            future.raise(<span class="keyword">new</span> GroupAuthorizationException(groupId));</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            log.debug(<span class="string">"Group coordinator lookup for group &#123;&#125; failed: &#123;&#125;"</span>, groupId, error.message());</div><div class="line">            future.raise(error);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e, RequestFuture&lt;Void&gt; future)</span> </span>&#123;</div><div class="line">        clearFindCoordinatorFuture();</div><div class="line">        <span class="keyword">super</span>.onFailure(e, future);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ensureActiveGroup"><a href="#ensureActiveGroup" class="headerlink" title="ensureActiveGroup()"></a><code>ensureActiveGroup()</code></h3><p>这个方法的作用是：向 GroupCoordinator 发送 join-group、sync-group 请求，获取 assign 的 tp list。</p>
<ul>
<li>如前面图中所示，ensureActiveGroup 方法的调用过程：ensureActiveGroup() –&gt; ensureCoordinatorReady() –&gt; startHeartbeatThreadIfNeeded() –&gt; joinGroupIfNeeded()；</li>
<li><code>joinGroupIfNeeded()</code> 方法中最重要的方法是 <code>initiateJoinGroup()</code>，其方法的调用过程为：initiateJoinGroup() –&gt; sendJoinGroupRequest() –&gt; JoinGroupResponseHandler.handle().succeed –&gt; onJoinLeader()/onJoinFollower() –&gt; sendSyncGroupRequest() –&gt; SyncGroupResponseHandler。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 确保 Group 是 active,并且加入该 group</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ensureActiveGroup</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">// always ensure that the coordinator is ready because we may have been disconnected</span></div><div class="line">    <span class="comment">// when sending heartbeats and does not necessarily require us to rejoin the group.</span></div><div class="line">    ensureCoordinatorReady();<span class="comment">//<span class="doctag">NOTE:</span> 确保 GroupCoordinator 已经连接</span></div><div class="line">    startHeartbeatThreadIfNeeded();<span class="comment">//<span class="doctag">NOTE:</span> 启动心跳发送线程（并不一定发送心跳,满足条件后才会发送心跳）</span></div><div class="line">    joinGroupIfNeeded();<span class="comment">//<span class="doctag">NOTE:</span> 发送 JoinGroup 请求,并对返回的信息进行处理</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>join-group 的请求是在 <code>joinGroupIfNeeded()</code> 中实现的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: join group</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">joinGroupIfNeeded</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">while</span> (needRejoin() || rejoinIncomplete()) &#123;</div><div class="line">        ensureCoordinatorReady();</div><div class="line">        <span class="comment">// call onJoinPrepare if needed. We set a flag to make sure that we do not call it a second</span></div><div class="line">        <span class="comment">// time if the client is woken up before a pending rebalance completes. This must be called</span></div><div class="line">        <span class="comment">// on each iteration of the loop because an event requiring a rebalance (such as a metadata</span></div><div class="line">        <span class="comment">// refresh which changes the matched subscription set) can occur while another rebalance is</span></div><div class="line">        <span class="comment">// still in progress.</span></div><div class="line">        <span class="comment">//note: 触发 onJoinPrepare, 包括 offset commit 和 rebalance listener</span></div><div class="line">        <span class="keyword">if</span> (needsJoinPrepare) &#123;</div><div class="line">            onJoinPrepare(generation.generationId, generation.memberId);</div><div class="line">            needsJoinPrepare = <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// note: 初始化 JoinGroup 请求,并发送该请求</span></div><div class="line">        RequestFuture&lt;ByteBuffer&gt; future = initiateJoinGroup();</div><div class="line">        client.poll(future);</div><div class="line">        resetJoinGroupFuture();<span class="comment">//<span class="doctag">NOTE:</span> 重置 joinFuture 为空</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> (future.succeeded()) &#123;<span class="comment">//note: join succeed,这一步时,时间上 sync-group 已经成功了</span></div><div class="line">            needsJoinPrepare = <span class="keyword">true</span>;</div><div class="line">            onJoinComplete(generation.generationId, generation.memberId, generation.protocol, future.value());</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            RuntimeException exception = future.exception();</div><div class="line">            <span class="keyword">if</span> (exception <span class="keyword">instanceof</span> UnknownMemberIdException ||</div><div class="line">                    exception <span class="keyword">instanceof</span> RebalanceInProgressException ||</div><div class="line">                    exception <span class="keyword">instanceof</span> IllegalGenerationException)</div><div class="line">                <span class="keyword">continue</span>;</div><div class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (!future.isRetriable())</div><div class="line">                <span class="keyword">throw</span> exception;</div><div class="line">            time.sleep(retryBackoffMs);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>sendJoinGroupRequest()</code> 方法是由 <code>initiateJoinGroup()</code> 方法来调用的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 JoinGroup 的请求, 并添加 listener</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">initiateJoinGroup</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">// we store the join future in case we are woken up by the user after beginning the</span></div><div class="line">    <span class="comment">// rebalance in the call to poll below. This ensures that we do not mistakenly attempt</span></div><div class="line">    <span class="comment">// to rejoin before the pending rebalance has completed.</span></div><div class="line">    <span class="keyword">if</span> (joinFuture == <span class="keyword">null</span>) &#123;</div><div class="line">        <span class="comment">// fence off the heartbeat thread explicitly so that it cannot interfere with the join group.</span></div><div class="line">        <span class="comment">// Note that this must come after the call to onJoinPrepare since we must be able to continue</span></div><div class="line">        <span class="comment">// sending heartbeats if that callback takes some time.</span></div><div class="line">        <span class="comment">// note: rebalance 期间,心跳线程停止</span></div><div class="line">        disableHeartbeatThread();</div><div class="line"></div><div class="line">        state = MemberState.REBALANCING;<span class="comment">//<span class="doctag">NOTE:</span> 标记为 rebalance</span></div><div class="line">        joinFuture = sendJoinGroupRequest();<span class="comment">//<span class="doctag">NOTE:</span> 发送 JoinGroup 请求</span></div><div class="line">        joinFuture.addListener(<span class="keyword">new</span> RequestFutureListener&lt;ByteBuffer&gt;() &#123;</div><div class="line">            <span class="meta">@Override</span></div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ByteBuffer value)</span> </span>&#123;</div><div class="line">                <span class="comment">// handle join completion in the callback so that the callback will be invoked</span></div><div class="line">                <span class="comment">// even if the consumer is woken up before finishing the rebalance</span></div><div class="line">                <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</div><div class="line">                    log.info(<span class="string">"Successfully joined group &#123;&#125; with generation &#123;&#125;"</span>, groupId, generation.generationId);</div><div class="line">                    state = MemberState.STABLE;<span class="comment">//<span class="doctag">NOTE:</span> 标记 Consumer 为 stable</span></div><div class="line"></div><div class="line">                    <span class="keyword">if</span> (heartbeatThread != <span class="keyword">null</span>)</div><div class="line">                        heartbeatThread.enable();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="meta">@Override</span></div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">                <span class="comment">// we handle failures below after the request finishes. if the join completes</span></div><div class="line">                <span class="comment">// after having been woken up, the exception is ignored and we will rejoin</span></div><div class="line">                <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</div><div class="line">                    state = MemberState.UNJOINED;<span class="comment">//<span class="doctag">NOTE:</span> 标记 Consumer 为 Unjoined</span></div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> joinFuture;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>sendJoinGroupRequest() 及其处理如下所示。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Join the group and return the assignment for the next generation. This function handles both</div><div class="line"> * JoinGroup and SyncGroup, delegating to &#123;<span class="doctag">@link</span> #performAssignment(String, String, Map)&#125; if</div><div class="line"> * elected leader by the coordinator.</div><div class="line"> * <span class="doctag">@return</span> A request future which wraps the assignment returned from the group leader</div><div class="line"> */</div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 JoinGroup 请求并返回 the assignment for the next generation（这个是在 JoinGroupResponseHandler 中做的）</span></div><div class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">sendJoinGroupRequest</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (coordinatorUnknown())</div><div class="line">        <span class="keyword">return</span> RequestFuture.coordinatorNotAvailable();</div><div class="line"></div><div class="line">    <span class="comment">// send a join group request to the coordinator</span></div><div class="line">    log.info(<span class="string">"(Re-)joining group &#123;&#125;"</span>, groupId);</div><div class="line">    JoinGroupRequest.Builder requestBuilder = <span class="keyword">new</span> JoinGroupRequest.Builder(</div><div class="line">            groupId,</div><div class="line">            <span class="keyword">this</span>.sessionTimeoutMs,</div><div class="line">            <span class="keyword">this</span>.generation.memberId,</div><div class="line">            protocolType(),</div><div class="line">            metadata()).setRebalanceTimeout(<span class="keyword">this</span>.rebalanceTimeoutMs);</div><div class="line"></div><div class="line">    log.debug(<span class="string">"Sending JoinGroup (&#123;&#125;) to coordinator &#123;&#125;"</span>, requestBuilder, <span class="keyword">this</span>.coordinator);</div><div class="line">    <span class="keyword">return</span> client.send(coordinator, requestBuilder)</div><div class="line">            .compose(<span class="keyword">new</span> JoinGroupResponseHandler());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 处理 JoinGroup response 的 handler（同步 group 信息）</span></div><div class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinGroupResponseHandler</span> <span class="keyword">extends</span> <span class="title">CoordinatorResponseHandler</span>&lt;<span class="title">JoinGroupResponse</span>, <span class="title">ByteBuffer</span>&gt; </span>&#123;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(JoinGroupResponse joinResponse, RequestFuture&lt;ByteBuffer&gt; future)</span> </span>&#123;</div><div class="line">        Errors error = Errors.forCode(joinResponse.errorCode());</div><div class="line">        <span class="keyword">if</span> (error == Errors.NONE) &#123;</div><div class="line">            log.debug(<span class="string">"Received successful JoinGroup response for group &#123;&#125;: &#123;&#125;"</span>, groupId, joinResponse);</div><div class="line">            sensors.joinLatency.record(response.requestLatencyMs());</div><div class="line"></div><div class="line">            <span class="keyword">synchronized</span> (AbstractCoordinator.<span class="keyword">this</span>) &#123;</div><div class="line">                <span class="keyword">if</span> (state != MemberState.REBALANCING) &#123;<span class="comment">//<span class="doctag">NOTE:</span> 如果此时 Consumer 的状态不是 rebalacing,就引起异常</span></div><div class="line">                    <span class="comment">// if the consumer was woken up before a rebalance completes, we may have already left</span></div><div class="line">                    <span class="comment">// the group. In this case, we do not want to continue with the sync group.</span></div><div class="line">                    future.raise(<span class="keyword">new</span> UnjoinedGroupException());</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    AbstractCoordinator.<span class="keyword">this</span>.generation = <span class="keyword">new</span> Generation(joinResponse.generationId(),</div><div class="line">                            joinResponse.memberId(), joinResponse.groupProtocol());</div><div class="line">                    AbstractCoordinator.<span class="keyword">this</span>.rejoinNeeded = <span class="keyword">false</span>;</div><div class="line">                    <span class="comment">//<span class="doctag">NOTE:</span> join group 成功,下面需要进行 sync-group,获取分配的 tp 列表。</span></div><div class="line">                    <span class="keyword">if</span> (joinResponse.isLeader()) &#123;</div><div class="line">                        onJoinLeader(joinResponse).chain(future);</div><div class="line">                    &#125; <span class="keyword">else</span> &#123;</div><div class="line">                        onJoinFollower().chain(future);</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_LOAD_IN_PROGRESS) &#123;</div><div class="line">            log.debug(<span class="string">"Attempt to join group &#123;&#125; rejected since coordinator &#123;&#125; is loading the group."</span>, groupId,</div><div class="line">                    coordinator());</div><div class="line">            <span class="comment">// backoff and retry</span></div><div class="line">            future.raise(error);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.UNKNOWN_MEMBER_ID) &#123;</div><div class="line">            <span class="comment">// reset the member id and retry immediately</span></div><div class="line">            resetGeneration();</div><div class="line">            log.debug(<span class="string">"Attempt to join group &#123;&#125; failed due to unknown member id."</span>, groupId);</div><div class="line">            future.raise(Errors.UNKNOWN_MEMBER_ID);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_COORDINATOR_NOT_AVAILABLE</div><div class="line">                || error == Errors.NOT_COORDINATOR_FOR_GROUP) &#123;</div><div class="line">            <span class="comment">// re-discover the coordinator and retry with backoff</span></div><div class="line">            coordinatorDead();</div><div class="line">            log.debug(<span class="string">"Attempt to join group &#123;&#125; failed due to obsolete coordinator information: &#123;&#125;"</span>, groupId, error.message());</div><div class="line">            future.raise(error);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.INCONSISTENT_GROUP_PROTOCOL</div><div class="line">                || error == Errors.INVALID_SESSION_TIMEOUT</div><div class="line">                || error == Errors.INVALID_GROUP_ID) &#123;</div><div class="line">            <span class="comment">// log the error and re-throw the exception</span></div><div class="line">            log.error(<span class="string">"Attempt to join group &#123;&#125; failed due to fatal error: &#123;&#125;"</span>, groupId, error.message());</div><div class="line">            future.raise(error);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</div><div class="line">            future.raise(<span class="keyword">new</span> GroupAuthorizationException(groupId));</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="comment">// unexpected error, throw the exception</span></div><div class="line">            future.raise(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error in join group response: "</span> + error.message()));</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>sendJoinGroupRequest()：向 GroupCoordinator 发送 join-group 请求</p>
<ol>
<li>如果 group 是新的 group.id，那么此时 group 初始化的状态为 <strong>Empty</strong>；</li>
<li>当 GroupCoordinator 接收到 consumer 的 join-group 请求后，由于此时这个 group 的 member 列表还是空（group 是新建的，每个 consumer 实例被称为这个 group 的一个 member），第一个加入的 member 将被选为 leader，也就是说，对于一个新的 consumer group 而言，当第一个 consumer 实例加入后将会被选为 leader；</li>
<li>如果 GroupCoordinator 接收到 leader 发送 join-group 请求，将会触发 rebalance，group 的状态变为 <strong>PreparingRebalance</strong>；</li>
<li>此时，GroupCoordinator 将会等待一定的时间，如果在一定时间内，接收到 join-group 请求的 consumer 将被认为是依然存活的，此时 group 会变为 <strong>AwaitSync</strong> 状态，并且 GroupCoordinator 会向这个 group 的所有 member 返回其 response；</li>
<li>consumer 在接收到 GroupCoordinator 的 response 后，如果这个 consumer 是 group 的 leader，那么这个 consumer 将会负责为整个 group assign partition 订阅安排（默认是按 range 的策略，目前也可选 roundrobin），然后 leader 将分配后的信息以 <code>sendSyncGroupRequest()</code> 请求的方式发给 GroupCoordinator，而作为 follower 的 consumer 实例会发送一个空列表；</li>
<li>GroupCoordinator 在接收到 leader 发来的请求后，会将 assign 的结果返回给所有已经发送 sync-group 请求的 consumer 实例，并且 group 的状态将会转变为 <strong>Stable</strong>，如果后续再收到 sync-group 请求，由于 group 的状态已经是 Stable，将会直接返回其分配结果。</li>
</ol>
<p>sync-group 请求的发送及其实现如下所示。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 consumer 为 follower 时,从 GroupCoordinator 拉取分配结果</span></div><div class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">onJoinFollower</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">// send follower's sync group with an empty assignment</span></div><div class="line">    SyncGroupRequest.Builder requestBuilder =</div><div class="line">            <span class="keyword">new</span> SyncGroupRequest.Builder(groupId, generation.generationId, generation.memberId,</div><div class="line">                    Collections.&lt;String, ByteBuffer&gt;emptyMap());</div><div class="line">    log.debug(<span class="string">"Sending follower SyncGroup for group &#123;&#125; to coordinator &#123;&#125;: &#123;&#125;"</span>, groupId, <span class="keyword">this</span>.coordinator,</div><div class="line">            requestBuilder);</div><div class="line">    <span class="keyword">return</span> sendSyncGroupRequest(requestBuilder);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 当 consumer 客户端为 leader 时,对 group 下的所有实例进行分配,将 assign 的结果发送到 GroupCoordinator</span></div><div class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">onJoinLeader</span><span class="params">(JoinGroupResponse joinResponse)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// perform the leader synchronization and send back the assignment for the group</span></div><div class="line">        Map&lt;String, ByteBuffer&gt; groupAssignment = performAssignment(joinResponse.leaderId(), joinResponse.groupProtocol(),</div><div class="line">                joinResponse.members());<span class="comment">//<span class="doctag">NOTE:</span> 进行 assign 操作</span></div><div class="line"></div><div class="line">        SyncGroupRequest.Builder requestBuilder =</div><div class="line">                <span class="keyword">new</span> SyncGroupRequest.Builder(groupId, generation.generationId, generation.memberId, groupAssignment);</div><div class="line">        log.debug(<span class="string">"Sending leader SyncGroup for group &#123;&#125; to coordinator &#123;&#125;: &#123;&#125;"</span>,</div><div class="line">                groupId, <span class="keyword">this</span>.coordinator, requestBuilder);</div><div class="line">        <span class="keyword">return</span> sendSyncGroupRequest(requestBuilder);<span class="comment">//<span class="doctag">NOTE:</span> 发送 sync-group 请求</span></div><div class="line">    &#125; <span class="keyword">catch</span> (RuntimeException e) &#123;</div><div class="line">        <span class="keyword">return</span> RequestFuture.failure(e);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 SyncGroup 请求,获取对 partition 分配的安排</span></div><div class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;ByteBuffer&gt; <span class="title">sendSyncGroupRequest</span><span class="params">(SyncGroupRequest.Builder requestBuilder)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (coordinatorUnknown())</div><div class="line">        <span class="keyword">return</span> RequestFuture.coordinatorNotAvailable();</div><div class="line">    <span class="keyword">return</span> client.send(coordinator, requestBuilder)</div><div class="line">            .compose(<span class="keyword">new</span> SyncGroupResponseHandler());</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncGroupResponseHandler</span> <span class="keyword">extends</span> <span class="title">CoordinatorResponseHandler</span>&lt;<span class="title">SyncGroupResponse</span>, <span class="title">ByteBuffer</span>&gt; </span>&#123;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(SyncGroupResponse syncResponse,</span></span></div><div class="line">                       RequestFuture&lt;ByteBuffer&gt; future) &#123;</div><div class="line">        Errors error = Errors.forCode(syncResponse.errorCode());</div><div class="line">        <span class="keyword">if</span> (error == Errors.NONE) &#123;<span class="comment">//note: 同步成功</span></div><div class="line">            sensors.syncLatency.record(response.requestLatencyMs());</div><div class="line">            future.complete(syncResponse.memberAssignment());</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            requestRejoin();<span class="comment">//note: join 的标志位设置为 true</span></div><div class="line"></div><div class="line">            <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</div><div class="line">                future.raise(<span class="keyword">new</span> GroupAuthorizationException(groupId));</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.REBALANCE_IN_PROGRESS) &#123;<span class="comment">//<span class="doctag">NOTE:</span> group 正在进行 rebalance,任务失败</span></div><div class="line">                log.debug(<span class="string">"SyncGroup for group &#123;&#125; failed due to coordinator rebalance"</span>, groupId);</div><div class="line">                future.raise(error);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.UNKNOWN_MEMBER_ID</div><div class="line">                    || error == Errors.ILLEGAL_GENERATION) &#123;</div><div class="line">                log.debug(<span class="string">"SyncGroup for group &#123;&#125; failed due to &#123;&#125;"</span>, groupId, error);</div><div class="line">                resetGeneration();</div><div class="line">                future.raise(error);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_COORDINATOR_NOT_AVAILABLE</div><div class="line">                    || error == Errors.NOT_COORDINATOR_FOR_GROUP) &#123;</div><div class="line">                log.debug(<span class="string">"SyncGroup for group &#123;&#125; failed due to &#123;&#125;"</span>, groupId, error);</div><div class="line">                coordinatorDead();</div><div class="line">                future.raise(error);</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                future.raise(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error from SyncGroup: "</span> + error.message()));</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="onJoinComplete"><a href="#onJoinComplete" class="headerlink" title="onJoinComplete()"></a><code>onJoinComplete()</code></h3><p>经过上面的步骤，一个 consumer 实例就已经加入 group 成功了，加入 group 成功后，将会触发ConsumerCoordinator 的 <code>onJoinComplete()</code> 方法，其作用就是：更新订阅的 tp 列表、更新其对应的 metadata 及触发注册的 listener。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// note: 加入 group 成功</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onJoinComplete</span><span class="params">(<span class="keyword">int</span> generation,</span></span></div><div class="line">                              String memberId,</div><div class="line">                              String assignmentStrategy,</div><div class="line">                              ByteBuffer assignmentBuffer) &#123;</div><div class="line">    <span class="comment">// only the leader is responsible for monitoring for metadata changes (i.e. partition changes)</span></div><div class="line">    <span class="keyword">if</span> (!isLeader)</div><div class="line">        assignmentSnapshot = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">    PartitionAssignor assignor = lookupAssignor(assignmentStrategy);</div><div class="line">    <span class="keyword">if</span> (assignor == <span class="keyword">null</span>)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Coordinator selected invalid assignment protocol: "</span> + assignmentStrategy);</div><div class="line"></div><div class="line">    Assignment assignment = ConsumerProtocol.deserializeAssignment(assignmentBuffer);</div><div class="line"></div><div class="line">    <span class="comment">// set the flag to refresh last committed offsets</span></div><div class="line">    <span class="comment">//note: 设置是否需要拉取 last committed offsets 为 true</span></div><div class="line">    subscriptions.needRefreshCommits();</div><div class="line"></div><div class="line">    <span class="comment">// update partition assignment</span></div><div class="line">    <span class="comment">//note: 更新订阅的 tp list</span></div><div class="line">    subscriptions.assignFromSubscribed(assignment.partitions());</div><div class="line"></div><div class="line">    <span class="comment">// check if the assignment contains some topics that were not in the original</span></div><div class="line">    <span class="comment">// subscription, if yes we will obey what leader has decided and add these topics</span></div><div class="line">    <span class="comment">// into the subscriptions as long as they still match the subscribed pattern</span></div><div class="line">    <span class="comment">//</span></div><div class="line">    <span class="comment">// TODO this part of the logic should be removed once we allow regex on leader assign</span></div><div class="line">    Set&lt;String&gt; addedTopics = <span class="keyword">new</span> HashSet&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (TopicPartition tp : subscriptions.assignedPartitions()) &#123;</div><div class="line">        <span class="keyword">if</span> (!joinedSubscription.contains(tp.topic()))</div><div class="line">            addedTopics.add(tp.topic());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (!addedTopics.isEmpty()) &#123;</div><div class="line">        Set&lt;String&gt; newSubscription = <span class="keyword">new</span> HashSet&lt;&gt;(subscriptions.subscription());</div><div class="line">        Set&lt;String&gt; newJoinedSubscription = <span class="keyword">new</span> HashSet&lt;&gt;(joinedSubscription);</div><div class="line">        newSubscription.addAll(addedTopics);</div><div class="line">        newJoinedSubscription.addAll(addedTopics);</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.subscriptions.subscribeFromPattern(newSubscription);</div><div class="line">        <span class="keyword">this</span>.joinedSubscription = newJoinedSubscription;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// update the metadata and enforce a refresh to make sure the fetcher can start</span></div><div class="line">    <span class="comment">// fetching data in the next iteration</span></div><div class="line">    <span class="comment">//note: 更新 metadata,确保在下一次循环中可以拉取</span></div><div class="line">    <span class="keyword">this</span>.metadata.setTopics(subscriptions.groupSubscription());</div><div class="line">    client.ensureFreshMetadata();</div><div class="line"></div><div class="line">    <span class="comment">// give the assignor a chance to update internal state based on the received assignment</span></div><div class="line">    assignor.onAssignment(assignment);</div><div class="line"></div><div class="line">    <span class="comment">// reschedule the auto commit starting from now</span></div><div class="line">    <span class="keyword">this</span>.nextAutoCommitDeadline = time.milliseconds() + autoCommitIntervalMs;</div><div class="line"></div><div class="line">    <span class="comment">// execute the user's callback after rebalance</span></div><div class="line">    <span class="comment">//note: 执行 listener</span></div><div class="line">    ConsumerRebalanceListener listener = subscriptions.listener();</div><div class="line">    log.info(<span class="string">"Setting newly assigned partitions &#123;&#125; for group &#123;&#125;"</span>, subscriptions.assignedPartitions(), groupId);</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        Set&lt;TopicPartition&gt; assigned = <span class="keyword">new</span> HashSet&lt;&gt;(subscriptions.assignedPartitions());</div><div class="line">        listener.onPartitionsAssigned(assigned);</div><div class="line">    &#125; <span class="keyword">catch</span> (WakeupException | InterruptException e) &#123;</div><div class="line">        <span class="keyword">throw</span> e;</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">        log.error(<span class="string">"User provided listener &#123;&#125; for group &#123;&#125; failed on partition assignment"</span>,</div><div class="line">                listener.getClass().getName(), groupId, e);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>至此，一个 consumer 实例算是真正上意义上加入 group 成功。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;距离上一篇博客（2017-09-10），到现在已经过去一个多月了，理论上这篇文章在上个月就应该写完，无奈拖延症又犯了，一直以这部分过于复杂为借口拖了好久，这两天逼了自己一把，先整理出其中的一篇，后续要加把劲，要不然今年的年度计划（年底前把这个系列写完）就完不成了，废话到此为
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Producer 单 Partition 顺序性实现及配置说明（五）</title>
    <link href="http://matt33.com/2017/09/10/produccer-end/"/>
    <id>http://matt33.com/2017/09/10/produccer-end/</id>
    <published>2017-09-10T15:39:01.000Z</published>
    <updated>2017-09-10T16:02:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天把 Kafka Producer 最后一部分给讲述一下，Producer 大部分内容都已经在前面几篇文章介绍过了，这里简单做个收尾，但并不是对前面的总结，本文从两块来讲述：RecordAccumulator 类的实现、Kafka Producer 如何保证其顺序性以及 Kafka Producer 的配置说明，每个 Producer 线程都会有一个 RecordAccumulator 对象，它负责缓存要发送 RecordBatch、记录发送的状态并且进行相应的处理，这里会详细讲述 Kafka Producer 如何保证单 Partition 的有序性。最后，简单介绍一下 Producer 的参数配置说明，只有正确地理解 Producer 相关的配置参数，才能更好地使用 Producer，发挥其相应的作用。</p>
<h2 id="RecordAccumulator"><a href="#RecordAccumulator" class="headerlink" title="RecordAccumulator"></a>RecordAccumulator</h2><p>这里再看一下 RecordAccumulator 的数据结构，如下图所示，每个 topic-partition 都有一个对应的 deque，deque 中存储的是 RecordBatch，它是发送的基本单位，只有这个 topic-partition 的 RecordBatch 达到大小或时间要求才会触发发送操作（但并不是只有达到这两个条件之一才会被发送，这点要理解清楚）。</p>
<p><img src="/images/kafka/recordbatch.png" alt="RecordAccumulator 模型"></p>
<p>再看一下 RecordAccumulator 类的主要方法介绍，如下图所示。</p>
<p><img src="/images/kafka/RecordAccumulator.png" alt="RecordAccumulator 主要方法及其说明"></p>
<p>这张图基本上涵盖了 RecordAccumulator 的主要方法，下面会选择其中几个方法详细讲述，会围绕着 Kafka Producer 如何实现单 Partition 顺序性这个主题来讲述。</p>
<h3 id="mutePartition-与-unmutePartition"><a href="#mutePartition-与-unmutePartition" class="headerlink" title="mutePartition() 与 unmutePartition()"></a>mutePartition() 与 unmutePartition()</h3><p>先看下 <code>mutePartition()</code> 与 <code>unmutePartition()</code> 这两个方法，它们是保证有序性关键之一，其主要做用就是将指定的 topic-partition 从 muted 集合中加入或删除，后面会看到它们的作用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">final</span> Set&lt;TopicPartition&gt; muted;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mutePartition</span><span class="params">(TopicPartition tp)</span> </span>&#123;</div><div class="line">    muted.add(tp);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unmutePartition</span><span class="params">(TopicPartition tp)</span> </span>&#123;</div><div class="line">    muted.remove(tp);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里先说一下这两个方法调用的条件，这样的话，下面在介绍其他方法时才会更容易理解：</p>
<ul>
<li><code>mutePartition()</code>：如果要求保证顺序性，那么这个 tp 对应的 RecordBatch 如果要开始发送，就将这个 tp 加入到 <code>muted</code> 集合中；</li>
<li><code>unmutePartition()</code>：如果 tp 对应的 RecordBatch 发送完成，tp 将会从 <code>muted</code> 集合中移除。</li>
</ul>
<p>也就是说，<code>muted</code> 是用来记录这个 tp 是否有还有未完成的 RecordBatch。</p>
<h3 id="ready"><a href="#ready" class="headerlink" title="ready()"></a>ready()</h3><p><code>ready()</code> 是在 Sender 线程中调用的，其作用选择那些可以发送的 node，也就是说，如果这个 tp 对应的 batch 可以发送（达到时间或大小要求），就把 tp 对应的 leader 选出来。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> ReadyCheckResult <span class="title">ready</span><span class="params">(Cluster cluster, <span class="keyword">long</span> nowMs)</span> </span>&#123;</div><div class="line">    Set&lt;Node&gt; readyNodes = <span class="keyword">new</span> HashSet&lt;&gt;();</div><div class="line">    <span class="keyword">long</span> nextReadyCheckDelayMs = Long.MAX_VALUE;</div><div class="line">    Set&lt;String&gt; unknownLeaderTopics = <span class="keyword">new</span> HashSet&lt;&gt;();</div><div class="line"></div><div class="line">    <span class="keyword">boolean</span> exhausted = <span class="keyword">this</span>.free.queued() &gt; <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; entry : <span class="keyword">this</span>.batches.entrySet()) &#123;</div><div class="line">        TopicPartition part = entry.getKey();</div><div class="line">        Deque&lt;RecordBatch&gt; deque = entry.getValue();</div><div class="line"></div><div class="line">        Node leader = cluster.leaderFor(part);</div><div class="line">        <span class="keyword">synchronized</span> (deque) &#123;</div><div class="line">            <span class="keyword">if</span> (leader == <span class="keyword">null</span> &amp;&amp; !deque.isEmpty()) &#123;</div><div class="line">                <span class="comment">// This is a partition for which leader is not known, but messages are available to send.</span></div><div class="line">                <span class="comment">// Note that entries are currently not removed from batches when deque is empty.</span></div><div class="line">                unknownLeaderTopics.add(part.topic());</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!readyNodes.contains(leader) &amp;&amp; !muted.contains(part)) &#123;<span class="comment">//note: part 如果 mute 就不会遍历</span></div><div class="line">                RecordBatch batch = deque.peekFirst();</div><div class="line">                <span class="keyword">if</span> (batch != <span class="keyword">null</span>) &#123;</div><div class="line">                    <span class="keyword">boolean</span> backingOff = batch.attempts &gt; <span class="number">0</span> &amp;&amp; batch.lastAttemptMs + retryBackoffMs &gt; nowMs;</div><div class="line">                    <span class="comment">//note: 是否是在重试</span></div><div class="line">                    <span class="keyword">long</span> waitedTimeMs = nowMs - batch.lastAttemptMs;</div><div class="line">                    <span class="keyword">long</span> timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;</div><div class="line">                    <span class="keyword">long</span> timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, <span class="number">0</span>);</div><div class="line">                    <span class="keyword">boolean</span> full = deque.size() &gt; <span class="number">1</span> || batch.isFull(); <span class="comment">//note: batch 满了</span></div><div class="line">                    <span class="keyword">boolean</span> expired = waitedTimeMs &gt;= timeToWaitMs; <span class="comment">//note: batch 超时</span></div><div class="line">                    <span class="keyword">boolean</span> sendable = full || expired || exhausted || closed || flushInProgress();</div><div class="line">                    <span class="keyword">if</span> (sendable &amp;&amp; !backingOff) &#123;</div><div class="line">                        readyNodes.add(leader);<span class="comment">// note: 将可以发送的 leader 添加到集合中</span></div><div class="line">                    &#125; <span class="keyword">else</span> &#123;</div><div class="line">                        <span class="comment">// Note that this results in a conservative estimate since an un-sendable partition may have</span></div><div class="line">                        <span class="comment">// a leader that will later be found to have sendable data. However, this is good enough</span></div><div class="line">                        <span class="comment">// since we'll just wake up and then sleep again for the remaining time.</span></div><div class="line">                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到这一行 <code>(!readyNodes.contains(leader) &amp;&amp; !muted.contains(part))</code>，如果 <code>muted</code> 集合包含这个 tp，那么在遍历时将不会处理它对应的 deque，也就是说，如果一个 tp 加入了 <code>muted</code> 集合中，即使它对应的 RecordBatch 可以发送了，也不会触发引起其对应的 leader 被选择出来。</p>
<h3 id="drain"><a href="#drain" class="headerlink" title="drain()"></a>drain()</h3><p><code>drain()</code> 是用来遍历可发送请求的 node，然后再遍历在这个 node 上所有 tp，如果 tp 对应的 deque 有数据，将会被选择出来直到超过一个请求的最大长度（<code>max.request.size</code>）为止，也就说说即使 RecordBatch 没有达到条件，但为了保证每个 request 尽快多地发送数据提高发送效率，这个 RecordBatch 依然会被提前选出来并进行发送。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 返回该 node 对应的可以发送的 RecordBatch 的 batches,并从 queue 中移除（最大的大小为maxSize,超过的话,下次再发送）</span></div><div class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;RecordBatch&gt;&gt; drain(Cluster cluster,</div><div class="line">                                             Set&lt;Node&gt; nodes,</div><div class="line">                                             <span class="keyword">int</span> maxSize,</div><div class="line">                                             <span class="keyword">long</span> now) &#123;</div><div class="line">    <span class="keyword">if</span> (nodes.isEmpty())</div><div class="line">        <span class="keyword">return</span> Collections.emptyMap();</div><div class="line"></div><div class="line">    Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (Node node : nodes) &#123;</div><div class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</div><div class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</div><div class="line">        List&lt;RecordBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        <span class="comment">/* to make starvation less likely this loop doesn't start at 0 */</span></div><div class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</div><div class="line">        <span class="keyword">do</span> &#123;</div><div class="line">            PartitionInfo part = parts.get(drainIndex);</div><div class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</div><div class="line">            <span class="comment">// Only proceed if the partition has no in-flight batches.</span></div><div class="line">            <span class="keyword">if</span> (!muted.contains(tp)) &#123;<span class="comment">//note: 被 mute 的 tp 依然不会被遍历</span></div><div class="line">                Deque&lt;RecordBatch&gt; deque = getDeque(<span class="keyword">new</span> TopicPartition(part.topic(), part.partition()));</div><div class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) &#123;</div><div class="line">                    <span class="keyword">synchronized</span> (deque) &#123;</div><div class="line">                        RecordBatch first = deque.peekFirst();</div><div class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) &#123;</div><div class="line">                            <span class="keyword">boolean</span> backoff = first.attempts &gt; <span class="number">0</span> &amp;&amp; first.lastAttemptMs + retryBackoffMs &gt; now;</div><div class="line">                            <span class="comment">// Only drain the batch if it is not during backoff period.</span></div><div class="line">                            <span class="keyword">if</span> (!backoff) &#123;</div><div class="line">                                <span class="keyword">if</span> (size + first.sizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) &#123;</div><div class="line">                                    <span class="comment">// there is a rare case that a single batch size is larger than the request size due</span></div><div class="line">                                    <span class="comment">// to compression; in this case we will still eventually send this batch in a single</span></div><div class="line">                                    <span class="comment">// request</span></div><div class="line">                                    <span class="keyword">break</span>;</div><div class="line">                                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                                    RecordBatch batch = deque.pollFirst();</div><div class="line">                                    batch.close();</div><div class="line">                                    size += batch.sizeInBytes();</div><div class="line">                                    ready.add(batch);</div><div class="line">                                    batch.drainedMs = now;</div><div class="line">                                &#125;</div><div class="line">                            &#125;</div><div class="line">                        &#125;</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</div><div class="line">        &#125; <span class="keyword">while</span> (start != drainIndex);</div><div class="line">        batches.put(node.id(), ready);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> batches;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在遍历 node 的所有 tp 时，可以看到是有条件的 —— <code>!muted.contains(tp)</code>，如果这个 tp 被添加到 <code>muted</code> 集合中，那么它将不会被遍历，也就不会作为 request 一部分被发送出去，这也就保证了 tp 如果还有未完成的 RecordBatch，那么其对应 deque 中其他 RecordBatch 即使达到条件也不会被发送，就保证了 tp 在任何时刻只有一个 RecordBatch 在发送。</p>
<h3 id="顺序性如何保证？"><a href="#顺序性如何保证？" class="headerlink" title="顺序性如何保证？"></a>顺序性如何保证？</h3><p>是否保证顺序性，还是在 Sender 线程中实现的，<code>mutePartition()</code> 与 <code>unmutePartition()</code> 也都是在 Sender 中调用的，这里看一下 KafkaProducer 是如何初始化一个 Sender 对象的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// from KafkaProducer</span></div><div class="line"><span class="keyword">this</span>.sender = <span class="keyword">new</span> Sender(client,</div><div class="line">                         <span class="keyword">this</span>.metadata,</div><div class="line">                         his.accumulator,</div><div class="line">                         config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION) == <span class="number">1</span>,</div><div class="line">                         config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),</div><div class="line">                         (<span class="keyword">short</span>) parseAcks(config.getString(ProducerConfig.ACKS_CONFIG)),</div><div class="line">                         config.getInt(ProducerConfig.RETRIES_CONFIG),</div><div class="line">                         <span class="keyword">this</span>.metrics,</div><div class="line">                         Time.SYSTEM,</div><div class="line">                         <span class="keyword">this</span>.requestTimeoutMs);<span class="comment">//<span class="doctag">NOTE:</span> Sender 实例,发送请求的后台线程</span></div><div class="line"></div><div class="line"><span class="comment">// from Sender</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Sender</span><span class="params">(KafkaClient client,</span></span></div><div class="line">              Metadata metadata,</div><div class="line">              RecordAccumulator accumulator,</div><div class="line">              <span class="keyword">boolean</span> guaranteeMessageOrder,</div><div class="line">              <span class="keyword">int</span> maxRequestSize,</div><div class="line">              <span class="keyword">short</span> acks,</div><div class="line">              <span class="keyword">int</span> retries,</div><div class="line">              Metrics metrics,</div><div class="line">              Time time,</div><div class="line">              <span class="keyword">int</span> requestTimeout) &#123;</div><div class="line">        <span class="keyword">this</span>.client = client;</div><div class="line">        <span class="keyword">this</span>.accumulator = accumulator;</div><div class="line">        <span class="keyword">this</span>.metadata = metadata;</div><div class="line">        <span class="keyword">this</span>.guaranteeMessageOrder = guaranteeMessageOrder;</div><div class="line">        <span class="keyword">this</span>.maxRequestSize = maxRequestSize;</div><div class="line">        <span class="keyword">this</span>.running = <span class="keyword">true</span>; <span class="comment">//note: 默认为 true</span></div><div class="line">        <span class="keyword">this</span>.acks = acks;</div><div class="line">        <span class="keyword">this</span>.retries = retries;</div><div class="line">        <span class="keyword">this</span>.time = time;</div><div class="line">        <span class="keyword">this</span>.sensors = <span class="keyword">new</span> SenderMetrics(metrics);</div><div class="line">        <span class="keyword">this</span>.requestTimeout = requestTimeout;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于上述过程可以这样进行解读</p>
<p><code>this.guaranteeMessageOrder = (config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION) == 1)</code></p>
<p>如果 KafkaProducer 的 <code>max.in.flight.requests.per.connection</code> 设置为1，那么就可以保证其顺序性，否则的话，就不保证顺序性，从下面这段代码也可以看出。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//from Sender</span></div><div class="line"><span class="comment">//note: max.in.flight.requests.per.connection 设置为1时会保证</span></div><div class="line"><span class="keyword">if</span> (guaranteeMessageOrder) &#123;</div><div class="line">    <span class="comment">// Mute all the partitions draine</span></div><div class="line">    <span class="keyword">for</span> (List&lt;RecordBatch&gt; batchList : batches.values()) &#123;</div><div class="line">         <span class="keyword">for</span> (RecordBatch batch : batchList)</div><div class="line">             <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>也就是说，如果要保证单 Partition 的顺序性，需要在 Producer 中配置 <code>max.in.flight.requests.per.connection=1</code>，而其实现机制则是在 RecordAccumulator 中实现的。</p>
<h2 id="Producer-Configs"><a href="#Producer-Configs" class="headerlink" title="Producer Configs"></a>Producer Configs</h2><p>这里是关于 Kafka Producer 一些配置的说明，内容来自官方文档<a href="http://kafka.apache.org/0102/documentation.html#producerconfigs" target="_blank" rel="external">Producer Configs</a>以及自己的一些个人理解，这里以官方文档保持一致，按其重要性分为三个级别进行讲述（涉及到权限方面的参数，这里先不介绍）。</p>
<h3 id="high-importance"><a href="#high-importance" class="headerlink" title="high importance"></a>high importance</h3><table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>bootstrap.servers</td>
<td>Kafka Broker 的一个列表，不用包含所有的 Broker，它用于初始化连接时，通过这几个 broker 来获取集群的信息，比如：<code>127.0.0.1：9092,127.0.0.2：9092,127.0.0.3：9092</code></td>
<td>-</td>
</tr>
<tr>
<td>key.serializer</td>
<td>对 key 进行序列化的 class，一般使用<code>StringSerializer</code></td>
<td>-</td>
</tr>
<tr>
<td>value.serializer</td>
<td>对 value 进行序列化的 class，一般使用 <code>StringDeserializer</code></td>
<td>-</td>
</tr>
<tr>
<td>acks</td>
<td>用于设置在什么情况一条才被认为已经发送成功了。acks=0：msg 只要被 producer 发送出去就认为已经发送完成了；acks=1：如果 leader 接收到消息并发送 ack （不会等会该 msg 是否同步到其他副本）就认为 msg 发送成功了； acks=all或者-1：leader 接收到 msg 并从所有 isr 接收到 ack 后再向 producer 发送 ack，这样才认为 msg 发送成功了，这是最高级别的可靠性保证。</td>
<td>1</td>
</tr>
<tr>
<td>buffer.memory</td>
<td>producer 可以使用的最大内存，如果超过这个值，producer 将会 block <code>max.block.ms</code> 之后抛出异常。</td>
<td>33554432（32MB）</td>
</tr>
<tr>
<td>compression.type</td>
<td>Producer 数据的压缩格式，可以选择 none、gzip、snappy、lz4</td>
<td>none</td>
</tr>
<tr>
<td>retries</td>
<td>msg 发送失败后重试的次数，允许重试，如果 <code>max.in.flight.requests.per.connection</code> 设置不为1，可能会导致乱序</td>
<td>0</td>
</tr>
</tbody>
</table>
<h3 id="medium-importance"><a href="#medium-importance" class="headerlink" title="medium importance"></a>medium importance</h3><p>下面的这些参数虽然被描述为 medium，但实际上对 Producer 的吞吐量等影响也同样很大，在实践中跟 high 参数的重要性基本一样。</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch.size</td>
<td>producer 向 partition 发送数据时，是以 batch 形式的发送数据，当 batch 的大小超过 <code>batch.size</code> 或者时间达到 <code>linger.ms</code> 就会发送 batch，根据经验，设置为1MB 吞吐会更高，太小的话吞吐小，太大的话导致内存浪费进而影响吞吐量</td>
<td>16384（16KB）</td>
</tr>
<tr>
<td>linger.ms</td>
<td>在一个 batch 达不到 <code>batch.size</code> 时，这个 batch 最多将会等待 <code>linger.ms</code> 时间，超过这个时间这个 batch 就会被发送，但也会带来相应的延迟，可以根据具体的场景进行设置</td>
<td>0</td>
</tr>
<tr>
<td>client.id</td>
<td>client 的 id，主要用于追踪 request 的来源</td>
<td>null</td>
</tr>
<tr>
<td>connections.max.idle.ms</td>
<td>如果 connection 连续空闲时间超过了这个值，将会被关闭，主要使用 Selector 的 <code>maybeCloseOldestConnection</code> 方法</td>
<td>540000（9min）</td>
</tr>
<tr>
<td>max.block.ms</td>
<td>控制 <code>KafkaProducer.send()</code> 和 <code>KafkaProducer.partitionsFor()</code> block 的最大时间，block 的原因是 buffer 满了或者 metadata 不可用导致。</td>
<td>60000</td>
</tr>
<tr>
<td>max.request.size</td>
<td>一个请求的最大长度</td>
<td>1048576（1MB）</td>
</tr>
<tr>
<td>partitioner.class</td>
<td>获取 topic 分区的 class</td>
<td>org.apache.kafka.clients.producer.internals.DefaultPartitioner</td>
</tr>
<tr>
<td>receive.buffer.bytes</td>
<td>在读取数据时 TCP receive buffer （SO_RCVBUF）的大小</td>
<td>32768（32KB）</td>
</tr>
<tr>
<td>request.timeout.ms</td>
<td>如果 producer 超过这么长时间没有收到 response，将会再次发送请求</td>
<td>30000</td>
</tr>
<tr>
<td>timeout.ms</td>
<td>用于配置 leader 等待 isr 返回 ack 的最大时间，如果超过了这个时间，将会返回给 producer 一个错误。</td>
<td>30000</td>
</tr>
</tbody>
</table>
<h3 id="low-importance"><a href="#low-importance" class="headerlink" title="low importance"></a>low importance</h3><table>
<thead>
<tr>
<th>参数名</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>block.on.buffer.full</td>
<td>当 Producer 使用 buffer 达到最大设置时，如果设置为 false，将会 block <code>max.block.ms</code> 后然后抛出 <code>TimeoutException</code> 异常，如果设置为 true，将会把 <code>max.block.ms</code> 设置为 <code>Long.MAX_VALUE</code>。</td>
<td>false</td>
</tr>
<tr>
<td>interceptor.classes</td>
<td>使用拦截器，实现这个 <code>ProducerInterceptor</code> 接口，可以对 topic 进行简单的处理。</td>
<td>null</td>
</tr>
<tr>
<td>max.in.flight.requests.per.connection</td>
<td>对一个 connection，同时发送最大请求数，不为1时，不能保证顺序性。</td>
<td>5</td>
</tr>
<tr>
<td>metadata.fetch.timeout.ms</td>
<td>获取 metadata 时的超时时间</td>
<td>60000</td>
</tr>
<tr>
<td>metadata.max.age.ms</td>
<td>强制 metadata 定时刷新的间隔</td>
<td>300000（5min）</td>
</tr>
<tr>
<td>metric.reporters</td>
<td>A list of classes to use as metrics reporters. Implementing the MetricReporter interface，JmxReporter 是默认被添加的。</td>
<td>“”</td>
</tr>
<tr>
<td>metrics.num.samples</td>
<td>统计 metrics 时采样的次数</td>
<td>2</td>
</tr>
<tr>
<td>metrics.sample.window.ms</td>
<td>metrics 采样计算的时间窗口</td>
<td>30000</td>
</tr>
<tr>
<td>reconnect.backoff.ms</td>
<td>重新建立建立连接的间隔</td>
<td>50</td>
</tr>
<tr>
<td>retry.backoff.ms</td>
<td>发送重试的间隔</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>对于不同的场景，合理配置相应的 Kafka Producer 参数。</p>
<p>至此，Kafka Producer 部分的源码分析已经结束，从下周开始将开始对 Kafka Consumer 部分进行分析。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天把 Kafka Producer 最后一部分给讲述一下，Producer 大部分内容都已经在前面几篇文章介绍过了，这里简单做个收尾，但并不是对前面的总结，本文从两块来讲述：RecordAccumulator 类的实现、Kafka Producer 如何保证其顺序性以及 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 最佳实践【译】</title>
    <link href="http://matt33.com/2017/09/04/kafka-best-pratice/"/>
    <id>http://matt33.com/2017/09/04/kafka-best-pratice/</id>
    <published>2017-09-04T14:07:58.000Z</published>
    <updated>2017-09-05T00:59:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里翻译一篇关于 Kafka 实践的文章，内容来自 DataWorks Summit/Hadoop Summit（<a href="https://dataworkssummit.com/munich-2017/sessions/apache-kafka-best-practices/" target="_blank" rel="external">Hadoop Summit</a>）上一篇分享，PPT 见<a href="https://www.slideshare.net/HadoopSummit/apache-kafka-best-practices" target="_blank" rel="external">Apache Kafka Best Pratices</a>，里面讲述了很多关于 Kafka 配置、监控、优化的内容，绝对是在实践中总结出的精华，有很大的借鉴参考意义，本文主要是根据 PPT 的内容进行翻译及适当补充。</p>
<p>Kafka 的架构这里就不多做介绍了，直接不如正题。</p>
<h2 id="Kafka-基本配置及性能优化"><a href="#Kafka-基本配置及性能优化" class="headerlink" title="Kafka 基本配置及性能优化"></a>Kafka 基本配置及性能优化</h2><p>这里主要是 Kafka 集群基本配置的相关内容。</p>
<h3 id="硬件要求"><a href="#硬件要求" class="headerlink" title="硬件要求"></a>硬件要求</h3><p>Kafka 集群基本硬件的保证</p>
<table>
<thead>
<tr>
<th></th>
<th>集群规模</th>
<th>内存</th>
<th>CPU</th>
<th>存储</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kafka Brokers</td>
<td>3+</td>
<td>24GB+（小规模）；64GB+（大规模）</td>
<td>多核（12CPU+），并允许超线程</td>
<td>6+ 1TB 的专属磁盘（RAID 或 JBOD）</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>3（小规模）；5（大规模）</td>
<td>8GB+（小规模）；24GB+（大规模）</td>
<td>2核+</td>
<td>SSD 用于中间的日志传输</td>
</tr>
</tbody>
</table>
<h3 id="OS-调优"><a href="#OS-调优" class="headerlink" title="OS 调优"></a>OS 调优</h3><ul>
<li>OS page cache：应当可以缓存所有活跃的 Segment（Kafka 中最基本的数据存储单位）；</li>
<li>fd 限制：100k+；</li>
<li>禁用 swapping：简单来说，swap 作用是当内存的使用达到一个临界值时就会将内存中的数据移动到 swap 交换空间，但是此时，内存可能还有很多空余资源，swap 走的是磁盘 IO，对于内存读写很在意的系统，最好禁止使用 swap 分区（参考<a href="https://www.quora.com/What-is-swapping-in-an-OS" target="_blank" rel="external">What is swapping in an OS?</a>）；</li>
<li>TCP 调优；</li>
<li>JVM 配置<ol>
<li>JDK 8 并且使用 G1 垃圾收集器；</li>
<li>至少要分配 6-8 GB 的堆内存。</li>
</ol>
</li>
</ul>
<h3 id="Kafka-磁盘存储"><a href="#Kafka-磁盘存储" class="headerlink" title="Kafka 磁盘存储"></a>Kafka 磁盘存储</h3><ul>
<li>使用多块磁盘，并配置为 Kafka 专用的磁盘；</li>
<li>JBOD vs RAID10；</li>
<li>JBOD（Just a Bunch of Disks，简单来说它表示一个没有控制软件提供协调控制的磁盘集合，它将多个物理磁盘串联起来，提供一个巨大的逻辑磁盘，数据是按序存储，它的性能与单块磁盘类似）</li>
<li>JBOD 的一些缺陷：<ul>
<li>任何磁盘的损坏都会导致异常关闭，并且需要较长的时间恢复；</li>
<li>数据不保证一致性；</li>
<li>多级目录；</li>
</ul>
</li>
<li>社区也正在解决这么问题，可以关注 KIP 112、113：<ul>
<li>必要的工具用于管理 JBOD；</li>
<li>自动化的分区管理；</li>
<li>磁盘损坏时，Broker 可以将 replicas 迁移到好的磁盘上；</li>
<li>在同一个 Broker 的磁盘间 reassign replicas；</li>
</ul>
</li>
<li>RAID 10 的特点：<ul>
<li>可以允许单磁盘的损坏；</li>
<li>性能和保护；</li>
<li>不同磁盘间的负载均衡；</li>
<li>高命中来减少 space；</li>
<li>单一的 mount point；</li>
</ul>
</li>
<li>文件系统：<ul>
<li>使用 EXT 或 XFS；</li>
<li>SSD；</li>
</ul>
</li>
</ul>
<h3 id="基本的监控"><a href="#基本的监控" class="headerlink" title="基本的监控"></a>基本的监控</h3><p>Kafka 集群需要监控的一些指标，这些指标反应了集群的健康度。</p>
<ul>
<li>CPU 负载；</li>
<li>Network Metrics；</li>
<li>File Handle 使用；</li>
<li>磁盘空间；</li>
<li>磁盘 IO 性能；</li>
<li>GC 信息；</li>
<li>ZooKeeper 监控。</li>
</ul>
<h2 id="Kafka-replica-相关配置及监控"><a href="#Kafka-replica-相关配置及监控" class="headerlink" title="Kafka replica 相关配置及监控"></a>Kafka replica 相关配置及监控</h2><h3 id="Kafka-Replication"><a href="#Kafka-Replication" class="headerlink" title="Kafka Replication"></a>Kafka Replication</h3><ul>
<li>Partition 有两种副本：Leader，Follower；</li>
<li>Leader 负责维护 in-sync-replicas(ISR)<ul>
<li><code>replica.lag.time.max.ms</code>：默认为10000，如果 follower 落后于 leader 的消息数超过这个数值时，leader 就将 follower 从 isr 列表中移除；</li>
<li><code>num.replica.fetchers</code>，默认为1，用于从 leader 同步数据的 fetcher 线程数；</li>
<li><code>min.insync.replica</code>：Producer 端使用来用于保证 Durability（持久性）；</li>
</ul>
</li>
</ul>
<h3 id="Under-Replicated-Partitions"><a href="#Under-Replicated-Partitions" class="headerlink" title="Under Replicated Partitions"></a>Under Replicated Partitions</h3><p>当发现 replica 的配置与集群的不同时，一般情况都是集群上的 replica 少于配置数时，可以从以下几个角度来排查问题：</p>
<ul>
<li>JMX 监控项：kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions；</li>
<li>可能的原因：<ul>
<li>Broker 挂了？</li>
<li>Controller 的问题？</li>
<li>ZooKeeper 的问题？</li>
<li>Network 的问题？</li>
</ul>
</li>
<li>解决办法：<ul>
<li>调整 ISR 的设置；</li>
<li>Broker 扩容。</li>
</ul>
</li>
</ul>
<h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><ul>
<li>负责管理 partition 生命周期；</li>
<li>避免 Controller’s ZK 会话超时：<ul>
<li>ISR 抖动；</li>
<li>ZK Server 性能问题；</li>
<li>Broker 长时间的 GC；</li>
<li>网络 IO 问题；</li>
</ul>
</li>
<li>监控：<ul>
<li>kafka.controller:type=KafkaController,name=ActiveControllerCount，应该为1；</li>
<li>LeaderElectionRate。</li>
</ul>
</li>
</ul>
<h3 id="Unclean-leader-选举"><a href="#Unclean-leader-选举" class="headerlink" title="Unclean leader 选举"></a>Unclean leader 选举</h3><p>允许不在 isr 中 replica 被选举为 leader。</p>
<ul>
<li>这是 Availability 和 Correctness 之间选择，Kafka 默认选择了可用性；</li>
<li><code>unclean.leader.election.enable</code>：默认为 true，即允许不在 isr 中 replica 选为 leader，这个配置可以全局配置，也可以在 topic 级别配置；</li>
<li>监控：kafka.controller:type=ControllerStats,name=UncleanLeaderElectionsPerSec。</li>
</ul>
<h2 id="Broker-配置"><a href="#Broker-配置" class="headerlink" title="Broker 配置"></a>Broker 配置</h2><p>Broker 级别有几个比较重要的配置，一般需要根据实际情况进行相应配置的：</p>
<ul>
<li><code>log.retention.{ms, minutes, hours}</code> , <code>log.retention.bytes</code>：数据保存时间；</li>
<li><code>message.max.bytes</code>, <code>replica.fetch.max.bytes</code>；</li>
<li><code>delete.topic.enable</code>：默认为 false，是否允许通过 admin tool 来删除 topic；</li>
<li><code>unclean.leader.election.enable</code> = false，参见上面；</li>
<li><code>min.insync.replicas</code> = 2：当 Producer 的 acks 设置为 all 或 -1 时，<code>min.insync.replicas</code> 代表了必须进行确认的最小 replica 数，如果不够的话 Producer 将会报 <code>NotEnoughReplicas</code> 或 <code>NotEnoughReplicasAfterAppend</code> 异常；</li>
<li><code>replica.lag.time.max.ms</code>（超过这个时间没有发送请求的话，follower 将从 isr 中移除）, num.replica.fetchers；</li>
<li><code>replica.fetch.response.max.bytes</code>；</li>
<li><code>zookeeper.session.timeout.ms</code> = 30s；</li>
<li><code>num.io.threads</code>：默认为8，KafkaRequestHandlerPool 的大小。</li>
</ul>
<h2 id="Kafka-相关资源的评估"><a href="#Kafka-相关资源的评估" class="headerlink" title="Kafka 相关资源的评估"></a>Kafka 相关资源的评估</h2><h3 id="集群评估"><a href="#集群评估" class="headerlink" title="集群评估"></a>集群评估</h3><ul>
<li>Broker 评估<ul>
<li>每个 Broker 的 Partition 数不应该超过2k；</li>
<li>控制 partition 大小（不要超过25GB）；</li>
</ul>
</li>
<li>集群评估（Broker 的数量根据以下条件配置）<ul>
<li>数据保留时间；</li>
<li>集群的流量大小；</li>
</ul>
</li>
<li>集群扩容：<ul>
<li>磁盘使用率应该在 60% 以下；</li>
<li>网络使用率应该在 75% 以下；</li>
</ul>
</li>
<li>集群监控<ul>
<li>保持负载均衡；</li>
<li>确保 topic 的 partition 均匀分布在所有 Broker 上；</li>
<li>确保集群的阶段没有耗尽磁盘或带宽。</li>
</ul>
</li>
</ul>
<h3 id="Broker-监控"><a href="#Broker-监控" class="headerlink" title="Broker 监控"></a>Broker 监控</h3><ul>
<li>Partition 数：kafka.server:type=ReplicaManager,name=PartitionCount；</li>
<li>Leader 副本数：kafka.server:type=ReplicaManager,name=LeaderCount；</li>
<li>ISR 扩容/缩容率：kafka.server:type=ReplicaManager,name=IsrExpandsPerSec；</li>
<li>读写速率：Message in rate/Byte in rate/Byte out rate；</li>
<li>网络请求的平均空闲率：NetworkProcessorAvgIdlePercent；</li>
<li>请求处理平均空闲率：RequestHandlerAvgIdlePercent。</li>
</ul>
<h3 id="Topic-评估"><a href="#Topic-评估" class="headerlink" title="Topic 评估"></a>Topic 评估</h3><ul>
<li>partition 数<ul>
<li>Partition 数应该至少与最大 consumer group 中 consumer 线程数一致；</li>
<li>对于使用频繁的 topic，应该设置更多的 partition；</li>
<li>控制 partition 的大小（25GB 左右）；</li>
<li>考虑应用未来的增长（可以使用一种机制进行自动扩容）；</li>
</ul>
</li>
<li>使用带 key 的 topic；</li>
<li>partition 扩容：当 partition 的数据量超过一个阈值时应该自动扩容（实际上还应该考虑网络流量）。</li>
</ul>
<h3 id="合理地设置-partition"><a href="#合理地设置-partition" class="headerlink" title="合理地设置 partition"></a>合理地设置 partition</h3><ul>
<li>根据吞吐量的要求设置 partition 数：<ul>
<li>假设 Producer 单 partition 的吞吐量为 P；</li>
<li>consumer 消费一个 partition 的吞吐量为 C；</li>
<li>而要求的吞吐量为 T；</li>
<li>那么 partition 数至少应该大于 T/P、T/c 的最大值；</li>
</ul>
</li>
<li>更多的 partition，意味着：<ul>
<li>更多的 fd；</li>
<li>可能增加 Unavailability（可能会增加不可用的时间）；</li>
<li>可能增加端到端的延迟；</li>
<li>client 端将会使用更多的内存。</li>
</ul>
</li>
</ul>
<p>关于 Partition 的设置可以参考这篇文章<a href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/" target="_blank" rel="external">How to choose the number of topics/partitions in a Kafka cluster?</a>，这里简单讲述一下，Partition 的增加将会带来以下几个优点和缺点：</p>
<ol>
<li>增加吞吐量：对于 consumer 来说，一个 partition 只能被一个 consumer 线程所消费，适当增加 partition 数，可以增加 consumer 的并发，进而增加系统的吞吐量；</li>
<li>需要更多的 fd：对于每一个 segment，在 broker 都会有一个对应的 index 和实际数据文件，而对于 Kafka Broker，它将会对于每个 segment 每个 index 和数据文件都会打开相应的 file handle（可以理解为 fd），因此，partition 越多，将会带来更多的 fd；</li>
<li>可能会增加数据不可用性（主要是指增加不可用时间）：主要是指 broker 宕机的情况，越多的 partition 将会意味着越多的 partition 需要 leader 选举（leader 在宕机这台 broker 的 partition 需要重新选举），特别是如果刚好 controller 宕机，重新选举的 controller 将会首先读取所有 partition 的 metadata，然后才进行相应的 leader 选举，这将会带来更大不可用时间；</li>
<li>可能增加 End-to-end 延迟：一条消息只有其被同步到 isr 的所有 broker 上后，才能被消费，partition 越多，不同节点之间同步就越多，这可能会带来毫秒级甚至数十毫秒级的延迟；</li>
<li>Client 将会需要更多的内存：Producer 和 Consumer 都会按照 partition 去缓存数据，每个 partition 都会带来数十 KB 的消耗，partition 越多, Client 将会占用更多的内存。</li>
</ol>
<h2 id="Producer-的相关配置、性能调优及监控"><a href="#Producer-的相关配置、性能调优及监控" class="headerlink" title="Producer 的相关配置、性能调优及监控"></a>Producer 的相关配置、性能调优及监控</h2><h3 id="Quotas"><a href="#Quotas" class="headerlink" title="Quotas"></a>Quotas</h3><ul>
<li>避免被恶意 Client 攻击，保证 SLA；</li>
<li>设置 produce 和 fetch 请求的字节速率阈值；</li>
<li>可以应用在 user、client-id、或者 user 和 client-id groups；</li>
<li>Broker 端的 metrics 监控：throttle-rate、byte-rate；</li>
<li><code>replica.fetch.response.max.bytes</code>：用于限制 replica 拉取请求的内存使用；</li>
<li>进行数据迁移时限制贷款的使用，<code>kafka-reassign-partitions.sh -- -throttle option</code>。</li>
</ul>
<h3 id="Kafka-Producer"><a href="#Kafka-Producer" class="headerlink" title="Kafka Producer"></a>Kafka Producer</h3><ul>
<li>使用 Java 版的 Client；</li>
<li>使用 <code>kafka-producer-perf-test.sh</code> 测试你的环境；</li>
<li>设置内存、CPU、batch 压缩；<ul>
<li>batch.size：该值设置越大，吞吐越大，但延迟也会越大；</li>
<li>linger.ms：表示 batch 的超时时间，该值越大，吞吐越大、但延迟也会越大；</li>
<li><code>max.in.flight.requests.per.connection</code>：默认为5，表示 client 在 blocking 之前向单个连接（broker）发送的未确认请求的最大数，超过1时，将会影响数据的顺序性；</li>
<li><code>compression.type</code>：压缩设置，会提高吞吐量；</li>
<li><code>acks</code>：数据 durability 的设置；</li>
</ul>
</li>
<li>避免大消息<ul>
<li>会使用更多的内存；</li>
<li>降低 Broker 的处理速度；</li>
</ul>
</li>
</ul>
<h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h3><ul>
<li>如果吞吐量小于网络带宽<ul>
<li>增加线程；</li>
<li>提高 batch.size；</li>
<li>增加更多 producer 实例；</li>
<li>增加 partition 数；</li>
</ul>
</li>
<li>设置 acks=-1 时，如果延迟增大：可以增大 <code>num.replica.fetchers</code>（follower 同步数据的线程数）来调解；</li>
<li>跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。</li>
</ul>
<h3 id="Prodcuer-监控"><a href="#Prodcuer-监控" class="headerlink" title="Prodcuer 监控"></a>Prodcuer 监控</h3><ul>
<li>batch-size-avg</li>
<li>compression-rate-avg</li>
<li>waiting-threads</li>
<li>buffer-available-bytes</li>
<li>record-queue-time-max</li>
<li>record-send-rate</li>
<li>records-per-request-avg</li>
</ul>
<h2 id="Kafka-Consumer-配置、性能调优及监控"><a href="#Kafka-Consumer-配置、性能调优及监控" class="headerlink" title="Kafka Consumer 配置、性能调优及监控"></a>Kafka Consumer 配置、性能调优及监控</h2><h3 id="Kafka-Consumer"><a href="#Kafka-Consumer" class="headerlink" title="Kafka Consumer"></a>Kafka Consumer</h3><ul>
<li>使用 <code>kafka-consumer-perf-test.sh</code> 测试环境；</li>
<li>吞吐量问题：<ul>
<li>partition 数太少；</li>
<li>OS page cache：分配足够的内存来缓存数据；</li>
<li>应用的处理逻辑；</li>
</ul>
</li>
<li>offset topic（<code>__consumer_offsets</code>）<ul>
<li><code>offsets.topic.replication.factor</code>：默认为3；</li>
<li><code>offsets.retention.minutes</code>：默认为1440，即 1day；<br>– MonitorISR，topicsize；</li>
</ul>
</li>
<li>offset commit较慢：异步 commit 或 手动 commit。</li>
</ul>
<h3 id="Consumer-配置"><a href="#Consumer-配置" class="headerlink" title="Consumer 配置"></a>Consumer 配置</h3><ul>
<li><code>fetch.min.bytes</code> 、<code>fetch.max.wait.ms</code>；</li>
<li><code>max.poll.interval.ms</code>：调用 <code>poll()</code> 之后延迟的最大时间，超过这个时间没有调用 <code>poll()</code> 的话，就会认为这个 consumer 挂掉了，将会进行 rebalance；</li>
<li><code>max.poll.records</code>：当调用 <code>poll()</code> 之后返回最大的 record 数，默认为500；</li>
<li><code>session.timeout.ms</code>；</li>
<li>Consumer Rebalance<br>– check timeouts<br>– check processing times/logic<br>– GC Issues</li>
<li>网络配置；</li>
</ul>
<h3 id="Consumer-监控"><a href="#Consumer-监控" class="headerlink" title="Consumer 监控"></a>Consumer 监控</h3><p>consumer 是否跟得上数据的发送速度。</p>
<ul>
<li>Consumer Lag：consumer offset 与 the end of log（partition 可以消费的最大 offset） 的差值；</li>
<li>监控<ul>
<li>metric 监控：records-lag-max；</li>
<li>通过 <code>bin/kafka-consumer-groups.sh</code> 查看；</li>
<li>用于 consumer 监控的 LinkedIn’s Burrow；</li>
</ul>
</li>
<li>减少 Lag<ul>
<li>分析 consumer：是 GC 问题还是 Consumer hang 住了；</li>
<li>增加 Consumer 的线程；</li>
<li>增加分区数和 consumer 线程；</li>
</ul>
</li>
</ul>
<h2 id="如何保证数据不丢"><a href="#如何保证数据不丢" class="headerlink" title="如何保证数据不丢"></a>如何保证数据不丢</h2><p>这个是常用的配置，这里截了 PPT 中的内容</p>
<p><img src="/images/kafka/not-data-less.png" alt="Kafka 数据不丢配置"></p>
<ul>
<li><code>block.on.buffer.full</code>：默认设置为 false，当达到内存设置时，可能通过 block 停止接受新的 record 或者抛出一些错误，默认情况下，Producer 将不会抛出  BufferExhaustException，而是当达到 <code>max.block.ms</code> 这个时间后直接抛出 TimeoutException。设置为 true 的意义就是将 <code>max.block.ms</code> 设置为 Long.MAX_VALUE，未来版本中这个设置将被遗弃，推荐设置 <code>max.block.ms</code>。</li>
</ul>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://www.slideshare.net/HadoopSummit/apache-kafka-best-practices" target="_blank" rel="external">Apache Kafka Best Pratices</a>；</li>
<li>胡夕-<a href="http://www.cnblogs.com/huxi2b/p/6720292.html" target="_blank" rel="external">【译】Kafka最佳实践 / Kafka Best Practices</a>；</li>
<li><a href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/" target="_blank" rel="external">How to choose the number of topics/partitions in a Kafka cluster?</a>；</li>
<li><a href="https://www.zhihu.com/question/20131784" target="_blank" rel="external">raid有哪几种有什么区别？希望讲通俗点</a>；</li>
<li><a href="https://stackoverflow.com/questions/33536061/file-descriptors-and-file-handles-and-c#" target="_blank" rel="external">File Descriptors and File Handles (and C)</a>.</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里翻译一篇关于 Kafka 实践的文章，内容来自 DataWorks Summit/Hadoop Summit（&lt;a href=&quot;https://dataworkssummit.com/munich-2017/sessions/apache-kafka-best-prac
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Producer NIO 网络模型（四）</title>
    <link href="http://matt33.com/2017/08/22/producer-nio/"/>
    <id>http://matt33.com/2017/08/22/producer-nio/</id>
    <published>2017-08-22T13:58:25.000Z</published>
    <updated>2018-05-25T10:36:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是 Kafka 源码解析的第四篇，在写这篇文章之前，专门看了一下 Java NIO 相关的内容，只有理解了 Java NIO 模型才能更好地理解 NIO 在 Kafka 中是如何应用的以及 Producer 如何利用 Java NIO 构建其网络模型（不了解的，可以先看一下上一篇文章：<a href="http://matt33.com/2017/08/12/java-nio/">谈一谈 Java IO 模型</a>），同时，本文也是对 Producer 整个流程的一个总结，主要讲述以下两个问题：</p>
<ol>
<li>Producer 的大概网络模型，与 Java NIO 模型之间关系；</li>
<li>Producer 整体流程及其整体流程详解。</li>
</ol>
<h1 id="Producer-的网络模型"><a href="#Producer-的网络模型" class="headerlink" title="Producer 的网络模型"></a>Producer 的网络模型</h1><p>KafkaProducer 通过 Sender 进行相应的 IO 操作，而 Sender 又调用 NetworkClient 来进行 IO 操作，NetworkClient 底层是对 Java NIO 进行相应的封装，其网络模型如下图所示（该图参考：<a href="http://blog.csdn.net/chunlongyu/article/details/52636762" target="_blank" rel="external">Kafka源码深度解析－序列3 －Producer －Java NIO</a>，在其基础上增加一个 KafkaProducer 成员变量的图形）。</p>
<p><img src="/images/kafka/producer-network.png" alt="Prodcuer 网络模型"></p>
<p>从图中可以看出，Sender 为最上层的接口，即调用层，Sender 调用 NetworkClient，NetworkClient 调用 Selector，而 Selector 底层封装了 Java NIO 的相关接口，从右边的图也可以看出它们之间的关系。</p>
<h1 id="Producer-整体流程"><a href="#Producer-整体流程" class="headerlink" title="Producer 整体流程"></a>Producer 整体流程</h1><p>有了对 Producer 网络模型的大概框架认识之后，下面再深入进去，看一下它们之间的调用关系以及 Producer 是如何调用 Java NIO 的相关接口，Producer 端的整体流程如下图所示。</p>
<p><img src="/images/kafka/producer-nio-flow.png" alt="Producer 整体流程"></p>
<p>这里涉及到的主要方法是：</p>
<ul>
<li><code>KafkaProducer.dosend()</code>；</li>
<li><code>Sender.run()</code>；</li>
<li><code>NetworkClient.poll()</code>（<code>NetworkClient.dosend()</code>）；</li>
<li><code>Selector.poll()</code>；</li>
</ul>
<p>下面会结合上图，对这几个方法做详细的讲解，本文下面的内容都是结合上图进行讲解。</p>
<h2 id="KafkaProducer-dosend"><a href="#KafkaProducer-dosend" class="headerlink" title="KafkaProducer.dosend()"></a>KafkaProducer.dosend()</h2><p><code>dosend()</code> 方法是读懂 Producer 的入口，具体可以参考 <a href="http://matt33.com/2017/06/25/kafka-producer-send-module/#Producer-的-doSend-实现">dosend()</a>，<code>dosend()</code> 主要做了两个事情：</p>
<ol>
<li><code>waitOnMetadata()</code>：请求更新 tp（topic-partition） meta，中间会调用 <code>sender.wakeup()</code>；</li>
<li><code>accumulator.append()</code>：将 msg 写入到其 tp 对应的 deque 中，如果该 tp 对应的 deque 新建了一个 Batch，最后也会调用 <code>sender.wakeup()</code>。</li>
</ol>
<p>这里主要关注的是 <code>sender.wakeup()</code> 方法，它的作用是将 Sender 线程从阻塞中唤醒。</p>
<h3 id="sender-wakeup-方法"><a href="#sender-wakeup-方法" class="headerlink" title="sender.wakeup() 方法"></a><code>sender.wakeup()</code> 方法</h3><p>这里来看一下 <code>sender.wakeup()</code> 具体实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// org.apache.kafka.clients.producer.internals.Sender</span></div><div class="line"><span class="comment">/**</span></div><div class="line">* Wake up the selector associated with this send thread</div><div class="line">*/</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.client.wakeup();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// org.apache.kafka.clients.NetworkClient</span></div><div class="line"><span class="comment">/**</span></div><div class="line">* Interrupt the client if it is blocked waiting on I/O.</div><div class="line">*/</div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.selector.wakeup();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// org.apache.kafka.common.network.Selector</span></div><div class="line"><span class="comment">/**</span></div><div class="line">* Interrupt the nioSelector if it is blocked waiting to do I/O.</div><div class="line">*/</div><div class="line"><span class="comment">//note: 如果 selector 是阻塞的话,就唤醒</span></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.nioSelector.wakeup();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法很简单，但也很有意思，其调用过程是下面这个样子：</p>
<ul>
<li>Sender -&gt; NetworkClient -&gt; Selector(Kafka 封装的) -&gt; Selector(Java NIO)</li>
</ul>
<p>跟上面两张图中 KafkaProducer 的总体调用过程大概一致，它的作用就是将 Sender 线程从 <code>select()</code> 方法的阻塞中唤醒，<code>select()</code> 方法的作用是轮询注册在多路复用器上的 Channel，它会一直阻塞在这个方法上，除非满足下面条件中的一个：</p>
<ul>
<li>at least one channel is selected;</li>
<li>this selector’s {@link #wakeup wakeup} method is invoked;</li>
<li>the current thread is interrupted;</li>
<li>the given timeout period expires.</li>
</ul>
<p>否则 <code>select()</code> 将会一直轮询，阻塞在这个地方，直到条件满足。</p>
<p>分析到这里，KafkaProducer 中 <code>dosend()</code> 方法调用 <code>sender.wakeup()</code> 方法作用就很明显的，作用就是：当有新的 RecordBatch 创建后，旧的 RecordBatch 就可以发送了（或者此时有 Metadata 请求需要发送），如果线程阻塞在 <code>select()</code> 方法中，就将其唤醒，Sender 重新开始运行 <code>run()</code> 方法，在这个方法中，旧的 RecordBatch （或相应的 Metadata 请求）将会被选中，进而可以及时将这些请求发送出去。</p>
<h2 id="Sender-run"><a href="#Sender-run" class="headerlink" title="Sender.run()"></a>Sender.run()</h2><p>每次循环都是从 Sender 的 <code>run()</code> 方法开始，具体代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Sender 线程每次循环具体执行的地方</span></div><div class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">        Cluster cluster = metadata.fetch();</div><div class="line">        <span class="comment">//note: Step1 获取那些已经可以发送的 RecordBatch 对应的 nodes</span></div><div class="line">        RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</div><div class="line"></div><div class="line">        <span class="comment">//note: Step2  如果有 topic-partition 的 leader 是未知的,就强制 metadata 更新</span></div><div class="line">        <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</div><div class="line">            <span class="keyword">for</span> (String topic : result.unknownLeaderTopics)</div><div class="line">                <span class="keyword">this</span>.metadata.add(topic);</div><div class="line">            <span class="keyword">this</span>.metadata.requestUpdate();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 如果与node 没有连接（如果可以连接,会初始化该连接）,暂时先移除该 node</span></div><div class="line">        Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</div><div class="line">        <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</div><div class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</div><div class="line">            Node node = iter.next();</div><div class="line">            <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) &#123;<span class="comment">//note: 没有建立连接的 broker,这里会与其建立连接</span></div><div class="line">                iter.remove();</div><div class="line">                notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.connectionDelay(node, now));</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: Step3  返回该 node 对应的所有可以发送的 RecordBatch 组成的 batches（key 是 node.id,这些 batches 将会在一个 request 中发送）</span></div><div class="line">        Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster,</div><div class="line">                                                                         result.readyNodes,</div><div class="line">                                                                         <span class="keyword">this</span>.maxRequestSize,</div><div class="line">                                                                         now);</div><div class="line">        <span class="comment">//note: 保证一个 tp 只有一个 RecordBatch 在发送,保证有序性</span></div><div class="line">        <span class="comment">//note: max.in.flight.requests.per.connection 设置为1时会保证</span></div><div class="line">        <span class="keyword">if</span> (guaranteeMessageOrder) &#123;</div><div class="line">            <span class="comment">// Mute all the partitions draine</span></div><div class="line">            <span class="keyword">for</span> (List&lt;RecordBatch&gt; batchList : batches.values()) &#123;</div><div class="line">                <span class="keyword">for</span> (RecordBatch batch : batchList)</div><div class="line">                    <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 将由于元数据不可用而导致发送超时的 RecordBatch 移除</span></div><div class="line">        List&lt;RecordBatch&gt; expiredBatches = <span class="keyword">this</span>.accumulator.abortExpiredBatches(<span class="keyword">this</span>.requestTimeout, now);</div><div class="line">        <span class="keyword">for</span> (RecordBatch expiredBatch : expiredBatches)</div><div class="line">            <span class="keyword">this</span>.sensors.recordErrors(expiredBatch.topicPartition.topic(), expiredBatch.recordCount);</div><div class="line"></div><div class="line">        sensors.updateProduceRequestMetrics(batches);</div><div class="line"></div><div class="line">        <span class="keyword">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</div><div class="line">        <span class="keyword">if</span> (!result.readyNodes.isEmpty()) &#123;</div><div class="line">            log.trace(<span class="string">"Nodes with data ready to send: &#123;&#125;"</span>, result.readyNodes);</div><div class="line">            pollTimeout = <span class="number">0</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: Step4 发送 RecordBatch</span></div><div class="line">        sendProduceRequests(batches, now);</div><div class="line"></div><div class="line">        <span class="comment">//note: 如果有 partition 可以立马发送数据,那么 pollTimeout 为0.</span></div><div class="line">        <span class="comment">//note: Step5 关于 socket 的一些实际的读写操作</span></div><div class="line">        <span class="keyword">this</span>.client.poll(pollTimeout, now);</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p><code>Sender.run()</code> 的大概流程总共有以下五步：</p>
<ol>
<li><code>accumulator.ready()</code>：遍历所有的 tp（topic-partition），如果其对应的 RecordBatch 可以发送（大小达到 <code>batch.size</code> 大小或时间达到 <code>linger.ms</code>），就将其对应的 leader 选出来，最后会返回一个可以发送 Produce request 的 <code>Set&lt;Node&gt;</code>（实际返回的是 <code>ReadyCheckResult</code> 实例，不过 <code>Set&lt;Node&gt;</code> 是最主要的成员变量）；</li>
<li>如果发现有 tp 没有 leader，那么这里就调用 <code>requestUpdate()</code> 方法更新 metadata，实际上还是在第一步对 tp 的遍历中，遇到没有 leader 的 tp 就将其加入到一个叫做  <code>unknownLeaderTopics</code> 的 set 中，然后会请求这个 tp 的 meta（meta 的更新策略可以参考之前的一篇博客 <a href="http://matt33.com/2017/07/08/kafka-producer-metadata/#Producer-Metadata-的更新策略">Producer Metadata 的更新策略</a>）；</li>
<li><code>accumulator.drain()</code>：遍历每个 leader （第一步中选出）上的所有 tp，如果该 tp 对应的 RecordBatch 不在 backoff 期间（没有重试过，或者重试了但是间隔已经达到了 retryBackoffMs ），并且加上这个 RecordBatch 其大小不超过 maxSize（一个 request 的最大限制，默认为 1MB），那么就把这个 RecordBatch 添加 list 中，最终返回的类型为 <code>Map&lt;Integer, List&lt;RecordBatch&gt;&gt;</code>，key 为 leader.id，value 为要发送的 RecordBatch 的列表；</li>
<li><code>sendProduceRequests()</code>：发送 Produce 请求，从图中，可以看出，这个方法会调用 <code>NetworkClient.send()</code> 来发送 clientRequest；</li>
<li><code>NetworkClient.poll()</code>：关于 socket 的 IO 操作都是在这个方法进行的，它还是调用 Selector 进行的相应操作，而 Selector 底层则是封装的 Java NIO 的相关接口，这个下面会详细讲述。</li>
</ol>
<p>在第三步中，可以看到，如果要向一个 leader 发送 Produce 请求，那么这 leader 对应 tp，如果其 RecordBatch 没有达到要求（<code>batch.size</code> 或 <code>linger.ms</code> 都没达到）还是可能会发送，这样做的好处是：可以减少 request 的频率，有利于提供发送效率。</p>
<h2 id="NetworkClient-poll"><a href="#NetworkClient-poll" class="headerlink" title="NetworkClient.poll()"></a>NetworkClient.poll()</h2><p>这个方法也是一个非常重要的方法，其作用简单来说有三点：</p>
<ul>
<li>如果需要更新 Metadata，那么就发送 Metadata 请求；</li>
<li>调用 Selector 进行相应的 IO 操作；</li>
<li>处理 Server 端的 response 及一些其他的操作。</li>
</ul>
<p>具体代码如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> List&lt;ClientResponse&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout, <span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">        <span class="comment">//note: Step1 判断是否需要更新 meta,如果需要就更新（请求更新 metadata 的地方）</span></div><div class="line">        <span class="keyword">long</span> metadataTimeout = metadataUpdater.maybeUpdate(now);</div><div class="line">        <span class="comment">//note: Step2 调用 Selector.poll() 进行 socket 相关的 IO 操作</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">this</span>.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            log.error(<span class="string">"Unexpected error during I/O"</span>, e);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: Step3 处理完成后的操作</span></div><div class="line">        <span class="keyword">long</span> updatedNow = <span class="keyword">this</span>.time.milliseconds();</div><div class="line">        List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        handleAbortedSends(responses);</div><div class="line">        <span class="comment">//note: 处理已经完成的 send（不需要 response 的 request,如 send）</span></div><div class="line">        handleCompletedSends(responses, updatedNow);<span class="comment">//note: 通过 selector 中获取 Server 端的 response</span></div><div class="line">        <span class="comment">//note: 处理从 server 端接收到 Receive（如 Metadata 请求）</span></div><div class="line">        handleCompletedReceives(responses, updatedNow);<span class="comment">//note: 在返回的 handler 中，会处理 metadata 的更新</span></div><div class="line">        <span class="comment">//note: 处理连接失败那些连接,重新请求 meta</span></div><div class="line">        handleDisconnections(responses, updatedNow);</div><div class="line">        <span class="comment">//note: 处理新建立的那些连接（还不能发送请求,比如:还未认证）</span></div><div class="line">        handleConnections();</div><div class="line">        handleInitiateApiVersionRequests(updatedNow);</div><div class="line">        handleTimedOutRequests(responses, updatedNow);</div><div class="line"></div><div class="line">        <span class="comment">// invoke callbacks</span></div><div class="line">        <span class="keyword">for</span> (ClientResponse response : responses) &#123;</div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">                response.onComplete();</div><div class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">                log.error(<span class="string">"Uncaught error in request completion:"</span>, e);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> responses;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>这个方法大致分为三步，这里详述讲述一下：</p>
<ol>
<li><code>metadataUpdater.maybeUpdate()</code>：如果 Metadata 需要更新，那么就选择连接数最小的 node，发送 Metadata 请求，详细流程可以参考之前那篇博客<a href="http://matt33.com/2017/07/08/kafka-producer-metadata/#Producer-的-Metadata-更新流程">Producer 的 Metadata 更新流程</a>；</li>
<li><code>selector.poll()</code>：进行 socket IO 相关的操作，下面会详细讲述；</li>
<li>process completed actions：在一个 <code>select()</code> 过程之后的相关处理。<ul>
<li><code>handleAbortedSends(responses)</code>：处理那么在发送过程出现 <code>UnsupportedVersionException</code> 异常的 request；</li>
<li><code>handleCompletedSends(responses, updatedNow)</code>：处理那些已经完成的 request，如果是那些不需要 response 的 request 的话，这里直接调用 <code>request.completed()</code>，标志着这个 request 发送处理完成；</li>
<li><code>handleCompletedReceives(responses, updatedNow)</code>：处理那些从 Server 端接收的 Receive，metadata 更新就是在这里处理的（以及 <code>ApiVersionsResponse</code>）；</li>
<li><code>handleDisconnections(responses, updatedNow)</code>：处理连接失败那些连接,重新请求 metadata；</li>
<li><code>handleConnections()</code>：处理新建立的那些连接（还不能发送请求,比如:还未认证）；</li>
<li><code>handleInitiateApiVersionRequests(updatedNow)</code>：对那些新建立的连接，发送 apiVersionRequest（默认情况：第一次建立连接时，需要向 Broker 发送 ApiVersionRequest 请求）；</li>
<li><code>handleTimedOutRequests(responses, updatedNow)</code>：处理 timeout 的连接，关闭该连接，并刷新 Metadata。</li>
</ul>
</li>
</ol>
<h2 id="Selector-poll"><a href="#Selector-poll" class="headerlink" title="Selector.poll()"></a>Selector.poll()</h2><p> Selector 类是 Kafka 对 Java NIO 相关接口的封装，socket IO 相关的操作都是这个类中完成的，这里先看一下 <code>poll()</code> 方法，主要的操作都是这个方法中调用的，其代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        <span class="keyword">if</span> (timeout &lt; <span class="number">0</span>)</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"timeout should be &gt;= 0"</span>);</div><div class="line"></div><div class="line">        <span class="comment">//note: Step1 清除相关记录</span></div><div class="line">        clear();</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (hasStagedReceives() || !immediatelyConnectedKeys.isEmpty())</div><div class="line">            timeout = <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="comment">/* check ready keys */</span></div><div class="line">        <span class="comment">//note: Step2 获取就绪事件的数</span></div><div class="line">        <span class="keyword">long</span> startSelect = time.nanoseconds();</div><div class="line">        <span class="keyword">int</span> readyKeys = select(timeout);</div><div class="line">        <span class="keyword">long</span> endSelect = time.nanoseconds();</div><div class="line">        <span class="keyword">this</span>.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());</div><div class="line"></div><div class="line">        <span class="comment">//note: Step3 处理 io 操作</span></div><div class="line">        <span class="keyword">if</span> (readyKeys &gt; <span class="number">0</span> || !immediatelyConnectedKeys.isEmpty()) &#123;</div><div class="line">            pollSelectionKeys(<span class="keyword">this</span>.nioSelector.selectedKeys(), <span class="keyword">false</span>, endSelect);</div><div class="line">            pollSelectionKeys(immediatelyConnectedKeys, <span class="keyword">true</span>, endSelect);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: Step4 将处理得到的 stagedReceives 添加到 completedReceives 中</span></div><div class="line">        addToCompletedReceives();</div><div class="line"></div><div class="line">        <span class="keyword">long</span> endIo = time.nanoseconds();</div><div class="line">        <span class="keyword">this</span>.sensors.ioTime.record(endIo - endSelect, time.milliseconds());</div><div class="line"></div><div class="line">        <span class="comment">// we use the time at the end of select to ensure that we don't close any connections that</span></div><div class="line">        <span class="comment">// have just been processed in pollSelectionKeys</span></div><div class="line">        <span class="comment">//note: 每次 poll 之后会调用一次</span></div><div class="line">        <span class="comment">//<span class="doctag">TODO:</span> 连接虽然关闭了,但是 Client 端的缓存依然存在</span></div><div class="line">        maybeCloseOldestConnection(endSelect);</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p><code>Selector.poll()</code> 方法会进行四步操作，这里分别来介绍一些。</p>
<h3 id="clear"><a href="#clear" class="headerlink" title="clear()"></a>clear()</h3><p><code>clear()</code> 方法是在每次 <code>poll()</code> 执行的第一步，它作用的就是清理上一次 poll 过程产生的部分缓存。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 每次 poll 调用前都会清除以下缓存</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">this</span>.completedSends.clear();</div><div class="line">    <span class="keyword">this</span>.completedReceives.clear();</div><div class="line">    <span class="keyword">this</span>.connected.clear();</div><div class="line">    <span class="keyword">this</span>.disconnected.clear();</div><div class="line">    <span class="comment">// Remove closed channels after all their staged receives have been processed or if a send was requested</span></div><div class="line">    <span class="keyword">for</span> (Iterator&lt;Map.Entry&lt;String, KafkaChannel&gt;&gt; it = closingChannels.entrySet().iterator(); it.hasNext(); ) &#123;</div><div class="line">        KafkaChannel channel = it.next().getValue();</div><div class="line">        Deque&lt;NetworkReceive&gt; deque = <span class="keyword">this</span>.stagedReceives.get(channel);</div><div class="line">        <span class="keyword">boolean</span> sendFailed = failedSends.remove(channel.id());</div><div class="line">        <span class="keyword">if</span> (deque == <span class="keyword">null</span> || deque.isEmpty() || sendFailed) &#123;</div><div class="line">            doClose(channel, <span class="keyword">true</span>);</div><div class="line">            it.remove();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">this</span>.disconnected.addAll(<span class="keyword">this</span>.failedSends);</div><div class="line">    <span class="keyword">this</span>.failedSends.clear();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="select"><a href="#select" class="headerlink" title="select()"></a>select()</h3><p>Selector 的 <code>select()</code> 方法在实现上底层还是调用 Java NIO 原生的接口，这里的 <code>nioSelector</code> 其实就是 <code>java.nio.channels.Selector</code> 的实例对象，这个方法最坏情况下，会阻塞 ms 的时间，如果在一次轮询，只要有一个 Channel 的事件就绪，它就会立马返回。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">select</span><span class="params">(<span class="keyword">long</span> ms)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    <span class="keyword">if</span> (ms &lt; <span class="number">0L</span>)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"timeout should be &gt;= 0"</span>);</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (ms == <span class="number">0L</span>)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.nioSelector.selectNow();</div><div class="line">    <span class="keyword">else</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.nioSelector.select(ms);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="pollSelectionKeys"><a href="#pollSelectionKeys" class="headerlink" title="pollSelectionKeys()"></a>pollSelectionKeys()</h3><p>这部分是 socket IO 的主要部分，发送 Send 及接收 Receive 都是在这里完成的，在 <code>poll()</code> 方法中，这个方法会调用两次：</p>
<ol>
<li>第一次调用的目的是：处理已经就绪的事件，进行相应的 IO 操作；</li>
<li>第二次调用的目的是：处理新建立的那些连接，添加缓存及传输层（Kafka 又封装了一次，这里后续文章会讲述）的握手与认证。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">pollSelectionKeys</span><span class="params">(Iterable&lt;SelectionKey&gt; selectionKeys,</span></span></div><div class="line">                                   <span class="keyword">boolean</span> isImmediatelyConnected,</div><div class="line">                                   <span class="keyword">long</span> currentTimeNanos) &#123;</div><div class="line">        Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator();</div><div class="line">        <span class="keyword">while</span> (iterator.hasNext()) &#123;</div><div class="line">            SelectionKey key = iterator.next();</div><div class="line">            iterator.remove();</div><div class="line">            KafkaChannel channel = channel(key);</div><div class="line"></div><div class="line">            <span class="comment">// register all per-connection metrics at once</span></div><div class="line">            sensors.maybeRegisterConnectionMetrics(channel.id());</div><div class="line">            <span class="keyword">if</span> (idleExpiryManager != <span class="keyword">null</span>)</div><div class="line">                idleExpiryManager.update(channel.id(), currentTimeNanos);</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">                <span class="comment">/* complete any connections that have finished their handshake (either normally or immediately) */</span></div><div class="line">                <span class="comment">//note: 处理一些刚建立 tcp 连接的 channel</span></div><div class="line">                <span class="keyword">if</span> (isImmediatelyConnected || key.isConnectable()) &#123;</div><div class="line">                    <span class="keyword">if</span> (channel.finishConnect()) &#123;<span class="comment">//note: 连接已经建立</span></div><div class="line">                        <span class="keyword">this</span>.connected.add(channel.id());</div><div class="line">                        <span class="keyword">this</span>.sensors.connectionCreated.record();</div><div class="line">                        SocketChannel socketChannel = (SocketChannel) key.channel();</div><div class="line">                        log.debug(<span class="string">"Created socket with SO_RCVBUF = &#123;&#125;, SO_SNDBUF = &#123;&#125;, SO_TIMEOUT = &#123;&#125; to node &#123;&#125;"</span>,</div><div class="line">                                socketChannel.socket().getReceiveBufferSize(),</div><div class="line">                                socketChannel.socket().getSendBufferSize(),</div><div class="line">                                socketChannel.socket().getSoTimeout(),</div><div class="line">                                channel.id());</div><div class="line">                    &#125; <span class="keyword">else</span></div><div class="line">                        <span class="keyword">continue</span>;</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="comment">/* if channel is not ready finish prepare */</span></div><div class="line">                <span class="comment">//note: 处理 tcp 连接还未完成的连接,进行传输层的握手及认证</span></div><div class="line">                <span class="keyword">if</span> (channel.isConnected() &amp;&amp; !channel.ready())</div><div class="line">                    channel.prepare();</div><div class="line"></div><div class="line">                <span class="comment">/* if channel is ready read from any connections that have readable data */</span></div><div class="line">                <span class="keyword">if</span> (channel.ready() &amp;&amp; key.isReadable() &amp;&amp; !hasStagedReceive(channel)) &#123;</div><div class="line">                    NetworkReceive networkReceive;</div><div class="line">                    <span class="keyword">while</span> ((networkReceive = channel.read()) != <span class="keyword">null</span>)<span class="comment">//note: 知道读取一个完整的 Receive,才添加到集合中</span></div><div class="line">                        addToStagedReceives(channel, networkReceive);<span class="comment">//note: 读取数据</span></div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="comment">/* if channel is ready write to any sockets that have space in their buffer and for which we have data */</span></div><div class="line">                <span class="keyword">if</span> (channel.ready() &amp;&amp; key.isWritable()) &#123;</div><div class="line">                    Send send = channel.write();</div><div class="line">                    <span class="keyword">if</span> (send != <span class="keyword">null</span>) &#123;</div><div class="line">                        <span class="keyword">this</span>.completedSends.add(send);<span class="comment">//note: 将完成的 send 添加到 list 中</span></div><div class="line">                        <span class="keyword">this</span>.sensors.recordBytesSent(channel.id(), send.size());</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="comment">/* cancel any defunct sockets */</span></div><div class="line">                <span class="comment">//note: 关闭断开的连接</span></div><div class="line">                <span class="keyword">if</span> (!key.isValid())</div><div class="line">                    close(channel, <span class="keyword">true</span>);</div><div class="line"></div><div class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">                String desc = channel.socketDescription();</div><div class="line">                <span class="keyword">if</span> (e <span class="keyword">instanceof</span> IOException)</div><div class="line">                    log.debug(<span class="string">"Connection with &#123;&#125; disconnected"</span>, desc, e);</div><div class="line">                <span class="keyword">else</span></div><div class="line">                    log.warn(<span class="string">"Unexpected error from &#123;&#125;; closing connection"</span>, desc, e);</div><div class="line">                close(channel, <span class="keyword">true</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<h3 id="addToCompletedReceives"><a href="#addToCompletedReceives" class="headerlink" title="addToCompletedReceives()"></a>addToCompletedReceives()</h3><p>这个方法的目的是处理接收到的 Receive，由于 Selector 这个类在 Client 和 Server 端都会调用，这里分两种情况讲述一下：</p>
<ol>
<li>应用在 Server 端时，后续文章会详细介绍，这里简单说一下，Server 为了保证消息的时序性，在 Selector 中提供了两个方法：<code>mute(String id)</code> 和 <code>unmute(String id)</code>，对该 KafkaChannel 做标记来保证同时只能处理这个 Channel 的一个 request（可以理解为排它锁）。当 Server 端接收到 request 后，先将其放入 <code>stagedReceives</code> 集合中，此时该 Channel 还未 mute，这个 Receive 会被放入 <code>completedReceives</code> 集合中。Server 在对 <code>completedReceives</code> 集合中的 request 进行处理时，会先对该 Channel mute，处理后的 response 发送完成后再对该 Channel unmute，然后才能处理该 Channel 其他的请求；</li>
<li>应用在 Client 端时，Client 并不会调用 Selector 的 <code>mute()</code> 和 <code>unmute()</code> 方法，client 的时序性而是通过 <code>InFlightRequests</code> 和 RecordAccumulator 的 <code>mutePartition</code> 来保证的（下篇文章会讲述），因此对于 Client 端而言，这里接收到的所有 Receive 都会被放入到 <code>completedReceives</code> 的集合中等待后续处理。</li>
</ol>
<p>这个方法只有配合 Server 端的调用才能看明白其作用，它统一 Client 和 Server 调用的 api，使得都可以使用 Selector 这个类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * checks if there are any staged receives and adds to completedReceives</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addToCompletedReceives</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!<span class="keyword">this</span>.stagedReceives.isEmpty()) &#123;<span class="comment">//note: 处理 stagedReceives</span></div><div class="line">        Iterator&lt;Map.Entry&lt;KafkaChannel, Deque&lt;NetworkReceive&gt;&gt;&gt; iter = <span class="keyword">this</span>.stagedReceives.entrySet().iterator();</div><div class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</div><div class="line">            Map.Entry&lt;KafkaChannel, Deque&lt;NetworkReceive&gt;&gt; entry = iter.next();</div><div class="line">            KafkaChannel channel = entry.getKey();</div><div class="line">            <span class="keyword">if</span> (!channel.isMute()) &#123;</div><div class="line">                Deque&lt;NetworkReceive&gt; deque = entry.getValue();</div><div class="line">                addToCompletedReceives(channel, deque);</div><div class="line">                <span class="keyword">if</span> (deque.isEmpty())</div><div class="line">                    iter.remove();</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addToCompletedReceives</span><span class="params">(KafkaChannel channel, Deque&lt;NetworkReceive&gt; stagedDeque)</span> </span>&#123;</div><div class="line">    NetworkReceive networkReceive = stagedDeque.poll();</div><div class="line">    <span class="keyword">this</span>.completedReceives.add(networkReceive); <span class="comment">//note: 添加到 completedReceives 中</span></div><div class="line">    <span class="keyword">this</span>.sensors.recordBytesReceived(channel.id(), networkReceive.payload().limit());</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Network-send-方法"><a href="#Network-send-方法" class="headerlink" title="Network.send() 方法"></a>Network.send() 方法</h2><p>至此，文章的主要内容已经讲述得差不多了，第二张图中最上面的那个调用关系已经讲述完，下面讲述一下另外一个小分支，也就是从 <code>Sender.run()</code> 调用 <code>NetworkClient.send()</code> 开始的那部分，其调用过程如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Sender.run()</div><div class="line">Sender.sendProduceRequests()</div><div class="line">NetworkClient.send()</div><div class="line">NetworkClient.dosend()</div><div class="line">Selector.send()</div><div class="line">KafkaChannel.setSend()</div></pre></td></tr></table></figure>
<h3 id="NetworkClient-dosend"><a href="#NetworkClient-dosend" class="headerlink" title="NetworkClient.dosend()"></a>NetworkClient.dosend()</h3><p>Producer 端的请求都是通过 <code>NetworkClient.dosend()</code> 来发送的，其作用就是：</p>
<ul>
<li>检查版本信息，并根据 <code>apiKey()</code> 构建 Request；</li>
<li>创建 <code>NetworkSend</code> 实例；</li>
<li>调用 <code>Selector.send</code> 发送该 Send。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送请求</span></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSend</span><span class="params">(ClientRequest clientRequest, <span class="keyword">boolean</span> isInternalRequest, <span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">        String nodeId = clientRequest.destination();</div><div class="line">        <span class="keyword">if</span> (!isInternalRequest) &#123;</div><div class="line">            <span class="comment">// If this request came from outside the NetworkClient, validate</span></div><div class="line">            <span class="comment">// that we can send data.  If the request is internal, we trust</span></div><div class="line">            <span class="comment">// that that internal code has done this validation.  Validation</span></div><div class="line">            <span class="comment">// will be slightly different for some internal requests (for</span></div><div class="line">            <span class="comment">// example, ApiVersionsRequests can be sent prior to being in</span></div><div class="line">            <span class="comment">// READY state.)</span></div><div class="line">            <span class="keyword">if</span> (!canSendRequest(nodeId))</div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Attempt to send a request to node "</span> + nodeId + <span class="string">" which is not ready."</span>);</div><div class="line">        &#125;</div><div class="line">        AbstractRequest request = <span class="keyword">null</span>;</div><div class="line">        AbstractRequest.Builder&lt;?&gt; builder = clientRequest.requestBuilder();</div><div class="line">        <span class="comment">//note: 构建 AbstractRequest, 检查其版本信息</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            NodeApiVersions versionInfo = nodeApiVersions.get(nodeId);</div><div class="line">            <span class="comment">// Note: if versionInfo is null, we have no server version information. This would be</span></div><div class="line">            <span class="comment">// the case when sending the initial ApiVersionRequest which fetches the version</span></div><div class="line">            <span class="comment">// information itself.  It is also the case when discoverBrokerVersions is set to false.</span></div><div class="line">            <span class="keyword">if</span> (versionInfo == <span class="keyword">null</span>) &#123;</div><div class="line">                <span class="keyword">if</span> (discoverBrokerVersions &amp;&amp; log.isTraceEnabled())</div><div class="line">                    log.trace(<span class="string">"No version information found when sending message of type &#123;&#125; to node &#123;&#125;. "</span> +</div><div class="line">                            <span class="string">"Assuming version &#123;&#125;."</span>, clientRequest.apiKey(), nodeId, builder.version());</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                <span class="keyword">short</span> version = versionInfo.usableVersion(clientRequest.apiKey());</div><div class="line">                builder.setVersion(version);</div><div class="line">            &#125;</div><div class="line">            <span class="comment">// The call to build may also throw UnsupportedVersionException, if there are essential</span></div><div class="line">            <span class="comment">// fields that cannot be represented in the chosen version.</span></div><div class="line">            request = builder.build();<span class="comment">//note: 当为 Produce 请求时,转化为 ProduceRequest,Metadata 请求时,转化为 Metadata 请求</span></div><div class="line">        &#125; <span class="keyword">catch</span> (UnsupportedVersionException e) &#123;</div><div class="line">            <span class="comment">// If the version is not supported, skip sending the request over the wire.</span></div><div class="line">            <span class="comment">// Instead, simply add it to the local queue of aborted requests.</span></div><div class="line">            log.debug(<span class="string">"Version mismatch when attempting to send &#123;&#125; to &#123;&#125;"</span>,</div><div class="line">                    clientRequest.toString(), clientRequest.destination(), e);</div><div class="line">            ClientResponse clientResponse = <span class="keyword">new</span> ClientResponse(clientRequest.makeHeader(),</div><div class="line">                    clientRequest.callback(), clientRequest.destination(), now, now,</div><div class="line">                    <span class="keyword">false</span>, e, <span class="keyword">null</span>);</div><div class="line">            abortedSends.add(clientResponse);</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        RequestHeader header = clientRequest.makeHeader();</div><div class="line">        <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</div><div class="line">            <span class="keyword">int</span> latestClientVersion = ProtoUtils.latestVersion(clientRequest.apiKey().id);</div><div class="line">            <span class="keyword">if</span> (header.apiVersion() == latestClientVersion) &#123;</div><div class="line">                log.trace(<span class="string">"Sending &#123;&#125; to node &#123;&#125;."</span>, request, nodeId);</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                log.debug(<span class="string">"Using older server API v&#123;&#125; to send &#123;&#125; to node &#123;&#125;."</span>,</div><div class="line">                    header.apiVersion(), request, nodeId);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: Send是一个接口，这里返回的是 NetworkSend，而 NetworkSend 继承 ByteBufferSend</span></div><div class="line">        Send send = request.toSend(nodeId, header);</div><div class="line">        InFlightRequest inFlightRequest = <span class="keyword">new</span> InFlightRequest(</div><div class="line">                header,</div><div class="line">                clientRequest.createdTimeMs(),</div><div class="line">                clientRequest.destination(),</div><div class="line">                clientRequest.callback(),</div><div class="line">                clientRequest.expectResponse(),</div><div class="line">                isInternalRequest,</div><div class="line">                send,</div><div class="line">                now);</div><div class="line">        <span class="keyword">this</span>.inFlightRequests.add(inFlightRequest);</div><div class="line">        <span class="comment">//note: 将 send 和对应 kafkaChannel 绑定起来，并开启该 kafkaChannel 底层 socket 的写事件</span></div><div class="line">        selector.send(inFlightRequest.send);</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<h3 id="Selector-send"><a href="#Selector-send" class="headerlink" title="Selector.send()"></a>Selector.send()</h3><p>这个方法就比较容易理解了，它的作用就是获取该 Send 对应的 KafkaChannel，调用 <code>setSend()</code> 向 KafkaChannel 注册一个 <code>Write</code> 事件。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送请求</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(Send send)</span> </span>&#123;</div><div class="line">    String connectionId = send.destination();</div><div class="line">    <span class="keyword">if</span> (closingChannels.containsKey(connectionId))</div><div class="line">        <span class="keyword">this</span>.failedSends.add(connectionId);</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">        KafkaChannel channel = channelOrFail(connectionId, <span class="keyword">false</span>);</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            channel.setSend(send);</div><div class="line">        &#125; <span class="keyword">catch</span> (CancelledKeyException e) &#123;</div><div class="line">            <span class="keyword">this</span>.failedSends.add(connectionId);</div><div class="line">            close(channel, <span class="keyword">false</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="KafkaChannel-setSend"><a href="#KafkaChannel-setSend" class="headerlink" title="KafkaChannel.setSend()"></a>KafkaChannel.setSend()</h3><p><code>setSend()</code> 方法需要配合 <code>write()</code>（该方法是在 <code>Selector.poll()</code> 中调用的） 方法一起来看</p>
<ul>
<li><code>setSend()</code>：将当前 KafkaChannel 的 Send 赋值为要发送的 Send，并注册一个 <code>OP_WRITE</code> 事件；</li>
<li><code>write()</code>：发送当前的 Send，发送完后删除注册的 <code>OP_WRITE</code> 事件。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 每次调用时都会注册一个 OP_WRITE 事件</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSend</span><span class="params">(Send send)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.send != <span class="keyword">null</span>)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Attempt to begin a send operation with prior send operation still in progress."</span>);</div><div class="line">    <span class="keyword">this</span>.send = send;</div><div class="line">    <span class="keyword">this</span>.transportLayer.addInterestOps(SelectionKey.OP_WRITE);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 调用 send() 发送 Send</span></div><div class="line"><span class="function"><span class="keyword">public</span> Send <span class="title">write</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    Send result = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">if</span> (send != <span class="keyword">null</span> &amp;&amp; send(send)) &#123;</div><div class="line">        result = send;</div><div class="line">        send = <span class="keyword">null</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 发送完成后,就删除这个 WRITE 事件</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">send</span><span class="params">(Send send)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    send.writeTo(transportLayer);</div><div class="line">    <span class="keyword">if</span> (send.completed())</div><div class="line">        transportLayer.removeInterestOps(SelectionKey.OP_WRITE);</div><div class="line"></div><div class="line">    <span class="keyword">return</span> send.completed();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，简单总结一下，可以回过头再看一下第一张图，对于 KafkaProducer 而言，其直接调用是 Sender，而 Sender 底层调用的是 NetworkClient，NetworkClient 则是通过 Selector 实现，Selector 则是对 Java NIO 原生接口的封装。</p>
<hr>
<p>参考文献：</p>
<ul>
<li><a href="http://blog.csdn.net/chunlongyu/article/details/52636762" target="_blank" rel="external">Kafka源码深度解析－序列3 －Producer －Java NIO</a></li>
<li><a href="http://blog.csdn.net/chunlongyu/article/details/52651960" target="_blank" rel="external"> Kafka源码深度解析－序列4 －Producer －network层核心原理</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是 Kafka 源码解析的第四篇，在写这篇文章之前，专门看了一下 Java NIO 相关的内容，只有理解了 Java NIO 模型才能更好地理解 NIO 在 Kafka 中是如何应用的以及 Producer 如何利用 Java NIO 构建其网络模型（不了解的，可以先看
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>谈一谈 Java IO 模型</title>
    <link href="http://matt33.com/2017/08/12/java-nio/"/>
    <id>http://matt33.com/2017/08/12/java-nio/</id>
    <published>2017-08-12T09:51:56.000Z</published>
    <updated>2017-08-18T14:03:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Java IO 模型对于 Java 开发工程师来说，是日常工作中经常接触的内容，特别是随着分布式系统的兴起，IO 也显得越来越重要，Java 的 IO 模型本质上还是利用操作系统提供的接口来实现，不熟悉这一部分内容的话，可以先看一下上篇文章<a href="http://matt33.com/2017/08/06/unix-io/">Unix 网络 IO 模型及 Linux 的 IO 多路复用模型</a>，本文跟上篇的内容是紧密相连的，特别是本文的重点 —— Java NIO 部分，其底层原理就是 UNIX 的 IO 多路复用，IO 多路复用在上篇文章中讲述了很多。</p>
<p>这篇文章大概内容如下：</p>
<ol>
<li>Java IO 模型的简单介绍；</li>
<li>BIO 、NIO、AIO 模型的介绍，会详细介绍 NIO；</li>
<li>几种 IO 模型的对比。</li>
</ol>
<h1 id="Java-IO-模型介绍"><a href="#Java-IO-模型介绍" class="headerlink" title="Java IO 模型介绍"></a>Java IO 模型介绍</h1><p>在 JDK 推出 Java NIO 之前，基于 Java 的所有 Socket 通信都采用了同步阻塞模式（BIO），这种一对一的通信模型虽然简化了开发的难度，但在性能和可靠性方面却存在这巨大的瓶颈，特别是无法处理高并发的场景，使得 Java 在服务器端应用十分有限。</p>
<p>正是由于 Java 传统 BIO 的拙劣表现，使得 Java 不得不去开发新版的 IO 模型，最终，JDK1.4 提供了新的 NIO 类库，Java 可以支持非阻塞 IO；之后，JDK1.7 正式发布，不但对 NIO 进行了升级，还提供了 AIO 功能。本文就是在对 Java 这些 IO 模型学习后，总结的一篇笔记。</p>
<h2 id="网络编程"><a href="#网络编程" class="headerlink" title="网络编程"></a>网络编程</h2><p>网络编程的基本模型是 Client/Server 模型，也就是两个进程之间进行相互通信，其中服务端提供位置信息（绑定的 IP 地址和端口），客户端通过连接操作向服务端监听的地址发起连接请求，通过三次握手建立连接，如果连接成功，双方就可以通过网络套接字（socket）进行通信（可以参考<a href="http://localhost:8080/2016/08/30/http-protocol/" target="_blank" rel="external">TCP的三次握手和四次挥手</a>），下面先看一下两种对 IO 模型常见的分类方式。</p>
<h2 id="同步与异步"><a href="#同步与异步" class="headerlink" title="同步与异步"></a>同步与异步</h2><p>描述的是用户线程与内核的交互方式，与消息的通知机制有关：</p>
<ol>
<li>同步：当一个同步调用发出后，需要等待返回消息（用户线程不断去询问），才能继续进行；</li>
<li>异步：当一个异步调用发出后，调用者不能立即得到返回消息，完成后会通过状态、通知和回调来通知调用者。</li>
</ol>
<p>简单来说就是：</p>
<ol>
<li>同步：同步等待消息通知，消息返回才能继续进行；</li>
<li>异步：异步等待消息通知，完成后被调系统通过回调等来通过调用者。</li>
</ol>
<h2 id="阻塞与非阻塞"><a href="#阻塞与非阻塞" class="headerlink" title="阻塞与非阻塞"></a>阻塞与非阻塞</h2><p>阻塞和非阻塞指的是不能立刻得到结果之前，会不会阻塞当前线程。</p>
<ol>
<li>阻塞：当前线程会被挂起，直到结果返回；</li>
<li>非阻塞：指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回（会导致线程切换的增加）。</li>
</ol>
<p>举个栗子说明：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>示例</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>同步阻塞</td>
<td>在银行排队，不干别的事情</td>
<td>效率最低</td>
</tr>
<tr>
<td>同步非阻塞</td>
<td>排队时，边打电话边抬头看是否到自己了</td>
<td>效率低下</td>
</tr>
<tr>
<td>异步阻塞</td>
<td>在银行领一个号后，在银行里等，不能做别的事情</td>
<td></td>
</tr>
<tr>
<td>异步非阻塞</td>
<td>领完号后，在忙着自己的事情，直到柜台通知</td>
<td>效率较高</td>
</tr>
</tbody>
</table>
<h1 id="BIO"><a href="#BIO" class="headerlink" title="BIO"></a>BIO</h1><p>BIO 模型是 Java IO 最开始提供的一种 IO 模型，BIO 又可以细分为两种模型，一是传统的同步阻塞模型，二是在对传统 BIO 模型的基本上进行的优化，又称为伪异步 IO 模型。</p>
<h2 id="传统的-BIO-模型"><a href="#传统的-BIO-模型" class="headerlink" title="传统的 BIO 模型"></a>传统的 BIO 模型</h2><p>传统 BIO 中，ServerSocket 负责绑定 IP 地址，启动监听端口；Socket 负责发起连接操作，连接成功后，双方通过输入和输出流进行同步阻塞通信。采用 BIO 通信模型的 Server，通常由一个独立的 Acceptor 线程负责监听 Client 端的连接，它接受到 Client 端连接请求后为每个 Client 创建一个新的线程进行处理，处理完之后，通过输出流返回给 Client 端，线程销毁，过程如下图所示（图来自《Netty 权威指南》）。</p>
<p><img src="/images/java/BIO.png" alt="传统 Java BIO 模型"></p>
<p>这个模型最大的问题是：</p>
<ul>
<li>缺乏扩展性，不能处理高性能、高并发场景，线程是 JVM 中非常宝贵的资源，当线程数膨胀后，系统的性能就会急剧下降，随着并发访问量的继续增大，系统就会出现线程堆栈溢出、创建新线程失败等问题，导致 Server 不能对外提供服务。</li>
</ul>
<p>示例代码参考 <a href="https://github.com/wangzzu/ProgramlLearn/tree/aab89008091660f1f231763660eb329eb5928bde/java_learn/java_socket/src/main/java/bio/" target="_blank" rel="external">Java BIO 示例</a>。</p>
<h2 id="伪异步-IO-模型"><a href="#伪异步-IO-模型" class="headerlink" title="伪异步 IO 模型"></a>伪异步 IO 模型</h2><p>为了改进这种一对一的连接模型，后来又演进出了一种通过线程池或者消息队列实现 1 个或者多个线程处理所有 Client 请求的模型，由于它底层依然是同步阻塞 IO，所以被称为【伪异步 IO 模型】。相比于传统 BIO 后端不断创建新的线程处理 Client 请求，它在后端使用一个<strong>线程池</strong>来代替，通过线程池可以灵活的调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程资源耗尽，过程如下图所示（图来自《Netty 权威指南》）。</p>
<p><img src="/images/java/BIO2.png" alt="伪异步 IO 模型"></p>
<p>看似这个模型解决了 BIO 面对的问题，实际上，由于它是面向数据流的模型，底层依然是同步阻塞模型，在处理一个 socket 输入流，它会一直阻塞下去，除非：有数据可读、可用数据读取完毕、有异常，否则会一直一直阻塞下去。这个模型最大的问题是：</p>
<ul>
<li>阻塞的时间取决于对应 IO 线程的处理速度和网络 IO 的传输速度，处理效率不可控。</li>
</ul>
<h1 id="Java-NIO"><a href="#Java-NIO" class="headerlink" title="Java NIO"></a>Java NIO</h1><p>Java NIO 是 Java IO 模型中最重要的 IO 模型，也是本文主要讲述的内容，正式由于 NIO 的出现，Java 才能在服务端获得跟 C 和 C++ 一样的运行效率，NIO 是 New IO（或者 Non-block IO）的简称。</p>
<p>与 Socket 类和 ServerSocket 类相对应，NIO 也提供了 SocketChannel 和 ServerSocketChannel 两种不同套接字通道的实现，它们都支持阻塞和非阻塞两种模式。一般来说，低负载、低并发的应用程序可以选择同步阻塞 IO 以降低复杂度，但是高负载、高并发的网络应用，需要使用 NIO 的非阻塞模式进行开发。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>在 NIO 中有三种非常重要的概念：</p>
<ul>
<li>缓冲区（buffer）：本质上是一个数组，它包含一些要读写的数据；</li>
<li>通道（channel）：是一个通道，通过它读写数据，类似于自来水管；</li>
<li>多路复用器（selector）：用于选择已经就绪的任务，selector 会轮询注册在其上的 channel，选出已经就绪的 channel。</li>
</ul>
<p><img src="/images/java/NIO.png" alt="NIO 的简单模型"></p>
<p>三者之间的关系如上图所示，这里先简单概括一下：</p>
<ul>
<li>Buffer：是缓冲区，任何时候访问 NIO 数据，都是通过 Buffer 进行；</li>
<li>Channel：通过它读写 Buffer 中的数据，可以用于读、写或同时读写；</li>
<li>Selector：多路复用器，Selector 不断轮询注册在其上的 Channel，如果某个 Channel 有新的 TCP 链接接入、读和写事件，这个 Channel 就处于就绪状态，会被 Selector 轮组出来，然后通过<code>SelectionKey()</code> 可以获取就绪 Channel 的集合，进行后续的 IO 操作。</li>
</ul>
<p>下面详细介绍一下这三个概念。</p>
<h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>Channel 是全双工的，可以比流更好地映射底层操作系统的 API，与流也非常相似，有以下几点区别：</p>
<ul>
<li>Channel 可以读也可以写，但流（InputStream 或 OutputStream）是单向的；</li>
<li>通道可以异步读写；</li>
<li>它是基于缓冲区（Buffer）进行读写；</li>
</ul>
<p>在 Java 中提供以下几种 Channel：</p>
<ol>
<li>FileChannel：用于文件的读写；</li>
<li>DatagramChannel：用于 UDP 数据读写；</li>
<li>SocketChannel：用于 Socket 数据读写；</li>
<li>ServerSocketChannel：监听 TCP 连接请求。</li>
</ol>
<p>这些 Channel 类之间的继承关系如下图所示</p>
<p><img src="/images/java/channel.png" alt="Channel 之间的继承关系"></p>
<p>从上图中，可以看出，Channel 可以分为两大类：用于网络读写的 <code>SelectableChannel</code> 和用于文件操作的 <code>FileChannel</code>。</p>
<p>其中，FileChannel 只能在阻塞模式下工作，具体可以参考<a href="http://wiki.jikexueyuan.com/project/java-nio-zh/java-nio-filechannel.html" target="_blank" rel="external">Java NIO FileChannel文件通道</a>。</p>
<h4 id="NIO-Scatter-Gather"><a href="#NIO-Scatter-Gather" class="headerlink" title="NIO Scatter/Gather"></a>NIO Scatter/Gather</h4><p>Java NIO 发布时内置了对 scatter/gather的支持：</p>
<ul>
<li>Scattering read 指的是从通道读取的操作能把数据写入多个 Buffer，也就是 sctters 代表了数据从一个 Channel 到多个 Buffer的过程。</li>
<li>Gathering write 则正好相反，表示的是从多个 Buffer 把数据写入到一个 Channel中。</li>
</ul>
<p><img src="/images/java/scatter-gather.png" alt="Channel 之间的继承关系"></p>
<p>示例如下，具体参考 <a href="http://wiki.jikexueyuan.com/project/java-nio-zh/java-nio-scatter-gather.html" target="_blank" rel="external">Java NIO Scatter / Gather</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Scattering read</span></div><div class="line">ByteBuffer header = ByteBuffer.allocate(<span class="number">128</span>);</div><div class="line">ByteBuffer body   = ByteBuffer.allocate(<span class="number">1024</span>);</div><div class="line"></div><div class="line">ByteBuffer[] bufferArray = &#123; header, body &#125;;</div><div class="line">channel.read(bufferArray);</div><div class="line"></div><div class="line"><span class="comment">// Gathering write</span></div><div class="line">ByteBuffer header = ByteBuffer.allocate(<span class="number">128</span>);</div><div class="line">ByteBuffer body   = ByteBuffer.allocate(<span class="number">1024</span>);</div><div class="line"></div><div class="line">ByteBuffer[] bufferArray = &#123; header, body &#125;;</div><div class="line">channel.write(bufferArray);</div></pre></td></tr></table></figure>
<h3 id="Buffer"><a href="#Buffer" class="headerlink" title="Buffer"></a>Buffer</h3><p>Buffer，本质上是一块内存区，可以用来读写数据，它包含一些要写入或者要读出的数据。在 NIO 中，所有数据都是通过 Buffer 处理的，读取数据时，它是直接读到缓冲区中，写入数据时，写入到缓冲区。</p>
<p>最常用的缓冲区是 ByteBuffer，一个 ByteBuffer 提供了一组功能用于操作 byte 数组，除了 ByteBuffer，还有其他的一些 Buffer，如：CharBuffer、IntBuffer 等，它们之间的关系如下图所示。</p>
<p><img src="/images/java/buffer.png" alt="Buffer 之间的继承关系"></p>
<h4 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h4><p>Buffer 基本用法（读写数据过程）：</p>
<ol>
<li>把数据写入 Buffer；</li>
<li>调用 <code>flip()</code>，Buffer 由写模式变为读模式；</li>
<li>Buffer 中读取数据；</li>
<li>调用 <code>clear()</code> 清空 buffer，等待下次写入。</li>
</ol>
<p>示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">byte</span>[] req = <span class="string">"QUERY TIME ORDER"</span>.getBytes();</div><div class="line">ByteBuffer byteBuffer = ByteBuffer.allocate(req.length);</div><div class="line">byteBuffer.put(req);</div><div class="line">byteBuffer.flip();</div><div class="line"><span class="keyword">while</span> (byteBuffer.hasRemaining())&#123;</div><div class="line">     System.out.println((<span class="keyword">char</span>) byteBuffer.get());</div><div class="line">&#125;</div><div class="line">byteBuffer.clear();</div></pre></td></tr></table></figure>
<h4 id="Buffer-位置信息"><a href="#Buffer-位置信息" class="headerlink" title="Buffer 位置信息"></a>Buffer 位置信息</h4><p>Buffer 实质上就是一块内存，用于读写数据，这块内存被 NIO Buffer 管理，一个 Buffer 有三个属性是必须掌握的，分别是：</p>
<ul>
<li>capacity：容量；</li>
<li>position：位置；</li>
<li>limit：限制；</li>
</ul>
<p>其中，position 和 limit 的具体含义取决于当前 buffer 的模式，capacity 在两种模式下都表示容量，Buffer 读模式和写模式如下图所示。</p>
<p><img src="/images/java/buffer-position.png" alt="Buffer 的位置信息"></p>
<ol>
<li>容量（capacity）<ul>
<li>Buffer 有一块固定的内存，其大小就是 capacity，一旦 Buffer 写满，就需要清空已读数据以便下次继续写入新的数据；</li>
</ul>
</li>
<li>位置（Position）<ul>
<li>写模式时，当写入数据到 Buffer 的时候从一个确定的位置开始，初始化时这个位置 position 为0，写入数据后，position 的值就会指向数据之后的单元，position 最大的值可以达到 <code>capacity-1</code>；</li>
<li>读模式时，也需要从一个确定的位置开始，Buffer 从写模式变为读模式时，position 会归零，每次读取后，position 向后移动；</li>
</ul>
</li>
<li>上限（limit）<ul>
<li>写模式时，limit 就是能写入的最大数据量，等同于 Buffer 的容量；</li>
<li>读模式时，limit 代表我们能读取的最大容量，它的值等同于写模式下 position 位置。</li>
</ul>
</li>
</ol>
<h4 id="Buffer-常用方法"><a href="#Buffer-常用方法" class="headerlink" title="Buffer 常用方法"></a>Buffer 常用方法</h4><ul>
<li><code>flip()</code>：把 buffer 从模式调整为读模式，在读模式下，可以读取所有已经写入的数据；</li>
<li><code>clear()</code>：清空整个 buffer；</li>
<li><code>compact()</code>：只清空已读取的数据，未被读取的数据会被移动到 buffer 的开始位置，写入位置则紧跟着未读数据之后；</li>
<li><code>rewind()</code>：将 position 置为0，这样我们可以重复读取 Buffer 中的数据，limit 保持不变；</li>
<li><code>mark()</code>和<code>reset()</code>：通过mark方法可以标记当前的position，通过reset来恢复mark的位置</li>
<li><code>equals()</code>：判断两个 Buffer 是否相等，需满足：类型相同、Buffer 中剩余字节数相同、所有剩余字节相等；</li>
<li><code>compareTo()</code>：compareTo 比较 Buffer 中的剩余元素，只不过这个方法适用于比较排序的。</li>
</ul>
<h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><p>Selector 是 Java NIO 核心部分，简单来说，它的作用就是：Selector 不断轮询注册在其上的 Channel，如果某个 Channel 上面有新的 TCP 连接、读和写事件，这个 Channel 就处于就绪状态，会被 Selector 轮询出来，然后通过 <code>SelectorKey()</code> 可以获取就绪 Channel 的集合，进行后续的 IO 操作。</p>
<p>一个 Selector 可以轮询多个 Channel，由于 JDK 底层使用了 <code>epoll()</code> 实现，它并没有最大连接句柄 1024/2048 的限制，这就意味着只需要一个线程负责 Selector 的轮询，就可以连接上千上万的 Client。</p>
<h4 id="注册-Channel"><a href="#注册-Channel" class="headerlink" title="注册 Channel"></a>注册 Channel</h4><p>举一个栗子，简单介绍 <code>Selector</code> 的使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 创建一个 Selector</span></div><div class="line">Selector selector = Selector.open();</div><div class="line"><span class="comment">// 将一个 Channel 注册到 Selector 上</span></div><div class="line">channel.configureBlocking(<span class="keyword">false</span>);</div><div class="line">SelectionKey key = channel.register(selector, SelectionKey.OP_READ);</div></pre></td></tr></table></figure>
<p><code>register()</code> 的第二个参数代表的是 selector 监听的事件类型，Selector 可以监听事件类型总共有以下四种：</p>
<ol>
<li>SelectionKey.OP_CONNECT：只会注册一次，成功之后（TCP 连接建立之后），这个监听事件就取消了；</li>
<li>SelectionKey.OP_ACCEPT：主要用于服务端，就是监听是否有新的连接请求；</li>
<li>SelectionKey.OP_READ：注册之后不会取消，监听是否数据到来；</li>
<li>SelectionKey.OP_WRITE：最好的使用方法是每当发送数据时，就注册一次，然后再取消，否则每次 select 轮询时，注册 OP_WRITE 事件的 Channel 都是 ready 的，除非 socket send buffer 满了（参考 <a href="https://stackoverflow.com/questions/23136079/communicating-between-nio-op-read-and-op-write-operations" target="_blank" rel="external">Communicating between nio OP_READ and OP_WRITE operations</a>）。</li>
</ol>
<h4 id="SelectionKey"><a href="#SelectionKey" class="headerlink" title="SelectionKey"></a>SelectionKey</h4><p><code>Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys();</code> 返回的是已经就绪的 Channel 集合，<code>SelectionKey</code> 对象的详细属性如下图所示。</p>
<p><img src="/images/java/SelectionKey.png" alt="SelectionKey 的详情"></p>
<h2 id="NIO-原理"><a href="#NIO-原理" class="headerlink" title="NIO 原理"></a>NIO 原理</h2><p>Java NIO 实现的关键是 IO 多路复用（具体可以参考上篇文章：<a href="http://matt33.com/2017/08/06/unix-io/#Linux-的-IO-多路复用模型">Linux 的 IO 多路复用模型</a>），在 Linux 平台，Java NIO 是基于 epoll（2.6以上，之前是 Select） 来实现的。</p>
<p>Linux 的 select/epoll  使用的是 Reactor 网络 IO 模式。网络编程中，有两种常用的设计模式，它们都是基于事件驱动：</p>
<ul>
<li>Reactor 模式：主动模式，应用程序不断去轮询，问操作系统 IO 是否就绪，实际的 IO 操作还是由应用实现（IO 多路复用采用的模式）；</li>
<li>Proactor 模式：被动模式，操作系统把 IO 完成后通知应用程序，此时数据已经就绪。</li>
</ul>
<p>这两种模式详细内容可以参考<a href="http://daoluan.net/linux/%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/2013/08/20/two-high-performance-io-design-patterns.html" target="_blank" rel="external">两种高性能 I/O 设计模式 Reactor 和 Proactor</a>一文。</p>
<h2 id="NIO-编程"><a href="#NIO-编程" class="headerlink" title="NIO 编程"></a>NIO 编程</h2><p>关于 Java NIO，有两种最常见的使用方式：</p>
<ul>
<li>使用原生的 Java NIO（如 Kafka）；</li>
<li>使用 Netty（Hadoop 的 RPC 框架 Avro 底层使用 Netty 做通信框架）。</li>
</ul>
<p>在实际使用中，推荐第二种，使用 Netty 将会大大提高开发效率，后续会写篇关于 Netty 的文章，介绍一下 Netty 的具体内容，这里使用一个基于 Java 原生 NIO API 的小示例，讲述一下 NIO 的使用方法。</p>
<h3 id="Client-端"><a href="#Client-端" class="headerlink" title="Client 端"></a>Client 端</h3><p>NIO Client 创建序列图如下图所示（图片来自《Netty 权威指南》）。</p>
<p><img src="/images/java/nio-client.png" alt="NIO Client 端序列图"></p>
<p>具体的代码及注释参考：<a href="https://github.com/wangzzu/ProgramlLearn/tree/aab89008091660f1f231763660eb329eb5928bde/java_learn/java_socket/src/main/java/nio/client/" target="_blank" rel="external">NIO Client 端代码</a>。</p>
<h3 id="Server-端"><a href="#Server-端" class="headerlink" title="Server 端"></a>Server 端</h3><p>NIO Server 创建序列图如下图所示（图片来自《Netty 权威指南》）。</p>
<p><img src="/images/java/nio-server.png" alt="NIO Server 端序列图"></p>
<p>具体的代码及注释参考：<a href="https://github.com/wangzzu/ProgramlLearn/tree/aab89008091660f1f231763660eb329eb5928bde/java_learn/java_socket/src/main/java/nio/server/" target="_blank" rel="external">NIO Server 端代码</a>。</p>
<h1 id="IO-模型对比"><a href="#IO-模型对比" class="headerlink" title="IO 模型对比"></a>IO 模型对比</h1><p>在对比之前，先简单介绍 Java AIO 模型，这里就不再进行相应的展开了。</p>
<h2 id="AIO"><a href="#AIO" class="headerlink" title="AIO"></a>AIO</h2><p>NIO 2.0 中引入异步通道的概念，并提供了异步文件通道和异步套接字导通的实现，它是真正的异步非阻塞I IO，底层是利用事件驱动（AIO）实现，不需要多路复用器（Selector）对注册的通道进行轮组操作即可实现异步读写。</p>
<p>可以参考<a href="https://www.ibm.com/developerworks/cn/java/j-lo-nio2/" target="_blank" rel="external">在 Java 7 中体会 NIO.2 异步执行的快乐</a></p>
<h2 id="几种-IO-模型功能和特性对比"><a href="#几种-IO-模型功能和特性对比" class="headerlink" title="几种 IO 模型功能和特性对比"></a>几种 IO 模型功能和特性对比</h2><table>
<thead>
<tr>
<th></th>
<th>传统 BIO</th>
<th>伪异步 IO</th>
<th>NIO</th>
<th>AIO</th>
</tr>
</thead>
<tbody>
<tr>
<td>client 数：IO 线程数</td>
<td>1：1</td>
<td>M：N（M 可以大于 N）</td>
<td>M：1</td>
<td>M：0（不需要额外的线程，被动回调）</td>
</tr>
<tr>
<td>IO 类型（阻塞）</td>
<td>阻塞IO</td>
<td>阻塞IO</td>
<td>非阻塞IO</td>
<td>非阻塞IO</td>
</tr>
<tr>
<td>IO 类型（同步）</td>
<td>同步 IO</td>
<td>同步 IO</td>
<td>同步 IO（IO 多路复用）</td>
<td>异步 IO</td>
</tr>
<tr>
<td>可靠性</td>
<td>非常差</td>
<td>差</td>
<td>高</td>
<td>高</td>
</tr>
<tr>
<td>吞吐量</td>
<td>低</td>
<td>中</td>
<td>高</td>
<td>高</td>
</tr>
</tbody>
</table>
<p>本文主要是对 Java IO 模型总结，特别是对 NIO 模型的总结。</p>
<hr>
<p>参考</p>
<ul>
<li>《Netty 权威指南》；</li>
<li>Java NIO <a href="http://tutorials.jenkov.com/java-nio/index.html" target="_blank" rel="external">英文版</a>，<a href="http://ifeve.com/java-nio-all/" target="_blank" rel="external">中文版</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Java IO 模型对于 Java 开发工程师来说，是日常工作中经常接触的内容，特别是随着分布式系统的兴起，IO 也显得越来越重要，Java 的 IO 模型本质上还是利用操作系统提供的接口来实现，不熟悉这一部分内容的话，可以先看一下上篇文章&lt;a href=&quot;http://m
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="linux" scheme="http://matt33.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Unix 网络 IO 模型及 Linux 的 IO 多路复用模型</title>
    <link href="http://matt33.com/2017/08/06/unix-io/"/>
    <id>http://matt33.com/2017/08/06/unix-io/</id>
    <published>2017-08-06T14:19:56.000Z</published>
    <updated>2017-08-06T14:42:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>近段在看 Kafka 的网络模型时，遇到了很多 Java NIO 的内容，在学习 Java NIO 的过程中，发现需要把 UNIX 的这几种网络 IO 模型以及 Linux 的 IO 多路复用理解清楚，才能更好地理解 Java NIO，本文就是在学习 UNIX 的五种网络 IO 模型以及 Linux IO 多路复用模型后，做的一篇总结。</p>
<p>本文主要探讨的问题有以下两个：</p>
<ol>
<li>Unix 中的五种网络 IO 模型；</li>
<li>Linux 中 IO 多路复用的实现。</li>
</ol>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>在介绍网络模型之前，先简单介绍一些基本概念。</p>
<h3 id="文件描述符-fd"><a href="#文件描述符-fd" class="headerlink" title="文件描述符 fd"></a>文件描述符 fd</h3><p>文件描述符（file descriptor，简称 fd）在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。</p>
<p>在 Linux 中，内核将所有的外部设备都当做一个文件来进行操作，而对一个文件的读写操作会调用内核提供的系统命令，返回一个 fd，对一个 socket 的读写也会有相应的描述符，称为 socketfd（socket 描述符），实际上描述符就是一个数字，它指向内核中的一个结构体（文件路径、数据区等一些属性）。</p>
<h3 id="用户空间与内核空间、内核态与用户态"><a href="#用户空间与内核空间、内核态与用户态" class="headerlink" title="用户空间与内核空间、内核态与用户态"></a>用户空间与内核空间、内核态与用户态</h3><p>这个是经常提到的概念，具体含义可以参考这篇文章<a href="http://www.cnblogs.com/Anker/p/3269106.html" target="_blank" rel="external">用户空间与内核空间，进程上下文与中断上下文【总结】</a>，大概内容如下：</p>
<p>现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操心系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核，保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对 linux 操作系统而言（以32位操作系统为例）</p>
<ul>
<li>将最高的 1G 字节（从虚拟地址 0xC0000000 到 0xFFFFFFFF），供内核使用，称为内核空间；</li>
<li>将较低的 3G 字节（从虚拟地址 0x00000000 到 0xBFFFFFFF），供各个进程使用，称为用户空间。</li>
</ul>
<p>每个进程可以通过系统调用进入内核，因此，Linux 内核由系统内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有 4G 字节的虚拟空间。</p>
<ul>
<li>当一个任务（进程）执行系统调用而陷入内核代码中执行时，称进程处于内核运行态（<strong>内核态</strong>）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈，每个进程都有自己的内核栈；</li>
<li>当进程在执行用户自己的代码时，则称其处于用户运行态（<strong>用户态</strong>）。此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。</li>
</ul>
<h3 id="上下文切换"><a href="#上下文切换" class="headerlink" title="上下文切换"></a>上下文切换</h3><p>当一个进程在执行时，CPU 的所有寄存器中的值、进程的状态以及堆栈中的内容被称为该进程的上下文。</p>
<p>当内核需要切换到另一个进程时，它需要保存当前进程的所有状态，即保存当前进程的上下文，以便在再次执行该进程时，能够必得到切换时的状态执行下去。在 Linux 中，当前进程上下文均保存在进程的任务数据结构中。在发生中断时，内核就在被中断进程的上下文中，在内核态下执行中断服务例程。但同时会保留所有需要用到的资源，以便中继服务结束时能恢复被中断进程的执行。</p>
<h2 id="UNIX-的网络-IO-模型"><a href="#UNIX-的网络-IO-模型" class="headerlink" title="UNIX 的网络 IO 模型"></a>UNIX 的网络 IO 模型</h2><p>根据 UNIX 网络编程对 IO 模型的分类，UNIX 提供了以下 5 种 IO 模型。</p>
<h3 id="阻塞-IO-模型"><a href="#阻塞-IO-模型" class="headerlink" title="阻塞 IO 模型"></a>阻塞 IO 模型</h3><p>最常用的 IO 模型就是阻塞 IO 模型，在缺省条件下，所有文件操作都是阻塞的，以 socket 读为例来介绍一下此模型，如下图所示。</p>
<p><img src="/images/linux/BIO.png" alt="阻塞 IO 模型"></p>
<p>在用户空间调用 <code>recvfrom</code>，系统调用直到数据包达到且被复制到应用进程的缓冲区中或中间发生异常返回，在这个期间进程会一直等待。进程从调用 <code>recvfrom</code> 开始到它返回的整段时间内都是被阻塞的，因此，被称为阻塞 IO 模型。</p>
<h3 id="非阻塞-IO-模型"><a href="#非阻塞-IO-模型" class="headerlink" title="非阻塞 IO 模型"></a>非阻塞 IO 模型</h3><p><code>recvfrom</code> 从应用到内核的时，如果该缓冲区没有数据，就会直接返回 <code>EWOULDBLOCK</code> 错误，一般都对非阻塞 IO 模型进行轮询检查这个状态，看看内核是不是有数据到来，流程如下图所示。</p>
<p><img src="/images/linux/N-BIO.png" alt="非阻塞 IO 模型"></p>
<p>也就是说非阻塞的 <code>recvform</code> 系统调用调用之后，进程并没有被阻塞，内核马上返回给进程。</p>
<ul>
<li>如果数据还没准备好，此时会返回一个 error。进程在返回之后，可以干点别的事情，然后再发起 <code>recvform</code> 系统调用。重复上面的过程，循环往复的进行 <code>recvform</code> 系统调用，这个过程通常被称之为<strong>轮询</strong>。</li>
</ul>
<p>轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态。</p>
<p>在 Linux 下，可以通过设置 socket 使其变为 non-blocking。</p>
<h3 id="IO-多路复用模型"><a href="#IO-多路复用模型" class="headerlink" title="IO 多路复用模型"></a>IO 多路复用模型</h3><p>Linux 提供 select、poll、epoll，进程通过讲一个或者多个 fd 传递给 select、poll、epoll 系统调用，阻塞在 select 操作（这个是内核级别的调用）上，这样的话，可以同时监听多个 fd 是否处于就绪状态。其中，</p>
<ul>
<li>select/poll 是顺序扫描 fd 是否就绪，而且支持的 fd 数量有限；</li>
<li>epoll 是基于事件驱动方式代替顺序扫描性能更高。</li>
</ul>
<p>这个后面详细讲述，具体流程如下图所示。</p>
<p><img src="/images/linux/Multi-IO.png" alt="IO 多路复用模型"></p>
<p>多路复用的特点是通过一种机制一个进程能同时等待 IO 文件描述符，内核监视这些文件描述符（套接字描述符），其中的任意一个进入读就绪状态，select， poll，epoll 函数就可以返回，它最大的优势就是可以同时处理多个连接。</p>
<h3 id="信号驱动-IO-模型"><a href="#信号驱动-IO-模型" class="headerlink" title="信号驱动 IO 模型"></a>信号驱动 IO 模型</h3><p>首先需要开启 socket 信号驱动 IO 功能，并通过系统调用 <code>sigaction</code> 执行一个信号处理函数（非阻塞，立即返回）。当数据就绪时，会为该进程生成一个 SIGIO 信号，通过信号回调通知应用程序调用 <code>recvfrom</code> 来读取数据，并通知主循环喊出处理数据，流程如下图所示。</p>
<p><img src="/images/linux/single-IO.png" alt="信号驱动 IO 模型"></p>
<h3 id="异步-IO-模型"><a href="#异步-IO-模型" class="headerlink" title="异步 IO 模型"></a>异步 IO 模型</h3><p>告知内核启动某个事件，并让内核在整个操作完成后（包括将数据从内核复制到用户自己的缓冲区）通过我们，流程如下图所示。</p>
<p><img src="/images/linux/AIO.png" alt="异步 IO 模型"></p>
<p>与信号驱动模式的主要区别是：</p>
<ul>
<li>信号驱动 IO 由内核通知我们何时可以开始一个 IO 操作；</li>
<li>异步 IO 操作由内核通知我们 IO 何时完成。</li>
</ul>
<p>内核是通过向应用程序发送 signal 或执行一个基于线程的回调函数来完成这次 IO 处理过程，告诉用户 read 操作已经完成，在 Linux 中，通知的方式是信号：</p>
<ol>
<li>当进程正处于用户态时，应用需要立马进行处理，一般情况下，是先将事件登记一下，放进一个队列中；</li>
<li>当进程正处于内核态时，比如正在以同步阻塞模式读磁盘，那么只能先把这个通知挂起来，等内核态的事情完成之后，再触发信号通知；</li>
<li>如果这个进程现在被挂起来了，比如 sleep，那就把这个进程唤醒，等 CPU 空闲时，就会调度这个进程，触发信号通知。</li>
</ol>
<h3 id="几种-IO-模型比较"><a href="#几种-IO-模型比较" class="headerlink" title="几种 IO 模型比较"></a>几种 IO 模型比较</h3><p><img src="/images/linux/IO-compact.png" alt="几种模型的比较"></p>
<h2 id="Linux-的-IO-多路复用模型"><a href="#Linux-的-IO-多路复用模型" class="headerlink" title="Linux 的 IO 多路复用模型"></a>Linux 的 IO 多路复用模型</h2><p>IO 多路复用通过把多个 IO 阻塞复用到同一个 select 的阻塞上，从而使得系统在单线程的情况下，可以同时处理多个 client 请求，与传统的多线程/多进程模型相比，IO 多路复用的最大优势是系统开销小，系统不需要创建新的额外的进程或线程，也不需要维护这些进程和线程的运行，节省了系统资源，IO 多路复用的主要场景如下：</p>
<ol>
<li>Server 需要同时处理多个处于监听状态或者连接状态的 socket；</li>
<li>Server 需要同时处理多种网络协议的 socket。</li>
</ol>
<p>IO 多路复用实际上就是通过一种机制，一个进程可以监视多个描 fd，一旦某个 fd 就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作，目前支持 IO 多路复用的系统有 select、pselect、poll、epoll，但它们本质上都是同步 IO。</p>
<p>在 Linux 网络编程中，最初是选用 select 做轮询和网络事件通知，然而 select 的一些固有缺陷导致了它的应用受到了很大的限制，最终 Linux 选择 epoll。</p>
<h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p>select 函数监视的 fd 分3类，分别是 <code>writefds</code>、<code>readfds</code>、和 <code>exceptfds</code>。调用后select 函数会阻塞，直到有 fd 就绪（有数据 可读、可写、或者有 except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当select函数返回后，可以通过遍历 fdset，来找到就绪的 fd。</p>
<p>select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select 的一个最大的缺陷就是单个进程对打开的 fd 是有一定限制的，它由 <code>FD_SETSIZE</code> 限制，默认值是1024，如果修改的话，就需要重新编译内核，不过这会带来网络效率的下降。</p>
<p>select 和 poll 另一个缺陷就是随着 fd 数目的增加，可能只有很少一部分 socket 是活跃的，但是 select/poll 每次调用时都会线性扫描全部的集合，导致效率呈现线性的下降。</p>
<h3 id="poll"><a href="#poll" class="headerlink" title="poll"></a>poll</h3><p>poll 本质上和 select 没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个 fd 对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有 fd 后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历 fd。这个过程经历了多次无谓的遍历。</p>
<p>它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样以下两个缺点：</p>
<ol>
<li>大量的 fd 的数组被整体复制于用户态和内核地址空间之间；</li>
<li>poll 还有一个特点是【水平触发】，如果报告了 fd 后，没有被处理，那么下次 poll 时会再次报告该 fd；</li>
<li>fd 增加时，线性扫描导致性能下降。</li>
</ol>
<h3 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h3><p>epoll 支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些 fd 变为就绪态，并且只会通知一次。还有一个特点是，epoll 使用【事件】的就绪通知方式，通过 <code>epoll_ctl</code> 注册 fd，一旦该 fd 就绪，内核就会采用类似 callback 的回调机制来激活该 fd，<code>epoll_wait</code> 便可以收到通知。</p>
<p>epoll的优点：</p>
<ol>
<li>没有最大并发连接的限制，它支持的 fd 上限受操作系统最大文件句柄数；</li>
<li>效率提升，不是轮询的方式，不会随着 fd 数目的增加效率下降。epoll 只会对【活跃】的 socket 进行操作，这是因为在内核实现中 epoll 是根据每个 fd 上面的 callback 函数实现的，只有【活跃】的 socket 才会主动的去调用 callback 函数，其他 idle 状态的 socket 则不会。epoll 的性能不会受 fd 总数的限制。</li>
<li>select/poll 都需要内核把 fd 消息通知给用户空间，而 epoll 是通过内核和用户空间 <code>mmap</code> 同一块内存实现。</li>
</ol>
<p>epoll 对 fd 的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT 模式是默认模式，LT 模式与 ET 模式的区别如下：</p>
<ul>
<li>LT 模式：当 <code>epoll_wait</code> 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件，下次调用 <code>epoll_wait</code> 时，会再次响应应用程序并通知此事件；</li>
<li>ET 模式：当 <code>epoll_wait</code> 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件，如果不处理，下次调用 <code>epoll_wait</code> 时，不会再次响应应用程序并通知此事件。</li>
</ul>
<h3 id="三种模型的区别"><a href="#三种模型的区别" class="headerlink" title="三种模型的区别"></a>三种模型的区别</h3><table>
<thead>
<tr>
<th>类别</th>
<th>select</th>
<th>poll</th>
<th>epoll</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持的最大连接数</td>
<td>由 <code>FD_SETSIZE</code> 限制</td>
<td>基于链表存储，没有限制</td>
<td>受系统最大句柄数限制</td>
</tr>
<tr>
<td>fd 剧增的影响</td>
<td>线性扫描 fd 导致性能很低</td>
<td>同 select</td>
<td>基于 fd 上 callback 实现，没有性能下降的问题</td>
</tr>
<tr>
<td>消息传递机制</td>
<td>内核需要将消息传递到用户空间，需要内核拷贝</td>
<td>同 select</td>
<td>epoll 通过内核与用户空间共享内存来实现</td>
</tr>
</tbody>
</table>
<p>介绍完 IO 多路复用之后，后续我们看一下 Java 网络编程中的 NIO 模型及其背后的实现机制。</p>
<hr>
<p>参考</p>
<ul>
<li>《Netty 权威指南》</li>
<li><a href="http://www.cnblogs.com/Anker/p/3269106.html" target="_blank" rel="external">用户空间与内核空间，进程上下文与中断上下文【总结】</a></li>
<li><a href="http://www.jianshu.com/p/486b0965c296" target="_blank" rel="external">聊聊 Linux 中的五种 IO 模型</a></li>
<li><a href="http://www.jianshu.com/p/dfd940e7fca2" target="_blank" rel="external">聊聊IO多路复用之select、poll、epoll详解</a></li>
<li><a href="http://www.jianshu.com/p/2461535c38f3" target="_blank" rel="external">高性能Server—Reactor模型</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近段在看 Kafka 的网络模型时，遇到了很多 Java NIO 的内容，在学习 Java NIO 的过程中，发现需要把 UNIX 的这几种网络 IO 模型以及 Linux 的 IO 多路复用理解清楚，才能更好地理解 Java NIO，本文就是在学习 UNIX 的五种网络 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="linux" scheme="http://matt33.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 topic 创建过程（三）</title>
    <link href="http://matt33.com/2017/07/21/kafka-topic-create/"/>
    <id>http://matt33.com/2017/07/21/kafka-topic-create/</id>
    <published>2017-07-21T15:49:03.000Z</published>
    <updated>2018-06-10T15:29:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是 Kafka 源码解析的第三篇，主要讲述一个 topic 的创建过程，从 topic 是如何创建到 topic 真正创建成功的中间详细过程，文章主要内容可以分为以下几个部分：</p>
<ol>
<li>topic 是如何创建的？<ul>
<li>命令行创建；</li>
<li>Producer 发送数据时，自动创建；</li>
</ul>
</li>
<li>topic 创建时，replicas 是如何分配的？<ul>
<li>指定 replicas 的分配；</li>
<li>自动 replicas 分配；</li>
</ul>
</li>
<li>replicas 更新到 zk 后，底层如何创建一个 topic？<ul>
<li>创建 Partition 对象及状态更新；</li>
<li>创建 Partition 的 replica 对象及状态更新。</li>
</ul>
</li>
</ol>
<p>一个 topic 的完整创建过程如下图所示（以 topic 的 replicas 自动创建，且 broker 没有机架感知为例）</p>
<p><img src="/images/kafka/create_topic.png" alt="Topic 完整创建过程"></p>
<p>上图只是列出一些主要的方法调用，具体内容下面会详细讲述（在看下面的内容时，最后配合上面这张图来看）。</p>
<h2 id="topic-介绍"><a href="#topic-介绍" class="headerlink" title="topic 介绍"></a>topic 介绍</h2><p>topic 是 Kafka 中的一个消息队列的标识，也可以认为是消息队列的一个 id，用于区分不同的消息队列，一个 topic 由多个 partition 组成，这些 partition 是通常是分布在不同的多台 Broker 上的，为了保证数据的可靠性，一个 partition 又会设置为多个副本（replica），通常会设置两副本或三副本。如下图所示，这个一个名为『topic』的 topic，它由三个 partition 组成，两副本，假设 Kafka 集群有三台 Broker（replica 0_1 代表 partition 0 的第一个副本）。</p>
<p><img src="/images/kafka/topic-replicas.png" alt="Kafka Topic 的组成"></p>
<p>在设置副本时，副本数是必须大于集群的 Broker 数的，副本只有设置在不同的机器上才有作用。</p>
<h2 id="topic-如何创建"><a href="#topic-如何创建" class="headerlink" title="topic 如何创建"></a>topic 如何创建</h2><p>topic 在创建时有两种方式：</p>
<ol>
<li>通过 <code>kafka-topics.sh</code> 创建一个 topic，可以设置相应的副本数让 Server 端自动进行 replica 分配，也可以直接指定手动 replica 的分配；</li>
<li>Server 端如果 <code>auto.create.topics.enable</code> 设置为 true 时，那么当 Producer 向一个不存在的 topic 发送数据时，该 topic 同样会被创建出来，此时，副本数默认是1。</li>
</ol>
<p>下面看一下这两种方式的底层实现。</p>
<h3 id="kafka-topics-sh-创建-topic"><a href="#kafka-topics-sh-创建-topic" class="headerlink" title="kafka-topics.sh 创建 topic"></a>kafka-topics.sh 创建 topic</h3><p>在 Kafka 的安装目录下，通过下面这条命令可以创建一个 partition 为3，replica 为2的 topic（test）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --create --topic <span class="built_in">test</span> --zookeeper XXXX --partitions 3 --replication-factor 2</div></pre></td></tr></table></figure>
<p><code>kafka-topics.sh</code> 实际上是调用 <code>kafka.admin.TopicCommand</code> 的方法来创建 topic，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建 topic</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topic = opts.options.valueOf(opts.topicOpt)</div><div class="line">  <span class="keyword">val</span> configs = parseTopicConfigsToBeAdded(opts)</div><div class="line">  <span class="keyword">val</span> ifNotExists = opts.options.has(opts.ifNotExistsOpt)</div><div class="line">  <span class="keyword">if</span> (<span class="type">Topic</span>.hasCollisionChars(topic))</div><div class="line">    println(<span class="string">"WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both."</span>)</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">if</span> (opts.options.has(opts.replicaAssignmentOpt)) &#123;<span class="comment">//note: 指定 replica 的分配,直接向 zk 更新即可</span></div><div class="line">      <span class="keyword">val</span> assignment = parseReplicaAssignment(opts.options.valueOf(opts.replicaAssignmentOpt))</div><div class="line">      <span class="type">AdminUtils</span>.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, assignment, configs, update = <span class="literal">false</span>)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;<span class="comment">//note: 未指定 replica 的分配,调用自动分配算法进行分配</span></div><div class="line">      <span class="type">CommandLineUtils</span>.checkRequiredArgs(opts.parser, opts.options, opts.partitionsOpt, opts.replicationFactorOpt)</div><div class="line">      <span class="keyword">val</span> partitions = opts.options.valueOf(opts.partitionsOpt).intValue</div><div class="line">      <span class="keyword">val</span> replicas = opts.options.valueOf(opts.replicationFactorOpt).intValue</div><div class="line">      <span class="keyword">val</span> rackAwareMode = <span class="keyword">if</span> (opts.options.has(opts.disableRackAware)) <span class="type">RackAwareMode</span>.<span class="type">Disabled</span></div><div class="line">                          <span class="keyword">else</span> <span class="type">RackAwareMode</span>.<span class="type">Enforced</span></div><div class="line">      <span class="type">AdminUtils</span>.createTopic(zkUtils, topic, partitions, replicas, configs, rackAwareMode)</div><div class="line">    &#125;</div><div class="line">    println(<span class="string">"Created topic \"%s\"."</span>.format(topic))</div><div class="line">  &#125; <span class="keyword">catch</span>  &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">TopicExistsException</span> =&gt; <span class="keyword">if</span> (!ifNotExists) <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果指定了 partition 各个 replica 的分布，那么将 partition replicas 的结果验证之后直接更新到 zk 上，验证的 replicas 的代码是在 <code>parseReplicaAssignment</code> 中实现的，如下所示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseReplicaAssignment</span></span>(replicaAssignmentList: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]] = &#123;</div><div class="line">  <span class="keyword">val</span> partitionList = replicaAssignmentList.split(<span class="string">","</span>)</div><div class="line">  <span class="keyword">val</span> ret = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]]()</div><div class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until partitionList.size) &#123;</div><div class="line">    <span class="keyword">val</span> brokerList = partitionList(i).split(<span class="string">":"</span>).map(s =&gt; s.trim().toInt)</div><div class="line">    <span class="keyword">val</span> duplicateBrokers = <span class="type">CoreUtils</span>.duplicates(brokerList)</div><div class="line">    <span class="keyword">if</span> (duplicateBrokers.nonEmpty)<span class="comment">//note: 同一个 partition 对应的 replica 是不能相同的</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminCommandFailedException</span>(<span class="string">"Partition replica lists may not contain duplicate entries: %s"</span>.format(duplicateBrokers.mkString(<span class="string">","</span>)))</div><div class="line">    ret.put(i, brokerList.toList)</div><div class="line">    <span class="keyword">if</span> (ret(i).size != ret(<span class="number">0</span>).size)<span class="comment">//note: 同一个 topic 的副本数必须相同</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Partition "</span> + i + <span class="string">" has different replication factor: "</span> + brokerList)</div><div class="line">  &#125;</div><div class="line">  ret.toMap</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果没有指定 parittion replicas 分配的话，将会调用 <code>AdminUtils.createTopic</code> 方法创建 topic，这个方法首先会检测当前的 Kafka 集群是否机架感知，如果有的话先获取 Broker 的机架信息，接着再使用 Replica 自动分配算法来分配 Partition 的 replica，最后就跟指定 replica 方式一样，将 replicas 的结果更新到 zk 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkUtils: <span class="type">ZkUtils</span>,</div><div class="line">                topic: <span class="type">String</span>,</div><div class="line">                partitions: <span class="type">Int</span>,</div><div class="line">                replicationFactor: <span class="type">Int</span>,</div><div class="line">                topicConfig: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>,</div><div class="line">                rackAwareMode: <span class="type">RackAwareMode</span> = <span class="type">RackAwareMode</span>.<span class="type">Enforced</span>) &#123;</div><div class="line">  <span class="keyword">val</span> brokerMetadatas = getBrokerMetadatas(zkUtils, rackAwareMode)<span class="comment">//note: 有机架感知的情况下,返回 Broker 与机架之间的信息</span></div><div class="line">  <span class="keyword">val</span> replicaAssignment = <span class="type">AdminUtils</span>.assignReplicasToBrokers(brokerMetadatas, partitions, replicationFactor)<span class="comment">//note: 获取 partiiton 的 replicas 分配</span></div><div class="line">  <span class="type">AdminUtils</span>.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, replicaAssignment, topicConfig)<span class="comment">//note: 更新到 zk 上</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Producer-创建-topic"><a href="#Producer-创建-topic" class="headerlink" title="Producer 创建 topic"></a>Producer 创建 topic</h3><p>只有当 Server 端的 <code>auto.create.topics.enable</code> 设置为 true 时，Producer 向一个不存在的 topic 发送数据，该 topic 才会被自动创建。</p>
<p>当 Producer 在向一个 topic 发送 produce 请求前，会先通过发送 Metadata 请求来获取这个 topic 的 metadata。Server 端在处理 Metadata 请求时，如果发现要获取 metadata 的 topic 不存在但 Server 允许 producer 自动创建 topic 的话（如果开启权限时，要求 Producer 需要有相应权限：对 topic 有 Describe 权限，并且对当前集群有 Create 权限），那么 Server 将会自动创建该 topic.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 topic 的 metadata 信息</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getTopicMetadata</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>], listenerName: <span class="type">ListenerName</span>, errorUnavailableEndpoints: <span class="type">Boolean</span>): <span class="type">Seq</span>[<span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> topicResponses = metadataCache.getTopicMetadata(topics, listenerName, errorUnavailableEndpoints)</div><div class="line">  <span class="keyword">if</span> (topics.isEmpty || topicResponses.size == topics.size) &#123;</div><div class="line">    topicResponses</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">val</span> nonExistentTopics = topics -- topicResponses.map(_.topic).toSet<span class="comment">//note: 集群上暂时不存在的 topic 列表</span></div><div class="line">    <span class="keyword">val</span> responsesForNonExistentTopics = nonExistentTopics.map &#123; topic =&gt;</div><div class="line">      <span class="keyword">if</span> (topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>) &#123;</div><div class="line">        createGroupMetadataTopic()</div><div class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (config.autoCreateTopicsEnable) &#123;<span class="comment">//note: auto.create.topics.enable 为 true 时,即允许自动创建 topic</span></div><div class="line">        createTopic(topic, config.numPartitions, config.defaultReplicationFactor)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>, topic, <span class="literal">false</span>,</div><div class="line">          java.util.<span class="type">Collections</span>.emptyList())</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    topicResponses ++ responsesForNonExistentTopics</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中 <code>createTopic</code> 还是调用了 <code>AdminUtils.createTopic</code> 来创建 topic，与命令行创建的底层实现是一样。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(topic: <span class="type">String</span>,</div><div class="line">                        numPartitions: <span class="type">Int</span>,</div><div class="line">                        replicationFactor: <span class="type">Int</span>,</div><div class="line">                        properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()): <span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span> = &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">//note: 还是调用 AdminUtils 命令创建 topic</span></div><div class="line">    <span class="type">AdminUtils</span>.createTopic(zkUtils, topic, numPartitions, replicationFactor, properties, <span class="type">RackAwareMode</span>.<span class="type">Safe</span>)</div><div class="line">    info(<span class="string">"Auto creation of topic %s with %d partitions and replication factor %d is successful"</span></div><div class="line">      .format(topic, numPartitions, replicationFactor))</div><div class="line">    <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>(<span class="type">Errors</span>.<span class="type">LEADER_NOT_AVAILABLE</span>, topic, <span class="type">Topic</span>.isInternal(topic),</div><div class="line">      java.util.<span class="type">Collections</span>.emptyList())</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> _: <span class="type">TopicExistsException</span> =&gt; <span class="comment">// let it go, possibly another broker created this topic</span></div><div class="line">      <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>(<span class="type">Errors</span>.<span class="type">LEADER_NOT_AVAILABLE</span>, topic, <span class="type">Topic</span>.isInternal(topic),</div><div class="line">        java.util.<span class="type">Collections</span>.emptyList())</div><div class="line">    <span class="keyword">case</span> ex: <span class="type">Throwable</span>  =&gt; <span class="comment">// Catch all to prevent unhandled errors</span></div><div class="line">      <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>(<span class="type">Errors</span>.forException(ex), topic, <span class="type">Topic</span>.isInternal(topic),</div><div class="line">        java.util.<span class="type">Collections</span>.emptyList())</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="replica-如何分配"><a href="#replica-如何分配" class="headerlink" title="replica 如何分配"></a>replica 如何分配</h2><p>通过前面的内容，可以看到，无论使用哪种方式，最后都是通过 <code>AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK()</code> 将 topic 的 Partition replicas 的更新到 zk 上，这中间关键的一点在于：Partition 的 replicas 是如何分配的。在创建时，我们既可以指定相应 replicas 分配，也可以使用默认的算法自动分配。</p>
<h3 id="创建时指定-replicas-分配"><a href="#创建时指定-replicas-分配" class="headerlink" title="创建时指定 replicas 分配"></a>创建时指定 replicas 分配</h3><p>在创建 topic 时，可以通过以下形式直接指定 topic 的 replica</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --create --topic <span class="built_in">test</span> --zookeeper XXXX --replica-assignment 1:2,3:4,5:6</div></pre></td></tr></table></figure>
<p>该 topic 有三个 partition，其中，partition 0 的 replica 分布在1和2上，partition 1 的 replica 分布在3和4上，partition 3 的 replica 分布在4和5上。</p>
<p>这样情况下，在创建 topic 时，Server 端会将该 replica 分布直接更新到 zk 上。</p>
<h3 id="replicas-自动分配算法"><a href="#replicas-自动分配算法" class="headerlink" title="replicas 自动分配算法"></a>replicas 自动分配算法</h3><p>在创建 topic 时，Server 通过 <code>AdminUtils.assignReplicasToBrokers()</code> 方法来获取该 topic partition 的 replicas 分配。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * 副本分配时,有三个原则:</div><div class="line">   * 1. 将副本平均分布在所有的 Broker 上;</div><div class="line">   * 2. partition 的多个副本应该分配在不同的 Broker 上;</div><div class="line">   * 3. 如果所有的 Broker 有机架信息的话, partition 的副本应该分配到不同的机架上。</div><div class="line">   *</div><div class="line">   * 为实现上面的目标,在没有机架感知的情况下，应该按照下面两个原则分配 replica:</div><div class="line">   * 1. 从 broker.list 随机选择一个 Broker,使用 round-robin 算法分配每个 partition 的第一个副本;</div><div class="line">   * 2. 对于这个 partition 的其他副本,逐渐增加 Broker.id 来选择 replica 的分配。</div><div class="line">   *</div><div class="line">   * @param brokerMetadatas</div><div class="line">   * @param nPartitions</div><div class="line">   * @param replicationFactor</div><div class="line">   * @param fixedStartIndex</div><div class="line">   * @param startPartitionId</div><div class="line">   * @return</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokers</span></span>(brokerMetadatas: <span class="type">Seq</span>[<span class="type">BrokerMetadata</span>],</div><div class="line">                             nPartitions: <span class="type">Int</span>,</div><div class="line">                             replicationFactor: <span class="type">Int</span>,</div><div class="line">                             fixedStartIndex: <span class="type">Int</span> = <span class="number">-1</span>,</div><div class="line">                             startPartitionId: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</div><div class="line">   <span class="keyword">if</span> (nPartitions &lt;= <span class="number">0</span>) <span class="comment">// note: 要增加的 partition 数需要大于0</span></div><div class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidPartitionsException</span>(<span class="string">"number of partitions must be larger than 0"</span>)</div><div class="line">   <span class="keyword">if</span> (replicationFactor &lt;= <span class="number">0</span>) <span class="comment">//note: replicas 应该大于0</span></div><div class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">"replication factor must be larger than 0"</span>)</div><div class="line">   <span class="keyword">if</span> (replicationFactor &gt; brokerMetadatas.size) <span class="comment">//note: replicas 超过了 broker 数</span></div><div class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">s"replication factor: <span class="subst">$replicationFactor</span> larger than available brokers: <span class="subst">$&#123;brokerMetadatas.size&#125;</span>"</span>)</div><div class="line">   <span class="keyword">if</span> (brokerMetadatas.forall(_.rack.isEmpty))<span class="comment">//note: 没有开启机架感知</span></div><div class="line">     assignReplicasToBrokersRackUnaware(nPartitions, replicationFactor, brokerMetadatas.map(_.id), fixedStartIndex,</div><div class="line">       startPartitionId)</div><div class="line">   <span class="keyword">else</span> &#123; <span class="comment">//note: 机架感知的情况</span></div><div class="line">     <span class="keyword">if</span> (brokerMetadatas.exists(_.rack.isEmpty)) <span class="comment">//note: 并不是所有的机架都有机架感知</span></div><div class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">"Not all brokers have rack information for replica rack aware assignment"</span>)</div><div class="line">     assignReplicasToBrokersRackAware(nPartitions, replicationFactor, brokerMetadatas, fixedStartIndex,</div><div class="line">       startPartitionId)</div><div class="line">   &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>这里没有开启机架感知模式来介绍 topic partition replicas 的分配情况，其分配算法主要是 <code>assignReplicasToBrokersRackUnaware()</code> 方法中实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: partition 分配</span></div><div class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokersRackUnaware</span></span>(nPartitions: <span class="type">Int</span>,</div><div class="line">                                                replicationFactor: <span class="type">Int</span>,</div><div class="line">                                                brokerList: <span class="type">Seq</span>[<span class="type">Int</span>],</div><div class="line">                                                fixedStartIndex: <span class="type">Int</span>,</div><div class="line">                                                startPartitionId: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</div><div class="line">   <span class="keyword">val</span> ret = mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]]()</div><div class="line">   <span class="keyword">val</span> brokerArray = brokerList.toArray</div><div class="line">   <span class="keyword">val</span> startIndex = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length) <span class="comment">//note: 随机选择一个Broker</span></div><div class="line">   <span class="keyword">var</span> currentPartitionId = math.max(<span class="number">0</span>, startPartitionId) <span class="comment">//note: 开始增加的第一个 partition</span></div><div class="line">   <span class="keyword">var</span> nextReplicaShift = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</div><div class="line">   <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until nPartitions) &#123; <span class="comment">//note: 对每个 partition 进行分配</span></div><div class="line">     <span class="keyword">if</span> (currentPartitionId &gt; <span class="number">0</span> &amp;&amp; (currentPartitionId % brokerArray.length == <span class="number">0</span>))</div><div class="line">       nextReplicaShift += <span class="number">1</span> <span class="comment">//note: 防止 partition 过大时,其中某些 partition 的分配（leader、follower）完全一样</span></div><div class="line">     <span class="keyword">val</span> firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length <span class="comment">//note: partition 的第一个 replica</span></div><div class="line">     <span class="keyword">val</span> replicaBuffer = mutable.<span class="type">ArrayBuffer</span>(brokerArray(firstReplicaIndex))</div><div class="line">     <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until replicationFactor - <span class="number">1</span>) <span class="comment">//note: 其他 replica 的分配</span></div><div class="line">       replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length))</div><div class="line">     ret.put(currentPartitionId, replicaBuffer)</div><div class="line">     currentPartitionId += <span class="number">1</span></div><div class="line">   &#125;</div><div class="line">   ret</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">//note: 为 partition 设置完第一个 replica 后,其他 replica 分配的计算</span></div><div class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex: <span class="type">Int</span>, secondReplicaShift: <span class="type">Int</span>, replicaIndex: <span class="type">Int</span>, nBrokers: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</div><div class="line">   <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>)<span class="comment">//note: 在 secondReplicaShift 的基础上增加一个 replicaIndex</span></div><div class="line">   (firstReplicaIndex + shift) % nBrokers</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>这里举一个栗子，假设一个 Kafka 集群有5个节点，新建的 topic 有10个 partition，并且是三副本，假设最初随机选择的 <code>startIndex</code> 和 <code>nextReplicaShift</code> 节点均为0</p>
<ul>
<li>partition 为0时，那第一副本在 <code>(0+0)%5=0</code>，第二个副本在 <code>(0+(1+(0+0)%5)))%5=1</code>，第三副本在 <code>(0+(1+(0+1)%5)))%5=2</code>；</li>
<li>partition 为2时，那第一副本在 <code>(0+2)%5=2</code>，第二个副本在 <code>(2+(1+(0+0)%5)))%5=3</code>，第三副本在 <code>(2+(1+(0+1)%5)))%5=4</code>；</li>
<li>partition 为5时，那第一副本在 <code>(0+5)%5=0</code>，第二个副本在 <code>(0+(1+(1+0)%5)))%5=2</code>，第三副本在 <code>(0+(1+(1+1)%5)))%5=3</code>（partition 数是 Broker 数一倍时，<code>nextReplicaShift</code> 值会增加1）；</li>
<li>partition 为8时，那第一副本在 <code>(0+8)%5=3</code>，第二个副本在 <code>(3+(1+(1+0)%5)))%5=0</code>，第三副本在 <code>(3+(1+(1+1)%5)))%5=1</code>。</li>
</ul>
<p>分配如下表所示：</p>
<table>
<thead>
<tr>
<th>broker-0</th>
<th>broker-1</th>
<th>broker-2</th>
<th>broker-3</th>
<th>broker-4</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>p0</td>
<td>p1</td>
<td>p2</td>
<td>p3</td>
<td>p4</td>
<td>(1st replica)</td>
</tr>
<tr>
<td>p5</td>
<td>p6</td>
<td>p7</td>
<td>p8</td>
<td>p9</td>
<td>(1st replica)</td>
</tr>
<tr>
<td>p4</td>
<td>p0</td>
<td>p1</td>
<td>p2</td>
<td>p3</td>
<td>(2nd replica)</td>
</tr>
<tr>
<td>p8</td>
<td>p9</td>
<td>p5</td>
<td>p6</td>
<td>p7</td>
<td>(2nd replica)</td>
</tr>
<tr>
<td>p3</td>
<td>p4</td>
<td>p0</td>
<td>p1</td>
<td>p2</td>
<td>(3nd replica)</td>
</tr>
<tr>
<td>p7</td>
<td>p8</td>
<td>p9</td>
<td>p5</td>
<td>p6</td>
<td>(3nd replica)</td>
</tr>
</tbody>
</table>
<h2 id="replicas-更新到-zk-后触发的操作"><a href="#replicas-更新到-zk-后触发的操作" class="headerlink" title="replicas 更新到 zk 后触发的操作"></a>replicas 更新到 zk 后触发的操作</h2><p>这一部分的内容是由 Kafka Controller 来控制的（Kafka Controller 将会在后续文章中讲解），当一个 topic 的 replicas 更新到 zk 上后，监控 zk 这个目录的方法会被触发（<code>TopicChangeListener.doHandleChildChange()</code>方法），可以配合文章第一张图来看。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 zk 上 topic 节点上有变更时,这个方法就会调用</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">val</span> currentChildren = &#123;</div><div class="line">              debug(<span class="string">"Topic change listener fired for path %s with children %s"</span>.format(parentPath, children.mkString(<span class="string">","</span>)))</div><div class="line">              children.toSet</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">val</span> newTopics = currentChildren -- controllerContext.allTopics<span class="comment">//note: 新创建的 topic 列表</span></div><div class="line">            <span class="keyword">val</span> deletedTopics = controllerContext.allTopics -- currentChildren<span class="comment">//note: 已经删除的 topic 列表</span></div><div class="line">            controllerContext.allTopics = currentChildren</div><div class="line"></div><div class="line">            <span class="comment">//note: 新创建 topic 对应的 partition 列表</span></div><div class="line">            <span class="keyword">val</span> addedPartitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(newTopics.toSeq)</div><div class="line">            controllerContext.partitionReplicaAssignment = controllerContext.partitionReplicaAssignment.filter(p =&gt;</div><div class="line">              !deletedTopics.contains(p._1.topic))<span class="comment">//note: 把已经删除 partition 过滤掉</span></div><div class="line">            controllerContext.partitionReplicaAssignment.++=(addedPartitionReplicaAssignment)<span class="comment">//note: 将新增的 tp-replicas 更新到缓存中</span></div><div class="line">            info(<span class="string">"New topics: [%s], deleted topics: [%s], new partition replica assignment [%s]"</span>.format(newTopics,</div><div class="line">              deletedTopics, addedPartitionReplicaAssignment))</div><div class="line">            <span class="keyword">if</span> (newTopics.nonEmpty)<span class="comment">//note: 处理新建的 topic</span></div><div class="line">              controller.onNewTopicCreation(newTopics, addedPartitionReplicaAssignment.keySet)</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling new topic"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>这个方法主要做了以下内容：</p>
<ul>
<li>获取 zk 的 topic 变更信息，得到新创建的 topic 列表（<code>newTopics</code>）以及被删除的 topic 列表（<code>deletedTopics</code>）；</li>
<li>将 <code>deletedTopics</code> 的 replicas 从 controller 的缓存中删除，并将新增 topic 的 replicas 更新到 controller 的缓存中；</li>
<li>调用 KafkaController 的 <code>onNewTopicCreation()</code> 创建 partition 和 replica 对象。</li>
</ul>
<p>KafkaController 中 <code>onNewTopicCreation()</code> 方法先对这些 topic 注册 <code>PartitionChangeListener</code>，然后再调用 <code>onNewPartitionCreation()</code> 方法创建 partition 和 replicas 的实例对象，<strong>topic 创建的主要实现是在 KafkaController <code>onNewPartitionCreation()</code> 这个方法中</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 partition state machine 监控到有新 topic 或 partition 时,这个方法将会被调用</span></div><div class="line"> <span class="comment">/**</span></div><div class="line">   * 1. 注册 partition change listener;</div><div class="line">   * 2. 触发 the new partition callback,也即是 onNewPartitionCreation()</div><div class="line">   * 3. 发送 metadata 请求给所有的 Broker</div><div class="line">   * @param topics</div><div class="line">   * @param newPartitions</div><div class="line">   */</div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">onNewTopicCreation</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>], newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">   info(<span class="string">"New topic creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">   <span class="comment">// subscribe to partition changes</span></div><div class="line">   topics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">   onNewPartitionCreation(newPartitions)</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> <span class="comment">//note: topic 变化时,这个方法将会被调用</span></div><div class="line"> <span class="comment">//note: 1. 将新创建的 partition 置为 NewPartition 状态; 2.从 NewPartition 改为 OnlinePartition 状态</span></div><div class="line"> <span class="comment">//note: 1. 将新创建的 Replica 置为 NewReplica 状态; 2.从 NewReplica 改为 OnlineReplica 状态</span></div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">onNewPartitionCreation</span></span>(newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">   info(<span class="string">"New partition creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">   partitionStateMachine.handleStateChanges(newPartitions, <span class="type">NewPartition</span>)</div><div class="line">   replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">NewReplica</span>)</div><div class="line">   partitionStateMachine.handleStateChanges(newPartitions, <span class="type">OnlinePartition</span>, offlinePartitionSelector)</div><div class="line">   replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">OnlineReplica</span>)</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>在详细介绍这四个方法的调用之前，先简单详述一下 Partition 和 Replica 状态机的变化。</p>
<h3 id="Partition-状态机"><a href="#Partition-状态机" class="headerlink" title="Partition 状态机"></a>Partition 状态机</h3><p>关于 Partition 状态的变化可以参考 Kafka 中的这个方法 <a href="https://github.com/apache/kafka/blob/0.10.2/core/src/main/scala/kafka/controller/PartitionStateMachine.scala" target="_blank" rel="external">PartitionStateMachine</a>，状态机的具体转换情况如下图所示</p>
<p><img src="/images/kafka/partition_state.png" alt="Partition 状态机"></p>
<p>一个 Partition 对象有四种状态：</p>
<ol>
<li><code>NonExistentPartition</code>：这个 partition 不存在；</li>
<li><code>NewPartition</code>：这个 partition 刚创建，有对应的 replicas，但还没有 leader 和 isr；</li>
<li><code>OnlinePartition</code>：这个 partition 的 leader 已经选举出来了，处理正常的工作状态；</li>
<li><code>OfflinePartition</code>：partition 的 leader 挂了。</li>
</ol>
<p>partition 只有在 <code>OnlinePartition</code> 这个状态时，才是可用状态。</p>
<h3 id="Replica-状态机"><a href="#Replica-状态机" class="headerlink" title="Replica 状态机"></a>Replica 状态机</h3><p>关于 Replica 状态的变化可以参考 Kafka 中的这个方法 <a href="https://github.com/apache/kafka/blob/0.10.2/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala" target="_blank" rel="external">ReplicaStateMachine</a>，，状态机的具体转换情况如下图所示</p>
<p><img src="/images/kafka/replica_state.png" alt="Replica 状态机"></p>
<p>Replica 对象有七种状态，中文解释的比较难以理解，直接上原文对这几种状态的解释。</p>
<ol>
<li><code>NewReplica</code>：The controller can create new replicas during partition reassignment. In this state, a replica can only get become follower state change request.</li>
<li><code>OnlineReplica</code>：Once a replica is started and part of the assigned replicas for its partition, it is in this state. In this state, it can get either become leader or become follower state change requests.</li>
<li><code>OfflineReplica</code>：If a replica dies, it moves to this state. This happens when the broker hosting the replica is down.</li>
<li><code>ReplicaDeletionStarted</code>：If replica deletion starts, it is moved to this state.</li>
<li><code>ReplicaDeletionSuccessful</code>：If replica responds with no error code in response to a delete replica request, it is moved to this state.</li>
<li><code>ReplicaDeletionIneligible</code>：If replica deletion fails, it is moved to this state.</li>
<li><code>NonExistentReplica</code>：If a replica is deleted successfully, it is moved to this state.</li>
</ol>
<h3 id="onNewPartitionCreation-详解"><a href="#onNewPartitionCreation-详解" class="headerlink" title="onNewPartitionCreation() 详解"></a><code>onNewPartitionCreation()</code> 详解</h3><p>这个方法有以下四步操作：</p>
<ol>
<li><code>partitionStateMachine.handleStateChanges(newPartitions, NewPartition)</code>： 创建 Partition 对象，并将其状态置为 <code>NewPartition</code> 状态</li>
<li><code>replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), NewReplica)</code>：创建 Replica 对象，并将其状态置为 <code>NewReplica</code> 状态；</li>
<li><code>partitionStateMachine.handleStateChanges(newPartitions, OnlinePartition, offlinePartitionSelector)</code>：将 partition 对象从 <code>NewPartition</code> 改为 <code>OnlinePartition</code> 状态；</li>
<li><code>replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), OnlineReplica)</code>：将 Replica 对象从 <code>NewReplica</code> 改为 <code>OnlineReplica</code> 状态。</li>
</ol>
<h4 id="partitionStateMachine-gt-NewPartition"><a href="#partitionStateMachine-gt-NewPartition" class="headerlink" title="partitionStateMachine &gt; NewPartition"></a>partitionStateMachine &gt; NewPartition</h4><p>这部分的作用是，创建分区对象，并将其状态设置为 <code>NewPartition</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</div><div class="line">  <span class="comment">//note: 新建一个 partition</span></div><div class="line">  assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</div><div class="line">  partitionState.put(topicAndPartition, <span class="type">NewPartition</span>) <span class="comment">//note: 缓存 partition 的状态</span></div><div class="line">  <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</div><div class="line">  stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></div><div class="line">                            .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</div><div class="line">                                    assignedReplicas))</div></pre></td></tr></table></figure>
<h4 id="replicaStateMachine-gt-NewReplica"><a href="#replicaStateMachine-gt-NewReplica" class="headerlink" title="replicaStateMachine &gt; NewReplica"></a>replicaStateMachine &gt; NewReplica</h4><p>这部分是为每个 Partition 创建对应的 replica 对象，并将其状态设置为 <code>NewReplica</code>，参照状态机的变化图更好理解。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="type">NewReplica</span> =&gt;</div><div class="line">          assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">NonExistentReplica</span>), targetState)  <span class="comment">//note: 验证</span></div><div class="line">          <span class="comment">// start replica as a follower to the current leader for its partition</span></div><div class="line">          <span class="keyword">val</span> leaderIsrAndControllerEpochOpt = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topic, partition)</div><div class="line">          leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">              <span class="keyword">if</span>(leaderIsrAndControllerEpoch.leaderAndIsr.leader == replicaId)<span class="comment">//note: 这个状态的 Replica 不能作为 leader</span></div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Replica %d for partition %s cannot be moved to NewReplica"</span></div><div class="line">                  .format(replicaId, topicAndPartition) + <span class="string">"state as it is being requested to become leader"</span>)</div><div class="line">              <span class="comment">//note: 向所有 replicaId 发送 LeaderAndIsr 请求,这个方法同时也会向所有的 broker 发送 updateMeta 请求</span></div><div class="line">              brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId),</div><div class="line">                                                                  topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">                                                                  replicaAssignment)</div><div class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// new leader request will be sent to this replica when one gets elected</span></div></pre></td></tr></table></figure>
<h4 id="partitionStateMachine-gt-OnlinePartition"><a href="#partitionStateMachine-gt-OnlinePartition" class="headerlink" title="partitionStateMachine &gt; OnlinePartition"></a>partitionStateMachine &gt; OnlinePartition</h4><p>这个方法的主要的作用是将 partition 对象的状态由 <code>NewPartition</code> 设置为 <code>OnlinePartition</code>，从状态机图中可以看到，会有以下两步操作：</p>
<ol>
<li>初始化 leader 和 isr，replicas 中的第一个 replica 将作为 leader，所有 replica 作为 isr，并把 leader 和 isr 信息更新到 zk；</li>
<li>发送 LeaderAndIsr 请求给所有的 replica，发送 UpdateMetadata 给所有 Broker。</li>
</ol>
<p>具体操作如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// post: partition has been assigned replicas</span></div><div class="line">       <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt;</div><div class="line">         assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</div><div class="line">         partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">           <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</div><div class="line">             <span class="comment">// initialize leader and isr path for new partition</span></div><div class="line">             initializeLeaderAndIsrForPartition(topicAndPartition) <span class="comment">//note: 为新建的 partition 初始化 leader 和 isr</span></div><div class="line">           <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt;</div><div class="line">             electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">           <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></div><div class="line">             electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">           <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></div><div class="line">         &#125;</div></pre></td></tr></table></figure>
<p>实际的操作是在 <code>initializeLeaderAndIsrForPartition()</code> 方法中完成，这个方法是当 partition 对象的状态由 NewPartition 变为 OnlinePartition 时触发的，用来初始化该 partition 的 leader 和 isr。简单来说，就是选取 Replicas 中的第一个 Replica 作为 leader，所有的 Replica 作为 isr，最后调用 <code>brokerRequestBatch.addLeaderAndIsrRequestForBrokers</code> 向所有 replicaId 发送 LeaderAndIsr 请求以及向所有的 broker 发送 UpdateMetadata 请求（关于 Server 对 LeaderAndIsr 和 UpdateMetadata 请求的处理将会后续文章中讲述）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 partition 状态由 NewPartition 变为 OnlinePartition 时,将触发这一方法,用来初始化 partition 的 leader 和 isr</span></div><div class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeLeaderAndIsrForPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>) &#123;</div><div class="line">   <span class="keyword">val</span> replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">   <span class="keyword">val</span> liveAssignedReplicas = replicaAssignment.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">   liveAssignedReplicas.size <span class="keyword">match</span> &#123;</div><div class="line">     <span class="keyword">case</span> <span class="number">0</span> =&gt;</div><div class="line">       <span class="keyword">val</span> failMsg = (<span class="string">"encountered error during state change of partition %s from New to Online, assigned replicas are [%s], "</span> +</div><div class="line">                      <span class="string">"live brokers are [%s]. No assigned replica is alive."</span>)</div><div class="line">                        .format(topicAndPartition, replicaAssignment.mkString(<span class="string">","</span>), controllerContext.liveBrokerIds)</div><div class="line">       stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">     <span class="keyword">case</span> _ =&gt;</div><div class="line">       debug(<span class="string">"Live assigned replicas for partition %s are: [%s]"</span>.format(topicAndPartition, liveAssignedReplicas))</div><div class="line">       <span class="comment">// make the first replica in the list of assigned replicas, the leader</span></div><div class="line">       <span class="keyword">val</span> leader = liveAssignedReplicas.head <span class="comment">//note: replicas 中的第一个 replica 选做 leader</span></div><div class="line">       <span class="keyword">val</span> leaderIsrAndControllerEpoch = <span class="keyword">new</span> <span class="type">LeaderIsrAndControllerEpoch</span>(<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(leader, liveAssignedReplicas.toList),</div><div class="line">         controller.epoch)</div><div class="line">       debug(<span class="string">"Initializing leader and isr for partition %s to %s"</span>.format(topicAndPartition, leaderIsrAndControllerEpoch))</div><div class="line">       <span class="keyword">try</span> &#123;</div><div class="line">         zkUtils.createPersistentPath(</div><div class="line">           getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition),</div><div class="line">           zkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch))<span class="comment">//note: zk 上初始化节点信息</span></div><div class="line">         <span class="comment">// <span class="doctag">NOTE:</span> the above write can fail only if the current controller lost its zk session and the new controller</span></div><div class="line">         <span class="comment">// took over and initialized this partition. This can happen if the current controller went into a long</span></div><div class="line">         <span class="comment">// GC pause</span></div><div class="line">         controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch)</div><div class="line">         brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic,</div><div class="line">           topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment)<span class="comment">//note: 向 live 的 Replica 发送  LeaderAndIsr 请求</span></div><div class="line">       &#125; <span class="keyword">catch</span> &#123;</div><div class="line">         <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt;</div><div class="line">           <span class="comment">// read the controller epoch</span></div><div class="line">           <span class="keyword">val</span> leaderIsrAndEpoch = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topicAndPartition.topic,</div><div class="line">             topicAndPartition.partition).get</div><div class="line">           <span class="keyword">val</span> failMsg = (<span class="string">"encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already "</span> +</div><div class="line">                          <span class="string">"exists with value %s and controller epoch %d"</span>)</div><div class="line">                            .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch)</div><div class="line">           stateChangeLogger.error(<span class="string">"Controller %d epoch %d "</span>.format(controllerId, controller.epoch) + failMsg)</div><div class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(failMsg)</div><div class="line">       &#125;</div><div class="line">   &#125;</div></pre></td></tr></table></figure>
<h4 id="replicaStateMachine-gt-OnlineReplica"><a href="#replicaStateMachine-gt-OnlineReplica" class="headerlink" title="replicaStateMachine &gt; OnlineReplica"></a>replicaStateMachine &gt; OnlineReplica</h4><p>这一步也就是最后一步，将 Replica 对象的状态由 <code>NewReplica</code> 更新为 <code>OnlineReplica</code> 状态，这些 Replica 才真正可用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="type">OnlineReplica</span> =&gt;</div><div class="line">          assertValidPreviousStates(partitionAndReplica,</div><div class="line">            <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line">          replicaState(partitionAndReplica) <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> <span class="type">NewReplica</span> =&gt;</div><div class="line">              <span class="comment">// add this replica to the assigned replicas list for its partition</span></div><div class="line">              <span class="comment">//note: 向 the assigned replicas list 添加这个 replica（正常情况下这些 replicas 已经更新到 list 中了）</span></div><div class="line">              <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">              <span class="keyword">if</span>(!currentAssignedReplicas.contains(replicaId))</div><div class="line">                controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas :+ replicaId)</div><div class="line">              stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                                        .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                                targetState))</div></pre></td></tr></table></figure>
<p>一直到这一步，一个 topic 就才算真正被创建完成。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="http://www.cnblogs.com/huxi2b/p/5923252.html" target="_blank" rel="external">Kafka如何创建topic？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是 Kafka 源码解析的第三篇，主要讲述一个 topic 的创建过程，从 topic 是如何创建到 topic 真正创建成功的中间详细过程，文章主要内容可以分为以下几个部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;topic 是如何创建的？&lt;ul&gt;
&lt;li&gt;命令行创建；&lt;/li&gt;
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Linux 常用的一些系统命令</title>
    <link href="http://matt33.com/2017/07/16/linux-system-cmd/"/>
    <id>http://matt33.com/2017/07/16/linux-system-cmd/</id>
    <published>2017-07-16T02:31:47.000Z</published>
    <updated>2017-12-05T14:17:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章的内容，基本来自<a href="http://www.cnblogs.com/peida/tag/%E6%AF%8F%E6%97%A5%E4%B8%80linux%E5%91%BD%E4%BB%A4/" target="_blank" rel="external">每日一个 linux 命令</a>，选取了几个在工作常用的命令，有：top、iostat、netstat 、free 和 ps，本文的主要目的是在学习这几条命令的过程中，简单做一些记录，便于日后工作中更加熟练地使用这些命令。</p>
<h1 id="top"><a href="#top" class="headerlink" title="top"></a>top</h1><p>top 命令是 Linux 下面实时展示系统运行情况的一个命令，它也可以显示当前每个任务的系统信息。在对系统的性能进行分析，它是一个最常用的命令。</p>
<h2 id="命令常用参数"><a href="#命令常用参数" class="headerlink" title="命令常用参数"></a>命令常用参数</h2><ol>
<li>命令格式：<ul>
<li>top [参数]</li>
</ul>
</li>
<li>命令功能：<ul>
<li>显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等</li>
</ul>
</li>
<li>命令参数：<ul>
<li><code>-b</code>: 批处理</li>
<li><code>-c</code>: 显示完整的治命令</li>
<li><code>-I</code>: 忽略失效过程</li>
<li><code>-s</code>: 保密模式</li>
<li><code>-S</code>: 累积模式</li>
<li><code>-i&lt;时间&gt;</code>: 设置间隔时间</li>
<li><code>-u&lt;用户名&gt;</code>: 指定用户名</li>
<li><code>-p&lt;进程号&gt;</code>: 指定进程</li>
<li><code>-n&lt;次数&gt;</code>: 循环显示的次数</li>
</ul>
</li>
</ol>
<h2 id="显示说明"><a href="#显示说明" class="headerlink" title="显示说明"></a>显示说明</h2><p>在命令行输入 <code>top</code> 命令，终端会展示当前系统的信息，如下所示</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[matt@XXX ~]$ top</div><div class="line">top - 21:04:19 up 129 days, 20:31,  1 user,  load average: 58.32, 57.85, 57.50</div><div class="line">Tasks: 589 total,   1 running, 584 sleeping,   0 stopped,   4 zombie</div><div class="line">Cpu(s): 22.3%us, 11.8%sy,  0.0%ni, 63.2%id,  0.2%wa,  0.0%hi,  2.5%si,  0.0%st</div><div class="line">Mem:  132103752k total, 122070628k used, 10033124k free,    42940k buffers</div><div class="line">Swap:        0k total,        0k used,        0k free, 58734284k cached</div><div class="line"></div><div class="line">   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND</div><div class="line"> 51179	matt   20   0 70.6g  24g  80m S 704.9 19.2  39244,38 java</div><div class="line">     1 root      20   0 1155m 1.1g  592 S  0.0  0.9   4020:57 init</div><div class="line">     2 root      20   0     0    0    0 S  0.0  0.0   0:00.01 kthreadd</div></pre></td></tr></table></figure>
<p>上面只是截取了部分的信息，这里介绍以下上面的一些信息的说明。上面的前五行是当前系统情况整体的统计信息区。</p>
<ol>
<li>任务的队列信息，同 uptime 命令的执行结果<ul>
<li><code>21:04:19</code>: 当前系统时间；</li>
<li><code>up 129 days, 20:31</code>: 系统已经运行了129天20小时31分钟（这期间系统没有重启）；</li>
<li><code>1 users</code>: 当前有1个用户登录系统；</li>
<li><code>load average: 58.32, 57.85, 57.50</code>: load average 后面的三个数分别是1分钟、5分钟、15分钟的负载情况（<strong>这个数除以逻辑 CPU 的数量，结果高于5的时候就表明系统在超负荷运转</strong>）</li>
</ul>
</li>
<li>Tasks — 任务（进程）的统计信息<ul>
<li>系统现在共有589个进程，其中处于运行中的有1个，584个在休眠（sleep），stoped 状态的有0个，zombie 状态（僵尸）的有4个；</li>
</ul>
</li>
<li>cpu 的状态信息<ul>
<li><code>22.3%us</code>: 用户空间占用 CPU 的百分比</li>
<li><code>11.8% sy</code>: 内核空间占用 CPU 的百分比</li>
<li><code>0.0% ni</code>: 改变过优先级的进程占用 CPU 的百分比</li>
<li><code>63.2% id</code>: 空闲 CPU 百分比</li>
<li><code>0.2% wa</code>: IO 等待占用 CPU 的百分比</li>
<li><code>0.0% hi</code>: 硬中断（Hardware IRQ）占用 CPU 的百分比</li>
<li><code>2.5% si</code>: 软中断（Software Interrupts）占用 CPU 的百分比</li>
<li><code>0.0% st</code>: 虚拟机占用的百分比</li>
</ul>
</li>
<li>内存的状态信息<ul>
<li><code>132103752k total</code>： 物理内存总量（128GB）</li>
<li><code>122070628k used</code>： 使用中的内存总量（118GB）</li>
<li><code>10033124k free</code>： 空闲内存总量（10GB）</li>
<li><code>42940k buffers</code>： 缓存的内存量 （42M）</li>
</ul>
</li>
<li>swap交换分区信息<ul>
<li><code>0k total</code>: 交换区总量（0K）</li>
<li><code>0k used</code>: 使用的交换区总量（0K）</li>
<li><code>0k free</code>: 空闲交换区总量（0K）</li>
<li><code>58734284k cached</code>: 缓冲的交换区总量（56GB）</li>
</ul>
</li>
<li>空行，作为系统信息与进程信息的分界线</li>
<li>各进程（任务）的状态监控<ul>
<li><code>PID</code>: 进程id</li>
<li><code>USER</code>: 进程所有者</li>
<li><code>PR</code>: 进程优先级</li>
<li><code>NI</code>: nice 值，负值表示高优先级，正值表示低优先级</li>
<li><code>VIRT</code>: 进程使用的虚拟内存总量，单位 kb。VIRT=SWAP+RES</li>
<li><code>RES</code>: 进程使用的、未被换出的物理内存大小，单位 kb。RES=CODE+DATA</li>
<li><code>SHR</code>: 共享内存大小，单位 kb</li>
<li><code>S</code>: 进程状态。D= 不可中断的睡眠状态 R= 运行 S= 睡眠 T= 跟踪/停止 Z= 僵尸进程</li>
<li><code>%CPU</code>: 上次更新到现在的CPU时间占用百分比</li>
<li><code>%MEM</code>: 进程使用的物理内存百分比</li>
<li><code>TIME+</code>: 进程使用的CPU时间总计，单位1/100秒</li>
<li><code>COMMAND</code>: 进程名称</li>
</ul>
</li>
</ol>
<p>其中，第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到 free 中去，因此在 linux 上 free 内存会越来越少，但不用为此担心。</p>
<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><p>在工作中，常用的几个命令在 <a href="http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="external">每天一个linux命令（44）：top命令</a> 都已经介绍得比较全面了，这里，再简单重复一下。</p>
<h3 id="多-CPU-监控"><a href="#多-CPU-监控" class="headerlink" title="多 CPU 监控"></a>多 CPU 监控</h3><p>在 top 的基本视图中，按键盘数字<strong>1</strong>，可监控每个逻辑CPU的状况：</p>
<p><img src="/images/linux/top1.png" alt="TOP 多 CPU 监控"></p>
<h3 id="高亮显示当前进程"><a href="#高亮显示当前进程" class="headerlink" title="高亮显示当前进程"></a>高亮显示当前进程</h3><p>敲击键盘<strong>b</strong>（打开/关闭加亮效果），top 的视图变化如下（图来自<a href="http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="external">每天一个linux命令（44）：top命令</a>）。</p>
<p><img src="/images/linux/top2.png" alt="TOP 高亮"></p>
<h3 id="进程字段排序"><a href="#进程字段排序" class="headerlink" title="进程字段排序"></a>进程字段排序</h3><p>默认进入 top 时，各进程是按照 CPU 的占用量来排序的，敲击键盘<strong>x</strong>（打开/关闭排序列的加亮效果），top 的视图变化如下所示，会将 CPU 占用量这行高亮（图来自<a href="http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="external">每天一个linux命令（44）：top命令</a>）。</p>
<p><img src="/images/linux/top4.png" alt="TOP 按 CPU 占用量排序"></p>
<p>通过 <code>shift + &gt;</code>或<code>shift + &lt;</code>可以向右或左改变排序列，下图是按一次<code>shift + &gt;</code>的效果图，视图现在已经按照 <code>%MEM</code> 来排序（图来自<a href="http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="external">每天一个linux命令（44）：top命令</a>）。</p>
<p><img src="/images/linux/top5.png" alt="TOP 按内存排序"></p>
<h3 id="显示进程完成命令"><a href="#显示进程完成命令" class="headerlink" title="显示进程完成命令"></a>显示进程完成命令</h3><p>敲击键盘<strong>c</strong>（打开/关闭进程完成命令），top 的视图变化如下（图来自<a href="http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="external">每天一个linux命令（44）：top命令</a>）。</p>
<p><img src="/images/linux/top6.png" alt="TOP 显示完整命令"></p>
<h3 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h3><ul>
<li><code>top -p 574</code>: 显示指定的进程信息</li>
<li><code>top -d 3</code>: 设置信息更新时间</li>
<li><code>top -n 2</code>: 设置信息更新次数</li>
<li><code>top -S</code>: 以累积模式显示程序信息</li>
<li><code>top -Hp 2050</code>：显示该进程所有线程的详细信息</li>
</ul>
<h1 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h1><p>iostat 也即 I/O statistics（输入/输出统计），iostat 会对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出 CPU 使用情况。但它不能对某个进程进行深入分析，仅对系统的整体情况进行分析。</p>
<h2 id="命令常用参数-1"><a href="#命令常用参数-1" class="headerlink" title="命令常用参数"></a>命令常用参数</h2><p>1．命令格式：</p>
<ul>
<li>iostat [参数][时间][次数]<br>2．命令功能：</li>
<li>通过 iostat 方便查看 CPU、网卡、tty设备、磁盘、CD-ROM 等等设备的活动情况，负载信息。<br>3．命令参数：</li>
<li><code>-C</code>: 显示CPU使用情况</li>
<li><code>-d</code>: 显示磁盘使用情况</li>
<li><code>-k</code>: 以 KB 为单位显示</li>
<li><code>-m</code>: 以 M 为单位显示</li>
<li><code>-N</code>: 显示磁盘阵列(LVM) 信息</li>
<li><code>-n</code>: 显示NFS 使用情况</li>
<li><code>-p[磁盘]</code>: 显示磁盘和分区的情况</li>
<li><code>-t</code>: 显示终端和CPU的信息</li>
<li><code>-x</code>: 显示详细信息</li>
<li><code>-V</code>: 显示版本信息</li>
</ul>
<h2 id="显示说明-1"><a href="#显示说明-1" class="headerlink" title="显示说明"></a>显示说明</h2><p>使用 iostat 命令时，终端会显示很多很多的信息，这里介绍一下这些信息的含义。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[matt@XXX ~]$ iostat</div><div class="line">Linux 2.6.32-431.20.3.el6.mt20150216.x86_64 (XXX) 	2017年07月16日 	_x86_64_	(32 CPU)</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">          22.28    0.00   14.26    0.25    0.00   63.22</div><div class="line"></div><div class="line">Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn</div><div class="line">sda              20.65       205.78       452.71 9429827972 20744862776</div></pre></td></tr></table></figure>
<ol>
<li>CPU 属性值说明：<ul>
<li><code>%user</code>：CPU 处在用户模式下的时间百分比</li>
<li><code>%nice</code>：CPU 处在带 NICE 值的用户模式下的时间百分比</li>
<li><code>%system</code>：CPU 处在系统模式下的时间百分比</li>
<li><code>%iowait</code>：CPU 等待输入输出完成时间的百分比</li>
<li><code>%steal</code>：管理程序维护另一个虚拟处理器时，虚拟 CPU 的无意识等待时间百分比</li>
<li><code>%idle</code>：CPU 空闲时间百分比</li>
</ul>
</li>
<li>disk 属性<ul>
<li><code>tps</code>：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。</li>
<li><code>kB_read/s</code>：每秒从设备（drive expressed）读取的数据量；</li>
<li><code>kB_wrtn/s</code>：每秒向设备（drive expressed）写入的数据量；</li>
<li><code>kB_read</code>：读取的总数据量；</li>
<li><code>kB_wrtn</code>：写入的总数量数据量，这些单位都为Kilobytes。</li>
</ul>
</li>
</ol>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[matt@XXX ~]$ iostat -xm 3</div><div class="line">Linux 2.6.32-431.20.3.el6.mt20150216.x86_64 (XXX) 	2017年07月16日 	_x86_64_	(32 CPU)</div><div class="line"></div><div class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</div><div class="line">          22.28    0.00   14.26    0.25    0.00   63.21</div><div class="line"></div><div class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await  svctm  %util</div><div class="line">sda               0.50    42.34    6.47   14.18     0.10     0.22    31.90     0.02    1.20   0.44   0.90</div></pre></td></tr></table></figure>
<p>这里 disk 属性与上面的不太相同</p>
<ul>
<li><code>rrqm/s</code>: 每秒进行 merge 的读操作数目。即 rmerge/s</li>
<li><code>wrqm/s</code>: 每秒进行 merge 的写操作数目。即 wmerge/s</li>
<li><code>r/s</code>: 每秒完成的读 I/O 设备次数。即 rio/s</li>
<li><code>w/s</code>: 每秒完成的写 I/O 设备次数。即 wio/s</li>
<li><code>rsec/s</code>: 每秒读扇区数。即 rsect/s</li>
<li><code>wsec/s</code>: 每秒写扇区数。即 wsect/s</li>
<li><code>rkB/s</code>: 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。</li>
<li><code>wkB/s</code>: 每秒写K字节数。是 wsect/s 的一半。</li>
<li><code>avgrq-sz</code>: 平均每次设备 I/O 操作的数据大小 (扇区)。</li>
<li><code>avgqu-sz</code>: 平均 I/O 队列长度。</li>
<li><code>await</code>: 平均每次设备 I/O 操作的等待时间 (毫秒)。</li>
<li><code>svctm</code>: 平均每次设备 I/O 操作的服务时间 (毫秒)。</li>
<li><code>%util</code>: 一秒中有百分之多少的时间用于 I/O 操作，即被 IO 消耗的 CPU 百分比。</li>
</ul>
<p>其中，下面是在实践中积累的一些经验</p>
<ol>
<li>如果 <code>%iowait</code> 的值过高，表示硬盘存在 I/O 瓶颈；</li>
<li><code>%idle</code> 值高，表示 CPU 较空闲，如果 <code>%idle</code> 值高但系统响应慢时，有可能是 CPU 等待分配内存，此时应加大内存容量。<code>%idle</code> 值如果持续低于 10，那么系统的 CPU 处理能力相对较低，表明系统中最需要解决的资源是 CPU；</li>
<li>如果 <code>%util</code> 接近 100%，说明产生的I/O请求太多，I/O 系统已经满负荷，该磁盘可能存在瓶颈，其值大于 70% 时，磁盘的压力就很大了；</li>
<li>如果 <code>svctm</code> 比较接近 <code>await</code>，说明 I/O 几乎没有等待时间；如果 <code>await</code> 远大于 <code>svctm</code>，说明 I/O 队列太长，IO 响应太慢，则需要进行必要优化；</li>
<li>如果 <code>avgqu-sz</code> 比较大，也表示有当量 IO 在等待，它是 IO 调优时需要注意的地方，它是直接每次操作的数据的大小，如果次数多，但数据拿的小的话，其实 IO 也会很小；</li>
<li>await 的大小一般取决于服务时间(svctm) 以及 I/O 队列的长度和 I/O 请求的发出模式。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明 I/O 队列太长，应用得到的响应时间变慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核 elevator 算法，优化应用，或者升级 CPU。</li>
</ol>
<h2 id="常用示例"><a href="#常用示例" class="headerlink" title="常用示例"></a>常用示例</h2><ul>
<li><code>iostat 2 3</code>: 每隔 2 秒刷新显示，且显示 3 次；</li>
<li><code>iostat -d sda</code>: 显示指定磁盘信息；</li>
<li><code>iostat -t</code>: 显示 tty 和 CPU 信息；</li>
<li><code>iostat -m</code>: 以M为单位显示所有信息；</li>
<li><code>iostat -d -k 1 1</code>: 查看TPS和吞吐量信息；</li>
<li><code>iostat -c 1 3</code>: 查看cpu状态；</li>
</ul>
<h1 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h1><p>ps —— 是 process status 的简称，它列出的是当前时刻那些进程的快照，如果想要动态的显示进程信息内容，可以使用 top 命令。</p>
<p>使用 ps 可以做以下事情：</p>
<ul>
<li>确定有哪些进程正在运行、查看运行的状态；</li>
<li>进程是否结束；</li>
<li>进程有没有僵死；</li>
<li>哪些进程占用了过多的资源等等。</li>
</ul>
<h2 id="Linux-进程的五种状态"><a href="#Linux-进程的五种状态" class="headerlink" title="Linux 进程的五种状态"></a>Linux 进程的五种状态</h2><p>在 Linux 上进程有5种状态，每种状态对应着不同的标识，如下表所示：</p>
<table>
<thead>
<tr>
<th>Linux 上状态</th>
<th>ps 的状态码</th>
</tr>
</thead>
<tbody>
<tr>
<td>运行（正在运行或在运行队列中等待）</td>
<td>R（running or on run queue）</td>
</tr>
<tr>
<td>中断（休眠中、受阻、在等待某个条件的形成和接收到信号）</td>
<td>S（sleeping）</td>
</tr>
<tr>
<td>不可中断（收到信号不唤醒和不可运行, 进程必须等待直到有中断发生）</td>
<td>D（uninterruptible sleep）</td>
</tr>
<tr>
<td>僵死（进程已终止，但进程描述符存在，直到父进程调用wait4()后才会释放）</td>
<td>Z（a defunct zombie process）</td>
</tr>
<tr>
<td>停止（进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行）</td>
<td>T（traced or stopped）</td>
</tr>
</tbody>
</table>
<h2 id="命令常用参数-2"><a href="#命令常用参数-2" class="headerlink" title="命令常用参数"></a>命令常用参数</h2><ol>
<li>命令格式<ul>
<li>ps [参数]</li>
</ul>
</li>
<li>命令参数<ul>
<li><code>a</code>：显示所有进程</li>
<li><code>-a</code>：显示同一终端下的所有程序</li>
<li><code>-A</code>：显示所有进程</li>
<li><code>c</code>：显示进程的真实名称</li>
<li><code>-N</code>：反向选择</li>
<li><code>e</code>：显示环境变量</li>
<li><code>f</code>：显示程序间的关系</li>
<li><code>-H</code>：显示树状结构</li>
<li><code>r</code>：显示当前终端的进程</li>
<li><code>T</code>：显示当前终端的所有程序</li>
<li><code>u</code>：指定用户的所有进程</li>
<li><code>-au</code>：显示较详细的资讯</li>
<li><code>-aux</code>：显示所有包含其他使用者的行程</li>
<li><code>-C&lt;命令&gt;</code>：列出指定命令的状况</li>
<li><code>--lines&lt;行数&gt;</code>：每页显示的行数</li>
<li><code>--width&lt;字符数&gt;</code>：每页显示的字符数</li>
<li><code>--help</code>：显示帮助信息</li>
<li><code>--version</code>：显示版本显示</li>
</ul>
</li>
</ol>
<h2 id="显示说明-2"><a href="#显示说明-2" class="headerlink" title="显示说明"></a>显示说明</h2><h3 id="将当前这次登入的-PID-与相关信息列示出来"><a href="#将当前这次登入的-PID-与相关信息列示出来" class="headerlink" title="将当前这次登入的 PID 与相关信息列示出来"></a>将当前这次登入的 PID 与相关信息列示出来</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[matt@XXX ~]$ ps <span class="_">-l</span></div><div class="line">F S   UID    PID   PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD</div><div class="line">0 S   10 120177 120176  0  80   0 - 27078 <span class="built_in">wait</span>   pts/0    00:00:00 bash</div><div class="line">0 R   10 137912 120177  0  80   0 - 27031 -      pts/0    00:00:00 ps</div></pre></td></tr></table></figure>
<p>上面各个参数的含义：</p>
<ul>
<li><code>F</code>: 代表这个程序的旗标 (flag)， 4 代表使用者为 super user</li>
<li><code>S</code>: 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍</li>
<li><code>UID</code>: 程序被该 UID 所拥有</li>
<li><code>PID</code>: 就是这个程序的 ID</li>
<li><code>PPID</code>: 则是其上级父程序的ID</li>
<li><code>C</code>: CPU 使用的资源百分比</li>
<li><code>PRI</code>: 这个是 Priority (优先执行序) 的缩写</li>
<li><code>NI</code>: 这个是 Nice 值</li>
<li><code>ADDR</code>: 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“</li>
<li><code>SZ</code>: 使用掉的内存大小</li>
<li><code>WCHAN</code>: 目前这个程序是否正在运作当中，若为 - 表示正在运作</li>
<li><code>TTY</code>: 登入者的终端机位置</li>
<li><code>TIME</code>: 使用掉的 CPU 时间。</li>
<li><code>CMD</code>: 所下达的指令为何</li>
</ul>
<h3 id="ps-aux"><a href="#ps-aux" class="headerlink" title="ps aux"></a>ps aux</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[matt@XXX ~]$ ps aux</div><div class="line">USER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</div><div class="line">root          1  0.5  0.8 1184632 1166268 ?     Ss    2016 4029:48 /sbin/init</div><div class="line">root          2  0.0  0.0      0     0 ?        S     2016   0:00 [kthreadd]</div><div class="line">root          3  0.0  0.0      0     0 ?        S     2016  36:36 [migration/0]</div></pre></td></tr></table></figure>
<p>上面各个参数的含义：</p>
<ul>
<li><code>USER</code>：该 process 属于那个使用者账号的</li>
<li><code>PID</code>：该 process 的号码</li>
<li><code>%CPU</code>：该 process 使用掉的 CPU 资源百分比</li>
<li><code>%MEM</code>：该 process 所占用的物理内存百分比</li>
<li><code>VSZ</code>：该 process 使用掉的虚拟内存量 (Kbytes)</li>
<li><code>RSS</code>：该 process 占用的固定的内存量 (Kbytes)</li>
<li><code>TTY</code>：该 process 是在那个终端机上面运作，若与终端机无关，则显示 <code>?</code>，若为 <code>pts/0</code> 等等的，则表示为由网络连接进主机的程序。</li>
<li><code>STAT</code>：该程序目前的状态</li>
<li><code>START</code>：该 process 被触发启动的时间</li>
<li><code>TIME</code>：该 process 实际使用 CPU 运作的时间</li>
<li><code>COMMAND</code>：该程序的实际指令</li>
</ul>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><p>ps 的常用命令主要有以下几种用法：</p>
<ul>
<li><code>ps -A</code>: 显示所有进程信息</li>
<li><code>ps -u root</code>: 显示指定用户信息</li>
<li><code>ps -ef</code>: 显示所有进程信息，连同命令行</li>
<li>与 <code>grep</code> 一起，来查看指定的进程。</li>
</ul>
<h1 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h1><p>Netstat 是一款命令行工具，可用于列出系统上所有的网络套接字连接情况，包括 tcp, udp 以及 unix 套接字。</p>
<h2 id="命令常用参数-3"><a href="#命令常用参数-3" class="headerlink" title="命令常用参数"></a>命令常用参数</h2><ul>
<li>命令格式：<br>netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][–ip]</li>
<li>命令参数：<ol>
<li>-a 或 –all 显示所有连线中的Socket。</li>
<li>-A &lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。</li>
<li>-c 或 –continuous 持续列出网络状态。</li>
<li>-C 或 –cache 显示路由器配置的快取信息。</li>
<li>-e 或 –extend 显示网络其他相关信息。</li>
<li>-F 或 –fib 显示FIB。</li>
<li>-g 或 –groups 显示多重广播功能群组组员名单。</li>
<li>-h 或 –help 在线帮助。</li>
<li>-i 或 –interfaces 显示网络界面信息表单。</li>
<li>-l 或 –listening 显示监控中的服务器的Socket。</li>
<li>-M 或 –masquerade 显示伪装的网络连线。</li>
<li>-n 或 –numeric 直接使用IP地址，而不通过域名服务器。</li>
<li>-N 或 –netlink 或 –symbolic 显示网络硬件外围设备的符号连接名称。</li>
<li>-o 或 –timers 显示计时器。</li>
<li>-p 或 –programs 显示正在使用Socket的程序识别码和程序名称。</li>
<li>-r 或 –route 显示Routing Table。</li>
<li>-s 或 –statistice 显示网络工作信息统计表。</li>
<li>-t 或 –tcp 显示TCP传输协议的连线状况。</li>
<li>-u 或 –udp 显示UDP传输协议的连线状况。</li>
<li>-v 或 –verbose 显示指令执行过程。</li>
<li>-V 或 –version 显示版本信息。</li>
<li>-w 或 –raw 显示RAW传输协议的连线状况。</li>
<li>-x 或 –unix 此参数的效果和指定”-A unix”参数相同。</li>
<li>–ip 或 –inet 此参数的效果和指定”-A inet”参数相同。</li>
</ol>
</li>
</ul>
<h2 id="显示说明-3"><a href="#显示说明-3" class="headerlink" title="显示说明"></a>显示说明</h2><p>这里看下在命令行下输入 <code>netstat</code> 显示内容的含义：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[matt@XXXX ~]$ netstat</div><div class="line">Active Internet connections (w/o servers)</div><div class="line">Proto Recv-Q Send-Q Local Address               Foreign Address             State</div><div class="line">tcp        0      0 0.0.0.0:55030 0.0.0.0:XmlIpcRegSvc ESTABLISHED</div><div class="line">tcp        0      0 localhost:18103             localhost:tr-rsrb-p2        TIME_WAIT</div><div class="line">tcp        0      0 localhost:18137             localhost:tr-rsrb-p2        TIME_WAIT</div><div class="line">tcp        0      0 0.0.0.0:5266 0.0.0.0:16011 ESTABLISHED</div><div class="line"></div><div class="line">Active UNIX domain sockets (w/o servers)</div><div class="line">Proto RefCnt Flags       Type       State         I-Node Path</div><div class="line">unix  11     [ ]         DGRAM                    72685074 /dev/<span class="built_in">log</span></div><div class="line">unix  2      [ ]         DGRAM                    8702   /var/run/portreserve/socket</div><div class="line">unix  2      [ ]         DGRAM                    7565   @/org/kernel/udev/udevd</div><div class="line">unix  2      [ ]         DGRAM                    339609275</div><div class="line">unix  2      [ ]         DGRAM                    339608880</div><div class="line">unix  3      [ ]         STREAM     CONNECTED     339608854 /var/run/nss-cache.sock</div></pre></td></tr></table></figure>
<p>netstat 的输出结果可分为两部分：</p>
<ol>
<li>是 Active Internet connections，称为有源 TCP 连接，其中 <code>Recv-Q</code> 和 <code>Send-Q</code> 指的是接收队列和发送队列，这些数字一般都应该是0，如果不是则表示软件包正在队列中堆积，这种情况只能在非常少的情况见到；</li>
<li>另一个是 Active UNIX domain sockets，称为有源 Unix 域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍。</li>
</ol>
<p>其中：</p>
<ul>
<li>Proto 显示连接使用的协议；</li>
<li>RefCnt 表示连接到本套接口上的进程号；</li>
<li>Types 显示套接口的类型；</li>
<li>State 显示套接口当前的状态；</li>
<li>Path 表示连接到套接口的其它进程使用的路径名。</li>
</ul>
<p>套接口类型：</p>
<ul>
<li>-t ：TCP</li>
<li>-u ：UDP</li>
<li>-raw ：RAW类型</li>
<li>–unix ：UNIX域类型</li>
<li>–ax25 ：AX25类型</li>
<li>–ipx ：ipx类型</li>
<li>–netrom ：netrom类型</li>
</ul>
<p>状态说明：</p>
<ul>
<li>LISTEN：侦听来自远方的TCP端口的连接请求</li>
<li>SYN-SENT：再发送连接请求后等待匹配的连接请求（如果有大量这样的状态包，检查是否中招了）</li>
<li>SYN-RECEIVED：再收到和发送一个连接请求后等待对方对连接请求的确认（如有大量此状态，估计被flood攻击了）</li>
<li>ESTABLISHED：代表一个打开的连接</li>
<li>FIN-WAIT-1：等待远程TCP连接中断请求，或先前的连接中断请求的确认</li>
<li>FIN-WAIT-2：从远程TCP等待连接中断请求</li>
<li>CLOSE-WAIT：等待从本地用户发来的连接中断请求</li>
<li>CLOSING：等待远程TCP对连接中断的确认</li>
<li>LAST-ACK：等待原来的发向远程TCP的连接中断请求的确认（不是什么好东西，此项出现，检查是否被攻击）</li>
<li>TIME-WAIT：等待足够的时间以确保远程TCP接收到连接中断请求的确认</li>
<li>CLOSED：没有任何连接状态</li>
</ul>
<h2 id="常用命令-1"><a href="#常用命令-1" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="列出所有的符合某些条件的连接"><a href="#列出所有的符合某些条件的连接" class="headerlink" title="列出所有的符合某些条件的连接"></a>列出所有的符合某些条件的连接</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 累出所有连接</span></div><div class="line">netstat <span class="_">-a</span></div><div class="line"></div><div class="line"><span class="comment"># 只列出 tcp</span></div><div class="line">netstat -at</div><div class="line"></div><div class="line"><span class="comment"># 只列出 udp</span></div><div class="line">netstat -au</div><div class="line"></div><div class="line"><span class="comment"># 只列出 tcp，加-n 禁止域名解析，之查看 ip 地址</span></div><div class="line">netstat -ant</div><div class="line"></div><div class="line"><span class="comment"># 只列出监听中的连接，-l：只列出监听的套接字</span></div><div class="line">netstat -tnl</div><div class="line"></div><div class="line"><span class="comment"># netstat 的 -c 选项持续输出信息</span></div><div class="line">netstat -ct</div><div class="line"></div><div class="line"><span class="comment"># 显示 pid</span></div><div class="line">netstat -pt</div></pre></td></tr></table></figure>
<h3 id="统计数据"><a href="#统计数据" class="headerlink" title="统计数据"></a>统计数据</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat <span class="_">-s</span></div></pre></td></tr></table></figure>
<h3 id="显示路由信息"><a href="#显示路由信息" class="headerlink" title="显示路由信息"></a>显示路由信息</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat -rn</div></pre></td></tr></table></figure>
<h3 id="显示UDP端口号的使用情况"><a href="#显示UDP端口号的使用情况" class="headerlink" title="显示UDP端口号的使用情况"></a>显示UDP端口号的使用情况</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat -apu</div></pre></td></tr></table></figure>
<h3 id="统计机器中网络连接各个状态个数"><a href="#统计机器中网络连接各个状态个数" class="headerlink" title="统计机器中网络连接各个状态个数"></a>统计机器中网络连接各个状态个数</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat <span class="_">-a</span> | awk <span class="string">'/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;'</span></div></pre></td></tr></table></figure>
<h3 id="找出运行在指定端口的进程"><a href="#找出运行在指定端口的进程" class="headerlink" title="找出运行在指定端口的进程"></a>找出运行在指定端口的进程</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">netstat -anpt | grep <span class="string">':16064'</span></div></pre></td></tr></table></figure>
<h1 id="kill"><a href="#kill" class="headerlink" title="kill"></a>kill</h1><p>kill 主要使用结束 linux 后台进程的命令，kill命令是通过向进程发送指定的信号来结束相应进程的。</p>
<h2 id="命令常用参数-4"><a href="#命令常用参数-4" class="headerlink" title="命令常用参数"></a>命令常用参数</h2><ul>
<li>命令格式<ul>
<li>kill[参数][进程号]</li>
</ul>
</li>
<li>命令功能<ul>
<li>发送指定的信号到相应进程。不指定信号时将发送 <code>SIGTERM</code>（15）终止指定进程。如果任无法终止该程序，可使用发送的信号为 <code>SIGKILL</code>(9) ，将强制结束进程，使用 ps 命令或者 jbs 命令可以查看进程号。</li>
</ul>
</li>
<li>命令参数：<ol>
<li>-l  信号，若果不加信号的编号参数，则使用 <code>-l</code> 参数会列出全部的信号名称;</li>
<li>-a  当处理当前进程时，不限制命令名和进程号的对应关系</li>
<li>-p  指定 kill 命令只打印相关进程的进程号，而不发送任何信号</li>
<li>-s  指定发送信号</li>
<li>-u  指定用户</li>
</ol>
</li>
</ul>
<p>注意：</p>
<ol>
<li>kill 命令可以带信号号码选项，也可以不带。如果没有信号，kill 命令就会发出终止信号(15)，这个信号可以被进程捕获，使得进程在退出之前可以清理并释放资源。也可以用 kill 向进程发送特定的信号。</li>
<li>kill 可以带有进程 PID 号作为参数。当用 kill 向这些进程发送信号时，必须是这些进程的主人。</li>
<li>可以向多个进程发信号或终止它们。</li>
<li>应注意，信号使进程强行终止，这常会带来一些副作用，如数据丢失或者终端无法恢复到正常状态。发送信号时必须小心，只有在万不得已时，才用 kill 信号(9)，因为进程不能首先捕获它。要撤销所有的后台作业，可以输入kill 0。</li>
</ol>
<h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="所有的信号名称"><a href="#所有的信号名称" class="headerlink" title="所有的信号名称"></a>所有的信号名称</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">[XXX@XXX ~]$ <span class="built_in">kill</span> <span class="_">-l</span></div><div class="line"> 1) SIGHUP	 2) SIGINT	 3) SIGQUIT	 4) SIGILL	 5) SIGTRAP</div><div class="line"> 6) SIGABRT	 7) SIGBUS	 8) SIGFPE	 9) SIGKILL	10) SIGUSR1</div><div class="line">11) SIGSEGV	12) SIGUSR2	13) SIGPIPE	14) SIGALRM	15) SIGTERM</div><div class="line">16) SIGSTKFLT	17) SIGCHLD	18) SIGCONT	19) SIGSTOP	20) SIGTSTP</div><div class="line">21) SIGTTIN	22) SIGTTOU	23) SIGURG	24) SIGXCPU	25) SIGXFSZ</div><div class="line">26) SIGVTALRM	27) SIGPROF	28) SIGWINCH	29) SIGIO	30) SIGPWR</div><div class="line">31) SIGSYS	34) SIGRTMIN	35) SIGRTMIN+1	36) SIGRTMIN+2	37) SIGRTMIN+3</div><div class="line">38) SIGRTMIN+4	39) SIGRTMIN+5	40) SIGRTMIN+6	41) SIGRTMIN+7	42) SIGRTMIN+8</div><div class="line">43) SIGRTMIN+9	44) SIGRTMIN+10	45) SIGRTMIN+11	46) SIGRTMIN+12	47) SIGRTMIN+13</div><div class="line">48) SIGRTMIN+14	49) SIGRTMIN+15	50) SIGRTMAX-14	51) SIGRTMAX-13	52) SIGRTMAX-12</div><div class="line">53) SIGRTMAX-11	54) SIGRTMAX-10	55) SIGRTMAX-9	56) SIGRTMAX-8	57) SIGRTMAX-7</div><div class="line">58) SIGRTMAX-6	59) SIGRTMAX-5	60) SIGRTMAX-4	61) SIGRTMAX-3	62) SIGRTMAX-2</div><div class="line">63) SIGRTMAX-1	64) SIGRTMAX</div><div class="line"></div><div class="line"><span class="comment"># 列出指定的信号的值</span></div><div class="line">[XXX@XXX ~]$ <span class="built_in">kill</span> <span class="_">-l</span> TERM</div><div class="line">15</div></pre></td></tr></table></figure>
<p>其中，只有第 <strong>9</strong> 种信号( <code>SIGKILL</code> )才可以无条件终止进程，其他信号进程都有权利忽略。    下面是常用的信号：</p>
<ul>
<li>HUP 1 终端断线</li>
<li>INT 2 中断（同 Ctrl + C）</li>
<li>QUIT 3 退出（同 Ctrl + \）</li>
<li>TERM 15 终止</li>
<li>KILL 9 强制终止</li>
<li>CONT 18 继续（与STOP相反， fg/bg命令）</li>
<li>STOP 19 暂停（同 Ctrl + Z）</li>
</ul>
<h3 id="彻底杀死进程"><a href="#彻底杀死进程" class="headerlink" title="彻底杀死进程"></a>彻底杀死进程</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">kill</span> –9 3268</div></pre></td></tr></table></figure>
<hr>
<p>参考：</p>
<ul>
<li><a href="http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html" target="_blank" rel="external">每天一个linux命令（44）：top命令</a></li>
<li><a href="http://www.cnblogs.com/peida/archive/2013/03/08/2949194.html" target="_blank" rel="external">每天一个linux命令（56）：netstat命令</a></li>
<li><a href="http://www.cnblogs.com/peida/archive/2012/12/28/2837345.html" target="_blank" rel="external">每天一个linux命令（47）：iostat命令</a></li>
<li><a href="http://www.cnblogs.com/peida/archive/2012/12/19/2824418.html" target="_blank" rel="external">每天一个linux命令（41）：ps命令</a></li>
<li><a href="http://www.cnblogs.com/peida/archive/2012/12/25/2831814.html" target="_blank" rel="external">每天一个linux命令（45）：free 命令</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章的内容，基本来自&lt;a href=&quot;http://www.cnblogs.com/peida/tag/%E6%AF%8F%E6%97%A5%E4%B8%80linux%E5%91%BD%E4%BB%A4/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;每
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="linux" scheme="http://matt33.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Producer Metadata 更新机制（二）</title>
    <link href="http://matt33.com/2017/07/08/kafka-producer-metadata/"/>
    <id>http://matt33.com/2017/07/08/kafka-producer-metadata/</id>
    <published>2017-07-08T15:51:00.000Z</published>
    <updated>2018-03-15T06:46:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中，已经介绍了 Producer 的发送模型，Producer <code>dosend()</code> 方法中的第一步，就是获取相关的 topic 的 metadata，但在上篇中并没有深入展开，因为这部分的内容比较多，所以本文单独一篇文章进行介绍，本文主要来讲述以下三个问题：</p>
<ol>
<li>metadata 内容是什么；</li>
<li>Producer 更新 metadata 的流程；</li>
<li>Producer 在什么情况下会去更新 metadata；</li>
</ol>
<h2 id="Metadata-内容"><a href="#Metadata-内容" class="headerlink" title="Metadata 内容"></a>Metadata 内容</h2><p>Metadata 信息的内容可以通过源码看明白：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 这个类被 client 线程和后台 sender 所共享,它只保存了所有 topic 的部分数据,当我们请求一个它上面没有的 topic meta 时,它会通过发送 metadata update 来更新 meta 信息,</span></div><div class="line"><span class="comment">// 如果 topic meta 过期策略是允许的,那么任何 topic 过期的话都会被从集合中移除,</span></div><div class="line"><span class="comment">// 但是 consumer 是不允许 topic 过期的因为它明确地知道它需要管理哪些 topic</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Metadata</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = LoggerFactory.getLogger(Metadata.class);</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> TOPIC_EXPIRY_MS = <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000</span>;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> TOPIC_EXPIRY_NEEDS_UPDATE = -<span class="number">1L</span>;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> refreshBackoffMs; <span class="comment">// metadata 更新失败时,为避免频繁更新 meta,最小的间隔时间,默认 100ms</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> metadataExpireMs; <span class="comment">// metadata 的过期时间, 默认 60,000ms</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> version; <span class="comment">// 每更新成功1次，version自增1,主要是用于判断 metadata 是否更新</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshMs; <span class="comment">// 最近一次更新时的时间（包含更新失败的情况）</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastSuccessfulRefreshMs; <span class="comment">// 最近一次成功更新的时间（如果每次都成功的话，与前面的值相等, 否则，lastSuccessulRefreshMs &lt; lastRefreshMs)</span></div><div class="line">    <span class="keyword">private</span> Cluster cluster; <span class="comment">// 集群中一些 topic 的信息</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> needUpdate; <span class="comment">// 是都需要更新 metadata</span></div><div class="line">    <span class="comment">/* Topics with expiry time */</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, Long&gt; topics; <span class="comment">// topic 与其过期时间的对应关系</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Listener&gt; listeners; <span class="comment">// 事件监控者</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ClusterResourceListeners clusterResourceListeners; <span class="comment">//当接收到 metadata 更新时, ClusterResourceListeners的列表</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> needMetadataForAllTopics; <span class="comment">// 是否强制更新所有的 metadata</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> topicExpiryEnabled; <span class="comment">// 默认为 true, Producer 会定时移除过期的 topic,consumer 则不会移除</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 topic 的详细信息（leader 所在节点、replica 所在节点、isr 列表）都是在 <code>Cluster</code> 实例中保存的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 并不是一个全集,metadata的主要组成部分</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Cluster</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">// 从命名直接就看出了各个变量的用途</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> isBootstrapConfigured;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Node&gt; nodes; <span class="comment">// node 列表</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Set&lt;String&gt; unauthorizedTopics; <span class="comment">// 未认证的 topic 列表</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Set&lt;String&gt; internalTopics; <span class="comment">// 内置的 topic 列表</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;TopicPartition, PartitionInfo&gt; partitionsByTopicPartition; <span class="comment">// partition 的详细信息</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, List&lt;PartitionInfo&gt;&gt; partitionsByTopic; <span class="comment">// topic 与 partition 的对应关系</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, List&lt;PartitionInfo&gt;&gt; availablePartitionsByTopic; <span class="comment">//  可用（leader 不为 null）的 topic 与 partition 的对应关系</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, List&lt;PartitionInfo&gt;&gt; partitionsByNode; <span class="comment">// node 与 partition 的对应关系</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, Node&gt; nodesById; <span class="comment">// node 与 id 的对应关系</span></div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ClusterResource clusterResource;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// org.apache.kafka.common.PartitionInfo</span></div><div class="line"><span class="comment">// topic-partition: 包含 topic、partition、leader、replicas、isr</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionInfo</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node leader;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node[] replicas;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node[] inSyncReplicas;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>Cluster</code> 实例主要是保存：</p>
<ol>
<li>broker.id 与 <code>node</code> 的对应关系；</li>
<li>topic 与 partition （<code>PartitionInfo</code>）的对应关系；</li>
<li><code>node</code> 与 partition （<code>PartitionInfo</code>）的对应关系。</li>
</ol>
<h2 id="Producer-的-Metadata-更新流程"><a href="#Producer-的-Metadata-更新流程" class="headerlink" title="Producer 的 Metadata 更新流程"></a>Producer 的 Metadata 更新流程</h2><p>Producer 在调用 <code>dosend()</code> 方法时，第一步就是通过 <code>waitOnMetadata</code> 方法获取该 topic 的 metadata 信息.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 等待 metadata 的更新</span></div><div class="line"><span class="function"><span class="keyword">private</span> ClusterAndWaitTime <span class="title">waitOnMetadata</span><span class="params">(String topic, Integer partition, <span class="keyword">long</span> maxWaitMs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">    metadata.add(topic);<span class="comment">// 在 metadata 中添加 topic 后,如果 metadata 中没有这个 topic 的 meta，那么 metadata 的更新标志设置为了 true</span></div><div class="line">    Cluster cluster = metadata.fetch();</div><div class="line">    Integer partitionsCount = cluster.partitionCountForTopic(topic);<span class="comment">// 如果 topic 已经存在 meta 中,则返回该 topic 的 partition 数,否则返回 null</span></div><div class="line"></div><div class="line">    <span class="comment">// 当前 metadata 中如果已经有这个 topic 的 meta 的话,就直接返回</span></div><div class="line">    <span class="keyword">if</span> (partitionsCount != <span class="keyword">null</span> &amp;&amp; (partition == <span class="keyword">null</span> || partition &lt; partitionsCount))</div><div class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ClusterAndWaitTime(cluster, <span class="number">0</span>);</div><div class="line"></div><div class="line">    <span class="keyword">long</span> begin = time.milliseconds();</div><div class="line">    <span class="keyword">long</span> remainingWaitMs = maxWaitMs;</div><div class="line">    <span class="keyword">long</span> elapsed;</div><div class="line"></div><div class="line">    <span class="comment">// 发送 metadata 请求,直到获取了这个 topic 的 metadata 或者请求超时</span></div><div class="line">    <span class="keyword">do</span> &#123;</div><div class="line">        log.trace(<span class="string">"Requesting metadata update for topic &#123;&#125;."</span>, topic);</div><div class="line">        <span class="keyword">int</span> version = metadata.requestUpdate();<span class="comment">// 返回当前版本号,初始值为0,每次更新时会自增,并将 needUpdate 设置为 true</span></div><div class="line">        sender.wakeup();<span class="comment">// 唤起 sender，发送 metadata 请求</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            metadata.awaitUpdate(version, remainingWaitMs);<span class="comment">// 等待 metadata 的更新</span></div><div class="line">        &#125; <span class="keyword">catch</span> (TimeoutException ex) &#123;</div><div class="line">            <span class="comment">// Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs</span></div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"Failed to update metadata after "</span> + maxWaitMs + <span class="string">" ms."</span>);</div><div class="line">        &#125;</div><div class="line">        cluster = metadata.fetch();</div><div class="line">        elapsed = time.milliseconds() - begin;</div><div class="line">        <span class="keyword">if</span> (elapsed &gt;= maxWaitMs)</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"Failed to update metadata after "</span> + maxWaitMs + <span class="string">" ms."</span>);<span class="comment">// 超时</span></div><div class="line">        <span class="keyword">if</span> (cluster.unauthorizedTopics().contains(topic))<span class="comment">// 认证失败，对当前 topic 没有 Write 权限</span></div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TopicAuthorizationException(topic);</div><div class="line">        remainingWaitMs = maxWaitMs - elapsed;</div><div class="line">        partitionsCount = cluster.partitionCountForTopic(topic);</div><div class="line">    &#125; <span class="keyword">while</span> (partitionsCount == <span class="keyword">null</span>);<span class="comment">// 不停循环,直到 partitionsCount 不为 null（即直到 metadata 中已经包含了这个 topic 的相关信息）</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> (partition != <span class="keyword">null</span> &amp;&amp; partition &gt;= partitionsCount) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(</div><div class="line">                String.format(<span class="string">"Invalid partition given with record: %d is not in the range [0...%d)."</span>, partition, partitionsCount));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ClusterAndWaitTime(cluster, elapsed);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果 metadata 中不存在这个 topic 的 metadata，那么就请求更新 metadata，如果 metadata 没有更新的话，方法就一直处在 <code>do ... while</code> 的循环之中，在循环之中，主要做以下操作：</p>
<ol>
<li><code>metadata.requestUpdate()</code> 将 metadata 的 <code>needUpdate</code> 变量设置为 true（强制更新），并返回当前的版本号（version），通过版本号来判断 metadata 是否完成更新；</li>
<li><code>sender.wakeup()</code> 唤醒 sender 线程，sender 线程又会去唤醒 <code>NetworkClient</code> 线程，<code>NetworkClient</code> 线程进行一些实际的操作（后面详细介绍）；</li>
<li><code>metadata.awaitUpdate(version, remainingWaitMs)</code> 等待 metadata 的更新。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 更新 metadata 信息（根据当前 version 值来判断）</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">awaitUpdate</span><span class="params">(<span class="keyword">final</span> <span class="keyword">int</span> lastVersion, <span class="keyword">final</span> <span class="keyword">long</span> maxWaitMs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">    <span class="keyword">if</span> (maxWaitMs &lt; <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Max time to wait for metadata updates should not be &lt; 0 milli seconds"</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">long</span> begin = System.currentTimeMillis();</div><div class="line">    <span class="keyword">long</span> remainingWaitMs = maxWaitMs;</div><div class="line">    <span class="keyword">while</span> (<span class="keyword">this</span>.version &lt;= lastVersion) &#123;<span class="comment">// 不断循环,直到 metadata 更新成功,version 自增</span></div><div class="line">        <span class="keyword">if</span> (remainingWaitMs != <span class="number">0</span>)</div><div class="line">            wait(remainingWaitMs);<span class="comment">// 阻塞线程，等待 metadata 的更新</span></div><div class="line">        <span class="keyword">long</span> elapsed = System.currentTimeMillis() - begin;</div><div class="line">        <span class="keyword">if</span> (elapsed &gt;= maxWaitMs)<span class="comment">// timeout</span></div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"Failed to update metadata after "</span> + maxWaitMs + <span class="string">" ms."</span>);</div><div class="line">        remainingWaitMs = maxWaitMs - elapsed;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>Metadata.awaitUpdate()</code> 方法中，线程会阻塞在 <code>while</code> 循环中，直到 metadata 更新成功或者 timeout。</p>
<p>从前面可以看出，此时 Producer 线程会阻塞在两个 <code>while</code> 循环中，直到 metadata 信息更新，那么 metadata 是如何更新的呢？如果有印象的话，前面应该已经介绍过了，主要是通过 <code>sender.wakeup()</code> 来唤醒 sender 线程，间接唤醒 NetworkClient 线程，NetworkClient 线程来负责发送 Metadata 请求，并处理 Server 端的响应。</p>
<p>在 <a href="http://matt33.com/2017/06/25/kafka-producer-send-module/">Kafka 源码分析之 Producer 发送模型（一）</a> 中介绍 Producer 发送模型时，在第五步 <code>sender</code> 线程会调用 <code>NetworkClient.poll()</code> 方法进行实际的操作，其源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> List&lt;ClientResponse&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout, <span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">        <span class="keyword">long</span> metadataTimeout = metadataUpdater.maybeUpdate(now);<span class="comment">// 判断是否需要更新 meta,如果需要就更新（请求更新 metadata 的地方）</span></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">this</span>.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            log.error(<span class="string">"Unexpected error during I/O"</span>, e);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// process completed actions</span></div><div class="line">        <span class="keyword">long</span> updatedNow = <span class="keyword">this</span>.time.milliseconds();</div><div class="line">        List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        handleAbortedSends(responses);</div><div class="line">        handleCompletedSends(responses, updatedNow);<span class="comment">// 通过 selector 中获取 Server 端的 response</span></div><div class="line">        handleCompletedReceives(responses, updatedNow);<span class="comment">// 在返回的 handler 中，会处理 metadata 的更新</span></div><div class="line">        handleDisconnections(responses, updatedNow);</div><div class="line">        handleConnections();</div><div class="line">        handleInitiateApiVersionRequests(updatedNow);</div><div class="line">        handleTimedOutRequests(responses, updatedNow);</div><div class="line"></div><div class="line">        <span class="comment">// invoke callbacks</span></div><div class="line">        <span class="keyword">for</span> (ClientResponse response : responses) &#123;</div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">                response.onComplete();</div><div class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">                log.error(<span class="string">"Uncaught error in request completion:"</span>, e);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> responses;</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，主要会以下操作：</p>
<ul>
<li><code>metadataUpdater.maybeUpdate(now)</code>：判断是否需要更新 Metadata，如果需要更新的话，先与 Broker 建立连接，然后发送更新 metadata 的请求；</li>
<li>处理 Server 端的一些响应，这里主要讨论的是 <code>handleCompletedReceives(responses, updatedNow)</code> 方法，它会处理 Server 端返回的 Metadata 结果。</li>
</ul>
<p>先看一下 <code>metadataUpdater.maybeUpdate()</code> 的具体实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">        <span class="comment">// should we update our metadata?</span></div><div class="line">        <span class="comment">// metadata 是否应该更新</span></div><div class="line">        <span class="keyword">long</span> timeToNextMetadataUpdate = metadata.timeToNextUpdate(now);<span class="comment">// metadata 下次更新的时间（需要判断是强制更新还是 metadata 过期更新,前者是立马更新,后者是计算 metadata 的过期时间）</span></div><div class="line">        <span class="comment">// 如果一条 metadata 的 fetch 请求还未从 server 收到恢复,那么时间设置为 waitForMetadataFetch（默认30s）</span></div><div class="line">        <span class="keyword">long</span> waitForMetadataFetch = <span class="keyword">this</span>.metadataFetchInProgress ? requestTimeoutMs : <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="keyword">long</span> metadataTimeout = Math.max(timeToNextMetadataUpdate, waitForMetadataFetch);</div><div class="line">        <span class="keyword">if</span> (metadataTimeout &gt; <span class="number">0</span>) &#123;<span class="comment">// 时间未到时,直接返回下次应该更新的时间</span></div><div class="line">            <span class="keyword">return</span> metadataTimeout;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        Node node = leastLoadedNode(now);<span class="comment">// 选择一个连接数最小的节点</span></div><div class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</div><div class="line">            log.debug(<span class="string">"Give up sending metadata request since no node is available"</span>);</div><div class="line">            <span class="keyword">return</span> reconnectBackoffMs;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> maybeUpdate(now, node); <span class="comment">// 可以发送 metadata 请求的话,就发送 metadata 请求</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * Add a metadata request to the list of sends if we can make one</div><div class="line">     */</div><div class="line">    <span class="comment">// 判断是否可以发送请求,可以的话将 metadata 请求加入到发送列表中</span></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now, Node node)</span> </span>&#123;</div><div class="line">        String nodeConnectionId = node.idString();</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (canSendRequest(nodeConnectionId)) &#123;<span class="comment">// 通道已经 ready 并且支持发送更多的请求</span></div><div class="line">            <span class="keyword">this</span>.metadataFetchInProgress = <span class="keyword">true</span>; <span class="comment">// 准备开始发送数据,将 metadataFetchInProgress 置为 true</span></div><div class="line">            MetadataRequest.Builder metadataRequest; <span class="comment">// 创建 metadata 请求</span></div><div class="line">            <span class="keyword">if</span> (metadata.needMetadataForAllTopics())<span class="comment">// 强制更新所有 topic 的 metadata（虽然默认不会更新所有 topic 的 metadata 信息，但是每个 Broker 会保存所有 topic 的 meta 信息）</span></div><div class="line">                metadataRequest = MetadataRequest.Builder.allTopics();</div><div class="line">            <span class="keyword">else</span> <span class="comment">// 只更新 metadata 中的 topics 列表（列表中的 topics 由 metadata.add() 得到）</span></div><div class="line">                metadataRequest = <span class="keyword">new</span> MetadataRequest.Builder(<span class="keyword">new</span> ArrayList&lt;&gt;(metadata.topics()));</div><div class="line"></div><div class="line"></div><div class="line">            log.debug(<span class="string">"Sending metadata request &#123;&#125; to node &#123;&#125;"</span>, metadataRequest, node.id());</div><div class="line">            sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);/ 发送 metadata 请求</div><div class="line">            <span class="keyword">return</span> requestTimeoutMs;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// If there's any connection establishment underway, wait until it completes. This prevents</span></div><div class="line">        <span class="comment">// the client from unnecessarily connecting to additional nodes while a previous connection</span></div><div class="line">        <span class="comment">// attempt has not been completed.</span></div><div class="line">        <span class="keyword">if</span> (isAnyNodeConnecting()) &#123;<span class="comment">// 如果 client 正在与任何一个 node 的连接状态是 connecting,那么就进行等待</span></div><div class="line">            <span class="comment">// Strictly the timeout we should return here is "connect timeout", but as we don't</span></div><div class="line">            <span class="comment">// have such application level configuration, using reconnect backoff instead.</span></div><div class="line">            <span class="keyword">return</span> reconnectBackoffMs;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (connectionStates.canConnect(nodeConnectionId, now)) &#123;<span class="comment">// 如果没有连接这个 node,那就初始化连接</span></div><div class="line">            <span class="comment">// we don't have a connection to this node right now, make one</span></div><div class="line">            log.debug(<span class="string">"Initialize connection to node &#123;&#125; for sending metadata request"</span>, node.id());</div><div class="line">            initiateConnect(node, now);<span class="comment">// 初始化连接</span></div><div class="line">            <span class="keyword">return</span> reconnectBackoffMs;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> Long.MAX_VALUE;</div><div class="line">    &#125;</div><div class="line"></div><div class="line"> <span class="comment">// 发送 Metadata 请求   </span></div><div class="line"> <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendInternalMetadataRequest</span><span class="params">(MetadataRequest.Builder builder,</span></span></div><div class="line">                                         String nodeConnectionId, <span class="keyword">long</span> now) &#123;</div><div class="line">    ClientRequest clientRequest = newClientRequest(nodeConnectionId, builder, now, <span class="keyword">true</span>);<span class="comment">// 创建 metadata 请求</span></div><div class="line">    doSend(clientRequest, <span class="keyword">true</span>, now);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>所以，每次 Producer 请求更新 metadata 时，会有以下几种情况：</p>
<ol>
<li>如果 node 可以发送请求，则直接发送请求；</li>
<li>如果该 node 正在建立连接，则直接返回；</li>
<li>如果该 node 还没建立连接，则向 broker 初始化链接。</li>
</ol>
<p>而 KafkaProducer 线程之前是一直阻塞在两个 <code>while</code> 循环中，直到 metadata 更新</p>
<ol>
<li>sender 线程第一次调用 <code>poll()</code> 方法时，初始化与 node 的连接；</li>
<li>sender 线程第二次调用 <code>poll()</code> 方法时，发送 <code>Metadata</code> 请求；</li>
<li>sender 线程第三次调用 <code>poll()</code> 方法时，获取 <code>metadataResponse</code>，并更新 metadata。</li>
</ol>
<p>经过上述 sender 线程三次调用 <code>poll()</code>方法，所请求的 metadata 信息才会得到更新，此时 Producer 线程也不会再阻塞，开始发送消息。</p>
<p><code>NetworkClient</code> 接收到 Server 端对 Metadata 请求的响应后，更新 Metadata 信息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 处理任何已经完成的接收响应</span></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleCompletedReceives</span><span class="params">(List&lt;ClientResponse&gt; responses, <span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">        <span class="keyword">for</span> (NetworkReceive receive : <span class="keyword">this</span>.selector.completedReceives()) &#123;</div><div class="line">            String source = receive.source();</div><div class="line">            InFlightRequest req = inFlightRequests.completeNext(source);</div><div class="line">            AbstractResponse body = parseResponse(receive.payload(), req.header);</div><div class="line">            log.trace(<span class="string">"Completed receive from node &#123;&#125;, for key &#123;&#125;, received &#123;&#125;"</span>, req.destination, req.header.apiKey(), body);</div><div class="line">            <span class="keyword">if</span> (req.isInternalRequest &amp;&amp; body <span class="keyword">instanceof</span> MetadataResponse)<span class="comment">// 如果是 meta 响应</span></div><div class="line">                metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);</div><div class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (req.isInternalRequest &amp;&amp; body <span class="keyword">instanceof</span> ApiVersionsResponse)</div><div class="line">                handleApiVersionsResponse(responses, req, now, (ApiVersionsResponse) body); <span class="comment">// 如果是其他响应</span></div><div class="line">            <span class="keyword">else</span></div><div class="line">                responses.add(req.completed(body, now));</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">        <span class="comment">// 处理 Server 端对 Metadata 请求处理后的 response</span></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleCompletedMetadataResponse</span><span class="params">(RequestHeader requestHeader, <span class="keyword">long</span> now, MetadataResponse response)</span> </span>&#123;</div><div class="line">            <span class="keyword">this</span>.metadataFetchInProgress = <span class="keyword">false</span>;</div><div class="line">            Cluster cluster = response.cluster();</div><div class="line">            <span class="comment">// check if any topics metadata failed to get updated</span></div><div class="line">            Map&lt;String, Errors&gt; errors = response.errors();</div><div class="line">            <span class="keyword">if</span> (!errors.isEmpty())</div><div class="line">                log.warn(<span class="string">"Error while fetching metadata with correlation id &#123;&#125; : &#123;&#125;"</span>, requestHeader.correlationId(), errors);</div><div class="line"></div><div class="line">            <span class="comment">// don't update the cluster if there are no valid nodes...the topic we want may still be in the process of being</span></div><div class="line">            <span class="comment">// created which means we will get errors and no nodes until it exists</span></div><div class="line">            <span class="keyword">if</span> (cluster.nodes().size() &gt; <span class="number">0</span>) &#123;</div><div class="line">                <span class="keyword">this</span>.metadata.update(cluster, now);<span class="comment">// 更新 meta 信息</span></div><div class="line">            &#125; <span class="keyword">else</span> &#123;<span class="comment">// 如果 metadata 中 node 信息无效,则不更新 metadata 信息</span></div><div class="line">                log.trace(<span class="string">"Ignoring empty metadata response with correlation id &#123;&#125;."</span>, requestHeader.correlationId());</div><div class="line">                <span class="keyword">this</span>.metadata.failedUpdate(now);</div><div class="line">            &#125;</div><div class="line">        &#125;</div></pre></td></tr></table></figure>
<h2 id="Producer-Metadata-的更新策略"><a href="#Producer-Metadata-的更新策略" class="headerlink" title="Producer Metadata 的更新策略"></a>Producer Metadata 的更新策略</h2><p>Metadata 会在下面两种情况下进行更新</p>
<ol>
<li>KafkaProducer 第一次发送消息时强制更新，其他时间周期性更新，它会通过 Metadata 的 <code>lastRefreshMs</code>, <code>lastSuccessfulRefreshMs</code> 这2个字段来实现；</li>
<li>强制更新： 调用 <code>Metadata.requestUpdate()</code> 将 <code>needUpdate</code> 置成了 true 来强制更新。</li>
</ol>
<p>在 NetworkClient 的 <code>poll()</code> 方法调用时，就会去检查这两种更新机制，只要达到其中一种，就行触发更新操作。</p>
<p>Metadata 的强制更新会在以下几种情况下进行：</p>
<ol>
<li><code>initConnect</code> 方法调用时，初始化连接；</li>
<li><code>poll()</code> 方法中对 <code>handleDisconnections()</code> 方法调用来处理连接断开的情况，这时会触发强制更新；</li>
<li><code>poll()</code> 方法中对 <code>handleTimedOutRequests()</code> 来处理请求超时时；</li>
<li>发送消息时，如果无法找到 partition 的 leader；</li>
<li>处理 Producer 响应（<code>handleProduceResponse</code>），如果返回关于 Metadata 过期的异常，比如：没有 topic-partition 的相关 meta 或者 client 没有权限获取其 metadata。</li>
</ol>
<p>强制更新主要是用于处理各种异常情况。</p>
<p>参考文档：</p>
<ul>
<li><a href="http://blog.csdn.net/chunlongyu/article/details/52622422" target="_blank" rel="external">Kafka源码深度解析－序列2 －Producer －Metadata的数据结构与读取、更新策略</a>；</li>
<li><a href="http://luodw.cc/2017/05/02/kafka02/" target="_blank" rel="external">kafka源码分析之Producer</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章中，已经介绍了 Producer 的发送模型，Producer &lt;code&gt;dosend()&lt;/code&gt; 方法中的第一步，就是获取相关的 topic 的 metadata，但在上篇中并没有深入展开，因为这部分的内容比较多，所以本文单独一篇文章进行介绍，本文主要
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>买房感想</title>
    <link href="http://matt33.com/2017/07/02/buy-little-room/"/>
    <id>http://matt33.com/2017/07/02/buy-little-room/</id>
    <published>2017-07-02T13:38:38.000Z</published>
    <updated>2017-08-30T12:18:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>落户+买房的事情终于搞定得差不多了（就差贷款合同的签署），六月份这个月已经往来杭州3趟，从四月底开始看房到现在，这一路真是五味杂粮、感慨颇多，亲身地感觉到生活之不易，无忧无虑的生活从离开大学的那一刻开始就已经结束，读研这两年半看似浑浑噩噩，但早已没有大学时的那种轻松惬意，或许这就是成长。</p>
<h2 id="去库存历史进程"><a href="#去库存历史进程" class="headerlink" title="去库存历史进程"></a>去库存历史进程</h2><p>从四月底看房到7.1签合同，杭州房价整体至少涨了20%（间接地上涨，刚需买房的成本涨得更多），简单举几个例子，都是个人的亲身经历：</p>
<ul>
<li>万科杭宸，成品装修，同学三月购入，当时三成首付、单价 2w 出头，五月我去看房时，销售态度也很好，让等认筹通知，等到六月份时，杭宸通知只接受全款或七成首付（最后情况是只接受全款）；</li>
<li>西溪璞园，四月底去看，当时有特价房，125 平大概 245w（均价1.9w，位置还好），6月初通知有一套小户型房源，顶楼，单价2.2w，遂吧；</li>
<li>东海闲湖城，三月开盘三幢楼，由于政府限价，均价只有1.2w，结果很明显，两三天就被抢完（当时市场还并不是太热），后面由于备案价格上不去，开发商目前选择直接封盘（后面三幢据说下半年开盘），车位从5月开始由 13w 涨到 16w；</li>
<li>万家之星，六月初去看盘，销售不怎么搭理，直接明说，现在要买的话只能去找关系，让去找滨江或旭辉的内部人员；</li>
<li>雅居乐国际花园，六月初第一次看，开盘当天小户型全部售罄，只剩 128 平，不过户型很好，只是没有看上这个小区，感觉小区人太多、楼太高（32层，4000户），后来六月再次开盘时，小户型依然是难抢；</li>
<li>….</li>
</ul>
<p>上面都是自己去踩过的盘，也都是真实的信息，我们可以再看一下去年7月份之前一些楼盘的价格，比如杭宸1.8w（现在2.3w）、桃源小镇 8k（现在1.8w）、科技城 1.5w（现在中心地带已经 3w+）、蒋村二手 2w 多点（现在 3.7w+）…，如果按 2016 年初的价格现在基本平均已经翻了 2-3 倍，当然并不是仅仅是杭州，全国一线城市和强二线城市都一样，郑州东区均价都到 1.5w、北京东西城均价 13w+ 朝阳海淀均价 9w+（目前调控虽说降了，但没什么卵用）、天津杭州均价到了 2w（之前是 1w），浩浩荡荡，当一线二线城市开始猛涨时，我们才意识到原来特么这就是<strong>去库存</strong>，通过房子把政府的债务轻轻松松转移到了普通百姓的身上，我们拿着纸面上的财富开始为银行打工。当然如果你有机会去为银行打工，那证明你还算是幸运的，因为你抓住机会已经买了房，更多的人根本就没抓住机会（就像楼主我），有人在这一历史进程中 all in 杭州，在短短一年的时间赚了几百万。</p>
<p>能看到机会的人毕竟是少数，当所有人都意识到机会的时候，市场已经变得疯狂，而且各种限购政策开始实施，这时候入场的人能赚到大钱的已经很少，很多可能已经开始站岗。去库存这一历史进程，在共和国、乃至中华民族史上都是可以载入史册的大事件（好坏无法评论，很多东西只有过些年才能看清楚其意义），它让一线城市有房的人身价迅速加了 500w+（多套房的就增加了千万），强二线城市有房的人身价涨了 100w+，这一切短短只需要一年，在这之前恐怕很少人相信现实是这么疯狂。而对于那些房价没有翻倍的城市，从此消彼长的角度看相当于其财富洗劫了，被一线二线城市洗劫，而且即使三四线城市的房价翻一倍，也免不了被洗劫的命运，北京翻一倍就是几百万上千万，而三四线只是几十万，这没有任何可比性。去库存造就了多少百万富翁，实在想不出来建国以来除了改革开放还有什么能造就这么多富翁。</p>
<p>任何事情都有好坏两方面，对于政府来说，可以实现两个效果：一、地方债务转移；二、一二线城市开始驱逐低收入或者买不起房的人，表面上看对于政府都是利好的，深层次分析，就不好说了，长征五号连续发射失败对于政府应该是一个警钟。从个人的角度来说，买到房的人（上车的人）个人资产迅速翻倍，没有买到房的人离买房越来越遥远。而买到房抓住机会的永远是少数，大部分人是这次去库存的受害者，去库存前可能马上就攒够了首付的钱，但一年之后却发现首付涨得远远超过了攒钱的速度，收入高点的还好说，可以去二线城市占坑，但另外一些人，他们已经被遗忘了。</p>
<p>现实就是这么残酷，没有人会去同情，我们只能去依靠自己，努力工作，为自己也为家人，其他的我们又能做些什么呢？</p>
<p>亲身经历浩浩荡荡的去库存历史进程，也算是大开眼界，这种事情在全世界史上都不是很常见，我们再次用了极短的时间完成发达国家几十年甚至上百年的进程，不出意外，未来我们还会经历更多可以载入史册的事件，但此时我们希望的是自己能够抓住机会，历史告诉我们，大事件背后隐藏的都是巨大的机会（过去的已经成为历史，历史不会重复，但我们可以从历史中去学习经验）。就像雄安一出来，那么多人跑过去买房，但是雄安早已封盘，而且雄安的商品房规划可能会像新加坡一样，政府全面提供廉租房，商品房在雄安将会退出历史舞台，这并不是那些炒房客的机会，他们只是得了去库存的后遗症。</p>
<h2 id="一线赶人，二线抢人"><a href="#一线赶人，二线抢人" class="headerlink" title="一线赶人，二线抢人"></a>一线赶人，二线抢人</h2><p>小时候，我们一直被灌输一种概念：人口是负担，如果我们人少一些，我们也可以过上美国人一样的生活。但是，现在我们却发现，人口是国家财富，看看那些鬼城，没有人的话规划得再好，没有任何用。北上深，这三个一线城市，他们在过去三十年的快速发展，既有政府集全国之力去发展（深圳除外，主要指北京和上海），可以修建的东方明珠、鸟巢，花得是全国公民的钱，但是却修在了上海和北京，这就是政策利好，然后到现在，以北京为例，出了以下这些政策：</p>
<ul>
<li>北京落户名额每年都在缩减；</li>
<li>北京控地，近两年新增住宅用地达到历史最低水平；</li>
<li>北京高校对非京籍博士总额做限制；</li>
<li>疏解低端产业，未来北京人口减少到 X；</li>
<li>…</li>
</ul>
<p>这都在向我们传递一个信号：北京不是想呆就能呆的地方，曾经<code>北京欢迎你</code>的时代已经过去，现在是一个新的时代。同时，也可以看到政府规划了各种城市群，来缓解一线的压力，中国 2016 的城市化率是 57%，经验告诉我们，一旦达到这个值之后，后面的城市化率将会以更快速度的增长，在之前，大部分高校毕业生都去了一线城市，做起了北漂、上漂、深漂，然而这几个大城市的容量是有限的，早些年来的人已经在一线扎根，现在留给我们的就会已经很少了。现在政府开始鼓励毕业生去二线城市发展，比较牛的二线城市，像杭州、南京、武汉以及一线城市深圳，都开始搞出了各种吸引人才政策，希望能把相关专业的高校毕业生（所谓的才人）吸引到本地，杭州和深圳本地高校少，只能从外地吸引，武汉和南京本地高校多，想的是怎么把自己培养的毕业生留下来，不得不说这几个城市哪个能吸引到更多高校毕业生，哪个城市未来的潜力就会更大一些。对于高校毕业生来说，大家主要关心是两方面：一个就业机会、一个是房价，一线城市的房价已经远远超出普通人的承受能力，甚至可以说，年薪100w（税前）在北上深都很有压力。所以对于二线城市来说，拥有较好就业机会以及较低房价的城市，未来肯定会吸引到更多的人才，只有把人吸引来了，这个城市未来才能飞速发展，中国确实需要培养更多的大城市，仅仅靠北上深三个城市，是带不起整个中国的。</p>
<h2 id="生活之不易，最难的永远在后面"><a href="#生活之不易，最难的永远在后面" class="headerlink" title="生活之不易，最难的永远在后面"></a>生活之不易，最难的永远在后面</h2><p>在国内，这几年讨论最多的就是房价，记得高中时，房价就已经是家常便饭，尤其是《蜗居》这部电视剧热播，全民开始讨论房价，但从现在来看，那时的房价真叫一个便宜。房子是我们的生活必需品，更何况房子跟户口、学校都是绑定在一起的，这就让房子成为家庭的必需品，结婚要婚房、小孩上学要学区房。。。一套房贷款之后，基本上要还30年，未来就要为银行工作30年，如果不买房，自己就是在为房东打工，甚至随时都会面对房租的上涨，所有的一切都逼着你去买房。</p>
<p>昨天正式签了合同，刚需的问题总算解决了，买的位置不是很好，未来的前景很难说，但是不买又没办法，今天还能买这，过半年可能就只能买更远的地方了。买完之后，跟一个朋友聊天，突然觉得压力更大了，贷款的压力还可以接受。但未来一旦有了孩子，孩子的支出、上学等等，这些都不是小数目，真的怀疑自己能不能承受住这么大支出，现在真的很理解那些选择不要孩子的一族。经济越发达的地方，生育率就越低，尤其是东亚这边深受儒家文化影响的民族，像日本韩国生育率比欧洲、澳洲、美洲低很多，东亚民族普遍对孩子投入较多，导致父母压力更大，所以越来越多的人不愿意去生孩子。</p>
<p>眼前的问题解决了，去年定一个的最重要的年度计划完成了，但是更大、更难的问题还在后面，然而<strong>现在能做的只能是花更多的时间去投资自己</strong>，不然的话真的害怕自己没有足够的能力为家庭提供一个好的生活。内地一年毕业600w+的大学生，在这样的竞争压力下，你若不努力，就会被淘汰，这就是现实社会中的达尔文主义。</p>
<h2 id="寒门再难在一二线扎根"><a href="#寒门再难在一二线扎根" class="headerlink" title="寒门再难在一二线扎根"></a>寒门再难在一二线扎根</h2><p>之前看到一个说法，说的是：中国70-90年代的精英大多来自农村，而90年代后的精英大都来自中产阶级。以互联网为例，这些大佬们，马云、马化腾、李彦宏、周鸿祎、刘强东、王兴等，家境较差的也只有刘强东了，其他人的家庭背景至少都是中产，农村走出来的人，在目前的精英阶层中还占有一定比例，未来的话这个比例估计会越来越低，最明显的是今年（2017）的高考状元，没有一个农村出身（网上的新闻），虽然这个并不能完全反映，但至少间接地反映了一些问题。</p>
<p>回到本文的主题 —- 房子，现在普遍有一种说法，想在北京扎根，家庭至少要能提供 200w 资金，要不然几乎不太有可能在北京扎根，昌平顺义的房子现在都已经 4w+ 了，离地铁近的恐怕都得 5w+，一般家庭的人如何负担得起。在去库存的这一历史进程中，一二城市的房价都已经翻倍，没有房子的人，相当于自己的财富被洗劫了一番，因为他并没有增加任何财富，而自己未来购房的成本却翻了不止一倍。对于普通人，尤其是来自农村家庭的孩子，感觉未来真的很难在一二线城市扎根，农村的孩子受到的教育本来就比城市差一些，而且父母管教得也不多，如果这些孩子再不知道努力的话，他们以后如何突破？甚至有可能会导致一个恶性循环，就像美国贫民区一样。</p>
<p>以前的时候在政治、历史书老是看到什么农民阶级的愚昧无知，鲁迅先生经常就会批判，因为自己出身于农村，每次回到家里，其实都能明显感觉到这一点，他们对于很多新鲜的事务都不是很了解、不懂理财不懂投资、更不懂得去投资自己，想想这样教育出来好孩子的几率有多大？成长的环境、父母的局限性对孩子性格、思维的培养影响是很大的，可是又能怎么办？这些父母可能并不知道未来他们孩子的竞争压力有多大，今年一个高考状元的采访，大概就是说：自己在城市长大，能接触到更好的学习方法、教育资源，而且父母对自己教育也非常上心，但农村孩子，很多人大学之前可能连电脑都没摸过，很难跟这些城市的孩子去竞争。唉，这就是残酷的现实，未来随着更多人去二线城市扎根，相信二线城市的房价还会再上升一部分，而这些想二线城市扎根的人付出的成本将会更大，给寒门出身的孩子们，留得机会并不多了，希望他们都能努力些。</p>
<h2 id="杂想"><a href="#杂想" class="headerlink" title="杂想"></a>杂想</h2><p>想想自己有了孩子之后，可能要为孩子牺牲很多，然后去慢慢培养孩子，等孩子长大后孩子在接着重复着我之前的生活，那人生的意义到底在哪里？仅仅为了繁衍么？近段在看《未来简史》，现在人文主义主宰世界，犹如之前宗教之于世界，人类不断为自己的生命寻找意义，以前我们认为自己是上帝伟大计划的一部分，这就是我们人类的全部意义，可是现在，估计很少有人相信这些，每个人都在寻找自己的信仰——follow your heart，都在试图找寻自己人生的意义。每当想到这些，看到那些为了孩子牺牲自己很多的父母时，我感觉到很可悲，这样的人生意义在哪里？以前很不明白为什么这样，现在有些明白了，但这完全不是我想要的生活。之前网上听到北京四中的校长讲到：一个为孩子付出一切的家庭，最后得到的往往是悲剧。这句话用在农村里，真的不为过，我们这代人，父母多生于文革年代，记事时赶上了恢复高考、改革开放，村里抓住机会的那批人已经在一站二线城市扎根，有的甚至已经在国外定居，每个乡每个镇多多少少都会有这样的人，这些走出去的人现在被称为当今社会的精英阶层，能走去的这些人大部分靠得都是高考，少数靠得是头脑和商机，他们的生活让依然生活在农村的父母一代羡慕。有长远眼光的父母就会从小对孩子管理严格，教育上投入更大，当然，在农村，投入更大也只是意味着找关系送礼上个当地更好的学校，最多寒暑假再上个补习班，幸运的是，我的父母属于这一类，父母一直希望我能走出农村，虽然他们只是初中文凭，但他们知道未来没有学历没有知识在这个社会很难有好的出路，打我记事开始，父母工作就很努力（在镇上做小生意），他们希望的是当我未来买房或者结婚的时候，能给我提供更大的支持，希望我能在二线甚至一线生存下去，非常感谢自己的父母，这么多年来，父母对自己默默付出了那么多。现在每次看到曾经的小学同学早已结婚生子，现在很多都在大城市里打工，在农村，如果没有考上大学，大部分走的都是这条路，这种生活明显不是我想要的生活，但是少年时代的自己并不知道这些、并不知道什么是自己想要的什么不是自己想要的，很幸运自己能够考上大学，走了农村孩子的另外一条路。而大部分的农村孩子并没有这种幸运，当然也有一些孩子，父母不怎么管教最后走出了一条很好的路，一是这种孩子较少，另一个是一个孩子性格的养成离不开其生活环境，父母在教育上没有管理太多，但在其他方面（隐性方面）对孩子的投入不见得小，这种孩子也很幸运，因为他们进入了一个正循环中，但是最多的那种，是没有考上大学而父母也没有为其提供一个很好出路的农村孩子，他们父母在孩子的投入不见得比别的孩子少，但结果并不如想象的那样，这中间有个体差异的原因，但是更多的是可悲、是无可奈何、或者说是不公平，这些孩子跟城里孩子是一样的，然而不同的生活环境造就了不同的人生路径，出身对一个人的影响有多大是不言而喻的，正所谓橘生淮南则为橘、橘生淮北则为枳。BBC有个纪录片叫做《七年》，有兴趣的朋友可以看一下，让人生走向一个正循环多么重要，而一个社会底层的人要想让人生走向正循环比例很低，中产阶级的孩子比例会大很多倍，这些中产阶级的孩子从小会接受更好的教育，成长的环境也比农村孩子强太多倍，但未来这些孩子们都将在同一个赛道上竞争，公平在这里显得苍白无力，随着经济的发展，更多私立中小学的兴起，公平又会从何而来？内心感觉很悲凉，可又无可奈何，未来自己终究也会走入这样的漩涡，明知不知道自己想要的生活，但又无法摆脱这种束缚，当想到这时，就会不由自主地去质疑人生的意义何在？《未来简史》没有告诉我答案，有信仰的人为自己的信仰而生，而我这种没有信仰的人却又当如何？为房子而活？为未来的孩子而活？为家庭而活？这都不是我想要的答案，为民族而活这句话说出来我自己都不相信，这些都不是我人生的意义，但又该是什么呢？我自己现在也不知道，但我会不停地去寻找答案。</p>
<h2 id="杭州看房的一些建议"><a href="#杭州看房的一些建议" class="headerlink" title="杭州看房的一些建议"></a>杭州看房的一些建议</h2><p>最后，再说一下买房的一些建议吧，杭州看房的话，有两个网站比较推荐：</p>
<ol>
<li>透明房，新房的最新消息，预售证信息、备案家都会在这个网站发布；</li>
<li>口水楼市，杭州方式论坛楼盘的论坛，有很多重要信息。</li>
</ol>
<p>我在看房的时候，主要记录以下这些信息，大家可以自行参考，选择自己认为比较重要的部分。</p>
<table>
<thead>
<tr>
<th>楼盘名</th>
<th>板块</th>
<th>状态</th>
<th>单价（中间套/边套）</th>
<th>面积</th>
<th>大概总价</th>
<th>得房率</th>
<th>交房时间</th>
<th>精装/毛坯</th>
<th>楼盘优惠</th>
<th>地铁（规划）</th>
<th>幼儿园</th>
<th>小学</th>
</tr>
</thead>
<tbody>
<tr>
<td>滨江旭辉·万家之星</td>
<td>勾庄北、拱辰北</td>
<td>目前还有三幢等待开售，预计价格是2.1-2.2万</td>
<td>2.2万</td>
<td>89 平</td>
<td>195w</td>
<td>-</td>
<td>-</td>
<td>毛坯</td>
<td>-</td>
<td>4号线</td>
<td>良渚通运幼儿园(公办)、勾庄中心幼儿园(公办)、12班幼儿园(规划)</td>
<td>运河小学(在建)、良渚二小运河校区</td>
</tr>
<tr>
<td>海德公园</td>
<td>勾庄北、拱辰北</td>
<td>开盘都已经卖完，下个月会在开盘一幢</td>
<td>~</td>
<td>83、89平</td>
<td>-</td>
<td>76%</td>
<td>2019年中旬交付</td>
<td>毛坯</td>
<td>-</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>最后，这篇文章是在杭州回北京的火车上完成的，这一路真的是感慨颇多，文章也如流水账一番，逻辑性不是很强，或许几年之后回头再看，就会嘲笑自己当年怎么这么无知。。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;落户+买房的事情终于搞定得差不多了（就差贷款合同的签署），六月份这个月已经往来杭州3趟，从四月底开始看房到现在，这一路真是五味杂粮、感慨颇多，亲身地感觉到生活之不易，无忧无虑的生活从离开大学的那一刻开始就已经结束，读研这两年半看似浑浑噩噩，但早已没有大学时的那种轻松惬意，或
    
    </summary>
    
      <category term="随笔" scheme="http://matt33.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://matt33.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
