<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matt&#39;s Blog</title>
  <subtitle>王蒙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matt33.com/"/>
  <updated>2018-11-19T02:26:06.000Z</updated>
  <id>http://matt33.com/</id>
  
  <author>
    <name>Matt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka Exactly-Once 之事务性实现</title>
    <link href="http://matt33.com/2018/11/04/kafka-transaction/"/>
    <id>http://matt33.com/2018/11/04/kafka-transaction/</id>
    <published>2018-11-04T12:36:34.000Z</published>
    <updated>2018-11-19T02:26:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempotent Producer 不提供跨多个 Partition 和跨会话场景下的保证，因此，我们是需要一种更强的事务保证，能够原子处理多个 Partition 的写入操作，数据要么全部写入成功，要么全部失败，不期望出现中间状态。这就是 Kafka Transactions 希望解决的问题，简单来说就是能够实现 <code>atomic writes across partitions</code>，本文以 Apache Kafka 2.0.0 代码实现为例，深入分析一下 Kafka 是如何实现这一机制的。</p>
<p>Apache Kafka 在 Exactly-Once Semantics（EOS）上三种粒度的保证如下（来自 <a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="external">Exactly-once Semantics in Apache Kafka</a>）：</p>
<ol>
<li>Idempotent Producer：Exactly-once，in-order，delivery per partition；</li>
<li>Transactions：Atomic writes across partitions；</li>
<li>Exactly-Once stream processing across read-process-write tasks；</li>
</ol>
<p>第二种情况就是本文讲述的主要内容，在讲述整个事务处理流程时，也会随便分析第三种情况。</p>
<h2 id="Kafka-Transactions"><a href="#Kafka-Transactions" class="headerlink" title="Kafka Transactions"></a>Kafka Transactions</h2><p>Kafka 事务性最开始的出发点是为了在 Kafka Streams 中实现 Exactly-Once 语义的数据处理，这个问题提出之后，在真正的方案讨论阶段，社区又挖掘了更多的应用场景，也为了尽可能覆盖更多的应用场景，在真正的实现中，在很多地方做了相应的 tradeoffs，后面会写篇文章对比一下 RocketMQ 事务性的实现，就能明白 Kafka 事务性实现及应用场景的复杂性了。</p>
<p>Kafka 的事务处理，主要是允许应用可以把消费和生产的 batch 处理（涉及多个 Partition）在一个原子单元内完成，操作要么全部完成、要么全部失败。为了实现这种机制，我们需要应用能提供一个唯一 id，即使故障恢复后也不会改变，这个 id 就是 TransactionnalId（也叫 txn.id，后面会详细讲述），txn.id 可以跟内部的 PID 1:1 分配，它们不同的是 txn.id 是用户提供的，而 PID 是 Producer 内部自动生成的（并且故障恢复后这个 PID 会变化），有了 txn.id 这个机制，就可以实现多 partition、跨会话的 EOS 语义。</p>
<p>当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：</p>
<ol>
<li>跨会话的幂等性写入：即使中间故障，恢复后依然可以保持幂等性；</li>
<li>跨会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；</li>
<li>跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。</li>
</ol>
<p>上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer 端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：</p>
<ol>
<li>对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；</li>
<li>一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；</li>
<li>Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；</li>
<li>Consumer 可能没有订阅这个事务涉及的全部 Partition。</li>
</ol>
<p>简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：</p>
<ol>
<li>Atomic writes across multiple partitions.</li>
<li>All messages in a transaction are made visible together, or none are.</li>
<li>Consumers must be configured to skip uncommitted messages.</li>
</ol>
<h2 id="事务性示例"><a href="#事务性示例" class="headerlink" title="事务性示例"></a>事务性示例</h2><p>Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 <code>transactional.id</code>，通过 <code>initTransactions()</code> 初始化事务状态信息，再通过 <code>beginTransaction()</code> 标识一个事务的开始，然后通过 <code>commitTransaction()</code> 或 <code>abortTransaction()</code> 对事务进行 commit 或 abort，示例如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">Properties props = <span class="keyword">new</span> Properties();</div><div class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line">props.put(<span class="string">"client.id"</span>, <span class="string">"ProducerTranscationnalExample"</span>);</div><div class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</div><div class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"test-transactional"</span>);</div><div class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</div><div class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</div><div class="line">producer.initTransactions();</div><div class="line"></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    String msg = <span class="string">"matt test"</span>;</div><div class="line">    producer.beginTransaction();</div><div class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"0"</span>, msg.toString()));</div><div class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"1"</span>, msg.toString()));</div><div class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"2"</span>, msg.toString()));</div><div class="line">    producer.commitTransaction();</div><div class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException e1) &#123;</div><div class="line">    e1.printStackTrace();</div><div class="line">    producer.close();</div><div class="line">&#125; <span class="keyword">catch</span> (KafkaException e2) &#123;</div><div class="line">    e2.printStackTrace();</div><div class="line">    producer.abortTransaction();</div><div class="line">&#125;</div><div class="line">producer.close();</div></pre></td></tr></table></figure>
<p>事务性的 API 也同样保持了 Kafka 一直以来的简洁性，使用起来是非常方便的。</p>
<h2 id="事务性要解决的问题"><a href="#事务性要解决的问题" class="headerlink" title="事务性要解决的问题"></a>事务性要解决的问题</h2><p>回想一下，前面一篇文章中关于幂等性要解决的问题（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98">幂等性要解决的问题</a>），事务性其实更多的是解决幂等性中没有解决的问题，比如：</p>
<ol>
<li>在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），这相当于出现了中间的状态，这并不是我们期望的结果；</li>
<li>Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；</li>
</ol>
<p>再来分析一下，Kafka 提供的事务性是如何解决上面两个问题的：</p>
<ol>
<li>如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；</li>
<li>其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactlt-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactlt-Once（这只是设计的思想，具体的实现后续会有文章详细解揭秘）。</li>
</ol>
<h2 id="事务性实现的关键"><a href="#事务性实现的关键" class="headerlink" title="事务性实现的关键"></a>事务性实现的关键</h2><p>对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。对于一个事务操作而言，其会涉及到多个 Topic-Partition 数据的写入，如果是一个 long transaction 操作，可能会涉及到非常多的数据，如何才能保证这个事务操作的原子性（要么全部完成，要么全部失败）呢？</p>
<ol>
<li>关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，如果大家一致认为可以 commit，那么就值行 commit，否则执行 abort：<ul>
<li>我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；</li>
<li>Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；</li>
<li>这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 <strong>TransactionCoordinator</strong> 角色；</li>
</ul>
</li>
<li>有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？<ul>
<li>TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 <strong>事务状态信息如何恢复？</strong> 也就是事务的状态信息需要<strong>很强的容错性、一致性</strong>；</li>
<li>关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），那就是 <code>min.isr + ack</code> 机制；</li>
<li>分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 <code>__consumer_offset</code> 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，而对应事务数据（transaction log）就是 <code>__transaction_state</code> 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator 在做故障恢复也是从这个 topic 中恢复数据；</li>
</ul>
</li>
<li>有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：<ul>
<li>Kafka 在 Producer 端引入了一个 <strong>TransactionalId</strong> 来解决这个问题，这个 txn.id 是由应用来配置的；</li>
<li>TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方（同一个 TransactionCoordinator）进行处理；</li>
</ul>
</li>
<li>再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：<ul>
<li>这个就是<strong>事务状态转移</strong>，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态（前提是这个状态转移是有效的）。</li>
</ul>
</li>
</ol>
<blockquote>
<p>上面的分析都是个人见解，有问题欢迎指正~</p>
</blockquote>
<p>下面这节就讲述一下事务性实现的一些关键的实现机制（对这些细节不太感兴趣或者之前没有深入接触过 Kafka，可以直接跳过，直接去看下一节的事务流程处理，先去了解一下一个事务操作的主要流程步骤）。</p>
<h3 id="TransactionCoordinator"><a href="#TransactionCoordinator" class="headerlink" title="TransactionCoordinator"></a>TransactionCoordinator</h3><p>TransactionCoordinator 与 GroupCoordinator 有一些相似之处，它主要是处理来自 Transactional Producer 的一些与事务相关的请求，涉及的请求如下表所示（关于这些请求处理的详细过程会在下篇文章详细讲述，这里先有个大概的认识即可）：</p>
<table>
<thead>
<tr>
<th>请求类型</th>
<th>用途说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>ApiKeys.FIND_COORDINATOR</td>
<td>Transaction Producer 会发送这个 FindCoordinatorRequest 请求，来查询当前事务（txn.id）对应的 TransactionCoordinator，这个与 GroupCoordinator 查询类似，是根据 txn.id 的 hash 值取模找到对应 Partition 的 leader，这个 leader 就是该事务对应的 TransactionCoordinator</td>
</tr>
<tr>
<td>ApiKeys.INIT_PRODUCER_ID</td>
<td>Producer 初始化时，会发送一个 InitProducerIdRequest 请求，来获取其分配的 PID 信息，对于幂等性的 Producer，会随机选择一台 broker 发送请求，而对于 Transaction Producer 会选择向其对应的 TransactionCoordinator 发送该请求（目的是为了根据 txn.id 对应的事务状态做一些判断）</td>
</tr>
<tr>
<td>ApiKeys.ADD_PARTITIONS_TO_TXN</td>
<td>将这个事务涉及到的 topic-partition 列表添加到事务的 meta 信息中（通过 AddPartitionsToTxnRequest 请求），事务 meta 信息需要知道当前的事务操作涉及到了哪些 Topic-Partition 的写入</td>
</tr>
<tr>
<td>ApiKeys.ADD_OFFSETS_TO_TXN</td>
<td>Transaction Producer 的这个 AddOffsetsToTxnRequest 请求是由 <code>sendOffsetsToTransaction()</code> 接口触发的，它主要是用在 consume-process-produce 的场景中，这时候 consumer 也是整个事务的一部分，只有这个事务 commit 时，offset 才会被真正 commit（主要还是用于 Failover）</td>
</tr>
<tr>
<td>ApiKeys.END_TXN</td>
<td>当提交事务时， Transaction Producer 会向 TransactionCoordinator 发送一个 EndTxnRequest 请求，来 commit 或者 abort 事务</td>
</tr>
</tbody>
</table>
<p>TransactionCoordinator 对象中还有两个关键的对象，分别是:</p>
<ol>
<li>TransactionStateManager：这个对象，从名字应该就能大概明白其作用是关于事务的状态管理，它会维护分配到这个 TransactionCoordinator 的所有事务的 meta 信息；</li>
<li>TransactionMarkerChannelManager：这个主要是用于向其他的 Broker 发送 Transaction Marker 数据，关于 Transaction Marker，第一次接触的人，可能会有一些困惑，什么是 Transaction Marker，Transaction Marker 是用来解决什么问题的呢？这里先留一个疑问，后面会来解密。</li>
</ol>
<p>总结一下，TransactionCoordinator 主要的功能有三个，分别是：</p>
<ol>
<li>处理事务相关的请求；</li>
<li>维护事务的状态信息；</li>
<li>向其他 Broker 发送 Transaction Marker 数据。</li>
</ol>
<h3 id="Transaction-Log（-transaction-state）"><a href="#Transaction-Log（-transaction-state）" class="headerlink" title="Transaction Log（__transaction_state）"></a>Transaction Log（__transaction_state）</h3><p>在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 topic <code>__transaction_state</code>，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 <code>__transaction_state</code> 的 partition 数取模得到，<code>__transaction_state</code> Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 <code>__transaction_state</code> Partition 2 的 leader 来处理。</p>
<p>对于 <code>__transaction_state</code> 这个 topic 默认是由 Server 端的 <code>transaction.state.log.replication.factor</code> 参数来配置，默认是3，如果当前 leader 故障，需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？</p>
<p>正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 <code>__transaction_state</code> 中读取之前事务的日志信息，来恢复其状态信息，前提是要求事务日志写入做相应的不丢配置。这也是 <code>__transaction_state</code> 一个重要作用之一，用于 TransactionCoordinator 的恢复，<code>__transaction_state</code>  与 <code>__consumer_offsets</code> 一样是 compact 类型的 topic，其 scheme 如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Key =&gt; Version TransactionalId</div><div class="line">    Version =&gt; 0 (int16)</div><div class="line">    TransactionalId =&gt; String</div><div class="line"></div><div class="line">Value =&gt; Version ProducerId ProducerEpoch TxnTimeoutDuration TxnStatus [TxnPartitions] TxnEntryLastUpdateTime TxnStartTime</div><div class="line">    Version =&gt; 0 (int16)</div><div class="line">    ProducerId =&gt; int64</div><div class="line">    ProducerEpoch =&gt; int16</div><div class="line">    TxnTimeoutDuration =&gt; int32</div><div class="line">    TxnStatus =&gt; int8</div><div class="line">    TxnPartitions =&gt; [Topic [Partition]]</div><div class="line">        Topic =&gt; String</div><div class="line">        Partition =&gt; int32</div><div class="line">    TxnLastUpdateTime =&gt; int64</div><div class="line">    TxnStartTime =&gt; int64</div></pre></td></tr></table></figure>
<h3 id="Transaction-Marker"><a href="#Transaction-Marker" class="headerlink" title="Transaction Marker"></a>Transaction Marker</h3><p>终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？</p>
<p>Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）</p>
<ol>
<li>Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；</li>
<li><p>在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？</p>
<ul>
<li>这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；</li>
</ul>
<p>再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？</p>
<ol>
<li>首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，是可以共享这个 TCP 连接的；</li>
<li>有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 信息做相应的处理。</li>
</ol>
</li>
</ol>
<p>Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">ControlMessageKey =&gt; Version ControlMessageType</div><div class="line">    Version =&gt; int16</div><div class="line">    ControlMessageType =&gt; int16</div><div class="line"></div><div class="line">TransactionControlMessageValue =&gt; Version CoordinatorEpoch</div><div class="line">    Version =&gt; int16</div><div class="line">    CoordinatorEpoch =&gt; int32</div></pre></td></tr></table></figure>
<p>这里再讲一个额外的内容，对于事务写入的数据，为了给消息添加一个标识（标识这条消息是不是来自事务写入的），<strong>数据格式（消息协议）发生了变化</strong>，这个改动主要是在 Attribute 字段，对于 MessageSet，Attribute 是16位，新的格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">| Unused (6-15) | Control (5) | Transactional (4) | Timestamp Type (3) | Compression Type (0-2) |</div></pre></td></tr></table></figure>
<p>对于 Message，也就是单条数据存储时（其中 Marker 数据都是单条存储的），在 Kafka 中，只有 MessageSet 才可以做压缩，所以 Message 就没必要设置压缩字段，其格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">| Unused (1-7) | Control Flag(0) |</div></pre></td></tr></table></figure>
<h3 id="Server-端事务状态管理"><a href="#Server-端事务状态管理" class="headerlink" title="Server 端事务状态管理"></a>Server 端事务状态管理</h3><p>TransactionCoordinator 会维护相应的事务的状态信息（也就是 TxnStatus），对于一个事务，总共有以下几种状态：</p>
<table>
<thead>
<tr>
<th>状态</th>
<th>状态码</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Empty</td>
<td>0</td>
<td>Transaction has not existed yet</td>
</tr>
<tr>
<td>Ongoing</td>
<td>1</td>
<td>Transaction has started and ongoing</td>
</tr>
<tr>
<td>PrepareCommit</td>
<td>2</td>
<td>Group is preparing to commit</td>
</tr>
<tr>
<td>PrepareAbort</td>
<td>3</td>
<td>Group is preparing to abort</td>
</tr>
<tr>
<td>CompleteCommit</td>
<td>4</td>
<td>Group has completed commit</td>
</tr>
<tr>
<td>CompleteAbort</td>
<td>5</td>
<td>Group has completed abort</td>
</tr>
<tr>
<td>Dead</td>
<td>6</td>
<td>TransactionalId has expired and is about to be removed from the transaction cache</td>
</tr>
<tr>
<td>PrepareEpochFence</td>
<td>7</td>
<td>We are in the middle of bumping the epoch and fencing out older producers</td>
</tr>
</tbody>
</table>
<p>其相应有效的状态转移图如下：</p>
<p><img src="/images/kafka/server-txn.png" alt="Server 端 Transaction 的状态转移图"></p>
<p>正常情况下，对于一个事务而言，其状态状态流程应该是 Empty –&gt; Ongoing –&gt; PrepareCommit –&gt; CompleteCommit –&gt; Empty 或者是 Empty –&gt; Ongoing –&gt; PrepareAbort –&gt; CompleteAbort –&gt; Empty。</p>
<h3 id="Client-端事务状态管理"><a href="#Client-端事务状态管理" class="headerlink" title="Client 端事务状态管理"></a>Client 端事务状态管理</h3><p>Client 的事务状态信息主要记录本地事务的状态，当然跟其他的系统类似，本地的状态信息与 Server 端的状态信息并不完全一致（状态的设置，就像 GroupCoodinator 会维护一个 Group 的状态，每个 Consumer 也会维护本地的 Consumer 对象的状态一样）。Client 端的事务状态信息主要用于 Client 端的事务状态处理，其主要有以下几种：</p>
<ol>
<li>UNINITIALIZED：Transactional Producer 初始化时的状态，此时还没有事务处理；</li>
<li>INITIALIZING：Transactional Producer 调用 <code>initTransactions()</code> 方法初始化事务相关的内容，比如发送 InitProducerIdRequest 请求；</li>
<li>READY：对于新建的事务，Transactional Producer 收到来自 TransactionCoordinator 的 InitProducerIdResponse 后，其状态会置为 READY（对于已有的事务而言，是当前事务完成后 Client 的状态会转移为 READY）；</li>
<li>IN_TRANSACTION：Transactional Producer 调用 <code>beginTransaction()</code> 方法，开始一个事务，标志着一个事务开始初始化；</li>
<li>COMMITTING_TRANSACTION：Transactional Producer 调用 <code>commitTransaction()</code> 方法时，会先更新本地的状态信息；</li>
<li>ABORTING_TRANSACTION：Transactional Producer 调用 <code>abortTransaction()</code> 方法时，会先更新本地的状态信息；</li>
<li>ABORTABLE_ERROR：在一个事务操作中，如果有数据发送失败，本地状态会转移到这个状态，之后再自动 abort 事务；</li>
<li>FATAL_ERROR：转移到这个状态之后，再进行状态转移时，会抛出异常；</li>
</ol>
<p>Client 端状态如下图：</p>
<p><img src="/images/kafka/client-txn.png" alt="Client 端 Transaction 的状态转移图"></p>
<h2 id="事务性的整体流程"><a href="#事务性的整体流程" class="headerlink" title="事务性的整体流程"></a>事务性的整体流程</h2><p>有了前面对 Kafka 事务性关键实现的讲述之后，这里详细讲述一个事务操作的处理流程，当然这里只是重点讲述事务性相关的内容，官方版的流程图可参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-DataFlow" target="_blank" rel="external">Kafka Exactly-Once Data Flow</a>，这里我做了一些改动，其流程图如下：</p>
<p><img src="/images/kafka/txn-data-flow.png" alt="consume-process-produce 事务的处理流程"></p>
<p>这个流程是以 consume-process-produce 场景为例（主要是 kafka streams 的场景），图中红虚框及 4.3a 部分是关于 consumer 的操作，去掉这部分的话，就是只考虑写入情况的场景。这种只考虑写入场景的事务操作目前在业内应用也是非常广泛的，比如 Flink + Kafka 端到端的 Exactly-Once 实现就是这种场景，下面来详细讲述一下整个流程。</p>
<h3 id="1-Finding-a-TransactionCoordinator"><a href="#1-Finding-a-TransactionCoordinator" class="headerlink" title="1. Finding a TransactionCoordinator"></a>1. Finding a TransactionCoordinator</h3><p>对于事务性的处理，第一步首先需要做的就是找到这个事务 txn.id 对应的 TransactionCoordinator，Transaction Producer 会向 Broker （随机选择一台 broker，一般选择本地连接最少的这台 broker）发送 FindCoordinatorRequest 请求，获取其 TransactionCoordinator。</p>
<p>怎么找到对应的 TransactionCoordinator 呢？这个前面已经讲过了，主要是通过下面的方法获取 <code>__transaction_state</code> 的 Partition，该 Partition 对应的 leader 就是这个 txn.id 对应的 TransactionCoordinator。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionFor</span></span>(transactionalId: <span class="type">String</span>): <span class="type">Int</span> = <span class="type">Utils</span>.abs(transactionalId.hashCode) % transactionTopicPartitionCount</div></pre></td></tr></table></figure>
<h3 id="2-Getting-a-PID"><a href="#2-Getting-a-PID" class="headerlink" title="2. Getting a PID"></a>2. Getting a PID</h3><p>PID 这里就不再介绍了，不了解的可以看前面那篇文章（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#PID">Producer ID</a>）。</p>
<p>Transaction Producer 在 <code>initializeTransactions()</code> 方法中会向 TransactionCoordinator 发送 InitPidRequest 请求获取其分配的 PID，有了 PID，事务写入时可以保证幂等性，PID 如何分配可以参考 <a href="http://matt33.com/2018/10/24/kafka-idempotent/#Producer-PID-%E7%94%B3%E8%AF%B7">PID 分配</a>，但是 TransactionCoordinator 在给事务 Producer 分配 PID 会做一些判断，主要的内容是：</p>
<ol>
<li>如果这个 txn.id 之前没有相应的事务状态（new txn.id），那么会初始化其事务 meta 信息 TransactionMetadata（会给其分配一个 PID，初始的 epoch 为-1），如果有事务状态，获取之前的状态；</li>
<li>校验其 TransactionMetadata 的状态信息（参考下面代码中 <code>prepareInitProduceIdTransit()</code> 方法）：<ol>
<li>如果前面还有状态转移正在进行，直接返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果此时的状态为 PrepareAbort 或 PrepareCommit，返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果之前的状态为 CompleteAbort、CompleteCommit 或 Empty，那么先将状态转移为 Empty，然后更新一下 epoch 值；</li>
<li>如果之前的状态为 Ongoing，状态会转移成 PrepareEpochFence，然后再 abort 当前的事务，并向 client 返回 CONCURRENT_TRANSACTIONS 异常；</li>
<li>如果状态为 Dead 或 PrepareEpochFence，直接抛出相应的 FATAL 异常；</li>
</ol>
</li>
<li>将 txn.id 与相应的 TransactionMetadata 持久化到事务日志中，对于 new txn.id，这个持久化的数据主要时 txn.id 与 pid 关系信息，如图中的 3a 所示。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: producer 启用事务性的情况下，检测此时事务的状态信息</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareInitProduceIdTransit</span></span>(transactionalId: <span class="type">String</span>,</div><div class="line">                                        transactionTimeoutMs: <span class="type">Int</span>,</div><div class="line">                                        coordinatorEpoch: <span class="type">Int</span>,</div><div class="line">                                        txnMetadata: <span class="type">TransactionMetadata</span>): <span class="type">ApiResult</span>[(<span class="type">Int</span>, <span class="type">TxnTransitMetadata</span>)] = &#123;</div><div class="line">  <span class="keyword">if</span> (txnMetadata.pendingTransitionInProgress) &#123;</div><div class="line">    <span class="comment">// return a retriable exception to let the client backoff and retry</span></div><div class="line">    <span class="type">Left</span>(<span class="type">Errors</span>.<span class="type">CONCURRENT_TRANSACTIONS</span>)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// caller should have synchronized on txnMetadata already</span></div><div class="line">    txnMetadata.state <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">PrepareAbort</span> | <span class="type">PrepareCommit</span> =&gt;</div><div class="line">        <span class="comment">// reply to client and let it backoff and retry</span></div><div class="line">        <span class="type">Left</span>(<span class="type">Errors</span>.<span class="type">CONCURRENT_TRANSACTIONS</span>)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">CompleteAbort</span> | <span class="type">CompleteCommit</span> | <span class="type">Empty</span> =&gt; <span class="comment">//note: 此时需要将状态转移到 Empty（此时状态并没有转移，只是在 PendingState 记录了将要转移的状态）</span></div><div class="line">        <span class="keyword">val</span> transitMetadata = <span class="keyword">if</span> (txnMetadata.isProducerEpochExhausted) &#123;</div><div class="line">          <span class="keyword">val</span> newProducerId = producerIdManager.generateProducerId()</div><div class="line">          txnMetadata.prepareProducerIdRotation(newProducerId, transactionTimeoutMs, time.milliseconds())</div><div class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 增加 producer 的 epoch 值</span></div><div class="line">          txnMetadata.prepareIncrementProducerEpoch(transactionTimeoutMs, time.milliseconds())</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="type">Right</span>(coordinatorEpoch, transitMetadata)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Ongoing</span> =&gt; <span class="comment">//note: abort 当前的事务，并返回一个 CONCURRENT_TRANSACTIONS 异常，强制 client 去重试</span></div><div class="line">        <span class="comment">// indicate to abort the current ongoing txn first. Note that this epoch is never returned to the</span></div><div class="line">        <span class="comment">// user. We will abort the ongoing transaction and return CONCURRENT_TRANSACTIONS to the client.</span></div><div class="line">        <span class="comment">// This forces the client to retry, which will ensure that the epoch is bumped a second time. In</span></div><div class="line">        <span class="comment">// particular, if fencing the current producer exhausts the available epochs for the current producerId,</span></div><div class="line">        <span class="comment">// then when the client retries, we will generate a new producerId.</span></div><div class="line">        <span class="type">Right</span>(coordinatorEpoch, txnMetadata.prepareFenceProducerEpoch())</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Dead</span> | <span class="type">PrepareEpochFence</span> =&gt; <span class="comment">//note: 返回错误</span></div><div class="line">        <span class="keyword">val</span> errorMsg = <span class="string">s"Found transactionalId <span class="subst">$transactionalId</span> with state <span class="subst">$&#123;txnMetadata.state&#125;</span>. "</span> +</div><div class="line">          <span class="string">s"This is illegal as we should never have transitioned to this state."</span></div><div class="line">        fatal(errorMsg)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(errorMsg)</div><div class="line"></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-Starting-a-Transaction"><a href="#3-Starting-a-Transaction" class="headerlink" title="3. Starting a Transaction"></a>3. Starting a Transaction</h3><p>前面两步都是 Transaction Producer 调用 <code>initTransactions()</code> 部分，到这里，Producer 可以调用 <code>beginTransaction()</code> 开始一个事务操作，其实现方法如下面所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//KafkaProducer</span></div><div class="line"><span class="comment">//note: 应该在一个事务操作之前进行调用</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</div><div class="line">    throwIfNoTransactionManager();</div><div class="line">    transactionManager.beginTransaction();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// TransactionManager</span></div><div class="line"><span class="comment">//note: 在一个事务开始之前进行调用，这里实际上只是转换了状态（只在 producer 本地记录了状态的开始）</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> </span>&#123;</div><div class="line">    ensureTransactional();</div><div class="line">    maybeFailWithError();</div><div class="line">    transitionTo(State.IN_TRANSACTION);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里只是将本地事务状态转移成 IN_TRANSACTION，并没有与 Server 端进行交互，所以在流程图中没有体现出来（TransactionManager 初始化时，其状态为 UNINITIALIZED，Producer 调用 <code>initializeTransactions()</code> 方法，其状态转移成 INITIALIZING）。</p>
<h3 id="4-Consume-Porcess-Produce-Loop"><a href="#4-Consume-Porcess-Produce-Loop" class="headerlink" title="4. Consume-Porcess-Produce Loop"></a>4. Consume-Porcess-Produce Loop</h3><p>在这个阶段，Transaction Producer 会做相应的处理，主要包括：从 consumer 拉取数据、对数据做相应的处理、通过 Producer 写入到下游系统中（对于只有写入场景，忽略前面那一步即可），下面有一个示例（start 和 end 中间的部分），是一个典型的 consume-process-produce 场景：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">    ConsumerRecords records = consumer.poll(Long.MAX_VALUE);</div><div class="line">    producer.beginTransaction();</div><div class="line">    <span class="comment">//start</span></div><div class="line">    <span class="keyword">for</span> (ConsumerRecord record : records)&#123;</div><div class="line">        producer.send(producerRecord(“outputTopic1”, record));</div><div class="line">        producer.send(producerRecord(“outputTopic2”, record));</div><div class="line">    &#125;</div><div class="line">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</div><div class="line">    <span class="comment">//end</span></div><div class="line">    producer.commitTransaction();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面来结合前面的流程图来讲述一下这部分的实现。</p>
<h4 id="4-1-AddPartitionsToTxnRequest"><a href="#4-1-AddPartitionsToTxnRequest" class="headerlink" title="4.1. AddPartitionsToTxnRequest"></a>4.1. AddPartitionsToTxnRequest</h4><p>Producer 在调用 <code>send()</code> 方法时，Producer 会将这个对应的 Topic—Partition 添加到 TransactionManager 的记录中，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如何开启了幂等性或事务性，需要做一些处理</span></div><div class="line"><span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</div><div class="line">    transactionManager.maybeAddPartitionToTransaction(tp);</div></pre></td></tr></table></figure>
<p>如果这个 Topic-Partition 之前不存在，那么就添加到 newPartitionsInTransaction 集合中，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 tp 添加到 newPartitionsInTransaction 中，记录当前进行事务操作的 tp</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">maybeAddPartitionToTransaction</span><span class="params">(TopicPartition topicPartition)</span> </span>&#123;</div><div class="line">    failIfNotReadyForSend();</div><div class="line"></div><div class="line">    <span class="comment">//note: 如果 partition 已经添加到 partitionsInTransaction、pendingPartitionsInTransaction、newPartitionsInTransaction中</span></div><div class="line">    <span class="keyword">if</span> (isPartitionAdded(topicPartition) || isPartitionPendingAdd(topicPartition))</div><div class="line">        <span class="keyword">return</span>;</div><div class="line"></div><div class="line">    log.debug(<span class="string">"Begin adding new partition &#123;&#125; to transaction"</span>, topicPartition);</div><div class="line">    newPartitionsInTransaction.add(topicPartition);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Producer 端的 Sender 线程会将这个信息通过 AddPartitionsToTxnRequest 请求发送给 TransactionCoordinator，也就是图中的 4.1 过程，TransactionCoordinator 会将这个 Topic-Partition 列表更新到 txn.id 对应的 TransactionMetadata 中，并且会持久化到事务日志中，也就是图中的 4.1 a 部分，这里持久化的数据主要是 txn.id 与其涉及到的 Topic-Partition 信息。</p>
<h4 id="4-2-ProduceRequest"><a href="#4-2-ProduceRequest" class="headerlink" title="4.2. ProduceRequest"></a>4.2. ProduceRequest</h4><p>这一步与正常 Producer 写入基本上一样，就是相应的 Leader 在持久化数据时会在头信息中标识这条数据是不是来自事务 Producer 的写入（主要是数据协议有变动，Server 处理并不需要做额外的处理）。</p>
<h4 id="4-3-AddOffsetsToTxnRequest"><a href="#4-3-AddOffsetsToTxnRequest" class="headerlink" title="4.3. AddOffsetsToTxnRequest"></a>4.3. AddOffsetsToTxnRequest</h4><p>Producer 在调用 <code>sendOffsetsToTransaction()</code> 方法时，第一步会首先向 TransactionCoordinator 发送相应的 AddOffsetsToTxnRequest 请求，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//class KafkaProcducer</span></div><div class="line"><span class="comment">//note: 当你需要 batch 的消费-处理-写入消息，这个方法需要被使用</span></div><div class="line"><span class="comment">//note: 发送指定的 offset 给 group coordinator，用来标记这些 offset 是作为当前事务的一部分，只有这次事务成功时</span></div><div class="line"><span class="comment">//note: 这些 offset 才会被认为 commit 了</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></div><div class="line">                                     String consumerGroupId) <span class="keyword">throws</span> ProducerFencedException &#123;</div><div class="line">    throwIfNoTransactionManager();</div><div class="line">    TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);</div><div class="line">    sender.wakeup();</div><div class="line">    result.await();</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// class TransactionManager</span></div><div class="line"><span class="comment">//note: 发送 AddOffsetsToTxRequest</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></div><div class="line">                                                                        String consumerGroupId) &#123;</div><div class="line">    ensureTransactional();</div><div class="line">    maybeFailWithError();</div><div class="line">    <span class="keyword">if</span> (currentState != State.IN_TRANSACTION)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Cannot send offsets to transaction either because the producer is not in an "</span> +</div><div class="line">                <span class="string">"active transaction"</span>);</div><div class="line"></div><div class="line">    log.debug(<span class="string">"Begin adding offsets &#123;&#125; for consumer group &#123;&#125; to transaction"</span>, offsets, consumerGroupId);</div><div class="line">    AddOffsetsToTxnRequest.Builder builder = <span class="keyword">new</span> AddOffsetsToTxnRequest.Builder(transactionalId,</div><div class="line">            producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, consumerGroupId);</div><div class="line">    AddOffsetsToTxnHandler handler = <span class="keyword">new</span> AddOffsetsToTxnHandler(builder, offsets);</div><div class="line">    enqueueRequest(handler);</div><div class="line">    <span class="keyword">return</span> handler.result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>TransactionCoordinator 在收到这个请求时，处理方法与 4.1 中的一样，把这个 group.id 对应的 <code>__consumer_offsets</code> 的 Partition （与写入涉及的 Topic-Partition 一样）保存到事务对应的 meta 中，之后会持久化相应的事务日志，如图中 4.3a 所示。</p>
<h4 id="4-4-TxnOffsetsCommitRequest"><a href="#4-4-TxnOffsetsCommitRequest" class="headerlink" title="4.4. TxnOffsetsCommitRequest"></a>4.4. TxnOffsetsCommitRequest</h4><p>Producer 在收到 TransactionCoordinator 关于 AddOffsetsToTxnRequest 请求的结果后，后再次发送 TxnOffsetsCommitRequest 请求给对应的 GroupCoordinator，AddOffsetsToTxnHandler 的 <code>handleResponse()</code> 的实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleResponse</span><span class="params">(AbstractResponse response)</span> </span>&#123;</div><div class="line">    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;</div><div class="line">    Errors error = addOffsetsToTxnResponse.error();</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (error == Errors.NONE) &#123;</div><div class="line">        log.debug(<span class="string">"Successfully added partition for consumer group &#123;&#125; to transaction"</span>, builder.consumerGroupId());</div><div class="line"></div><div class="line">        <span class="comment">// note the result is not completed until the TxnOffsetCommit returns</span></div><div class="line">        <span class="comment">//note: AddOffsetsToTnxRequest 之后，还会再发送 TxnOffsetCommitRequest</span></div><div class="line">        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));</div><div class="line">        transactionStarted = <span class="keyword">true</span>;</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_NOT_AVAILABLE || error == Errors.NOT_COORDINATOR) &#123;</div><div class="line">        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);</div><div class="line">        reenqueue();</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.COORDINATOR_LOAD_IN_PROGRESS || error == Errors.CONCURRENT_TRANSACTIONS) &#123;</div><div class="line">        reenqueue();</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.INVALID_PRODUCER_EPOCH) &#123;</div><div class="line">        fatalError(error.exception());</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED) &#123;</div><div class="line">        fatalError(error.exception());</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error == Errors.GROUP_AUTHORIZATION_FAILED) &#123;</div><div class="line">        abortableError(<span class="keyword">new</span> GroupAuthorizationException(builder.consumerGroupId()));</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        fatalError(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error in AddOffsetsToTxnResponse: "</span> + error.message()));</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>GroupCoordinator 在收到相应的请求后，会将 offset 信息持久化到 consumer offsets log 中（包含对应的 PID 信息），但是<strong>不会更新到缓存</strong>中，除非这个事务 commit 了，这样的话就可以保证这个 offset 信息对 consumer 是不可见的（没有更新到缓存中的数据是不可见的，通过接口是获取的，这是 GroupCoordinator 本身来保证的）。</p>
<h3 id="5-Committing-or-Aborting-a-Transaction"><a href="#5-Committing-or-Aborting-a-Transaction" class="headerlink" title="5.Committing or Aborting a Transaction"></a>5.Committing or Aborting a Transaction</h3><p>在一个事务操作处理完成之后，Producer 需要调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p>
<h4 id="5-1-EndTxnRequest"><a href="#5-1-EndTxnRequest" class="headerlink" title="5.1. EndTxnRequest"></a>5.1. EndTxnRequest</h4><p>无论是 Commit 还是 Abort，对于 Producer 而言，都是向 TransactionCoordinator 发送 EndTxnRequest 请求，这个请求的内容里会标识是 commit 操作还是 abort 操作，Producer 的 <code>commitTransaction()</code> 方法实现如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//class KafkaProducer</span></div><div class="line"><span class="comment">//note: commit 正在进行的事务操作，这个方法在真正发送 commit 之后将会 flush 所有未发送的数据</span></div><div class="line"><span class="comment">//note: 如果在发送中遇到任何一个不能修复的错误，这个方法抛出异常，事务也不会被提交，所有 send 必须成功，这个事务才能 commit 成功</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</div><div class="line">    throwIfNoTransactionManager();</div><div class="line">    TransactionalRequestResult result = transactionManager.beginCommit();</div><div class="line">    sender.wakeup();</div><div class="line">    result.await();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// class TransactionManager</span></div><div class="line"><span class="comment">//note: 开始 commit，转移本地本地保存的状态以及发送相应的请求</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">beginCommit</span><span class="params">()</span> </span>&#123;</div><div class="line">    ensureTransactional();</div><div class="line">    maybeFailWithError();</div><div class="line">    transitionTo(State.COMMITTING_TRANSACTION);</div><div class="line">    <span class="keyword">return</span> beginCompletingTransaction(TransactionResult.COMMIT);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Producer 的 <code>abortTransaction()</code> 方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//class KafkaProducer</span></div><div class="line"><span class="comment">//note: 取消正在进行事务，任何没有 flush 的数据都会被丢弃</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</div><div class="line">    throwIfNoTransactionManager();</div><div class="line">    TransactionalRequestResult result = transactionManager.beginAbort();</div><div class="line">    sender.wakeup();</div><div class="line">    result.await();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// class TransactionManager</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult <span class="title">beginAbort</span><span class="params">()</span> </span>&#123;</div><div class="line">    ensureTransactional();</div><div class="line">    <span class="keyword">if</span> (currentState != State.ABORTABLE_ERROR)</div><div class="line">        maybeFailWithError();</div><div class="line">    transitionTo(State.ABORTING_TRANSACTION);</div><div class="line"></div><div class="line">    <span class="comment">// We're aborting the transaction, so there should be no need to add new partitions</span></div><div class="line">    newPartitionsInTransaction.clear();</div><div class="line">    <span class="keyword">return</span> beginCompletingTransaction(TransactionResult.ABORT);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它们最终都是调用了 TransactionManager 的 <code>beginCompletingTransaction()</code> 方法，这个方法会向其 待发送请求列表 中添加 EndTxnRequest 请求，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送 EndTxnRequest 请求，添加到 pending 队列中</span></div><div class="line"><span class="function"><span class="keyword">private</span> TransactionalRequestResult <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult transactionResult)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (!newPartitionsInTransaction.isEmpty())</div><div class="line">        enqueueRequest(addPartitionsToTransactionHandler());</div><div class="line">    EndTxnRequest.Builder builder = <span class="keyword">new</span> EndTxnRequest.Builder(transactionalId, producerIdAndEpoch.producerId,</div><div class="line">            producerIdAndEpoch.epoch, transactionResult);</div><div class="line">    EndTxnHandler handler = <span class="keyword">new</span> EndTxnHandler(builder);</div><div class="line">    enqueueRequest(handler);</div><div class="line">    <span class="keyword">return</span> handler.result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>TransactionCoordinator 在收到 EndTxnRequest 请求后，会做以下处理：</p>
<ol>
<li>更新事务的 meta 信息，状态转移成 PREPARE_COMMIT 或 PREPARE_ABORT，并将事务状态信息持久化到事务日志中；</li>
<li>根据事务 meta 信息，向其涉及到的所有 Topic-Partition 的 leader 发送 Transaction Marker 信息（也就是 WriteTxnMarkerRquest 请求，见下面的 5.2 分析）；</li>
<li>最后将事务状态更新为 COMMIT 或者 ABORT，并将事务的 meta 持久化到事务日志中，也就是 5.3 步骤。</li>
</ol>
<h4 id="5-2-WriteTxnMarkerRquest"><a href="#5-2-WriteTxnMarkerRquest" class="headerlink" title="5.2. WriteTxnMarkerRquest"></a>5.2. WriteTxnMarkerRquest</h4><p>WriteTxnMarkerRquest 是 TransactionCoordinator 收到 Producer 的 EndTxnRequest 请求后向其他 Broker 发送的请求，主要是告诉它们事务已经完成。不论是普通的 Topic-Partition 还是 <code>__consumer_offsets</code>，在收到这个请求后，都会把事务结果（Transaction Marker 的格数据式见前面）持久化到对应的日志文件中，这样下游 Consumer 在消费这个数据时，就知道这个事务是 commit 还是 abort。</p>
<h4 id="5-3-Writing-the-Final-Commit-or-Abort-Message"><a href="#5-3-Writing-the-Final-Commit-or-Abort-Message" class="headerlink" title="5.3. Writing the Final Commit or Abort Message"></a>5.3. Writing the Final Commit or Abort Message</h4><p>当这个事务涉及到所有 Topic-Partition 都已经把这个 marker 信息持久化到日志文件之后，TransactionCoordinator 会将这个事务的状态置为 COMMIT 或 ABORT，并持久化到事务日志文件中，到这里，这个事务操作就算真正完成了，TransactionCoordinator 缓存的很多关于这个事务的数据可以被清除了。</p>
<h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>在上面讲述完 Kafka 事务性处理之后，我们来思考一下以下这些问题，上面的流程可能会出现下面这些问题或者很多人可能会有下面的疑问：</p>
<ol>
<li>txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？</li>
<li>TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？</li>
<li>对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？</li>
<li>对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？</li>
<li>如果 txn.id 长期不使用，server 端怎么处理？</li>
<li>PID Snapshot 是做什么的？是用来解决什么问题？</li>
</ol>
<p>下面，来详细分析一下上面提到的这些问题。</p>
<h3 id="如果多个-Producer-使用同一个-txn-id-会出现什么情况？"><a href="#如果多个-Producer-使用同一个-txn-id-会出现什么情况？" class="headerlink" title="如果多个 Producer 使用同一个 txn.id 会出现什么情况？"></a>如果多个 Producer 使用同一个 txn.id 会出现什么情况？</h3><p>对于这个情况，我们这里直接做了一个相应的实验，两个 Producer 示例都使用了同一个 txn.id（为 test-transactional-matt），Producer 1 先启动，然后过一会再启动 Producer 2，这时候会发现一个现象，那就是 Producer 1 进程会抛出异常退出进程，其异常信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are <span class="keyword">in</span> an error state</div><div class="line">	at org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:784)</div><div class="line">	at org.apache.kafka.clients.producer.internals.TransactionManager.beginTransaction(TransactionManager.java:215)</div><div class="line">	at org.apache.kafka.clients.producer.KafkaProducer.beginTransaction(KafkaProducer.java:606)</div><div class="line">	at com.matt.test.kafka.producer.ProducerTransactionExample.main(ProducerTransactionExample.java:68)</div><div class="line">Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></div></pre></td></tr></table></figure>
<p>这里抛出了 ProducerFencedException 异常，如果打开相应的 Debug 日志，在 Producer 1 的日志文件会看到下面的日志信息</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">[2018-11-03 12:48:52,495] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Transition from state COMMITTING_TRANSACTION to error state FATAL_ERROR (org.apache.kafka.clients.producer.internals.TransactionManager)</div><div class="line">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></div><div class="line">[2018-11-03 12:48:52,498] ERROR [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)</div><div class="line">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.</div><div class="line">[2018-11-03 12:48:52,599] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)</div><div class="line">[2018-11-03 12:48:52,599] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Beginning shutdown of Kafka producer I/O thread, sending remaining records. (org.apache.kafka.clients.producer.internals.Sender)</div><div class="line">[2018-11-03 12:48:52,601] DEBUG Removed sensor with name connections-closed: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,601] DEBUG Removed sensor with name connections-created: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name successful-authentication: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name failed-authentication: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,602] DEBUG Removed sensor with name bytes-sent-received: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,603] DEBUG Removed sensor with name bytes-sent: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,603] DEBUG Removed sensor with name bytes-received: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name select-time: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name io-time: (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,604] DEBUG Removed sensor with name node--1.bytes-sent (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node--1.bytes-received (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node--1.latency (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,605] DEBUG Removed sensor with name node-33.bytes-sent (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-33.bytes-received (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-33.latency (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.bytes-sent (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.bytes-received (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,606] DEBUG Removed sensor with name node-35.latency (org.apache.kafka.common.metrics.Metrics)</div><div class="line">[2018-11-03 12:48:52,607] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Shutdown of Kafka producer I/O thread has completed. (org.apache.kafka.clients.producer.internals.Sender)</div><div class="line">[2018-11-03 12:48:52,607] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</div><div class="line">[2018-11-03 12:48:52,808] ERROR Forcing producer close! (com.matt.test.kafka.producer.ProducerTransactionExample)</div><div class="line">[2018-11-03 12:48:52,808] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)</div><div class="line">[2018-11-03 12:48:52,808] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="built_in">test</span>-transactional-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</div></pre></td></tr></table></figure>
<p>Producer 1 本地事务状态从 COMMITTING_TRANSACTION 变成了 FATAL_ERROR 状态，导致 Producer 进程直接退出了，出现这个异常的原因，就是抛出的 ProducerFencedException 异常，简单来说 Producer 1 被 Fencing 了（这是 Producer Fencing 的情况）。因此，这个问题的答案就很清除了，如果多个 Producer 共用一个 txn.id，那么最后启动的 Producer 会成功运行，会它之前启动的 Producer 都 Fencing 掉（至于为什么会 Fencing 下一小节会做分析）。</p>
<h3 id="Fencing"><a href="#Fencing" class="headerlink" title="Fencing"></a>Fencing</h3><p>关于 Fencing 这个机制，在分布式系统还是很常见的，我第一个见到这个机制是在 HDFS 中，可以参考我之前总结的一篇文章 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98">HDFS NN 脑裂问题</a>，Fencing 机制解决的主要也是这种类型的问题 —— 脑裂问题，简单来说就是，本来系统这个组件在某个时刻应该只有一个处于 active 状态的，但是在实际生产环境中，特别是切换期间，可能会同时出现两个组件处于 active 状态，这就是脑裂问题，在 Kafka 的事务场景下，用到 Fencing 机制有两个地方：</p>
<ol>
<li>TransactionCoordinator Fencing；</li>
<li>Producer Fencing；</li>
</ol>
<h4 id="TransactionCoordinator-Fencing"><a href="#TransactionCoordinator-Fencing" class="headerlink" title="TransactionCoordinator Fencing"></a>TransactionCoordinator Fencing</h4><p>TransactionCoordinator 在遇到上 long FGC 时，可能会导致 脑裂 问题，FGC 时会 stop-the-world，这时候可能会与 zk 连接超时导致临时节点消失进而触发 leader 选举，如果 <code>__transaction_state</code> 发生了 leader 选举，TransactionCoordinator 就会切换，如果此时旧的 TransactionCoordinator FGC 完成，在还没来得及同步到最细 meta 之前，会有一个短暂的时刻，对于一个 txn.id 而言就是这个时刻可能出现了两个 TransactionCoordinator。</p>
<p>相应的解决方案就是 TransactionCoordinator Fencing，这里 Fencing 策略不像离线场景 HDFS 这种直接 Kill 旧的 NN 进程或者强制切换状态这么暴力，而是通过 CoordinatorEpoch 来判断，每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 <code>__transaction_state</code> Partition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。</p>
<p>明白了 TransactionCoordinator 脑裂问题发生情况及解决方案之后，来分析下，Fencing 机制会在哪里发挥作用？仔细想想，是可以推断出来的，只可能是 TransactionCoordinator 向别人发请求时影响才会比较严重（特别是乱发 admin 命令）。有了 CoordinatorEpoch 之后，其他 Server 在收到请求时做相应的判断，如果发现 CoordinatorEpoch 值比缓存的最新的值小，那么 Fencing 就生效，拒绝这个请求，也就是 TransactionCoordinator 发送 WriteTxnMarkerRequest 时可能会触发这一机制。</p>
<h4 id="Producer-Fencing"><a href="#Producer-Fencing" class="headerlink" title="Producer Fencing"></a>Producer Fencing</h4><p>Producer Fencing 与前面的类似，如果对于相同 PID 和 txn.id 的 Producer，Server 端会记录最新的 Epoch 值，拒绝来自 zombie Producer （Epoch 值小的 Producer）的请求。前面第一个问题的情况，Producer 2 在启动时，会向 TransactionCoordinator 发送 InitPIDRequest 请求，此时 TransactionCoordinator 已经有了这个 txn.id 对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样 Producer 2 就被认为是最新的 Producer，而 Producer 1 就会被认为是 zombie Producer，因此，TransactionCoordinator 在处理 Producer 1 的事务请求时，会返回相应的异常信息。</p>
<h3 id="Consumer-端如何消费事务数据"><a href="#Consumer-端如何消费事务数据" class="headerlink" title="Consumer 端如何消费事务数据"></a>Consumer 端如何消费事务数据</h3><p>在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 <code>isolation.level</code> 配置，这个是配置对于事务性数据的消费策略，有以下两种可选配置：</p>
<ol>
<li><code>read_committed</code>: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.</li>
<li><code>read_uncommitted</code>: consume all available messages in offset ordering. This is the <strong>default value</strong>.</li>
</ol>
<p>简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），这部分的难点在于 read_committed 机制的实现。</p>
<h4 id="Last-Stable-Offset（LSO）"><a href="#Last-Stable-Offset（LSO）" class="headerlink" title="Last Stable Offset（LSO）"></a>Last Stable Offset（LSO）</h4><p>在事务机制的实现中，Kafka 又设置了一个新的 offset 概念，那就是 Last Stable Offset，简称 LSO（其他的 Offset 概念可参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Kafka Offset 那些事</a>），先看下 LSO 的定义：</p>
<blockquote>
<p>The LSO is defined as the latest offset such that the status of all transactional messages at lower offsets have been determined (i.e. committed or aborted).</p>
</blockquote>
<p>对于一个 Partition 而言，offset 小于 LSO 的数据，全都是已经确定的数据，这个主要是对于事务操作而言，在这个 offset 之前的事务操作都是已经完成的事务（已经 commit 或 abort），如果这个 Partition 没有涉及到事务数据，那么 LSO 就是其 HW（水位）。</p>
<h4 id="Server-处理-read-committed-类型的-Fetch-请求"><a href="#Server-处理-read-committed-类型的-Fetch-请求" class="headerlink" title="Server 处理 read_committed 类型的 Fetch 请求"></a>Server 处理 read_committed 类型的 Fetch 请求</h4><p>如果 Consumer 的消费策略设置的是 read_committed，其在向 Server 发送 Fetch 请求时，Server 端<strong>只会返回 LSO 之前的数据</strong>，在 LSO 之后的数据不会返回。</p>
<p>这种机制有没有什么问题呢？我现在能想到的就是如果有一个 long transaction，比如其 first offset 是 1000，另外有几个已经完成的小事务操作，比如：txn1（offset：1100~1200）、txn2（offset：1400~1500），假设此时的 LSO 是 1000，也就是说这个 long transaction 还没有完成，那么已经完成的 txn1、txn2 也会对 consumer 不可见（假设都是 commit 操作），此时<strong>受 long transaction 的影响可能会导致数据有延迟</strong>。</p>
<p>那么我们再来想一下，如果不设计 LSO，又会有什么问题呢？可能分两种情况：</p>
<ol>
<li>允许读未完成的事务：那么 Consumer 可以直接读取到 Partition 的 HW 位置，对于未完成的事务，因为设置的是 read_committed 机制，所以不能对用户可见，需要在 Consumer 端做缓存，这个缓存应该设置多大？（不限制肯定会出现 OOM 的情况，当然也可以现在 client 端持久化到硬盘，这样的设计太过于复杂，还需要考虑 client 端 IO、磁盘故障等风险），明显这种设计方案是不可行的；</li>
<li>如果不允许读未完成的事务：相当于还是在 Server 端处理，与前面的区别是，这里需要先把示例中的 txn1、txn2 的数据发送给 Consumer，这样的设计会带来什么问题呢？<ol>
<li>假设这个 long transaction commit 了，其 end offset 是 2000，这时候有两种方案：第一种是把 1000-2000 的数据全部读出来（可能是磁盘读），把这个 long transaction 的数据过滤出来返回给 Consumer；第二种是随机读，只读这个 long transaction 的数据，无论哪种都有多触发一次磁盘读的风险，可能影响影响 Server 端的性能；</li>
<li>Server 端需要维护每个 consumer group 有哪些事务读了、哪些事务没读的 meta 信息，因为 consumer 是随机可能挂掉，需要接上次消费的，这样实现就复杂很多了；</li>
<li>还有一个问题是，消费的顺序性无法保证，两次消费其读取到的数据顺序可能是不同的（两次消费启动时间不一样）；</li>
</ol>
</li>
</ol>
<p>从这些分析来看，个人认为 LSO 机制还是一种相当来说 实现起来比较简单、而且不影响原来 server 端性能、还能保证顺序性的一种设计方案，它不一定是最好的，但也不会差太多。在实际的生产场景中，尽量避免 long transaction 这种操作，而且 long transaction可能也会容易触发事务超时。</p>
<h4 id="Consumer-如何过滤-abort-的事务数据"><a href="#Consumer-如何过滤-abort-的事务数据" class="headerlink" title="Consumer 如何过滤 abort 的事务数据"></a>Consumer 如何过滤 abort 的事务数据</h4><p>Consumer 在拉取到相应的数据之后，后面该怎么处理呢？它拉取到的这批数据并不能保证都是完整的事务数据，很有可能是拉取到一个事务的部分数据（marker 数据还没有拉取到），这时候应该怎么办？难道 Consumer 先把这部分数据缓存下来，等后面的 marker 数据到来时再确认数据应该不应该丢弃？（还是又 OOM 的风险）有没有更好的实现方案？</p>
<p>Kafka 的设计总是不会让我们失望，这部分做的优化也是非常高明，Broker 会追踪每个 Partition 涉及到的 abort transactions，Partition 的每个 log segment 都会有一个单独只写的文件（append-only file）来存储 abort transaction 信息，因为 abort transaction 并不是很多，所以这个开销是可以可以接受的，之所以要持久化到磁盘，主要是为了故障后快速恢复，要不然 Broker 需要把这个 Partition 的所有数据都读一遍，才能直到哪些事务是 abort 的，这样的话，开销太大（如果这个 Partition 没有事务操作，就不会生成这个文件）。这个持久化的文件是以 <code>.txnindex</code> 做后缀，前面依然是这个 log segment 的 offset 信息，存储的数据格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TransactionEntry =&gt;</div><div class="line">    Version =&gt; int16</div><div class="line">    PID =&gt; int64</div><div class="line">    FirstOffset =&gt; int64</div><div class="line">    LastOffset =&gt; int64</div><div class="line">    LastStableOffset =&gt; int64</div></pre></td></tr></table></figure>
<p>有了这个设计，Consumer 在拉取数据时，Broker 会把这批数据涉及到的所有 abort transaction 信息都返回给 Consumer，Server 端会根据拉取的 offset 范围与 abort transaction 的 offset 做对比，返回涉及到的 abort transaction 集合，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAbortedTxns</span></span>(fetchOffset: <span class="type">Long</span>, upperBoundOffset: <span class="type">Long</span>): <span class="type">TxnIndexSearchResult</span> = &#123;</div><div class="line">  <span class="keyword">val</span> abortedTransactions = <span class="type">ListBuffer</span>.empty[<span class="type">AbortedTxn</span>]</div><div class="line">  <span class="keyword">for</span> ((abortedTxn, _) &lt;- iterator()) &#123;</div><div class="line">    <span class="keyword">if</span> (abortedTxn.lastOffset &gt;= fetchOffset &amp;&amp; abortedTxn.firstOffset &lt; upperBoundOffset)</div><div class="line">      abortedTransactions += abortedTxn <span class="comment">//note: 这个 abort 的事务有在在这个范围内，就返回</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> (abortedTxn.lastStableOffset &gt;= upperBoundOffset)</div><div class="line">      <span class="keyword">return</span> <span class="type">TxnIndexSearchResult</span>(abortedTransactions.toList, isComplete = <span class="literal">true</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="type">TxnIndexSearchResult</span>(abortedTransactions.toList, isComplete = <span class="literal">false</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Consumer 在拿到这些数据之后，会进行相应的过滤，大概的判断逻辑如下（Server 端返回的 abort transaction 列表就保存在 <code>abortedTransactions</code>  集合中，<code>abortedProducerIds</code>  最开始时是为空的）：</p>
<ol>
<li>如果这个数据是 control msg（也即是 marker 数据），是 ABORT 的话，那么与这个事务相关的 PID 信息从 <code>abortedProducerIds</code> 集合删掉，是 COMMIT 的话，就忽略（每个这个 PID 对应的 marker 数据收到之后，就从 <code>abortedProducerIds</code> 中清除这个 PID 信息）；</li>
<li>如果这个数据是正常的数据，把它的 PID 和 offset 信息与 <code>abortedTransactions</code> 队列（有序队列，头部 transaction 的 first offset 最小）第一个 transaction 做比较，如果 PID 相同，并且 offset 大于等于这个 transaction 的 first offset，就将这个 PID 信息添加到 <code>abortedProducerIds</code> 集合中，同时从 <code>abortedTransactions</code> 队列中删除这个 transaction，最后再丢掉这个 batch（它是 abort transaction 的数据）；</li>
<li>检查这个 batch 的 PID 是否在 <code>abortedProducerIds</code> 集合中，在的话，就丢弃，不在的话就返回上层应用。</li>
</ol>
<p>这部分的实现确实有些绕（有兴趣的可以慢慢咀嚼一下），它严重依赖了 Kafka 提供的下面两种保证：</p>
<ol>
<li>Consumer 拉取到的数据，在处理时，其 offset 是严格有序的；</li>
<li>同一个 txn.id（PID 相同）在某一个时刻最多只能有一个事务正在进行；</li>
</ol>
<p>这部分代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> Record <span class="title">nextFetchedRecord</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">        <span class="keyword">if</span> (records == <span class="keyword">null</span> || !records.hasNext()) &#123; <span class="comment">//note: records 为空（数据全部丢掉了），records 没有数据（是 control msg）</span></div><div class="line">            maybeCloseRecordStream();</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (!batches.hasNext()) &#123;</div><div class="line">                <span class="comment">// Message format v2 preserves the last offset in a batch even if the last record is removed</span></div><div class="line">                <span class="comment">// through compaction. By using the next offset computed from the last offset in the batch,</span></div><div class="line">                <span class="comment">// we ensure that the offset of the next fetch will point to the next batch, which avoids</span></div><div class="line">                <span class="comment">// unnecessary re-fetching of the same batch (in the worst case, the consumer could get stuck</span></div><div class="line">                <span class="comment">// fetching the same batch repeatedly).</span></div><div class="line">                <span class="keyword">if</span> (currentBatch != <span class="keyword">null</span>)</div><div class="line">                    nextFetchOffset = currentBatch.nextOffset();</div><div class="line">                drain();</div><div class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            currentBatch = batches.next();</div><div class="line">            maybeEnsureValid(currentBatch);</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (isolationLevel == IsolationLevel.READ_COMMITTED &amp;&amp; currentBatch.hasProducerId()) &#123;</div><div class="line">                <span class="comment">//note: 需要做相应的判断</span></div><div class="line">                <span class="comment">// remove from the aborted transaction queue all aborted transactions which have begun</span></div><div class="line">                <span class="comment">// before the current batch's last offset and add the associated producerIds to the</span></div><div class="line">                <span class="comment">// aborted producer set</span></div><div class="line">                <span class="comment">//note: 如果这个 batch 的 offset 已经大于等于 abortedTransactions 中第一事务的 first offset</span></div><div class="line">                <span class="comment">//note: 那就证明下个 abort transaction 的数据已经开始到来，将 PID 添加到 abortedProducerIds 中</span></div><div class="line">                consumeAbortedTransactionsUpTo(currentBatch.lastOffset());</div><div class="line"></div><div class="line">                <span class="keyword">long</span> producerId = currentBatch.producerId();</div><div class="line">                <span class="keyword">if</span> (containsAbortMarker(currentBatch)) &#123;</div><div class="line">                    abortedProducerIds.remove(producerId); <span class="comment">//note: 这个 PID（当前事务）涉及到的数据已经处理完</span></div><div class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isBatchAborted(currentBatch)) &#123; <span class="comment">//note: 丢弃这个数据</span></div><div class="line">                    log.debug(<span class="string">"Skipping aborted record batch from partition &#123;&#125; with producerId &#123;&#125; and "</span> +</div><div class="line">                                  <span class="string">"offsets &#123;&#125; to &#123;&#125;"</span>,</div><div class="line">                              partition, producerId, currentBatch.baseOffset(), currentBatch.lastOffset());</div><div class="line">                    nextFetchOffset = currentBatch.nextOffset();</div><div class="line">                    <span class="keyword">continue</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            records = currentBatch.streamingIterator(decompressionBufferSupplier);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            Record record = records.next();</div><div class="line">            <span class="comment">// skip any records out of range</span></div><div class="line">            <span class="keyword">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</div><div class="line">                <span class="comment">// we only do validation when the message should not be skipped.</span></div><div class="line">                maybeEnsureValid(record);</div><div class="line"></div><div class="line">                <span class="comment">// control records are not returned to the user</span></div><div class="line">                <span class="keyword">if</span> (!currentBatch.isControlBatch()) &#123; <span class="comment">//note: 过滤掉 marker 数据</span></div><div class="line">                    <span class="keyword">return</span> record;</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    <span class="comment">// Increment the next fetch offset when we skip a control batch.</span></div><div class="line">                    nextFetchOffset = record.offset() + <span class="number">1</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Consumer-消费数据时，其顺序如何保证"><a href="#Consumer-消费数据时，其顺序如何保证" class="headerlink" title="Consumer 消费数据时，其顺序如何保证"></a>Consumer 消费数据时，其顺序如何保证</h3><p>有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。</p>
<h3 id="如果-txn-id-长期不使用，server-端怎么处理？"><a href="#如果-txn-id-长期不使用，server-端怎么处理？" class="headerlink" title="如果 txn.id 长期不使用，server 端怎么处理？"></a>如果 txn.id 长期不使用，server 端怎么处理？</h3><p>Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 <code>transaction.timeout.ms</code>，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间（参数是 <code>transaction.timeout.ms</code>，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。</p>
<p>上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，Server 端提供了一个 <code>transaction.id.expiration.ms</code> 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，那么 TransactionCoordinator 将会使这个 txn.id 过期。</p>
<h3 id="PID-Snapshot-是做什么的？用来解决什么问题？"><a href="#PID-Snapshot-是做什么的？用来解决什么问题？" class="headerlink" title="PID Snapshot 是做什么的？用来解决什么问题？"></a>PID Snapshot 是做什么的？用来解决什么问题？</h3><p>对于每个 Topic-Partition，Broker 都会在内存中维护其 PID 与 sequence number（最后成功写入的 msg 的 sequence number）的对应关系（这个在上面幂等性文章应讲述过，主要是为了不丢补充的实现）。</p>
<p>Broker 重启时，如果想恢复上面的状态信息，那么它读取所有的 log 文件。相比于之下，定期对这个 state 信息做 checkpoint（Snapshot），明显收益是非常大的，此时如果 Broker 重启，只需要读取最近一个 Snapshot 文件，之后的数据再从 log 文件中恢复即可。</p>
<p>这个 PID Snapshot 样式如 00000000000235947656.snapshot，以 <code>.snapshot</code> 作为后缀，其数据格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[matt@XXX-35 app.matt_test_transaction_json_3-2]$ /usr/<span class="built_in">local</span>/java18/bin/java -Djava.ext.dirs=/XXX/kafka/libs kafka.tools.DumpLogSegments --files 00000000000235947656.snapshot</div><div class="line">Dumping 00000000000235947656.snapshot</div><div class="line">producerId: 2000 producerEpoch: 1 coordinatorEpoch: 4 currentTxnFirstOffset: None firstSequence: 95769510 lastSequence: 95769511 lastOffset: 235947654 offsetDelta: 1 timestamp: 1541325156503</div><div class="line">producerId: 3000 producerEpoch: 5 coordinatorEpoch: 6 currentTxnFirstOffset: None firstSequence: 91669662 lastSequence: 91669666 lastOffset: 235947651 offsetDelta: 4 timestamp: 1541325156454</div></pre></td></tr></table></figure>
<p>在实际的使用中，这个 snapshot 文件一般只会保存最近的两个文件。</p>
<h3 id="中间流程故障如何恢复"><a href="#中间流程故障如何恢复" class="headerlink" title="中间流程故障如何恢复"></a>中间流程故障如何恢复</h3><p>对于上面所讲述的一个事务操作流程，实际生产环境中，任何一个地方都有可能出现的失败：</p>
<ol>
<li>Producer 在发送 <code>beginTransaction()</code> 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li>
<li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li>
<li>Producer 发送 <code>commitTransaction()</code> 时出现 timeout 或者错误：Producer 应该重试这个请求；</li>
<li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li>
</ol>
<p>陆陆续续写了几天，终于把这篇文章总结完了。</p>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="external">Idempotent Producer</a>；</li>
<li><a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="external">Exactly-once Semantics in Apache Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka" target="_blank" rel="external">Transactional Messaging in Kafka</a>；</li>
<li><a href="https://www.confluent.io/blog/transactions-apache-kafka/" target="_blank" rel="external">Transactions in Apache Kafka</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempo
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 事务性之幂等性实现</title>
    <link href="http://matt33.com/2018/10/24/kafka-idempotent/"/>
    <id>http://matt33.com/2018/10/24/kafka-idempotent/</id>
    <published>2018-10-24T06:11:25.000Z</published>
    <updated>2018-11-04T12:38:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 Kafka 事务性系列的文章我们只重点关注前两种层面上的事务性，与 Kafka Streams 相关的内容暂时不做讨论。社区从开始讨论事务性，前后持续近半年时间，相关的设计文档有六十几页（参考 <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>）。事务性这部分的实现也是非常复杂的，之前 Producer 端的代码实现其实是非常简单的，增加事务性的逻辑之后，这部分代码复杂度提高了很多，本篇及后面几篇关于事务性的文章会以 2.0.0 版的代码实现为例，对这部分做了一下分析，计划分为五篇文章：</p>
<ol>
<li>第一篇：Kafka 幂等性实现；</li>
<li>第二篇：Kafka 事务性实现；</li>
<li>第三篇：Kafka 事务性相关处理请求在 Server 端如何处理及其实现细节；</li>
<li>第四篇：关于 Kafka 事务性实现的一些思考，也会简单介绍一下 RocketMQ 事务性的实现，做一下对比；</li>
<li>第五篇：Flink + Kafka 如何实现 Exactly Once；</li>
</ol>
<p>这篇是 Kafka 事务性系列的第一篇文章，主要讲述幂等性实现的整体流程，幂等性的实现相对于事务性的实现简单很多，也是事务性实现的基础。</p>
<h2 id="Producer-幂等性"><a href="#Producer-幂等性" class="headerlink" title="Producer 幂等性"></a>Producer 幂等性</h2><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p>
<ul>
<li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li>
<li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li>
</ul>
<p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p>
<h2 id="幂等性示例"><a href="#幂等性示例" class="headerlink" title="幂等性示例"></a>幂等性示例</h2><p>Producer 使用幂等性的示例非常简单，与正常情况下 Producer 使用相比变化不大，只需要把 Producer 的配置 enable.idempotence 设置为 true 即可，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Properties props = <span class="keyword">new</span> Properties();</div><div class="line">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class="string">"true"</span>);</div><div class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>); <span class="comment">// 当 enable.idempotence 为 true，这里默认为 all</span></div><div class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</div><div class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line"></div><div class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</div><div class="line"></div><div class="line">producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"test"</span>);</div></pre></td></tr></table></figure>
<p>Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好。</p>
<h2 id="幂等性要解决的问题"><a href="#幂等性要解决的问题" class="headerlink" title="幂等性要解决的问题"></a>幂等性要解决的问题</h2><p>在看 Producer 是如何实现幂等性之前，首先先考虑一个问题：<strong>幂等性是来解决什么问题的？</strong> 在 0.11.0 之前，Kafka 通过 Producer 端和 Server 端的相关配置可以做到<strong>数据不丢</strong>，也就是 at least once，但是在一些情况下，可能会导致数据重复，比如：网络请求延迟等导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。</p>
<p>对于大多数应用而言，数据保证不丢是可以满足其需求的，但是对于一些其他的应用场景（比如支付数据等），它们是要求精确计数的，这时候如果上游数据有重复，下游应用只能在消费数据时进行相应的去重操作，应用在去重时，最常用的手段就是根据唯一 id 键做 check 去重。</p>
<p>在这种场景下，因为上游生产导致的数据重复问题，会导致所有有精确计数需求的下游应用都需要做这种复杂的、重复的去重处理。试想一下：如果在发送时，系统就能保证 exactly once，这对下游将是多么大的解脱。这就是幂等性要解决的问题，主要是解决数据重复的问题，正如前面所述，数据重复问题，通用的解决方案就是加唯一 id，然后根据 id 判断数据是否重复，Producer 的幂等性也是这样实现的，这一小节就让我们看下 Kafka 的 Producer 如何保证数据的 exactly once 的。</p>
<h2 id="幂等性的实现原理"><a href="#幂等性的实现原理" class="headerlink" title="幂等性的实现原理"></a>幂等性的实现原理</h2><p>在讲述幂等性处理流程之前，先看下 Producer 是如何来保证幂等性的，正如前面所述，幂等性要解决的问题是：Producer 设置 at least once 时，由于异常触发重试机制导致数据重复，幂等性的目的就是为了解决这个数据重复的问题，简单来说就是：</p>
<p><strong>at least once + 幂等 = exactly once</strong></p>
<p>通过在 al least once 的基础上加上 幂等性来坐到 exactly once，当然这个层面的 exactly once 是有限制的，比如它会要求单会话内有效或者跨会话使用事务性有效等。这里我们先分析最简单的情况，那就是在单会话内如何做到幂等性，进而保证 exactly once。</p>
<p>要做到幂等性，要解决下面的问题：</p>
<ol>
<li>系统需要有能力鉴别一条数据到底是不是重复的数据？常用的手段是通过 <strong>唯一键/唯一 id</strong> 来判断，这时候系统一般是需要缓存已经处理的唯一键记录，这样才能更有效率地判断一条数据是不是重复；</li>
<li>唯一键应该选择什么粒度？对于分布式存储系统来说，肯定不能用全局唯一键（全局是针对集群级别），核心的解决思路依然是 <strong>分而治之</strong>，数据密集型系统为了实现分布式都是有分区概念的，而分区之间是有相应的隔离，对于 Kafka 而言，这里的解决方案就是在分区的维度上去做，重复数据的判断让 partition 的 leader 去判断处理，前提是 Produce 请求需要把唯一键值告诉 leader；</li>
<li>分区粒度实现唯一键会不会有其他问题？这里需要考虑的问题是当一个 Partition 有来自多个 client 写入的情况，这些 client 之间是很难做到使用同一个唯一键（一个是它们之间很难做到唯一键的实时感知，另一个是这样实现是否有必要）。而如果系统在实现时做到了  <strong>client + partition</strong> 粒度，这样实现的好处是每个 client 都是完全独立的（它们之间不需要有任何的联系，这是非常大的优点），只是在 Server 端对不同的 client 做好相应的区分即可，当然同一个 client 在处理多个 Topic-Partition 时是完全可以使用同一个 PID 的。</li>
</ol>
<p>有了上面的分析（都是个人见解，如果有误，欢迎指教），就不难理解 Producer 幂等性的实现原理，Kafka Producer 在实现时有以下两个重要机制：</p>
<ol>
<li>PID（Producer ID），用来标识每个 producer client；</li>
<li>sequence numbers，client 发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复。</li>
</ol>
<p>下面详细讲述这两个实现机制。</p>
<h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>每个 Producer 在初始化时都会被分配一个唯一的 PID，这个 PID 对应用是透明的，完全没有暴露给用户。对于一个给定的 PID，sequence number 将会从0开始自增，每个 Topic-Partition 都会有一个独立的 sequence number。Producer 在发送数据时，将会给每条 msg 标识一个 sequence number，Server 也就是通过这个来验证数据是否重复。这里的 PID 是全局唯一的，Producer 故障后重新启动后会被分配一个新的 PID，这也是幂等性无法做到跨会话的一个原因。</p>
<h4 id="Producer-PID-申请"><a href="#Producer-PID-申请" class="headerlink" title="Producer PID 申请"></a>Producer PID 申请</h4><p>这里看下 PID 在 Server 端是如何分配的？Client 通过向 Server 发送一个 InitProducerIdRequest 请求获取 PID（幂等性时，是选择一台连接数最少的 Broker 发送这个请求），这里看下 Server 端是如何处理这个请求的？KafkaApis 中 <code>handleInitProducerIdRequest()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleInitProducerIdRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> initProducerIdRequest = request.body[<span class="type">InitProducerIdRequest</span>]</div><div class="line">  <span class="keyword">val</span> transactionalId = initProducerIdRequest.transactionalId</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (transactionalId != <span class="literal">null</span>) &#123; <span class="comment">//note: 设置 txn.id 时，验证对 txn.id 的权限</span></div><div class="line">    <span class="keyword">if</span> (!authorize(request.session, <span class="type">Write</span>, <span class="type">Resource</span>(<span class="type">TransactionalId</span>, transactionalId, <span class="type">LITERAL</span>))) &#123;</div><div class="line">      sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">TRANSACTIONAL_ID_AUTHORIZATION_FAILED</span>.exception)</div><div class="line">      <span class="keyword">return</span></div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!authorize(request.session, <span class="type">IdempotentWrite</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123; <span class="comment">//note: 没有设置 txn.id 时，验证对集群是否有幂等性权限</span></div><div class="line">    sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.exception)</div><div class="line">    <span class="keyword">return</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(result: <span class="type">InitProducerIdResult</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createResponse</span></span>(requestThrottleMs: <span class="type">Int</span>): <span class="type">AbstractResponse</span> = &#123;</div><div class="line">      <span class="keyword">val</span> responseBody = <span class="keyword">new</span> <span class="type">InitProducerIdResponse</span>(requestThrottleMs, result.error, result.producerId, result.producerEpoch)</div><div class="line">      trace(<span class="string">s"Completed <span class="subst">$transactionalId</span>'s InitProducerIdRequest with result <span class="subst">$result</span> from client <span class="subst">$&#123;request.header.clientId&#125;</span>."</span>)</div><div class="line">      responseBody</div><div class="line">    &#125;</div><div class="line">    sendResponseMaybeThrottle(request, createResponse)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 生成相应的了 pid，返回给 producer</span></div><div class="line">  txnCoordinator.handleInitProducerId(transactionalId, initProducerIdRequest.transactionTimeoutMs, sendResponseCallback)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里实际上是调用了 TransactionCoordinator （Broker 在启动 server 服务时都会初始化这个实例）的 <code>handleInitProducerId()</code> 方法做了相应的处理，其实现如下（这里只关注幂等性的处理）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleInitProducerId</span></span>(transactionalId: <span class="type">String</span>,</div><div class="line">                         transactionTimeoutMs: <span class="type">Int</span>,</div><div class="line">                         responseCallback: <span class="type">InitProducerIdCallback</span>): <span class="type">Unit</span> = &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (transactionalId == <span class="literal">null</span>) &#123; <span class="comment">//note: 只设置幂等性时，直接分配 pid 并返回</span></div><div class="line">    <span class="comment">// if the transactional id is null, then always blindly accept the request</span></div><div class="line">    <span class="comment">// and return a new producerId from the producerId manager</span></div><div class="line">    <span class="keyword">val</span> producerId = producerIdManager.generateProducerId()</div><div class="line">    responseCallback(<span class="type">InitProducerIdResult</span>(producerId, producerEpoch = <span class="number">0</span>, <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Server 在给一个 client 初始化 PID 时，实际上是通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID。</p>
<h4 id="Server-PID-管理"><a href="#Server-PID-管理" class="headerlink" title="Server PID 管理"></a>Server PID 管理</h4><p>如前面所述，在幂等性的情况下，直接通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID，其中 ProducerIdManager 是在 TransactionCoordinator 对象初始化时初始化的，这个对象主要是用来管理 PID 信息：</p>
<ul>
<li>在本地的 PID 端用完了或者处于新建状态时，申请 PID 段（默认情况下，每次申请 1000 个 PID）；</li>
<li>TransactionCoordinator 对象通过 <code>generateProducerId()</code> 方法获取下一个可以使用的 PID；</li>
</ul>
<p><strong>PID 端申请是向 ZooKeeper 申请</strong>，zk 中有一个 <code>/latest_producer_id_block</code> 节点，每个 Broker 向 zk 申请一个 PID 段后，都会把自己申请的 PID 段信息写入到这个节点，这样当其他 Broker 再申请 PID 段时，会首先读写这个节点的信息，然后根据 block_end 选择一个 PID 段，最后再把信息写会到 zk 的这个节点，这个节点信息格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="attr">"version"</span>:<span class="number">1</span>,<span class="attr">"broker"</span>:<span class="number">35</span>,<span class="attr">"block_start"</span>:<span class="string">"4000"</span>,<span class="attr">"block_end"</span>:<span class="string">"4999"</span>&#125;</div></pre></td></tr></table></figure>
<p>ProducerIdManager 向 zk 申请 PID 段的方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getNewProducerIdBlock</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">var</span> zkWriteComplete = <span class="literal">false</span></div><div class="line">  <span class="keyword">while</span> (!zkWriteComplete) &#123; <span class="comment">//note: 直到从 zk 拿取到分配的 PID 段</span></div><div class="line">    <span class="comment">// refresh current producerId block from zookeeper again</span></div><div class="line">    <span class="keyword">val</span> (dataOpt, zkVersion) = zkClient.getDataAndVersion(<span class="type">ProducerIdBlockZNode</span>.path)</div><div class="line"></div><div class="line">    <span class="comment">// generate the new producerId block</span></div><div class="line">    currentProducerIdBlock = dataOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(data) =&gt;</div><div class="line">        <span class="comment">//note: 从 zk 获取当前最新的 pid 信息，如果后面更新失败，这里也会重新从 zk 获取</span></div><div class="line">        <span class="keyword">val</span> currProducerIdBlock = <span class="type">ProducerIdManager</span>.parseProducerIdBlockData(data)</div><div class="line">        debug(<span class="string">s"Read current producerId block <span class="subst">$currProducerIdBlock</span>, Zk path version <span class="subst">$zkVersion</span>"</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (currProducerIdBlock.blockEndId &gt; <span class="type">Long</span>.<span class="type">MaxValue</span> - <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span>) &#123;<span class="comment">//note: 不足以分配1000个 PID</span></div><div class="line">          <span class="comment">// we have exhausted all producerIds (wow!), treat it as a fatal error</span></div><div class="line">          <span class="comment">//note: 当 PID 分配超过限制时，直接报错了（每秒分配1个，够用2百亿年了）</span></div><div class="line">          fatal(<span class="string">s"Exhausted all producerIds as the next block's end producerId is will has exceeded long type limit (current block end producerId is <span class="subst">$&#123;currProducerIdBlock.blockEndId&#125;</span>)"</span>)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Have exhausted all producerIds."</span>)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="type">ProducerIdBlock</span>(brokerId, currProducerIdBlock.blockEndId + <span class="number">1</span>L, currProducerIdBlock.blockEndId + <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span>)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//note: 该节点还不存在，第一次初始化</span></div><div class="line">        debug(<span class="string">s"There is no producerId block yet (Zk path version <span class="subst">$zkVersion</span>), creating the first block"</span>)</div><div class="line">        <span class="type">ProducerIdBlock</span>(brokerId, <span class="number">0</span>L, <span class="type">ProducerIdManager</span>.<span class="type">PidBlockSize</span> - <span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> newProducerIdBlockData = <span class="type">ProducerIdManager</span>.generateProducerIdBlockJson(currentProducerIdBlock)</div><div class="line"></div><div class="line">    <span class="comment">// try to write the new producerId block into zookeeper</span></div><div class="line">    <span class="comment">//note: 将新的 pid 信息写入到 zk，如果写入失败（写入之前会比对 zkVersion，如果这个有变动，证明这期间有别的 Broker 在操作，那么写入失败），重新申请</span></div><div class="line">    <span class="keyword">val</span> (succeeded, version) = zkClient.conditionalUpdatePath(<span class="type">ProducerIdBlockZNode</span>.path,</div><div class="line">      newProducerIdBlockData, zkVersion, <span class="type">Some</span>(checkProducerIdBlockZkData))</div><div class="line">    zkWriteComplete = succeeded</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (zkWriteComplete)</div><div class="line">      info(<span class="string">s"Acquired new producerId block <span class="subst">$currentProducerIdBlock</span> by writing to Zk with path version <span class="subst">$version</span>"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ProducerIdManager 申请 PID 段的流程如下：</p>
<ol>
<li>先从 zk 的 <code>/latest_producer_id_block</code> 节点读取最新已经分配的 PID 段信息；</li>
<li>如果该节点不存在，直接从 0 开始分配，选择 0~1000 的 PID 段（ProducerIdManager 的 PidBlockSize 默认为 1000，即是每次申请的 PID 段大小）；</li>
<li>如果该节点存在，读取其中数据，根据 block_end 选择 <block_end+1, block_end+1000=""> 这个 PID 段（如果 PID 段超过 Long 类型的最大值，这里会直接返回一个异常）；</block_end+1,></li>
<li>在选择了相应的 PID 段后，将这个 PID 段信息写回到 zk 的这个节点中，如果写入成功，那么 PID 段就证明申请成功，如果写入失败（写入时会判断当前节点的 zkVersion 是否与步骤1获取的 zkVersion 相同，如果相同，那么可以成功写入，否则写入就会失败，证明这个节点被修改过），证明此时可能其他的 Broker 已经更新了这个节点（当前的 PID 段可能已经被其他 Broker 申请），那么从步骤 1 重新开始，直到写入成功。</li>
</ol>
<p>明白了 ProducerIdManager 如何申请 PID 段之后，再看 <code>generateProducerId()</code> 这个方法就简单很多了，这个方法在每次调用时，都会更新 nextProducerId 值（下一次可以使用 PID 值），如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateProducerId</span></span>(): <span class="type">Long</span> = &#123;</div><div class="line">  <span class="keyword">this</span> synchronized &#123;</div><div class="line">    <span class="comment">// grab a new block of producerIds if this block has been exhausted</span></div><div class="line">    <span class="keyword">if</span> (nextProducerId &gt; currentProducerIdBlock.blockEndId) &#123;</div><div class="line">      <span class="comment">//note: 如果分配的 pid 用完了，重新再向 zk 申请一批</span></div><div class="line">      getNewProducerIdBlock()</div><div class="line">      nextProducerId = currentProducerIdBlock.blockStartId + <span class="number">1</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      nextProducerId += <span class="number">1</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    nextProducerId - <span class="number">1</span> <span class="comment">//note: 返回当前分配的 pid</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里就是 Producer PID 如何申请（事务性情况下 PID 的申请会复杂一些，下篇文章再讲述）以及 Server 端如何管理 PID 的。</p>
<h3 id="sequence-numbers"><a href="#sequence-numbers" class="headerlink" title="sequence numbers"></a>sequence numbers</h3><p>再有了 PID 之后，在 PID + Topic-Partition 级别上添加一个 sequence numbers 信息，就可以实现 Producer 的幂等性了。ProducerBatch 也提供了一个 <code>setProducerState()</code> 方法，它可以给一个 batch 添加一些 meta 信息（pid、baseSequence、isTransactional），这些信息是会伴随着 ProduceRequest 发到 Server 端，Server 端也正是通过这些 meta 来做相应的判断，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// ProducerBatch</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(ProducerIdAndEpoch producerIdAndEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</div><div class="line">    recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// MemoryRecordsBuilder</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(<span class="keyword">long</span> producerId, <span class="keyword">short</span> producerEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (isClosed()) &#123;</div><div class="line">        <span class="comment">// Sequence numbers are assigned when the batch is closed while the accumulator is being drained.</span></div><div class="line">        <span class="comment">// If the resulting ProduceRequest to the partition leader failed for a retriable error, the batch will</span></div><div class="line">        <span class="comment">// be re queued. In this case, we should not attempt to set the state again, since changing the producerId and sequence</span></div><div class="line">        <span class="comment">// once a batch has been sent to the broker risks introducing duplicates.</span></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Trying to set producer state of an already closed batch. This indicates a bug on the client."</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">this</span>.producerId = producerId;</div><div class="line">    <span class="keyword">this</span>.producerEpoch = producerEpoch;</div><div class="line">    <span class="keyword">this</span>.baseSequence = baseSequence;</div><div class="line">    <span class="keyword">this</span>.isTransactional = isTransactional;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="幂等性实现整体流程"><a href="#幂等性实现整体流程" class="headerlink" title="幂等性实现整体流程"></a>幂等性实现整体流程</h2><p>在前面讲述完 Kafka 幂等性的两个实现机制（PID+sequence numbers）之后，这里详细讲述一下，幂等性时其整体的处理流程，主要讲述幂等性相关的内容，其他的部分会简单介绍（可以参考前面【Kafka 源码分析系列文章】了解 Producer 端处理流程以及 Server 端关于 ProduceRequest 请求的处理流程），其流程如下图所示：</p>
<p><img src="/images/kafka/kafka-idemoptent.png" alt="Producer 幂等性时处理流程"></p>
<p>这个图只展示了幂等性情况下，Producer 的大概流程，很多部分在前面的文章中做过分析，本文不再讲述，这里重点关注与幂等性相关的内容（事务性实现更加复杂，后面的文章再讲述），首先 KafkaProducer 在初始化时会初始化一个 TransactionManager 实例，它的作用有以下几个部分：</p>
<ol>
<li>记录本地的事务状态（事务性时必须）；</li>
<li>记录一些状态信息以保证幂等性，比如：每个 topic-partition 对应的下一个 sequence numbers 和 last acked batch（最近一个已经确认的 batch）的最大的 sequence number 等；</li>
<li>记录 ProducerIdAndEpoch 信息（PID 信息）。</li>
</ol>
<h3 id="Client-幂等性时发送流程"><a href="#Client-幂等性时发送流程" class="headerlink" title="Client 幂等性时发送流程"></a>Client 幂等性时发送流程</h3><p>如前面图中所示，幂等性时，Producer 的发送流程如下：</p>
<ol>
<li>应用通过 KafkaProducer 的 <code>send()</code> 方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的；</li>
<li>Producer 后台发送线程 Sender，在 <code>run()</code> 方法中，会先根据 TransactionManager 的 <code>shouldResetProducerStateAfterResolvingSequences()</code> 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有 topic-partition 的 batch 重试多次失败最后因为超时而被移除，这时 sequence number 将无法做到连续，因为 sequence number 有部分已经分配出去，这时系统依赖自身的机制无法继续进行下去（因为幂等性是要保证不丢不重的），相当于程序遇到了一个 fatal 异常，PID 会进行重置，TransactionManager 相关的缓存信息被清空（Producer 不会重启），只是保存状态信息的 TransactionManager 做了 <code>clear+new</code> 操作，遇到这个问题时是无法保证 exactly once 的（有数据已经发送失败了，并且超过了重试次数）；</li>
<li>Sender 线程通过 <code>maybeWaitForProducerId()</code> 方法判断是否需要申请 PID，如果需要的话，这里会阻塞直到获取到相应的 PID 信息；</li>
<li>Sender 线程通过 <code>sendProducerData()</code> 方法发送数据，整体流程与之前的 Producer 流程相似，不同的地方是在 RecordAccumulator 的 <code>drain()</code> 方法中，在加了幂等性之后，<code>drain()</code> 方法多了如下几步判断：<ol>
<li>常规的判断：判断这个 topic-partition 是否可以继续发送（如果出现前面2中的情况是不允许发送的）、判断 PID 是否有效、如果这个 batch 是重试的 batch，那么需要判断这个 batch 之前是否还有 batch 没有发送完成，如果有，这里会先跳过这个 Topic-Partition 的发送，直到前面的 batch 发送完成，<strong>最坏情况下，这个 Topic-Partition 的 in-flight request 将会减少到1</strong>（这个涉及也是考虑到 server 端的一个设置，文章下面会详细分析）；</li>
<li>如果这个 ProducerBatch 还没有这个相应的 PID 和 sequence number 信息，会在这里进行相应的设置；</li>
</ol>
</li>
<li>最后 Sender 线程再调用 <code>sendProduceRequests()</code> 方法发送 ProduceRequest 请求，后面的就跟之前正常的流程保持一致了。</li>
</ol>
<p>这里看下几个关键方法的实现，首先是 Sender 线程获取 PID 信息的方法  <code>maybeWaitForProducerId()</code> ，其实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 等待直到 Producer 获取到相应的 PID 和 epoch 信息</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">maybeWaitForProducerId</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">while</span> (!transactionManager.hasProducerId() &amp;&amp; !transactionManager.hasError()) &#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            Node node = awaitLeastLoadedNodeReady(requestTimeoutMs); <span class="comment">//note: 选取 node（本地连接数最少的 node）</span></div><div class="line">            <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</div><div class="line">                ClientResponse response = sendAndAwaitInitProducerIdRequest(node); <span class="comment">//note: 发送 InitPidRequest</span></div><div class="line">                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();</div><div class="line">                Errors error = initProducerIdResponse.error();</div><div class="line">                <span class="keyword">if</span> (error == Errors.NONE) &#123; <span class="comment">//note: 更新 Producer 的 PID 和 epoch 信息</span></div><div class="line">                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">new</span> ProducerIdAndEpoch(</div><div class="line">                            initProducerIdResponse.producerId(), initProducerIdResponse.epoch());</div><div class="line">                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);</div><div class="line">                    <span class="keyword">return</span>;</div><div class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error.exception() <span class="keyword">instanceof</span> RetriableException) &#123;</div><div class="line">                    log.debug(<span class="string">"Retriable error from InitProducerId response"</span>, error.message());</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    transactionManager.transitionToFatalError(error.exception());</div><div class="line">                    <span class="keyword">break</span>;</div><div class="line">                &#125;</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                log.debug(<span class="string">"Could not find an available broker to send InitProducerIdRequest to. "</span> +</div><div class="line">                        <span class="string">"We will back off and try again."</span>);</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (UnsupportedVersionException e) &#123;</div><div class="line">            transactionManager.transitionToFatalError(e);</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            log.debug(<span class="string">"Broker &#123;&#125; disconnected while awaiting InitProducerId response"</span>, e);</div><div class="line">        &#125;</div><div class="line">        log.trace(<span class="string">"Retry InitProducerIdRequest in &#123;&#125;ms."</span>, retryBackoffMs);</div><div class="line">        time.sleep(retryBackoffMs);</div><div class="line">        metadata.requestUpdate();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>再看下 RecordAccumulator 的 <code>drain()</code> 方法，重点需要关注的是关于幂等性和事务性相关的处理，具体如下所示，这里面关于事务性相关的判断在上面的流程中已经讲述。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Drain all the data for the given nodes and collate them into a list of batches that will fit within the specified</div><div class="line"> * size on a per-node basis. This method attempts to avoid choosing the same topic-node over and over.</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> cluster The current cluster metadata</div><div class="line"> * <span class="doctag">@param</span> nodes The list of node to drain</div><div class="line"> * <span class="doctag">@param</span> maxSize The maximum number of bytes to drain</div><div class="line"> * <span class="doctag">@param</span> now The current unix time in milliseconds</div><div class="line"> * <span class="doctag">@return</span> A list of &#123;<span class="doctag">@link</span> ProducerBatch&#125; for each node specified with total size less than the requested maxSize.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(Cluster cluster,</div><div class="line">                                               Set&lt;Node&gt; nodes,</div><div class="line">                                               <span class="keyword">int</span> maxSize,</div><div class="line">                                               <span class="keyword">long</span> now) &#123;</div><div class="line">    <span class="keyword">if</span> (nodes.isEmpty())</div><div class="line">        <span class="keyword">return</span> Collections.emptyMap();</div><div class="line"></div><div class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (Node node : nodes) &#123;</div><div class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</div><div class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</div><div class="line">        List&lt;ProducerBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        <span class="comment">/* to make starvation less likely this loop doesn't start at 0 */</span></div><div class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</div><div class="line">        <span class="keyword">do</span> &#123;</div><div class="line">            PartitionInfo part = parts.get(drainIndex);</div><div class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</div><div class="line">            <span class="comment">// Only proceed if the partition has no in-flight batches.</span></div><div class="line">            <span class="keyword">if</span> (!isMuted(tp, now)) &#123;</div><div class="line">                Deque&lt;ProducerBatch&gt; deque = getDeque(tp);</div><div class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) &#123;</div><div class="line">                    <span class="keyword">synchronized</span> (deque) &#123; <span class="comment">//note: 先判断有没有数据，然后后面真正处理时再加锁处理</span></div><div class="line">                        ProducerBatch first = deque.peekFirst();</div><div class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) &#123;</div><div class="line">                            <span class="keyword">boolean</span> backoff = first.attempts() &gt; <span class="number">0</span> &amp;&amp; first.waitedTimeMs(now) &lt; retryBackoffMs;</div><div class="line">                            <span class="comment">// Only drain the batch if it is not during backoff period.</span></div><div class="line">                            <span class="keyword">if</span> (!backoff) &#123;</div><div class="line">                                <span class="keyword">if</span> (size + first.estimatedSizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) &#123;</div><div class="line">                                    <span class="comment">// there is a rare case that a single batch size is larger than the request size due</span></div><div class="line">                                    <span class="comment">// to compression; in this case we will still eventually send this batch in a single</span></div><div class="line">                                    <span class="comment">// request</span></div><div class="line">                                    <span class="keyword">break</span>;</div><div class="line">                                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">null</span>;</div><div class="line">                                    <span class="keyword">boolean</span> isTransactional = <span class="keyword">false</span>;</div><div class="line">                                    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123; <span class="comment">//note: 幂等性或事务性时， 做一些检查判断</span></div><div class="line">                                        <span class="keyword">if</span> (!transactionManager.isSendToPartitionAllowed(tp))</div><div class="line">                                            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">                                        producerIdAndEpoch = transactionManager.producerIdAndEpoch();</div><div class="line">                                        <span class="keyword">if</span> (!producerIdAndEpoch.isValid()) <span class="comment">//note: pid 是否有效</span></div><div class="line">                                            <span class="comment">// we cannot send the batch until we have refreshed the producer id</span></div><div class="line">                                            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">                                        isTransactional = transactionManager.isTransactional();</div><div class="line"></div><div class="line">                                        <span class="keyword">if</span> (!first.hasSequence() &amp;&amp; transactionManager.hasUnresolvedSequence(first.topicPartition))</div><div class="line">                                            <span class="comment">//note: 当前这个 topic-partition 的数据出现过超时,不能发送,如果是新的 batch 数据直接跳过（没有 seq  number 信息）</span></div><div class="line">                                            <span class="comment">// Don't drain any new batches while the state of previous sequence numbers</span></div><div class="line">                                            <span class="comment">// is unknown. The previous batches would be unknown if they were aborted</span></div><div class="line">                                            <span class="comment">// on the client after being sent to the broker at least once.</span></div><div class="line">                                            <span class="keyword">break</span>;</div><div class="line"></div><div class="line">                                        <span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</div><div class="line">                                        <span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</div><div class="line">                                                &amp;&amp; first.baseSequence() != firstInFlightSequence)</div><div class="line">                                            <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></div><div class="line">                                            <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></div><div class="line">                                            <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></div><div class="line">                                            <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></div><div class="line">                                            <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></div><div class="line">                                            <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></div><div class="line">                                            <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></div><div class="line">                                            <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></div><div class="line">                                            <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></div><div class="line">                                            <span class="comment">// in flight request count to 1.</span></div><div class="line">                                            <span class="keyword">break</span>;</div><div class="line">                                    &#125;</div><div class="line"></div><div class="line">                                    ProducerBatch batch = deque.pollFirst();</div><div class="line">                                    <span class="keyword">if</span> (producerIdAndEpoch != <span class="keyword">null</span> &amp;&amp; !batch.hasSequence()) &#123;<span class="comment">//note: batch 的相关信息（seq id）是在这里设置的</span></div><div class="line">                                        <span class="comment">//note: 这个 batch 还没有 seq number 信息</span></div><div class="line">                                        <span class="comment">// If the batch already has an assigned sequence, then we should not change the producer id and</span></div><div class="line">                                        <span class="comment">// sequence number, since this may introduce duplicates. In particular,</span></div><div class="line">                                        <span class="comment">// the previous attempt may actually have been accepted, and if we change</span></div><div class="line">                                        <span class="comment">// the producer id and sequence here, this attempt will also be accepted,</span></div><div class="line">                                        <span class="comment">// causing a duplicate.</span></div><div class="line">                                        <span class="comment">//</span></div><div class="line">                                        <span class="comment">// Additionally, we update the next sequence number bound for the partition,</span></div><div class="line">                                        <span class="comment">// and also have the transaction manager track the batch so as to ensure</span></div><div class="line">                                        <span class="comment">// that sequence ordering is maintained even if we receive out of order</span></div><div class="line">                                        <span class="comment">// responses.</span></div><div class="line">                                        <span class="comment">//note: 给这个 batch 设置相应的 pid、seq id 等信息</span></div><div class="line">                                        batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);</div><div class="line">                                        transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount); <span class="comment">//note: 增加 partition 对应的下一个 seq id 值</span></div><div class="line">                                        log.debug(<span class="string">"Assigned producerId &#123;&#125; and producerEpoch &#123;&#125; to batch with base sequence "</span> +</div><div class="line">                                                        <span class="string">"&#123;&#125; being sent to partition &#123;&#125;"</span>, producerIdAndEpoch.producerId,</div><div class="line">                                                producerIdAndEpoch.epoch, batch.baseSequence(), tp);</div><div class="line"></div><div class="line">                                        transactionManager.addInFlightBatch(batch);</div><div class="line">                                    &#125;</div><div class="line">                                    batch.close();</div><div class="line">                                    size += batch.records().sizeInBytes();</div><div class="line">                                    ready.add(batch);</div><div class="line">                                    batch.drained(now);</div><div class="line">                                &#125;</div><div class="line">                            &#125;</div><div class="line">                        &#125;</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</div><div class="line">        &#125; <span class="keyword">while</span> (start != drainIndex);</div><div class="line">        batches.put(node.id(), ready);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> batches;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="幂等性时-Server-端如何处理-ProduceRequest-请求"><a href="#幂等性时-Server-端如何处理-ProduceRequest-请求" class="headerlink" title="幂等性时 Server 端如何处理 ProduceRequest 请求"></a>幂等性时 Server 端如何处理 ProduceRequest 请求</h3><p>如前面途中所示，当 Broker 收到 ProduceRequest 请求之后，会通过 <code>handleProduceRequest()</code> 做相应的处理，其处理流程如下（这里只讲述关于幂等性相关的内容）：</p>
<ol>
<li>如果请求是事务请求，检查是否对 TXN.id 有 Write 权限，没有的话返回 TRANSACTIONAL_ID_AUTHORIZATION_FAILED；</li>
<li>如果请求设置了幂等性，检查是否对 ClusterResource 有 IdempotentWrite 权限，没有的话返回 CLUSTER_AUTHORIZATION_FAILED；</li>
<li>验证对 topic 是否有 Write 权限以及 Topic 是否存在，否则返回 TOPIC_AUTHORIZATION_FAILED 或 UNKNOWN_TOPIC_OR_PARTITION 异常；</li>
<li>检查是否有 PID 信息，没有的话走正常的写入流程；</li>
<li>LOG 对象会在 <code>analyzeAndValidateProducerState()</code> 方法先根据 batch 的 sequence number 信息检查这个 batch 是否重复（server 端会缓存 PID 对应这个 Topic-Partition 的最近5个 batch 信息），如果有重复，这里当做写入成功返回（不更新 LOG 对象中相应的状态信息，比如这个 replica 的 the end offset 等）；</li>
<li>有了 PID 信息，并且不是重复 batch 时，在更新 producer 信息时，会做以下校验：<ol>
<li>检查该 PID 是否已经缓存中存在（主要是在 ProducerStateManager 对象中检查）；</li>
<li>如果不存在，那么判断 sequence number 是否 从0 开始，是的话，在缓存中记录 PID 的 meta（PID，epoch， sequence number），并执行写入操作，否则返回 UnknownProducerIdException（PID 在 server 端已经过期或者这个 PID 写的数据都已经过期了，但是 Client 还在接着上次的 sequence number 发送数据）；</li>
<li>如果该 PID 存在，先检查 PID epoch 与 server 端记录的是否相同；</li>
<li>如果不同并且 sequence number 不从 0 开始，那么返回 OutOfOrderSequenceException 异常；</li>
<li>如果不同并且 sequence number 从 0 开始，那么正常写入；</li>
<li>如果相同，那么根据缓存中记录的最近一次 sequence number（currentLastSeq）检查是否为连续（会区分为 0、Int.MaxValue 等情况），不连续的情况下返回 OutOfOrderSequenceException 异常。</li>
</ol>
</li>
<li>下面与正常写入相同。</li>
</ol>
<p>幂等性时，Broker 在处理 ProduceRequest 请求时，多了一些校验操作，这里重点看一下其中一些重要实现，先看下 <code>analyzeAndValidateProducerState()</code> 方法的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">analyzeAndValidateProducerState</span></span>(records: <span class="type">MemoryRecords</span>, isFromClient: <span class="type">Boolean</span>): (mutable.<span class="type">Map</span>[<span class="type">Long</span>, <span class="type">ProducerAppendInfo</span>], <span class="type">List</span>[<span class="type">CompletedTxn</span>], <span class="type">Option</span>[<span class="type">BatchMetadata</span>]) = &#123;</div><div class="line">  <span class="keyword">val</span> updatedProducers = mutable.<span class="type">Map</span>.empty[<span class="type">Long</span>, <span class="type">ProducerAppendInfo</span>]</div><div class="line">  <span class="keyword">val</span> completedTxns = <span class="type">ListBuffer</span>.empty[<span class="type">CompletedTxn</span>]</div><div class="line">  <span class="keyword">for</span> (batch &lt;- records.batches.asScala <span class="keyword">if</span> batch.hasProducerId) &#123; <span class="comment">//note: 有 pid 时,才会做相应的判断</span></div><div class="line">    <span class="keyword">val</span> maybeLastEntry = producerStateManager.lastEntry(batch.producerId)</div><div class="line"></div><div class="line">    <span class="comment">// if this is a client produce request, there will be up to 5 batches which could have been duplicated.</span></div><div class="line">    <span class="comment">// If we find a duplicate, we return the metadata of the appended batch to the client.</span></div><div class="line">    <span class="keyword">if</span> (isFromClient) &#123;</div><div class="line">      maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach &#123; duplicate =&gt;</div><div class="line">        <span class="keyword">return</span> (updatedProducers, completedTxns.toList, <span class="type">Some</span>(duplicate)) <span class="comment">//note: 如果这个 batch 已经收到过，这里直接返回</span></div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> maybeCompletedTxn = updateProducers(batch, updatedProducers, isFromClient = isFromClient) <span class="comment">//note: 这里</span></div><div class="line">    maybeCompletedTxn.foreach(completedTxns += _)</div><div class="line">  &#125;</div><div class="line">  (updatedProducers, completedTxns.toList, <span class="type">None</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果这个 batch 有 PID 信息，会首先检查这个 batch 是否为重复的 batch 数据，其实现如下，batchMetadata 会缓存最新 5个 batch 的数据（如果超过5个，添加时会进行删除，这个也是幂等性要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5 的原因，与这个值的设置有关），根据 batchMetadata 缓存的 batch 数据来判断这个 batch 是否为重复的数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">findDuplicateBatch</span></span>(batch: <span class="type">RecordBatch</span>): <span class="type">Option</span>[<span class="type">BatchMetadata</span>] = &#123;</div><div class="line">  <span class="keyword">if</span> (batch.producerEpoch != producerEpoch)</div><div class="line">     <span class="type">None</span></div><div class="line">  <span class="keyword">else</span></div><div class="line">    batchWithSequenceRange(batch.baseSequence, batch.lastSequence)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// Return the batch metadata of the cached batch having the exact sequence range, if any.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchWithSequenceRange</span></span>(firstSeq: <span class="type">Int</span>, lastSeq: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">BatchMetadata</span>] = &#123;</div><div class="line">  <span class="keyword">val</span> duplicate = batchMetadata.filter &#123; metadata =&gt;</div><div class="line">    firstSeq == metadata.firstSeq &amp;&amp; lastSeq == metadata.lastSeq</div><div class="line">  &#125;</div><div class="line">  duplicate.headOption</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatchMetadata</span></span>(batch: <span class="type">BatchMetadata</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (batchMetadata.size == <span class="type">ProducerStateEntry</span>.<span class="type">NumBatchesToRetain</span>)</div><div class="line">    batchMetadata.dequeue() <span class="comment">//note: 只会保留最近 5 个 batch 的记录</span></div><div class="line">  batchMetadata.enqueue(batch) <span class="comment">//note: 添加到 batchMetadata 中记录，便于后续根据 seq id 判断是否重复</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果 batch 不是重复的数据，<code>analyzeAndValidateProducerState()</code> 会通过 <code>updateProducers()</code> 更新 producer 的相应记录，在更新的过程中，会做一步校验，校验方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查 seq number</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkSequence</span></span>(producerEpoch: <span class="type">Short</span>, appendFirstSeq: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (producerEpoch != updatedEntry.producerEpoch) &#123; <span class="comment">//note: epoch 不同时</span></div><div class="line">    <span class="keyword">if</span> (appendFirstSeq != <span class="number">0</span>) &#123; <span class="comment">//note: 此时要求 seq number 必须从0开始（如果不是的话，pid 可能是新建的或者 PID 在 Server 端已经过期）</span></div><div class="line">      <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 不是-1，证明时原来的 pid 过期了）</span></div><div class="line">      <span class="keyword">if</span> (updatedEntry.producerEpoch != <span class="type">RecordBatch</span>.<span class="type">NO_PRODUCER_EPOCH</span>) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Invalid sequence number for new epoch: <span class="subst">$producerEpoch</span> "</span> +</div><div class="line">          <span class="string">s"(request epoch), <span class="subst">$appendFirstSeq</span> (seq. number)"</span>)</div><div class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 为-1，证明 server 端 meta 新建的，PID 在 server 端已经过期，client 还在接着上次的 seq 发数据）</span></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownProducerIdException</span>(<span class="string">s"Found no record of producerId=<span class="subst">$producerId</span> on the broker. It is possible "</span> +</div><div class="line">          <span class="string">s"that the last message with t（）he producerId=<span class="subst">$producerId</span> has been removed due to hitting the retention limit."</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">val</span> currentLastSeq = <span class="keyword">if</span> (!updatedEntry.isEmpty)</div><div class="line">      updatedEntry.lastSeq</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (producerEpoch == currentEntry.producerEpoch)</div><div class="line">      currentEntry.lastSeq</div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="type">RecordBatch</span>.<span class="type">NO_SEQUENCE</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> (currentLastSeq == <span class="type">RecordBatch</span>.<span class="type">NO_SEQUENCE</span> &amp;&amp; appendFirstSeq != <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">//note: 此时期望的 seq number 是从 0 开始,因为 currentLastSeq 是 -1,也就意味着这个 pid 还没有写入过数据</span></div><div class="line">      <span class="comment">// the epoch was bumped by a control record, so we expect the sequence number to be reset</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Out of order sequence number for producerId <span class="subst">$producerId</span>: found <span class="subst">$appendFirstSeq</span> "</span> +</div><div class="line">        <span class="string">s"(incoming seq. number), but expected 0"</span>)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inSequence(currentLastSeq, appendFirstSeq)) &#123;</div><div class="line">      <span class="comment">//note: 判断是否连续</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OutOfOrderSequenceException</span>(<span class="string">s"Out of order sequence number for producerId <span class="subst">$producerId</span>: <span class="subst">$appendFirstSeq</span> "</span> +</div><div class="line">        <span class="string">s"(incoming seq. number), <span class="subst">$currentLastSeq</span> (current end sequence number)"</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其校验逻辑如前面流程中所述。</p>
<h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>这里主要思考两个问题：</p>
<ol>
<li>Producer 在设置幂等性时，为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5，如果设置大于 5（不考虑 Producer 端参数校验的报错），会带来什么后果？</li>
<li>Producer 在设置幂等性时，如果我们设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，那么是否可以保证有序，如果可以，是怎么做到的？</li>
</ol>
<p>先说一下结论，问题 1 的这个设置要求其实上面分析的时候已经讲述过了，主要跟 server 端只会缓存最近 5 个 batch 的机制有关；问题 2，即使 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，幂等性时依然可以做到有序，下面来详细分析一下这两个问题。</p>
<h3 id="为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5"><a href="#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5" class="headerlink" title="为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档，忘记在哪了），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p>
<p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（相当于client 狂发错误请求）。</p>
<p>那有没有更好的方案呢？我认为是有的，那就是对于 OutOfOrderSequenceException 异常，再进行细分，区分这个 sequence number 是大于 nextSeq （期望的下次 sequence number  值）还是小于 nextSeq，如果是小于，那么肯定是重复的数据。</p>
<h3 id="当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序"><a href="#当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序" class="headerlink" title="当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序"></a>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序</h3><p>先来分析一下，在什么情况下 Producer 会出现乱序的问题？没有幂等性时，乱序的问题是在重试时出现的，举个例子：client 依然发送了 6 个请求 1、2、3、4、5、6（它们分别对应了一个 batch），这 6 个请求只有 2-6 成功 ack 了，1 失败了，这时候需要重试，重试时就会把 batch 1 的数据添加到待发送的数据列队中），那么下次再发送时，batch 1 的数据将会被发送，这时候数据就已经出现了乱序，因为 batch 1 的数据已经晚于了 batch 2-6。</p>
<p>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 1 时，是可以解决这个为题，因为同时只允许一个请求正在发送，只有当前的请求发送完成（成功 ack 后），才能继续下一条请求的发送，类似单线程处理这种模式，每次请求发送时都会等待上次的完成，效率非常差，但是可以解决乱序的问题（当然这里有序只是针对单 client 情况，多 client 并发写是无法做到的）。</p>
<p>系统能提供的方案，基本上就是有序性与性能之间二选一，无法做到兼容，实际上系统出现请求重试的几率是很小的（一般都是网络问题触发的），可能连 0.1% 的时间都不到，但是就是为了这 0.1% 时间都不到的情况，应用需要牺牲性能问题来解决，在大数据场景下，我们是希望有更友好的方式来解决这个问题。简单来说，就是当出现重试时，max-in-flight-request 可以动态减少到 1，在正常情况下还是按 5 （5是举例说明）来处理，这有点类似于分布式系统 CAP 理论中关于 P 的考虑，当出现问题时，可以容忍性能变差，但是其他的情况下，我们希望的是能拥有原来的性能，而不是一刀切。令人高兴的，在 Kafka 2.0.0 版本中，如果 Producer 开始了幂等性，Kafka 是可以做到这一点的，如果不开启幂等性，是无法做到的，因为它的实现是依赖了 sequence number。</p>
<p>当请求出现重试时，batch 会重新添加到队列中，这时候是根据 sequence number 添加到队列的合适位置（有些 batch 如果还没有 sequence number，那么就保持其相对位置不变），也就是队列中排在这个 batch 前面的 batch，其 sequence number 都比这个 batch 的 sequence number 小，其实现如下，这个方法保证了在重试时，其 batch 会被放到合适的位置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Re-enqueue the given record batch in the accumulator to retry</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reenqueue</span><span class="params">(ProducerBatch batch, <span class="keyword">long</span> now)</span> </span>&#123;</div><div class="line">    batch.reenqueued(now); <span class="comment">//note: 重试,更新相应的 meta</span></div><div class="line">    Deque&lt;ProducerBatch&gt; deque = getOrCreateDeque(batch.topicPartition);</div><div class="line">    <span class="keyword">synchronized</span> (deque) &#123;</div><div class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>)</div><div class="line">            insertInSequenceOrder(deque, batch); <span class="comment">//note: 将 batch 添加到队列的合适位置（根据 seq num 信息）</span></div><div class="line">        <span class="keyword">else</span></div><div class="line">            deque.addFirst(batch);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>另外 Sender 在发送请求时，会首先通过 RecordAccumulator 的 <code>drain()</code> 方法获取其发送的数据，在遍历 Topic-Partition 对应的 queue 中的 batch 时，如果发现 batch 已经有了 sequence number 的话，则证明这个 batch 是重试的 batch，因为没有重试的 batch 其 sequence number 还没有设置，这时候会做一个判断，会等待其 in-flight-requests 中请求发送完成，才允许再次发送这个 Topic-Partition 的数据，其判断实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 inFlightBatches 中第一个 batch 的 baseSequence, inFlightBatches 为 null 的话返回 RecordBatch.NO_SEQUENCE</span></div><div class="line"><span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</div><div class="line"><span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</div><div class="line">        &amp;&amp; first.baseSequence() != firstInFlightSequence)</div><div class="line">    <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></div><div class="line">    <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></div><div class="line">    <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></div><div class="line">    <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></div><div class="line">    <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></div><div class="line">    <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></div><div class="line">    <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></div><div class="line">    <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></div><div class="line">    <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></div><div class="line">    <span class="comment">// in flight request count to 1.</span></div><div class="line">    <span class="keyword">break</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>仅有 client 端这两个机制还不够，Server 端在处理 ProduceRequest 请求时，还会检查 batch 的 sequence number 值，它会要求这个值必须是连续的，如果不连续都会返回异常，Client 会进行相应的重试，举个栗子：假设 Client 发送的请求顺序是 1、2、3、4、5（分别对应了一个 batch），如果中间的请求 2 出现了异常，那么会导致 3、4、5 都返回异常进行重试（因为 sequence number 不连续），也就是说此时 2、3、4、5 都会进行重试操作添加到对应的 queue 中。</p>
<p>Producer 的 TransactionManager 实例的 inflightBatchesBySequence 成员变量会维护这个 Topic-Partition 与目前正在发送的 batch 的对应关系（通过 <code>addInFlightBatch()</code> 方法添加 batch 记录），只有这个 batch 成功 ack 后，才会通过 <code>removeInFlightBatch()</code> 方法将这个 batch 从 inflightBatchesBySequence 中移除。接着前面的例子，此时 inflightBatchesBySequence 中还有 2、3、4、5 这几个 batch（有顺序的，2 在前面），根据前面的 RecordAccumulator 的 <code>drain()</code> 方法可以知道只有这个 Topic-Partition 下次要发送的 batch 是 batch 2（跟 transactionManager 的这个 <code>firstInFlightSequence()</code> 方法获取 inFlightBatches 中第一个 batch 的 baseSequence 来判断） 时，才可以发送，否则会直接 break，跳过这个 Topic-Partition 的数据发送。这里相当于有一个等待，等待 batch 2 重新加入到 queue 中，才可以发送，不能跳过 batch 2，直接重试 batch 3、4、5，这是不允许的。</p>
<p>简单来说，其实现机制概括为：</p>
<ol>
<li>Server 端验证 batch 的 sequence number 值，不连续时，直接返回异常；</li>
<li>Client 端请求重试时，batch 在 reenqueue 时会根据 sequence number 值放到合适的位置（有序保证之一）；</li>
<li>Sender 线程发送时，在遍历 queue 中的 batch 时，会检查这个 batch 是否是重试的 batch，如果是的话，只有这个 batch 是最旧的那个需要重试的 batch，才允许发送，否则本次发送跳过这个 Topic-Partition 数据的发送等待下次发送。</li>
</ol>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="external">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="external">Idempotent Producer</a>；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 K
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>BookKeeper 集群搭建及使用</title>
    <link href="http://matt33.com/2018/10/19/bk-cluster-install-and-use/"/>
    <id>http://matt33.com/2018/10/19/bk-cluster-install-and-use/</id>
    <published>2018-10-19T15:23:35.000Z</published>
    <updated>2018-10-20T01:42:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>随着 Apache Pulsar 成为 Apache 的顶级开源项目，其存储层的解决方案 Apache BookKeeper 再次受到业界广泛关注。BookKeeper 在 Pulsar 之前也有很多成功的应用，比如使用 BookKeeper 实现了 HDFS NameNode 的 HA 机制（可能大部分公司使用的还是 Quorum Journal Manage 方案）、Twitter 开源的 DistributedLog 系统（可参考<a href="http://www.infoq.com/cn/news/2016/05/Twitter-Github-DistributedLog" target="_blank" rel="external">Twitter开源分布式高性能日志复制服务</a>），BookKeeper 作为一个高扩展、强容错、低延迟的存储服务（A scalable, fault-tolerant, and low-latency storage service optimized for real-time workloads），它相当于把底层的存储层系统服务化（BookKeeper 是更底层的存储服务，类似于 Kafka 的存储层）。这样可以使得依赖于 BookKeeper 实现的分布式存储系统（包括分布式消息队列）在设计时可以只关注其应用层和功能层的内容，存储层比较难解决的问题像一致性、容错等，BookKeeper 已经实现了，从这个层面看，BookKeeper 确实解决业内的一些问题，而且 BookKeeper （Ledger 化，Ledger 相当于 Kafka segment）天生适合云上部署，未来还是有很大潜力的。近段对 BookKeeper 做了一些相应的调研，做了一些总结，本文将会主要从集群部署和使用角度来介绍一下 Apache BookKeeper，后面准备再写一篇文章来深入讲述其架构设计及实现原理。</p>
<h2 id="BookKeeper-简介"><a href="#BookKeeper-简介" class="headerlink" title="BookKeeper 简介"></a>BookKeeper 简介</h2><p>这里先对 BookKeeper 的基本概念做一下介绍，下图是 BookKeeper 的架构图（图片来自 <a href="https://www.slideshare.net/streamlio/introduction-to-apache-bookkeeper-distributed-storage?qid=3cbd6bbf-9e04-4e38-9ab6-4619e4d8f61e&amp;v=&amp;b=&amp;from_search=1" target="_blank" rel="external">Introduction to Apache BookKeeper</a>）：</p>
<p><img src="/images/bookkeeper/bookkeeper.png" alt="Apache BookKeeper 架构图"></p>
<p>在 BookKeeper 中节点（Server）被称作 Bookie（类似于 Kafka 中 Broker，HDFS 中的 DN，但是 BookKeeper 没有 Master 节点，它是典型 Slave/Slave 架构），数据在 Bookie 上以 Ledger 的形式存储（类似 Kafka 中的 Segment，HDFS 中的 Block）， BookKeeper 相关的基本概念如下：</p>
<ol>
<li>Cluster: 所有的 Bookie 组成一个集群（连接到同一个 zk 地址的 Bookie 集合）；</li>
<li>Bookie：BookKeeper 的存储节点，也即 Server 节点；</li>
<li>Ledger：Ledger 是对一个 log 文件的抽象，它本质上是由一系列 Entry （类似与 Kafka 每条 msg）组成的，client 在向 BookKeeper 写数据时也是往 Ledger 中写的；</li>
<li>Entry：entry 本质上就是一条数据，它会有一个 id 做标识；</li>
<li>Journal: Write ahead log，数据是先写到 Journal 中，这个也是 BookKeeper 读写分离实现机制的一部分，后续会详细分析；</li>
<li>Ensemble: Set of Bookies across which a ledger is striped，一个 Ledger 所涉及的 Bookie 集合，初始化 Ledger 时，需要指定这个 Ledger 可以在几台 Bookie 上存储；</li>
<li>Write Quorum Size: Number of replicas，要写入的副本数；</li>
<li>Ack Quorum Size: Number of responses needed before client’s write is satisfied，当这么多副本写入成功后才会向 client 返回成功，比如副本数设置了 3，这个设置了2，client 会同时向三副本写入数据，当收到两个成功响应后，会认为数据已经写入成功；</li>
<li>LAC: Last Add Confirmed，Ledger 中已经确认的最近一条数据的 entry id。</li>
</ol>
<h2 id="BookKeeper-集群搭建"><a href="#BookKeeper-集群搭建" class="headerlink" title="BookKeeper 集群搭建"></a>BookKeeper 集群搭建</h2><p>关于 BookKeeper 集群的搭建可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/deployment/manual/#starting-up-bookies" target="_blank" rel="external">Apache BookKeeper Manual deployment</a> 这篇文章。</p>
<h3 id="集群搭建前准备"><a href="#集群搭建前准备" class="headerlink" title="集群搭建前准备"></a>集群搭建前准备</h3><p>BookKeeper 集群搭建需要：</p>
<ol>
<li>ZooKeeper 集群；</li>
<li>一些 Bookie 节点（在集群的模式下最好是选取三台）；</li>
<li>JDK 版本要求是 JDK8；</li>
</ol>
<p>这里先看下 BookKeeper 的目录结构，跟其他分布式系统也类似，命令在 bin 目录下，配置文件在 conf 目录下，lib 是其依赖的相关 jar 包，如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[matt@XXX2 bookkeeper]$ ll</div><div class="line">total 64</div><div class="line">drwxr-xr-x 2 matt matt  4096 Sep 20 18:35 bin</div><div class="line">drwxr-xr-x 2 matt matt  4096 Sep 20 18:35 conf</div><div class="line">drwxrwxr-x 9 matt matt  4096 Oct  9 21:41 deps</div><div class="line">drwxrwxr-x 2 matt matt 12288 Oct  9 21:41 lib</div><div class="line">-rw-r--r-- 1 matt matt 24184 Sep 20 18:35 LICENSE</div><div class="line">-rw-r--r-- 1 matt matt  5114 Sep 20 18:35 NOTICE</div><div class="line">-rw-r--r-- 1 matt matt  4267 Sep 20 18:35 README.md</div></pre></td></tr></table></figure>
<p>bin 目录下提供了 BookKeeper 相应的操作命令，这里用的命令主要是 <code>bin/bookkeeper*</code>（<code>bookkeeper-daemon.sh</code> 可以让 Bookie 进程在后台自动运行），可以在 <code>bin/common.sh</code> 配置一些通用的配置（比如 JAVA_HOME），关于 bookkeeper 命令的使用方法见 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/cli/#bookkeeper" target="_blank" rel="external">bookkeeper cli</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[matt@XXX2 bookkeeper]$ ll bin/</div><div class="line">total 56</div><div class="line">-rwxr-xr-x 1 matt matt 2319 Sep 20 18:35 bkctl</div><div class="line">-rwxr-xr-x 1 matt matt 5874 Sep 20 18:35 bookkeeper</div><div class="line">-rwxr-xr-x 1 matt matt 2869 Sep 20 18:35 bookkeeper-cluster.sh</div><div class="line">-rwxr-xr-x 1 matt matt 4590 Sep 20 18:35 bookkeeper-daemon.sh</div><div class="line">-rwxr-xr-x 1 matt matt 7785 Sep 20 18:35 common.sh</div><div class="line">-rwxr-xr-x 1 matt matt 4575 Sep 20 18:35 dlog</div><div class="line">-rwxr-xr-x 1 matt matt 1738 Sep 20 18:35 standalone</div><div class="line">-rwxr-xr-x 1 matt matt 5128 Sep 20 18:35 standalone.docker-compose</div><div class="line">-rwxr-xr-x 1 matt matt 1854 Sep 20 18:35 standalone.process</div></pre></td></tr></table></figure>
<p>在 bookkeper 命令中，又提供了 shell 的相关命令，这里提供的命令非常丰富，可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/cli/#the-bookkeeper-shell" target="_blank" rel="external">BookKeeper Shell</a>，如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">[matt@XXX2 bookkeeper]$ bin/bookkeeper shell</div><div class="line">Usage: bookkeeper shell [-localbookie [&lt;host:port&gt;]] [-ledgeridformat &lt;hex/long/uuid&gt;] [-entryformat &lt;hex/string&gt;] [-conf configuration] &lt;<span class="built_in">command</span>&gt;</div><div class="line"><span class="built_in">where</span> <span class="built_in">command</span> is one of:</div><div class="line">       autorecovery [-enable|-disable]</div><div class="line">       bookieformat [-nonInteractive] [-force] [-deleteCookie]</div><div class="line">       bookieinfo</div><div class="line">       bookiesanity [-entries N] [-timeout N]</div><div class="line">       convert-to-db-storage</div><div class="line">       convert-to-interleaved-storage</div><div class="line">       decommissionbookie [-bookieid &lt;bookieaddress&gt;]</div><div class="line">       deleteledger -ledgerid &lt;ledgerid&gt; [-force]</div><div class="line">       <span class="built_in">help</span>         [COMMAND]</div><div class="line">       initbookie</div><div class="line">       initnewcluster</div><div class="line">       lastmark</div><div class="line">       ledger       [-m] &lt;ledger_id&gt;</div><div class="line">       ledgermetadata -ledgerid &lt;ledgerid&gt;</div><div class="line">       listbookies  [-readwrite|-readonly] [-hostnames]</div><div class="line">       listfilesondisc  [-journal|-entrylog|-index]</div><div class="line">       listledgers  [-meta] [-bookieid &lt;bookieaddress&gt;]</div><div class="line">       listunderreplicated [[-missingreplica &lt;bookieaddress&gt;] [-excludingmissingreplica &lt;bookieaddress&gt;]] [-printmissingreplica] [-printreplicationworkerid]</div><div class="line">       lostbookierecoverydelay [-get|-set &lt;value&gt;]</div><div class="line">       metaformat   [-nonInteractive] [-force]</div><div class="line">       nukeexistingcluster -zkledgersrootpath &lt;zkledgersrootpath&gt; [-instanceid &lt;instanceid&gt; | -force]</div><div class="line">       readjournal [-dir] [-msg] &lt;journal_id | journal_file_name&gt;</div><div class="line">       readledger  [-bookie &lt;address:port&gt;]  [-msg] -ledgerid &lt;ledgerid&gt; [-firstentryid &lt;firstentryid&gt; [-lastentryid &lt;lastentryid&gt;]] [-force-recovery]</div><div class="line">       readlog      [-msg] &lt;entry_log_id | entry_log_file_name&gt; [-ledgerid &lt;ledgerid&gt; [-entryid &lt;entryid&gt;]] [-startpos &lt;startEntryLogBytePos&gt; [-endpos &lt;endEntryLogBytePos&gt;]]</div><div class="line">       readlogmetadata &lt;entry_log_id | entry_log_file_name&gt;</div><div class="line">       rebuild-db-ledger-locations-index</div><div class="line">       recover [-deleteCookie] &lt;bookieSrc[:bookieSrc]&gt;</div><div class="line">       simpletest   [-ensemble N] [-writeQuorum N] [-ackQuorum N] [-numEntries N]</div><div class="line">       triggeraudit</div><div class="line">       updatecookie [-bookieId &lt;hostname|ip&gt;] [-expandstorage] [-list] [-delete &lt;force&gt;]</div><div class="line">       updateledgers -bookieId &lt;hostname|ip&gt; [-updatespersec N] [-limit N] [-verbose <span class="literal">true</span>/<span class="literal">false</span>] [-printprogress N]</div><div class="line">       whatisinstanceid</div><div class="line">       whoisauditor</div></pre></td></tr></table></figure>
<p>conf 目录下是关于 BookKeeper 的相关配置，如下所示，主要配置在 <code>bk_server.conf</code> 中，这里可以提供的配置非常多，具体可配置的参数可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/config/" target="_blank" rel="external">BookKeeper Config</a>，</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[matt@XXX2 bookkeeper]$ ll conf/</div><div class="line">total 84</div><div class="line">-rw-r--r-- 1 matt matt  1804 Sep 20 18:35 bk_cli_env.sh</div><div class="line">-rw-r--r-- 1 matt matt  2448 Sep 20 18:35 bkenv.sh</div><div class="line">-rwxr-xr-x 1 matt matt 42269 Sep 20 18:35 bk_server.conf</div><div class="line">-rw-r--r-- 1 matt matt  1211 Sep 20 18:35 jaas_example.conf</div><div class="line">-rw-r--r-- 1 matt matt  2311 Sep 20 18:35 <span class="built_in">log</span>4j.cli.properties</div><div class="line">-rw-r--r-- 1 matt matt  2881 Sep 20 18:35 <span class="built_in">log</span>4j.properties</div><div class="line">-rw-r--r-- 1 matt matt  1810 Sep 20 18:35 <span class="built_in">log</span>4j.shell.properties</div><div class="line">-rw-r--r-- 1 matt matt  1117 Sep 20 18:35 nettyenv.sh</div><div class="line">-rwxr-xr-x 1 matt matt  1300 Sep 20 18:35 standalone.conf</div><div class="line">-rw-r--r-- 1 matt matt  3275 Sep 20 18:35 zookeeper.conf</div><div class="line">-rw-r--r-- 1 matt matt   843 Sep 20 18:35 zookeeper.conf.dynamic</div></pre></td></tr></table></figure>
<h3 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h3><p>在 <a href="https://bookkeeper.apache.org/releases/" target="_blank" rel="external">Apache BookKeeper Releases</a> 中下载 BookKeeper 最新的安装包（这里以 bookkeeper-server-4.8.0-bin.tar.gz 为例）。</p>
<p>将安装包在指定目录下解压后，启动的操作分为以下几步：</p>
<ol>
<li>修改相关配置（<code>zkServers</code>、<code>bookiePort</code>、<code>journalDir</code>、<code>ledgerDir</code> 等）；</li>
<li>在相应的机器上启动 Bookie 进程（使用 <code>./bin/bookkeeper-daemon.sh start bookie</code> 启动 Bookie）；</li>
<li>当所有的 Bookie 启动完成后，随便选择一台，初始化集群 meta 信息（使用 <code>bookkeeper-server/bin/bookkeeper shell metaformat</code> 命令初始化集群的 meta 信息，这里只需要初始化一次）。</li>
</ol>
<p>如果启动成功的话（如果有异常日志，即使 Bookie 进程存在，也可能没有启动成功），启动正常的情况下，在日志中，可以看到类似下面的信息：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-10-15 11:24:49,549 - INFO  [main:ComponentStarter@81] - Started component bookie-server.</div></pre></td></tr></table></figure>
<h3 id="Admin-REST-API"><a href="#Admin-REST-API" class="headerlink" title="Admin REST API"></a>Admin REST API</h3><p>BookKeeper 服务提供了相应的 Rest API，可供管理员使用，具体可以参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/admin/http/" target="_blank" rel="external">BookKeeper Admin REST API</a>，如果想要使用这个功能，首先需要 Bookie 服务将 bk_server.conf 中的 <code>httpServerEnabled</code> 配置设置为 true ，相关的配置参考 <a href="https://bookkeeper.apache.org/docs/4.8.0/reference/config/#http-server-settings" target="_blank" rel="external">Http server settings</a>。</p>
<h3 id="安装时踩的坑"><a href="#安装时踩的坑" class="headerlink" title="安装时踩的坑"></a>安装时踩的坑</h3><p>在搭建 BookKeeper 集群中，并没有想象中那么顺畅，遇到了一些小问题，记录如下：</p>
<h4 id="问题1：修改配置后重新启动失败"><a href="#问题1：修改配置后重新启动失败" class="headerlink" title="问题1：修改配置后重新启动失败"></a>问题1：修改配置后重新启动失败</h4><p>在使用  <code>./bin/bookkeeper-daemon.sh  stop bookie</code> 命令关闭 Bookie 进程，当关闭完 Bookie 进程后，再次启动时，发现无法启动，报出了下面的错误：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">2018-10-13 21:05:40,674 - ERROR [main:Main@221] - Failed to build bookie server</div><div class="line">org.apache.bookkeeper.bookie.BookieException<span class="variable">$InvalidCookieException</span>: instanceId 406a08e5-911e-4ab6-b97b-40e4a56279a8 is not matching with null</div><div class="line">	at org.apache.bookkeeper.bookie.Cookie.verifyInternal(Cookie.java:142)</div><div class="line">	at org.apache.bookkeeper.bookie.Cookie.verify(Cookie.java:147)</div><div class="line">	at org.apache.bookkeeper.bookie.Bookie.verifyAndGetMissingDirs(Bookie.java:381)</div><div class="line">	at org.apache.bookkeeper.bookie.Bookie.checkEnvironmentWithStorageExpansion(Bookie.java:444)</div><div class="line">	at org.apache.bookkeeper.bookie.Bookie.checkEnvironment(Bookie.java:262)</div><div class="line">	at org.apache.bookkeeper.bookie.Bookie.&lt;init&gt;(Bookie.java:646)</div><div class="line">	at org.apache.bookkeeper.proto.BookieServer.newBookie(BookieServer.java:133)</div><div class="line">	at org.apache.bookkeeper.proto.BookieServer.&lt;init&gt;(BookieServer.java:102)</div><div class="line">	at org.apache.bookkeeper.server.service.BookieService.&lt;init&gt;(BookieService.java:43)</div><div class="line">	at org.apache.bookkeeper.server.Main.buildBookieServer(Main.java:299)</div><div class="line">	at org.apache.bookkeeper.server.Main.doMain(Main.java:219)</div><div class="line">	at org.apache.bookkeeper.server.Main.main(Main.java:201)</div></pre></td></tr></table></figure>
<p>大概的意思就是说现在 zk 上的 instanceId 是 <code>406a08e5-911e-4ab6-b97b-40e4a56279a8</code>，而期望的 instanceId 是 null，索引因为验证失败导致进程无法启动，instanceId 是搭建集群第三步（初始化集群 meta 信息的地方）中初始化的。此时如果我们启动测试的 client 程序，会抛出以下异常，这是因为目前集群只有2台 Bookie 处在可用状态，而 ensSize 默认是 3，writeQuorumSize 是 2，ackQuorumSize 是2。在 client 的测试程序中，新建一个 Ledger 时，由于集群当前可用的 Bookie 为2，不满足相应的条件，所以抛出了一下的异常：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">org.apache.bookkeeper.client.BKException<span class="variable">$BKNotEnoughBookiesException</span>: Not enough non-faulty bookies available</div><div class="line">	at org.apache.bookkeeper.client.SyncCallbackUtils.finish(SyncCallbackUtils.java:83)</div><div class="line">	at org.apache.bookkeeper.client.SyncCallbackUtils<span class="variable">$SyncCreateCallback</span>.createComplete(SyncCallbackUtils.java:106)</div><div class="line">	at org.apache.bookkeeper.client.LedgerCreateOp.createComplete(LedgerCreateOp.java:238)</div><div class="line">	at org.apache.bookkeeper.client.LedgerCreateOp.initiate(LedgerCreateOp.java:142)</div><div class="line">	at org.apache.bookkeeper.client.BookKeeper.asyncCreateLedger(BookKeeper.java:891)</div><div class="line">	at org.apache.bookkeeper.client.BookKeeper.createLedger(BookKeeper.java:975)</div><div class="line">	at org.apache.bookkeeper.client.BookKeeper.createLedger(BookKeeper.java:930)</div><div class="line">	at org.apache.bookkeeper.client.BookKeeper.createLedger(BookKeeper.java:911)</div><div class="line">	at com.matt.test.bookkeeper.ledger.LedgerTest.createLedgerSync(LedgerTest.java:110)</div><div class="line">	at com.matt.test.bookkeeper.ledger.LedgerTest.main(LedgerTest.java:25)</div><div class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.NullPointerException</div><div class="line">	at com.matt.test.bookkeeper.ledger.LedgerTest.main(LedgerTest.java:26)</div></pre></td></tr></table></figure>
<p>关于这个 BookieException$InvalidCookieException 异常，google 了一下并没有找到相应的解决办法，所以就直接看了相应的代码，抛出异常的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">verifyInternal</span><span class="params">(Cookie c, <span class="keyword">boolean</span> checkIfSuperSet)</span> <span class="keyword">throws</span> BookieException.InvalidCookieException </span>&#123;</div><div class="line">    String errMsg;</div><div class="line">    <span class="keyword">if</span> (c.layoutVersion &lt; <span class="number">3</span> &amp;&amp; c.layoutVersion != layoutVersion) &#123;</div><div class="line">        errMsg = <span class="string">"Cookie is of too old version "</span> + c.layoutVersion;</div><div class="line">        LOG.error(errMsg);</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BookieException.InvalidCookieException(errMsg);</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!(c.layoutVersion &gt;= <span class="number">3</span> &amp;&amp; c.bookieHost.equals(bookieHost)</div><div class="line">        &amp;&amp; c.journalDirs.equals(journalDirs) &amp;&amp; verifyLedgerDirs(c, checkIfSuperSet))) &#123;</div><div class="line">        errMsg = <span class="string">"Cookie ["</span> + <span class="keyword">this</span> + <span class="string">"] is not matching with ["</span> + c + <span class="string">"]"</span>;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BookieException.InvalidCookieException(errMsg);</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((instanceId == <span class="keyword">null</span> &amp;&amp; c.instanceId != <span class="keyword">null</span>)</div><div class="line">            || (instanceId != <span class="keyword">null</span> &amp;&amp; !instanceId.equals(c.instanceId))) &#123;</div><div class="line">        <span class="comment">// instanceId should be same in both cookies</span></div><div class="line">        errMsg = <span class="string">"instanceId "</span> + instanceId</div><div class="line">                + <span class="string">" is not matching with "</span> + c.instanceId;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> BookieException.InvalidCookieException(errMsg); <span class="comment">// 由于 instanceId 不匹配，抛出了相应的异常</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里可以看到的是从 zk 上拿到的 instanceId 是 <code>406a08e5-911e-4ab6-b97b-40e4a56279a8</code>，而 Cookie 实例 c 中的 instanceId 为 null，那么 这个 Cookie 是如何初始化的呢？往上追一下代码，发现是在初始化 Bookie 时，会检查一下相应的运行环境，此时会从 journalDirectories 和 ledgerDirectories 中 <code>current/VERSION</code> 中初始化相应的 Cookie 对象，由于这个台机器之前启动过，所以这个文件已经创建了，文件的内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[matt@XXX2 bookkeeper]$ cat /tmp/bk-data/current/VERSION</div><div class="line">4</div><div class="line">bookieHost: &quot;XXX:3181&quot;</div><div class="line">journalDir: &quot;/tmp/bk-txn&quot;</div><div class="line">ledgerDirs: &quot;1\t/tmp/bk-data&quot;</div><div class="line">[matt@XXX2 bookkeeper]$ cat /tmp/bk-txn/current/VERSION</div><div class="line">4</div><div class="line">bookieHost: &quot;XXX:3181&quot;</div><div class="line">journalDir: &quot;/tmp/bk-txn&quot;</div><div class="line">ledgerDirs: &quot;1\t/tmp/bk-data&quot;</div></pre></td></tr></table></figure>
<p>Cookie 从文件加载相应文件，并初始化对象的实现方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Read cookie from registration manager for a given bookie &lt;i&gt;address&lt;/i&gt;.</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> rm registration manager</div><div class="line"> * <span class="doctag">@param</span> address bookie address</div><div class="line"> * <span class="doctag">@return</span> versioned cookie object</div><div class="line"> * <span class="doctag">@throws</span> BookieException when fail to read cookie</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Versioned&lt;Cookie&gt; <span class="title">readFromRegistrationManager</span><span class="params">(RegistrationManager rm,</span></span></div><div class="line">                                                     BookieSocketAddress address) <span class="keyword">throws</span> BookieException &#123;</div><div class="line">    Versioned&lt;<span class="keyword">byte</span>[]&gt; cookieData = rm.readCookie(address.toString());</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">try</span> (BufferedReader reader = <span class="keyword">new</span> BufferedReader(</div><div class="line">                <span class="keyword">new</span> StringReader(<span class="keyword">new</span> String(cookieData.getValue(), UTF_8)))) &#123;</div><div class="line">            Builder builder = parse(reader);</div><div class="line">            Cookie cookie = builder.build();</div><div class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Versioned&lt;Cookie&gt;(cookie, cookieData.getVersion());</div><div class="line">        &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> InvalidCookieException(ioe);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Builder <span class="title">parse</span><span class="params">(BufferedReader reader)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    Builder cBuilder = Cookie.newBuilder();</div><div class="line">    <span class="keyword">int</span> layoutVersion = <span class="number">0</span>;</div><div class="line">    String line = reader.readLine();</div><div class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == line) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> EOFException(<span class="string">"Exception in parsing cookie"</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        layoutVersion = Integer.parseInt(line.trim());</div><div class="line">        cBuilder.setLayoutVersion(layoutVersion);</div><div class="line">    &#125; <span class="keyword">catch</span> (NumberFormatException e) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Invalid string '"</span> + line.trim()</div><div class="line">                + <span class="string">"', cannot parse cookie."</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (layoutVersion == <span class="number">3</span>) &#123;</div><div class="line">        cBuilder.setBookieHost(reader.readLine());</div><div class="line">        cBuilder.setJournalDirs(reader.readLine());</div><div class="line">        cBuilder.setLedgerDirs(reader.readLine());</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (layoutVersion &gt;= <span class="number">4</span>) &#123; <span class="comment">//这里的版本默认为 4</span></div><div class="line">        CookieFormat.Builder cfBuilder = CookieFormat.newBuilder();</div><div class="line">        TextFormat.merge(reader, cfBuilder);</div><div class="line">        CookieFormat data = cfBuilder.build();</div><div class="line">        cBuilder.setBookieHost(data.getBookieHost());</div><div class="line">        cBuilder.setJournalDirs(data.getJournalDir());</div><div class="line">        cBuilder.setLedgerDirs(data.getLedgerDirs());</div><div class="line">        <span class="comment">// Since InstanceId is optional</span></div><div class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != data.getInstanceId() &amp;&amp; !data.getInstanceId().isEmpty()) &#123; <span class="comment">//如果文件中没有 instanceId 字段，这里就不会初始化到 Cookie 中</span></div><div class="line">            cBuilder.setInstanceId(data.getInstanceId());</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> cBuilder;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>解决的方法很简单，在 <code>current/VERSION</code> 文件中添加相应的 instanceId 字段后，Bookie 便可启动成功。但是这里还需要考虑的问题是：</p>
<ul>
<li>instanceId 在这里的作用是什么？instanceId 是在集群初始化时设置的，关于这个值的含义，我推测它的目的是对节点的上线做一个简单的认证，也就是说如果打算在集群中新添加一台 Bookie，需要知道当前的 instanceId 值，这样才能加入到这个集群中；</li>
<li>Bookie 服务的启动流程是什么样的？这里就需要看下代码的具体实现，追一下 Bookie 的启动流程了。</li>
</ul>
<h2 id="BookKeeper-API-使用"><a href="#BookKeeper-API-使用" class="headerlink" title="BookKeeper API 使用"></a>BookKeeper API 使用</h2><p>关于 BookKeeper API，总共提供了以下三种 API：</p>
<ol>
<li>The ledger API is a lower-level API that enables you to interact with ledgers directly，第一种是一种较为底层的 API 接口，直接与 Ledger 交互，见 <a href="https://bookkeeper.apache.org/docs/4.8.0/api/ledger-api/" target="_blank" rel="external">The Ledger API</a>；</li>
<li>The Ledger Advanced API is an advanced extension to Ledger API to provide more flexibilities to applications，第二种较高级的 API，提供了一些较高级的功能，见 <a href="https://bookkeeper.apache.org/docs/4.8.0/api/ledger-adv-apiThe Advanced Ledger API/" target="_blank" rel="external">The Advanced Ledger API</a>；</li>
<li>The DistributedLog API is a higher-level API that provides convenient abstractions，这种是关于 DistributedLog 的一些操作 API，见 <a href="https://bookkeeper.apache.org/docs/4.8.0/api/distributedlog-api/" target="_blank" rel="external">DistributedLog</a>。</li>
</ol>
<p>在这节，我们主要看下第一种的实现，会简单讲述一下第二种，第三种这里不再介绍。</p>
<h3 id="The-Ledger-API"><a href="#The-Ledger-API" class="headerlink" title="The Ledger API"></a>The Ledger API</h3><p>关于 Ledger API 基本操作主要有以下几种：</p>
<ol>
<li>创建 Ledger；</li>
<li>向 Ledger 写入数据（Entry）；</li>
<li>关闭 Ledger，Ledger 关闭后数据就不能再写入，Ledger 一旦关闭它的数据就是不可变的；</li>
<li>从 Ledger 中读取数据；</li>
<li>删除 Ledger。</li>
</ol>
<p>当然实现上述操作的前提是，需要先初始化一个 BookKeeper Client，下面开始慢慢讲述。</p>
<h4 id="初始化-BookKeeper-Client"><a href="#初始化-BookKeeper-Client" class="headerlink" title="初始化 BookKeeper Client"></a>初始化 BookKeeper Client</h4><p>BK Client 的初始化需要指定 zk 地址，BK Client 通过 zk 来连接到 BK 集群，具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 第一种初始化 BookKeeper Client 的方法</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    String connectionString = zkAddr; <span class="comment">// For a single-node, local ZooKeeper cluster</span></div><div class="line">    BookKeeper bkClient = <span class="keyword">new</span> BookKeeper(connectionString);</div><div class="line">    logger.info(<span class="string">"BookKeeper client init success."</span>);</div><div class="line">&#125; <span class="keyword">catch</span> (InterruptedException | IOException | BKException e) &#123;</div><div class="line">    e.printStackTrace();</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(</div><div class="line">            <span class="string">"There is an exception throw while creating the BookKeeper client."</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 第二种初始化 BookKeeper Client 的方法</span></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    ClientConfiguration config = <span class="keyword">new</span> ClientConfiguration();</div><div class="line">    config.setZkServers(zkAddr);</div><div class="line">    config.setAddEntryTimeout(<span class="number">2000</span>);</div><div class="line">    BookKeeper bkClient = <span class="keyword">new</span> BookKeeper(config);</div><div class="line">    logger.info(<span class="string">"BookKeeper client init success."</span>);</div><div class="line">&#125; <span class="keyword">catch</span> (InterruptedException | IOException | BKException e) &#123;</div><div class="line">    e.printStackTrace();</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(</div><div class="line">            <span class="string">"There is an exception throw while creating the BookKeeper client."</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="新建一个-Ledger"><a href="#新建一个-Ledger" class="headerlink" title="新建一个 Ledger"></a>新建一个 Ledger</h4><p>Ledger 的创建有两种，一种是同步创建，一种是异步创建（创建时需要指定相应的 password），其实现分别如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * create the ledger, default ensemble size is 3, write quorum size is 2, ack quorum size is 2</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> pw password</div><div class="line"> * <span class="doctag">@return</span> LedgerHandle</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> LedgerHandle <span class="title">createLedgerSync</span><span class="params">(String pw)</span> </span>&#123;</div><div class="line">    <span class="keyword">byte</span>[] password = pw.getBytes();</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        LedgerHandle handle = bkClient.createLedger(BookKeeper.DigestType.MAC, password);</div><div class="line">        <span class="keyword">return</span> handle;</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * create the ledger</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> pw password</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createLedgerAsync</span><span class="params">(String pw)</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">LedgerCreationCallback</span> <span class="keyword">implements</span> <span class="title">AsyncCallback</span>.<span class="title">CreateCallback</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createComplete</span><span class="params">(<span class="keyword">int</span> returnCode, LedgerHandle handle, Object ctx)</span> </span>&#123;</div><div class="line">            System.out.println(<span class="string">"Ledger successfully created"</span>);</div><div class="line">            logger.info(<span class="string">"Ledger successfully created async."</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    bkClient.asyncCreateLedger(</div><div class="line">            <span class="number">3</span>, <span class="comment">// ensSize</span></div><div class="line">            <span class="number">2</span>, <span class="comment">// writeQuorumSize and ackQuorumSize</span></div><div class="line">            BookKeeper.DigestType.MAC,</div><div class="line">            pw.getBytes(),</div><div class="line">            <span class="keyword">new</span> LedgerCreationCallback(),</div><div class="line">            <span class="string">"some context"</span></div><div class="line">    );</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>新建好 Ledger 之后，会返回一个 LedgerHandle 实例，对于 Ledger 的操作都是通过这个实例对象完成的，也可以通过 <code>LedgerHandle.getId()</code> 方法获取 Ledger 的 id，有了这个 id 就可以映射到具体的 Ledger，当需要读取数据时，通过 ledger id 初始化相应的 LedgerHandle 实例即可。</p>
<h4 id="向-Ledger-写入数据"><a href="#向-Ledger-写入数据" class="headerlink" title="向 Ledger 写入数据"></a>向 Ledger 写入数据</h4><p>有了 Ledger 对应的 LedgerHandle 实例之后，可以通过 <code>addEntry()</code> 方法直接向 Ledger 写数据，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">addEntry</span><span class="params">(LedgerHandle ledgerHandle, String msg)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">return</span> ledgerHandle.addEntry(msg.getBytes());</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="从-Ledger-读取数据"><a href="#从-Ledger-读取数据" class="headerlink" title="从 Ledger 读取数据"></a>从 Ledger 读取数据</h4><p>从 Ledger 读取数据时，也是通过 LedgerHandle 实例的方法实现，提供了以下三种方法：</p>
<ol>
<li>指定读取的 entry.id 范围消费；</li>
<li>从某一个 entry.id 一直读取到 LAC （LastAddConfirmed，该 Ledger 中最近的已经确认的数据）位置；</li>
<li>从某一个 entry.id 一直读取到 lastEntryIdExpectedToRead 位置，该位置可以比 LAC 大，前提是需要该值已经有对应的数据；</li>
</ol>
<p>方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * read entry from startId to endId</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> ledgerHandle the ledger</div><div class="line"> * <span class="doctag">@param</span> startId      start entry id</div><div class="line"> * <span class="doctag">@param</span> endId        end entry id</div><div class="line"> * <span class="doctag">@return</span> the entries, if occur exception, return null</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> Enumeration&lt;LedgerEntry&gt; <span class="title">readEntry</span><span class="params">(LedgerHandle ledgerHandle, <span class="keyword">int</span> startId, <span class="keyword">int</span> endId)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">return</span> ledgerHandle.readEntries(startId, endId);</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * read entry from 0 to the LAC</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> ledgerHandle the ledger</div><div class="line"> * <span class="doctag">@return</span> the entries, if occur exception, return null</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> Enumeration&lt;LedgerEntry&gt; <span class="title">readEntry</span><span class="params">(LedgerHandle ledgerHandle)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">return</span> ledgerHandle.readEntries(<span class="number">0</span>, ledgerHandle.getLastAddConfirmed());</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * read entry form 0 to lastEntryIdExpectedToRead which can larger than the LastAddConfirmed range</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> ledgerHandle              the handle</div><div class="line"> * <span class="doctag">@param</span> lastEntryIdExpectedToRead the last entry id</div><div class="line"> * <span class="doctag">@return</span> the entries, if occur exception, return null</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> Enumeration&lt;LedgerEntry&gt; <span class="title">readEntry</span><span class="params">(LedgerHandle ledgerHandle,</span></span></div><div class="line">                                          <span class="keyword">long</span> lastEntryIdExpectedToRead) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">return</span> ledgerHandle.readUnconfirmedEntries(<span class="number">0</span>, lastEntryIdExpectedToRead);</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="删除-Ledger"><a href="#删除-Ledger" class="headerlink" title="删除 Ledger"></a>删除 Ledger</h4><p>Ledger 的删除实现也很简洁，如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * delete the ledger</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> ledgerId the ledger id</div><div class="line"> * <span class="doctag">@return</span> if occur exception, return false</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deleteLedger</span><span class="params">(<span class="keyword">long</span> ledgerId)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        bkClient.deleteLedger(ledgerId);</div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="The-Ledger-Advanced-API"><a href="#The-Ledger-Advanced-API" class="headerlink" title="The Ledger Advanced API"></a>The Ledger Advanced API</h3><p>Ledger 的 Advanced API 在用法上与上面的实现差异不大，它向应用提供了更大的灵活性，比如：在创建 Ledger 时，应用可以指定 LedgerId，写入 Entry 时，应用也可以指定相应的 EntryID。</p>
<h4 id="新建-Ledger"><a href="#新建-Ledger" class="headerlink" title="新建 Ledger"></a>新建 Ledger</h4><p>在新建 Ledger 这部分，Advanced API 可以指定 LedgerId 创建相应的 Ledger，如下面示例的第三种实现。</p>
<p>假设当前 BK 集群的 LedgerId 已经到了5，这时候在新建 Ledger 时如果不指定 LedgerId，下一个被使用的 LedgerId 就是6，如果应用指定了 7，新建的 Leader 的 id 将会是设置的 7，id 6 会等待下次再被使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * create the ledger</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> password pw</div><div class="line"> * <span class="doctag">@return</span> LedgerHandleAdv</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> LedgerHandleAdv <span class="title">createLedger</span><span class="params">(String password)</span> </span>&#123;</div><div class="line">    <span class="keyword">byte</span>[] passwd = password.getBytes();</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        LedgerHandleAdv handle = (LedgerHandleAdv) bkClient.createLedgerAdv(</div><div class="line">                <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="comment">// replica settings</span></div><div class="line">                BookKeeper.DigestType.CRC32,</div><div class="line">                passwd);</div><div class="line">        <span class="keyword">return</span> handle;</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * create the ledger async</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> password</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createLedgerAsync</span><span class="params">(String password)</span> </span>&#123;</div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">LedgerCreationCallback</span> <span class="keyword">implements</span> <span class="title">AsyncCallback</span>.<span class="title">CreateCallback</span> </span>&#123;</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createComplete</span><span class="params">(<span class="keyword">int</span> returnCode, LedgerHandle handle, Object ctx)</span> </span>&#123;</div><div class="line">            System.out.println(<span class="string">"Ledger successfully created"</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    bkClient.asyncCreateLedgerAdv(</div><div class="line">            <span class="number">3</span>, <span class="comment">// ensemble size</span></div><div class="line">            <span class="number">3</span>, <span class="comment">// write quorum size</span></div><div class="line">            <span class="number">2</span>, <span class="comment">// ack quorum size</span></div><div class="line">            BookKeeper.DigestType.CRC32,</div><div class="line">            password.getBytes(),</div><div class="line">            <span class="keyword">new</span> LedgerCreationCallback(),</div><div class="line">            <span class="string">"some context"</span>,</div><div class="line">            <span class="keyword">null</span>);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * create the ledger on special ledgerId</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> password pw</div><div class="line"> * <span class="doctag">@param</span> ledgerId the ledger id, if the ledger id exist, it will return BKLedgerExistException</div><div class="line"> * <span class="doctag">@return</span> LedgerHandleAdv</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> LedgerHandleAdv <span class="title">createLedger</span><span class="params">(String password, <span class="keyword">long</span> ledgerId)</span> </span>&#123;</div><div class="line">    <span class="keyword">byte</span>[] passwd = password.getBytes();</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        LedgerHandleAdv handle = (LedgerHandleAdv) bkClient.createLedgerAdv(</div><div class="line">                ledgerId,</div><div class="line">                <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="comment">// replica settings</span></div><div class="line">                BookKeeper.DigestType.CRC32,</div><div class="line">                passwd,</div><div class="line">                <span class="keyword">null</span>);</div><div class="line">        <span class="keyword">return</span> handle;</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="向-Ledger-添加-Entry"><a href="#向-Ledger-添加-Entry" class="headerlink" title="向 Ledger 添加 Entry"></a>向 Ledger 添加 Entry</h4><p>向 Ledger 添加 Entry API 中，最吸引我的是可以指定 EntryId 写入（熟悉 Kafka 的同学知道，向 Kafka 写入数据是可以指定 Partition，但是不能指定 offset，如果可以指定 offset 写入，那么在做容灾时就可以实现 topic 的完全同步，下游可以根据 commit offset 随时切换数据源），其示例如下（注意，Advanced API 在写数据时是强制要指定 entryId 的）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * add the msg to the ledger on the special entryId</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> ledgerHandleAdv ledgerHandleAdv</div><div class="line"> * <span class="doctag">@param</span> entryId         the entry id</div><div class="line"> * <span class="doctag">@param</span> msg             msg</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addEntry</span><span class="params">(LedgerHandleAdv ledgerHandleAdv, <span class="keyword">long</span> entryId, String msg)</span> </span>&#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        ledgerHandleAdv.addEntry(entryId, msg.getBytes());</div><div class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125; <span class="keyword">catch</span> (BKException e) &#123;</div><div class="line">        e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于这个 API，社区官方文档有如下介绍：</p>
<ol>
<li>The entry id has to be non-negative.</li>
<li>Clients are okay to add entries out of order.</li>
<li>However, the entries are only acknowledged in a monotonic order starting from 0.</li>
</ol>
<p>首先，说下我对上面的理解：entry.id 要求是非负的，client 在添加 entry 时可以乱序，但是 entry 只有 0 开始单调顺序增加时才会被 ack。最开始，我以为是只要 entry.id 单调递增就可以，跑了一个测试用例，第一个 entry 的 id 设置为 0，第二个设置为 2，然后程序直接 hang 在那里了，相应日志信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:662 ] - [ DEBUG ]  Got Add response from bookie:XXX.230:3181 rc:EOK, ledger:8:entry:0</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:663 ] - [ DEBUG ]  Got Add response from bookie:XXX.247:3181 rc:EOK, ledger:8:entry:0</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:663 ] - [ DEBUG ]  Submit callback (lid:8, eid: 0). rc:0</div><div class="line">2018-10-19 16:58:34  [ main:663 ] - [ DEBUG ]  Adding entry [50, 32, 109, 97, 116, 116, 32, 116, 101, 115, 116]</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Got Add response from bookie:XXX.247:3181 rc:EOK, ledger:8:entry:2</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Got Add response from bookie:XXX.230:3181 rc:EOK, ledger:8:entry:2</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Head of the queue entryId: 2 is not the expected value: 1</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Got Add response from bookie:XXX.146:3181 rc:EOK, ledger:8:entry:0</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:673 ] - [ DEBUG ]  Head of the queue entryId: 2 is not the expected value: 1</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:681 ] - [ DEBUG ]  Got Add response from bookie:XXX.146:3181 rc:EOK, ledger:8:entry:2</div><div class="line">2018-10-19 16:58:34  [ BookKeeperClientWorker-OrderedExecutor-0-0:681 ] - [ DEBUG ]  Head of the queue entryId: 2 is not the expected value: 1</div><div class="line">2018-10-19 16:58:37  [ main-SendThread(zk01:2181):3702 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</div><div class="line">2018-10-19 16:58:40  [ main-SendThread(zk01:2181):7039 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</div><div class="line">2018-10-19 16:58:43  [ main-SendThread(zk01:2181):10374 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</div><div class="line">2018-10-19 16:58:47  [ main-SendThread(zk01:2181):13710 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</div><div class="line">2018-10-19 16:58:50  [ main-SendThread(zk01:2181):17043 ] - [ DEBUG ]  Got ping response <span class="keyword">for</span> sessionid: 0x3637dbff9e7486c after 0ms</div></pre></td></tr></table></figure>
<p>可以看到有这样的异常日志 <code>Head of the queue entryId: 2 is not the expected value: 1</code>，期望的 entry id 是 1，这里是 2，乱序了，导致程序直接 hang 住（hang 住的原因推测是这个 Entry 没有被 ack），该异常信息出现地方如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendAddSuccessCallbacks</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">// Start from the head of the queue and proceed while there are</span></div><div class="line">    <span class="comment">// entries that have had all their responses come back</span></div><div class="line">    PendingAddOp pendingAddOp;</div><div class="line"></div><div class="line">    <span class="keyword">while</span> ((pendingAddOp = pendingAddOps.peek()) != <span class="keyword">null</span></div><div class="line">           &amp;&amp; blockAddCompletions.get() == <span class="number">0</span>) &#123;</div><div class="line">        <span class="keyword">if</span> (!pendingAddOp.completed) &#123;</div><div class="line">            <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">                LOG.debug(<span class="string">"pending add not completed: &#123;&#125;"</span>, pendingAddOp);</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// Check if it is the next entry in the sequence.</span></div><div class="line">        <span class="keyword">if</span> (pendingAddOp.entryId != <span class="number">0</span> &amp;&amp; pendingAddOp.entryId != pendingAddsSequenceHead + <span class="number">1</span>) &#123;</div><div class="line">            <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">                LOG.debug(<span class="string">"Head of the queue entryId: &#123;&#125; is not the expected value: &#123;&#125;"</span>, pendingAddOp.entryId,</div><div class="line">                           pendingAddsSequenceHead + <span class="number">1</span>);</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        pendingAddOps.remove();</div><div class="line">        explicitLacFlushPolicy.updatePiggyBackedLac(lastAddConfirmed);</div><div class="line">        pendingAddsSequenceHead = pendingAddOp.entryId;</div><div class="line">        <span class="keyword">if</span> (!writeFlags.contains(WriteFlag.DEFERRED_SYNC)) &#123;</div><div class="line">            <span class="keyword">this</span>.lastAddConfirmed = pendingAddsSequenceHead;</div><div class="line">        &#125;</div><div class="line">        pendingAddOp.submitCallback(BKException.Code.OK);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果 entry id 出现了乱序，会导致这个 add 操作没有正常处理。但是如果这里强制要求 entry.id 从 0，而还有序，那么这个 API 跟前面的 API 有什么区别？这点没有搞懂，也向社区发一封邮件咨询，还在等待社区的响应。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着 Apache Pulsar 成为 Apache 的顶级开源项目，其存储层的解决方案 Apache BookKeeper 再次受到业界广泛关注。BookKeeper 在 Pulsar 之前也有很多成功的应用，比如使用 BookKeeper 实现了 HDFS NameNo
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="bk" scheme="http://matt33.com/tags/bk/"/>
    
  </entry>
  
  <entry>
    <title>YARN 架构学习总结</title>
    <link href="http://matt33.com/2018/09/01/yarn-architecture-learn/"/>
    <id>http://matt33.com/2018/09/01/yarn-architecture-learn/</id>
    <published>2018-09-01T14:39:47.000Z</published>
    <updated>2018-09-01T15:57:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>关于 Hadoop 的介绍，这里就不再多说，可以简答来说 Hadoop 的出现真正让更多的互联网公司开始有能力解决大数据场景下的问题，其中的 HDFS 和 YARN 已经成为大数据场景下存储和资源调度的统一解决方案（MR 现在正在被 Spark 所取代，Spark 在计算这块的地位也开始受到其他框架的冲击，流计算上有 Flink，AI 上有 Tensorflow，两面夹击，但是 Spark 的生态建设得很好，其他框架想要在生产环境立马取代还有很长的路要走）。本片文章就是关于 YARN 框架学习的简单总结，目的是希望自己能对分布式调度这块有更深入的了解，当然也希望也这篇文章能够对初学者有所帮助，文章的主要内容来自 <a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a> 和 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>。</p>
<h1 id="Yarn-背景"><a href="#Yarn-背景" class="headerlink" title="Yarn 背景"></a>Yarn 背景</h1><p>关于 YARN 出现的背景，还是得从 Hadoop1.0 说起，在 Hadoop1.0 中，MR 作业的调度还是有两个重要的组件：JobTracker 和 TaskTracker，其基础的架构如下图所示，从下图中可以大概看出原 MR 作业启动流程：</p>
<ol>
<li>首先用户程序 (Client) 提交了一个 job，job 的信息会发送到 JobTracker 中，JobTracker 是 Map-Reduce 框架的中心，它需要与集群中的机器定时通信 (心跳：heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理<strong>所有 job</strong> 失败、重启等操作；</li>
<li>TaskTracker 是 Map-Reduce 集群中每台机器都有的一个组件，它做的事情主要是监视自己所在机器的资源使用情况；</li>
<li>TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以便处理新提交的 job，来决定其应该分配运行在哪些机器上。</li>
</ol>
<p><img src="/images/hadoop/yarn10.png" alt="Hadoop 1.0 调度的架构图"></p>
<p>可以看出原来的调度框架实现非常简答明了，在 Hadoop 推出的最初几年，也获得业界的认可，但是随着集群规模的增大，很多的弊端开始显露出来，主要有以下几点：</p>
<ol>
<li>JobTracker 是 Map-Reduce 的集中处理点，存在<strong>单点故障</strong>；</li>
<li>JobTracker 赋予的功能太多，导致负载过重，1.0 时未将资源管理与作业控制（包括：作业监控、容错等）分开，导致负载重而且无法支撑更多的计算框架，当集群的作业非常多时，会有很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出 Hadoop1.0 的 Map-Reduce 只能支持 4000 节点主机上限的原因；</li>
<li>在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一个节点上，很容易出现 OOM；</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
</ol>
<p>Hadoop 2.0 中下一代 MR 框架的基本设计思想就是将 JobTracker 的两个主要功能，资源管理和作业控制（包括作业监控、容错等），分拆成两个独立的进程。资源管理与具体的应用程序无关，它负责整个集群的资源（内存、CPU、磁盘等）管理，而作业控制进程则是直接与应用程序相关的模块，且每个作业控制进程只负责管理一个作业，这样就是 YARN 诞生的背景，它是在 MapReduce 框架上衍生出的一个资源统一的管理平台。</p>
<h1 id="Yarn-架构"><a href="#Yarn-架构" class="headerlink" title="Yarn 架构"></a>Yarn 架构</h1><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：</p>
<p><img src="/images/hadoop/yarn20.gif" alt="YARN 基本架构"></p>
<h2 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h2><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：</p>
<ol>
<li>调度器：Scheduler；</li>
<li>应用程序管理器：Applications Manager，ASM。</li>
</ol>
<h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><p>调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 <strong>资源容器(Resource Container，也即 Container)</strong>，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。</p>
<h3 id="应用程序管理器"><a href="#应用程序管理器" class="headerlink" title="应用程序管理器"></a>应用程序管理器</h3><p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p>
<h2 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h2><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p>
<h2 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h2><p>提交的每个作业都会包含一个 AM，主要功能包括：</p>
<ol>
<li>与 RM 协商以获取资源（用 container 表示）；</li>
<li>将得到的任务进一步分配给内部的任务；</li>
<li>与 NM 通信以启动/停止任务；</li>
<li>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。</li>
</ol>
<p>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。</p>
<h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>
<h1 id="YARN-作业提交流程"><a href="#YARN-作业提交流程" class="headerlink" title="YARN 作业提交流程"></a>YARN 作业提交流程</h1><p>当用户向 YARN 中提交一个应用程序后，YARN 将分两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成，如下图所示（此图来自<a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a>）：</p>
<p><img src="/images/hadoop/yarn-flow.png" alt="YARN 工作流程"></p>
<p>上图所示的 YARN 工作流程分为以下几个步骤：</p>
<ol>
<li>用户向 YARN 提交应用程序，其中包括 ApplicationMaster 程序，启动 ApplicationMaster 命令、用户程序等；</li>
<li>RM 为该应用程序分配第一个 Container，并与对应的 NM 通信，要求它在这个 Container 中启动应用程序的 ApplicationMaster；</li>
<li>ApplicationMaster 首先向 RM 注册，这样用户可以直接通过 NM 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，一直重复下面的 4-7 步；</li>
<li>ApplicationMaster 采用轮询的方式通过 RPC 协议向 RM 申请和领取资源；</li>
<li>一旦 ApplicationMaster 申请到资源后，便与对应的 NM 通信，要求它启动任务；</li>
<li>NM 为任务设置好运行环境（包括环境变量、jar 包等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</li>
<li>应用程序运行完成后，ApplicationMaster 向 RM 注销并关闭自己（当然像 Storm、Flink 这种常驻应用程序列外）。</li>
</ol>
<h1 id="调度器-1"><a href="#调度器-1" class="headerlink" title="调度器"></a>调度器</h1><p>YARN 的调度器是一个可插拔的组件，目前社区已经提供了 FIFO Scheduler（先进先出调度器）、Capacity Scheduler（能力调度器）、Fair Scheduler（公平调度器），用户也可以继承 ResourceScheduler 的接口实现自定义的调度器，就像 app on yarn 流程一样，不同的应用可以自己去实现，这里只是简单讲述上述三种调度器的基本原理。</p>
<h2 id="FIFO-Scheduler"><a href="#FIFO-Scheduler" class="headerlink" title="FIFO Scheduler"></a>FIFO Scheduler</h2><p>FIFO 是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应的位置，在资源调度时，<strong>按照队列的先后顺序、先进先出地进行调度和资源分配</strong>。</p>
<p>很明显这种调度器过于简单，在实际的生产中，应用不是很多，毕竟需要调度的作业是有不同的优先级的。</p>
<h2 id="公平调度器（Fair-Scheduler）"><a href="#公平调度器（Fair-Scheduler）" class="headerlink" title="公平调度器（Fair Scheduler）"></a>公平调度器（Fair Scheduler）</h2><p>公平调度器先将用户的任务分配到多个资源池（Pool）中，每个资源池设定资源分配最低保障和最高上限，管理员也可以指定资源池的优先级，优先级高的资源池将会被分配更多的资源，当一个资源池有剩余时，可以临时将剩余资源共享给其他资源池。公平调度器的调度过程如下：</p>
<ol>
<li>根据每个资源池的最小资源保障，将系统中的部分资源分配给各个资源池；</li>
<li>根据资源池的指定优先级讲剩余资源按照比例分配给各个资源池；</li>
<li>在各个资源池中，按照作业的优先级或者根据公平策略将资源分配给各个作业；</li>
</ol>
<p>公平调度器有以下几个特点：</p>
<ol>
<li><strong>支持抢占式调度</strong>，即如果某个资源池长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的资源池的任务，以空出资源供这个资源池使用；</li>
<li><strong>强调作业之间的公平性</strong>：在每个资源池中，公平调度器默认使用公平策略来实现资源分配，这种公平策略是最大最小公平算法的一种具体实现，可以尽可能保证作业间的资源分配公平性；</li>
<li><strong>负载均衡</strong>：公平调度器提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到给各个节点上；</li>
<li><strong>调度策略配置灵活</strong>：允许管理员为每个队列单独设置调度策略；</li>
<li><strong>提高小应用程序响应时间</strong>：由于采用了最大最小公平算法，小作业可以快速获得资源并运行完成。</li>
</ol>
<h2 id="能力调度器（Capacity-Scheduler）"><a href="#能力调度器（Capacity-Scheduler）" class="headerlink" title="能力调度器（Capacity Scheduler）"></a>能力调度器（Capacity Scheduler）</h2><p>能力调度器是 Yahool 为 Hadoop 开发的多用户调度器，应用于用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。</p>
<p>它将用户和任务组织成多个队列，每个队列可以设定资源最低保障和使用上限，当一个队列的资源有剩余时，可以将剩余资源暂时分享给其他队列。调度器在调度时，优先将资源分配给资源使用率最低的队列（即队列已使用资源量占分配给队列的资源量比例最小的队列）；在队列内部，则按照作业优先级的先后顺序遵循 FIFO 策略进行调度。</p>
<p>能力调度器有以下几点特点：</p>
<ol>
<li><strong>容量保证</strong>：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源；</li>
<li><strong>灵活性</strong>：如果一个队列资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列；</li>
<li><strong>多重租赁</strong>：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增多多重约束；</li>
<li><strong>安全保证</strong>：每个队列有严格的 ACL 列表规定它访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序；</li>
<li><strong>动态更新配置文件</strong>：管理可以根据需要动态修改各种配置参数。</li>
</ol>
<h1 id="Yarn-容错"><a href="#Yarn-容错" class="headerlink" title="Yarn 容错"></a>Yarn 容错</h1><p>对于分布式系统，不论是调度系统还是其他系统，容错机制都是非常必要的，这里我们简单看下 YARN 的容错机制，YARN 需要做容错的地方，有以下四个地方：</p>
<ol>
<li>ApplicationMaster 容错：ResourceManager 会和 ApplicationMaster 保持通信，一旦发现 ApplicationMaster 失败或者超时，会为其重新分配资源并重启。重启后 ApplicationMaster 的运行状态需要自己恢复，比如 MRAppMaster 会把相关的状态记录到 HDFS 上，重启后从 HDFS 读取运行状态恢复；</li>
<li>NodeManager 容错：NodeManager 如果超时，则 ResourceManager 会认为它失败，将其上的所有 container 标记为失败并通知相应的 ApplicationMaster，由 AM 决定如何处理（可以重新分配任务，可以整个作业失败，重新拉起）；</li>
<li>container 容错：如果 ApplicationMaster 在一定时间内未启动分配的 container，RM 会将其收回，如果 Container 运行失败，RM 会告诉对应的 AM 由其处理；</li>
<li>RM 容错：RM 采用 HA 机制，这里详细讲述一下。</li>
</ol>
<h2 id="ResourceManager-HA"><a href="#ResourceManager-HA" class="headerlink" title="ResourceManager HA"></a>ResourceManager HA</h2><p>因为 RM 是 YARN 架构中的一个单点，所以他的容错很难做，一般是采用 HA 的方式，有一个 active master 和一个 standby master（可参考：<a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="external">ResourceManager High Availability</a>），HA 的架构方案如下图所示：</p>
<p><img src="/images/hadoop/rm-ha-overview.png" alt="YARN RM HA 机制"></p>
<p>关于 YARN 的 RM 的 HA 机制，其实现与 HDFS 的很像，可以参考前面关于 HDFS 文章的讲述 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-2-0-%E7%9A%84-HA-%E5%AE%9E%E7%8E%B0">HDFS NN HA 实现</a>。</p>
<h1 id="分布式调度器总结"><a href="#分布式调度器总结" class="headerlink" title="分布式调度器总结"></a>分布式调度器总结</h1><p>上面基本已经把 YARN 的相关内容总结完了，这个小节主要讲述一下分布式调度系统的一些内容（调度框架只是具体的一种实现方案），主要讲述分布式调度系统要解决的一些问题和分布式调度系统的调度模型。</p>
<h2 id="调度系统设计遇到的基本问题"><a href="#调度系统设计遇到的基本问题" class="headerlink" title="调度系统设计遇到的基本问题"></a>调度系统设计遇到的基本问题</h2><p>对于分布式调度系统，在实际的生产环境中，遇到的问题很相似，这个小节就是看下调度系统主要面对的问题。</p>
<h3 id="资源异构性与工作负载异构性"><a href="#资源异构性与工作负载异构性" class="headerlink" title="资源异构性与工作负载异构性"></a>资源异构性与工作负载异构性</h3><p>在资源管理与调度场景下，有两类异质性需要考虑：</p>
<ol>
<li>资源异质性：这个是从系统拥有资源的角度来看，对于数据中心来说非常常见，数据中心的机器很难保证完全一样的配置，有的配置会高一些，有的会低一些；</li>
<li>工作负载异质性：在大型互联网公司中很常见，因为各种服务和功能特性各异，对资源的需求千差万别。</li>
</ol>
<h3 id="数据局部性（Data-Locality）"><a href="#数据局部性（Data-Locality）" class="headerlink" title="数据局部性（Data Locality）"></a>数据局部性（Data Locality）</h3><p>在大数据场景下，还有一个基本的共识：将计算任务推送到数据所在地进行而不是反过来。因为数据的移动会产生大量低效的数据网络传输开销，而计算代码相比而言数据小得多，所以将计算任务推动到数据所在地是非常常见的，这就是<strong>数据局部性</strong>，在资源调度中，有三种类型的数据局部性，分别是：</p>
<ol>
<li>节点局部性（Node Locality）：计算任务分配到数据所在机器节点，无需任务网络传输；</li>
<li>机架局部性（Rack Locality）：虽然计算任务与数据分布在不同的节点，但这两个节点在同一个机架中，这也是效率较高的一种数据性；</li>
<li>全局局部性（Global Locality）：需要跨机架的传输，会产生较大的网络传输开销。</li>
</ol>
<h3 id="抢占式调度与非抢占式调度"><a href="#抢占式调度与非抢占式调度" class="headerlink" title="抢占式调度与非抢占式调度"></a>抢占式调度与非抢占式调度</h3><p>在多用户场景下，面对已经分配的资源，资源管理调度系统可以有两种不同类型的调度方式：</p>
<ol>
<li>抢占式调度：对于某个计算任务来说，如果空闲资源不足或者出现不同任务共同竞争同一资源，调度系统可以从比当前计算任务优先级低的其他任务中获取已经分配资源，而被抢占资源的计算任务则需要出让资源停止计算；</li>
<li>非抢占式调度：只允许从空闲资源中进行分配，如果当前空闲资源不足，则须等待其他任务释放资源后才能进行。</li>
</ol>
<h3 id="资源分配粒度（Allocation-Granularity）"><a href="#资源分配粒度（Allocation-Granularity）" class="headerlink" title="资源分配粒度（Allocation Granularity）"></a>资源分配粒度（Allocation Granularity）</h3><p>大数据场景下的计算任务往往由两层结构构成：作业级（Job）和任务级（Task），一个作业由多个并发任务构成，任务之间的依赖关系往往形成有向无环图（DAG），比如：MR 作业，关于作业资源分配的粒度，常见的有两种模式：</p>
<ol>
<li>群体分配（全分或不分）：需要将作业的所有所需资源一次性分配完成；</li>
<li>增量满足式分配策略：对于某个作业，只要分配部分资源就能启动一些任务开始运行，随着空闲资源的不断出现，可以逐步增量式分配给作业的其他任务以维护作业不断向后进行。</li>
</ol>
<p>还有一种策略是 <strong>资源储备策略</strong>，它指的是只有分配到一定量的资源资源才能启动，但是在未获得足够资源的时候，作业可以先持有目前已经分配的资源，并等待其他作业释放资源，这样从调度系统不断获取新资源并进行储备和累积，直到分配到的资源量达到最低标准后开始运行。</p>
<h3 id="饿死（Starvation）与死锁（Dead-Lock）问题"><a href="#饿死（Starvation）与死锁（Dead-Lock）问题" class="headerlink" title="饿死（Starvation）与死锁（Dead Lock）问题"></a>饿死（Starvation）与死锁（Dead Lock）问题</h3><p>饿死和死锁是一个合理的资源调度系统需要避免的两个问题：</p>
<ol>
<li>饿死：指的是这个计算任务持续上时间无法获得开始执行所需的最少资源量，导致一直处于等待执行的状态，比如在资源紧张的情形下，有些低优先级的任务始终无法获得资源分配机会，如果不断出现新提交的高优先级任务，则这些低优先级任务就会出现饿死现象；</li>
<li>死锁：指的是由于资源调度不当导致整个调度无法继续正常执行，比如前面提高的资源储备策略就有可能导致调度系统进入死锁状态，多个作业占有一定作业的情况下，都在等待新的资源释放。</li>
</ol>
<h3 id="资源隔离方法"><a href="#资源隔离方法" class="headerlink" title="资源隔离方法"></a>资源隔离方法</h3><p>目前对于资源隔离最常用的手段是 Linux 容器（Linux Container，LXC，可以参考<a href="https://www.redhat.com/zh/topics/containers/whats-a-linux-container" target="_blank" rel="external">什么是 Linux 容器？</a>），YARN 和 Mesos 都是采用了这种方式来实现资源隔离。LXC 是一种轻量级的内核虚拟化技术，可以用来进行资源和进程运行的隔离，通过 LXC 可以在一台物理机上隔离出多个互相隔离的容器。LXC 在资源管理方面依赖于 Linux 内核的 cgroups 子系统，cgroups 子系统是 Linux 内核提供的一个基于进程组的资源管理的框架，可以为特定的进程组限定可以使用的资源。</p>
<h2 id="调度器模型"><a href="#调度器模型" class="headerlink" title="调度器模型"></a>调度器模型</h2><p>关于资源管理与调度功能的实际功能，分布式调度器根据运行机制的不同进行分类，可以归纳为三种资源管理与调度系统泛型：</p>
<ol>
<li>集中式调度器；</li>
<li>两级调度器；</li>
<li>状态共享调度器。</li>
</ol>
<p>它们的区别与联系如下图所示：</p>
<p><img src="/images/hadoop/scheduler.png" alt="三种调度模型"></p>
<h3 id="集中式调度器"><a href="#集中式调度器" class="headerlink" title="集中式调度器"></a>集中式调度器</h3><p>集中式调度器在整个系统中只运行一个全局的中央调度器实例，所有之上的框架或者计算任务的资源请求全部经由中央调度器来满足（也就是说：资源的使用及任务的执行状态都由中央调度器管理），因此，整个调度系统缺乏并发性且所有调度逻辑全部由中央调度器来完成。集中式调度器有以下这些特点：</p>
<ol>
<li>适合批处理任务和吞吐量较大、运行时间较长的任务；</li>
<li>调度逻辑全部融入到了中央调度器，实现逻辑复杂，灵活性和策略的可扩展性不高；</li>
<li>并发性能较差，比较适合小规模的集群系统；</li>
<li>状态同步比较容易且稳定，这是因为资源使用和任务执行的状态被统一管理，降低了状态同步和并发控制的难度。</li>
</ol>
<h3 id="两级调度器"><a href="#两级调度器" class="headerlink" title="两级调度器"></a>两级调度器</h3><p>对于集中式调度器的不足之处，两级调度器是一个很好的解决方案，它可以看做一种策略下放的机制，它将整个系统的调度工作分为两个级别：</p>
<ol>
<li>中央调度器：中央调度器可以看到集群中所有机器的可用资源并管理其状态，它可以按照一定策略将集群中的所有资源更配各个计算框架，中央调度器级别的资源调度是一种粗粒度的资源调度方式；</li>
<li>框架调度器：各个计算框架在接收到所需资源后，可以根据自身计算任务的特性，使用自身的调度策略来进一步细粒度地分配从中央调度器获得的各种资源。</li>
</ol>
<p>在这种两级调度器架构中，只有中央调度器能够观察到所有集群资源的状态，而每个框架并无全局资源概念（不知道整个集群资源使用情况），只能看到由中央调度器分配给自己的资源，Mesos、YARN 和 Hadoop on Demand 系统是3个典型的两级调度器。两级调度的缺点也非常明显：</p>
<ol>
<li><strong>各个框架无法知道整个集群的实时资源使用情况</strong>：很多框架不需要知道整个集群的实时资源使用情况就可以运行得很顺畅，但是对于其他一些应用，为之提供实时资源使用情况可以挖掘潜在的优化空间；</li>
<li><strong>采用悲观锁，并发粒度小</strong>：悲观锁通常采用锁机制控制并发，这会大大降低性能。</li>
</ol>
<h3 id="状态共享调度器"><a href="#状态共享调度器" class="headerlink" title="状态共享调度器"></a>状态共享调度器</h3><p>通过前面两种模型的介绍，可以发现集群中需要管理的状态主要包括以下两种：</p>
<ol>
<li>系统中资源分配和使用的状态；</li>
<li>系统中任务调度和执行的状态</li>
</ol>
<p>在集中式调度器中，这两个状态都由中心调度器管理，并且一并集成了调度等功能；而在双层调度器中，这两个状态分别由中央调度器和框架调度器管理。集中式调度器可以容易地保证全局状态的一致性，但是可扩展性不够；双层调度器对共享状态的管理较难达到好的一致性保证，也不容易检测资源竞争和死锁。</p>
<p>这也就催生出了另一种调度器 —— 状态共享调度器（Shared-State Scheduler），它是 Google 的 Omega 调度系统提出的一种调度器模型。在这种调度器中，每个计算框架可以看到整个集群中的所有资源，并采用互相竞争的方式去获取自己所需的资源，根据自身特性采取不同的具体资源调整策略，同时系统采用了乐观并发控制手段解决不同框架在资源竞争过程中出现的需求冲突。这样，状态共享调度器在一下两个方面对两级调度器做了相应的优化：</p>
<ol>
<li><strong>乐观并发控制增加了系统的并发性能</strong>；</li>
<li>每个计算框架都可以获得全局的资源使用状况；</li>
</ol>
<p>与两级调度器对比，两者的根本区别在于 <strong>中央调度器功能的强弱不同</strong>，两级调度器依赖中央调度器来进行第一次资源分配，而 Omega 则严重弱化中央调度器的功能，只是维护一份可恢复的集群资源状态信息的主副本，这份数据被称为 <strong>单元状态（Cell State）</strong></p>
<ol>
<li>每个框架在自身内部会维护 单元状态 的一份私有并不断更新的副本信息，而框架对资源的需求则直接在这份副本信息上进行；</li>
<li>只要框架具有特定的优先级，就可以在这份副本信息上申请相应的闲置资源，也可以抢夺已经分配给其他比自身优先级低的计算任务的资源；</li>
<li>一旦框架做出资源决策，则可以改变私有 单元状态 信息并将其同步到全局的 单元状态 信息中区，这样就完成了资源申请并使得这种变化让其他框架可见；</li>
<li>上述资源竞争过程通过 <strong>事务操作</strong> 来进行，保证了操作的原子性。</li>
</ol>
<p>如果两个框架竞争同一份资源，因其决策过程都是在各自私有数据上做出的，并通过原子事务进行提交，系统保证此种情形下只有一个竞争胜出者，而失败者可以后续继续重新申请资源，这是一种乐观并发控制手段，可以增加系统的整体并发性能。</p>
<p>从上面的过程，可以看出，这种架构是一种 <strong>以效率优先，不太考虑资源分配公平性</strong> 的策略，很明显高优先级的任务总是能够在资源竞争过程中获胜，而低优先级的任务存在由于长时间无法竞争到所需资源而被【饿死】的风险。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a>;</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">Hadoop Yarn</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/" target="_blank" rel="external">Hadoop 新 MapReduce 框架 Yarn 详解</a>；</li>
<li><a href="http://shiyanjun.cn/archives/1119.html" target="_blank" rel="external">Hadoop YARN架构设计要点</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/data/library/bd-yarn-intro/index.html" target="_blank" rel="external">YARN 简介</a>；</li>
<li><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="external">理解Hadoop YARN架构</a>；</li>
<li><a href="https://dbaplus.cn/news-141-2004-1.html" target="_blank" rel="external">这里有7种主流案例，告诉你调度器架构设计通用法则</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于 Hadoop 的介绍，这里就不再多说，可以简答来说 Hadoop 的出现真正让更多的互联网公司开始有能力解决大数据场景下的问题，其中的 HDFS 和 YARN 已经成为大数据场景下存储和资源调度的统一解决方案（MR 现在正在被 Spark 所取代，Spark 在计算这
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hadoop" scheme="http://matt33.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>如何学习开源项目</title>
    <link href="http://matt33.com/2018/08/01/system-learn-summary/"/>
    <id>http://matt33.com/2018/08/01/system-learn-summary/</id>
    <published>2018-08-01T06:55:11.000Z</published>
    <updated>2018-11-19T02:30:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章的方法论内容基本来自订阅的极客时间-李运华老师的《从0开始学架构》中的一篇文章，会结合自己的学习经验、加上以 Flink 为例来做一个总结，也为了让自己再学习其他开源项目时能够按照这样的一个方法论高效的深入学习。先简单说一下开源项目，开源项目最早从上个世纪开始，我知道最早的是 linux 项目（其他的不是很了解），再到近几年大数据领域，发展非常迅速，开源给本公司带来的好处，首先是提高这家在技术界的影响力，然后如果这个项目比较受大家认可，那么这家公司的这个技术可能会成为业界的统一解决方案，就像 Hadoop、Kafka 等。对其他公司的好处是，节省成本、可以快速应用来解决业务中的问题。</p>
<p>但是对于公司技术员工不好的地方是，作为这些项目的维护者，以后真的可能变成运维工程师，社区这些项目发展一般都很快，小厂很难有足够的人力、能力在这上面做太多的研发，一般都是跟着社区升级，可能发展到最后的结果是: 项目的研发由社区（或者背后主导的公司）来负责，其他公司融入这一生态，做好运维工作或产品化的东西就可以了，稳定性、可靠性、新功能交给社区，随着项目逐渐庞大，到最后可能其他公司很少有人能对这些项目有较强的掌控力，研发完全依赖于社区。这些开源项目的接口变得越来越简单、内部越来越复杂时，虽然降低了开发者、维护者的门槛，但也降低对开发者、维护者的要求，这是一把双刃剑，对于在技术上对自己有一定要求的工程师，不应该仅限于使用、原理等。</p>
<p>上面是一些浅薄的想法，下面开始结合李运华老师的文章总结学习开源项目的方法论。首先在学习开源项目时，有几点需要明确的是：</p>
<ol>
<li>先树立正确观念，不管你是什么身份，都可以从开源项目中学到很多东西（比如：要学习 Redis 的网络模型，不需要我们成为 Redis 的开发者，也不需要一定要用到 Redis，只需要具备一定的网络编程基础，再通过阅读 Redis 源码，就可以学习 Redis 这种单进程的 Reactor 模型）；</li>
<li>不要只盯着数据结构和算法，这些在学习开源项目时并没有那么重要（Nginx 使用红黑树来管理定时器，对于大多数人只需要这一点就足够了，并不需要研究 Nginx 实现红黑树的源码是如何写的，除非需要修改这部分的逻辑代码）；</li>
<li>采取<strong>自顶向下</strong>的学习方法，源码不是第一步，而是最后一步（基本掌握了功能、原理、关键设计之后再去看源码，看源码的主要目的是为了学习其代码的写作方式以及关键技术的实现）。</li>
</ol>
<blockquote>
<p>例如，Redis 的 RDB 持久化模式「会将当前内存中的数据库快照保存到磁盘文件中」，那这里所谓的 “数据库快照” 到底是怎么做的呢？在 Linux 平台上其实就是 fork 一个子进程保存就可以了；那为何 fork 子进程就生成了数据库快照了呢？这又和 Linux 的父子进程机制以及 copy-on-write 技术相关了。通过这种学习方式，既能够快速掌握系统设计的关键点（Redis 和 RDB 模式），又能够掌握具体的变成技巧（内存快照）。</p>
</blockquote>
<p>下面来看下李运华老师的『自顶向下』的学习方法和步骤。</p>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>这里的安装并不是对着手册执行一下命令，而是要通过安装过程，获取到如下一些关键的信息：</p>
<ul>
<li>这个系统的依赖组件，而依赖的组件又是系统设计和实现的基础;</li>
<li>安装目录也能够提供一些使用和运行的基本信息；</li>
<li>系统提供了哪些工具方便我们使用（<strong>带着问题去学习效率是最高的</strong>）。</li>
</ul>
<p>以 Nginx 为例，源码安装时依赖的库有 pcre、pcre-devel、openssl、openssl-devel、zlib，光从名字上看能够了解一些信息，例如 openssl 可能和 https 有关，zlib 可能与压缩有关。再以 Memcache 为例，它最大的依赖库就是 libevent，而根据 libevent 是一个高性能的网络库，大概能够推测 Memcache 的网络实现应该是 Reactor 模型。</p>
<p>例如，flink1.5.0安装完成后，目录如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[XXX@matt@pro flink-1.5.0]$ ll</div><div class="line">total 52</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 bin</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:57 conf</div><div class="line">drwxr-xr-x 6 XXX XXX  4096 Jul  9 23:39 examples</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 lib</div><div class="line">-rw-r--r-- 1 XXX XXX 18197 Jul  9 23:39 LICENSE</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:57 <span class="built_in">log</span></div><div class="line">-rw-r--r-- 1 XXX XXX   779 Jul  9 23:39 NOTICE</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 opt</div><div class="line">-rw-r--r-- 1 XXX XXX  1308 Jul  9 23:39 README.txt</div></pre></td></tr></table></figure>
<p>上面 bin 是运行程序，conf 是配置文件的目录，lib 和 opt 是依赖的相关 jar 包，但为什么分为两个目录去放，这个我还不是很明白。下面是目录的详细内容:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">[XXX@matt@pro flink-1.5.0]$ ll bin/</div><div class="line">total 116</div><div class="line">-rwxr-xr-x 1 XXX XXX 23957 Jul  9 23:39 config.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2224 Jul  9 23:39 flink</div><div class="line">-rwxr-xr-x 1 XXX XXX  1271 Jul  9 23:39 flink.bat</div><div class="line">-rwxr-xr-x 1 XXX XXX  2823 Jul  9 23:39 flink-console.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  6407 Jul  9 23:39 flink-daemon.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1482 Jul  9 23:39 historyserver.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2652 Jul  9 23:39 jobmanager.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1802 Jul  9 23:39 mesos-appmaster-job.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1971 Jul  9 23:39 mesos-appmaster.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2013 Jul  9 23:39 mesos-taskmanager.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1164 Jul  9 23:39 pyflink.bat</div><div class="line">-rwxr-xr-x 1 XXX XXX  1107 Jul  9 23:39 pyflink.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1182 Jul  9 23:39 pyflink-stream.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  3434 Jul  9 23:39 sql-client.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  3364 Jul  9 23:39 start-cluster.bat</div><div class="line">-rwxr-xr-x 1 XXX XXX  1836 Jul  9 23:39 start-cluster.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2960 Jul  9 23:39 start-scala-shell.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1854 Jul  9 23:39 start-zookeeper-quorum.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1616 Jul  9 23:39 stop-cluster.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1845 Jul  9 23:39 stop-zookeeper-quorum.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  3543 Jul  9 23:39 taskmanager.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1674 Jul  9 23:39 yarn-session.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2281 Jul  9 23:39 zookeeper.sh</div><div class="line">[XXX@matt@pro flink-1.5.0]$ ll lib/</div><div class="line">total 88972</div><div class="line">-rw-r--r-- 1 XXX XXX 90458504 Jul  9 23:39 flink-dist_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   142041 Jul  9 23:39 flink-python_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   489884 Jul  9 23:39 <span class="built_in">log</span>4j-1.2.17.jar</div><div class="line">-rw-r--r-- 1 XXX XXX     8870 Jul  9 23:39 slf4j-log4j12-1.7.7.jar</div><div class="line">[XXX@matt@pro flink-1.5.0]$ ll opt/</div><div class="line">total 193956</div><div class="line">-rw-r--r-- 1 XXX XXX    48215 Jul  9 23:39 flink-avro-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   124115 Jul  9 23:39 flink-cep_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    49235 Jul  9 23:39 flink-cep-scala_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   630006 Jul  9 23:39 flink-gelly_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   759288 Jul  9 23:39 flink-gelly-scala_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    21140 Jul  9 23:39 flink-json-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    16835 Jul  9 23:39 flink-metrics-datadog-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   136599 Jul  9 23:39 flink-metrics-dropwizard-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   278137 Jul  9 23:39 flink-metrics-ganglia-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   161637 Jul  9 23:39 flink-metrics-graphite-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    89072 Jul  9 23:39 flink-metrics-prometheus-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX     6029 Jul  9 23:39 flink-metrics-slf4j-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX     7712 Jul  9 23:39 flink-metrics-statsd-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 27197071 Jul  9 23:39 flink-ml_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    17916 Jul  9 23:39 flink-queryable-state-runtime_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 30676687 Jul  9 23:39 flink<span class="_">-s</span>3-fs-hadoop-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 38244766 Jul  9 23:39 flink<span class="_">-s</span>3-fs-presto-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 18517471 Jul  9 23:39 flink-sql-client-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 37325999 Jul  9 23:39 flink-streaming-python_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 26088550 Jul  9 23:39 flink-swift-fs-hadoop-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 18172108 Jul  9 23:39 flink-table_2.11-1.5.0.jar</div><div class="line">[XXX@matt@pro flink-1.5.0]$ ll conf/</div><div class="line">total 56</div><div class="line">-rw-r--r-- 1 XXX XXX 9866 Jul  9 23:57 flink-conf.yaml</div><div class="line">-rw-r--r-- 1 XXX XXX 2138 Jul  9 23:39 <span class="built_in">log</span>4j-cli.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 1884 Jul  9 23:39 <span class="built_in">log</span>4j-console.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 1939 Jul  9 23:39 <span class="built_in">log</span>4j.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 1709 Jul  9 23:39 <span class="built_in">log</span>4j-yarn-session.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 2294 Jul  9 23:39 logback-console.xml</div><div class="line">-rw-r--r-- 1 XXX XXX 2331 Jul  9 23:39 logback.xml</div><div class="line">-rw-r--r-- 1 XXX XXX 1550 Jul  9 23:39 logback-yarn.xml</div><div class="line">-rw-r--r-- 1 XXX XXX   15 Jul  9 23:39 masters</div><div class="line">-rw-r--r-- 1 XXX XXX  120 Jul  9 23:39 slaves</div><div class="line">-rw-r--r-- 1 XXX XXX 2755 Jul  9 23:39 sql-client-defaults.yaml</div><div class="line">-rw-r--r-- 1 XXX XXX 1434 Jul  9 23:39 zoo.cfg</div></pre></td></tr></table></figure>
<p>比如这里我们想查一下 <code>sql-client.sh</code> 是做什么的？应该怎么使用？不同参数是什么意思，可以通过 help 信息查看。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">./bin/sql-client.sh</div><div class="line">./sql-client [MODE] [OPTIONS]</div><div class="line"></div><div class="line">The following options are available:</div><div class="line"></div><div class="line">Mode <span class="string">"embedded"</span> submits Flink <span class="built_in">jobs</span> from the <span class="built_in">local</span> machine.</div><div class="line"></div><div class="line">  Syntax: embedded [OPTIONS]</div><div class="line">  <span class="string">"embedded"</span> mode options:</div><div class="line">     <span class="_">-d</span>,--defaults &lt;environment file&gt;      The environment properties with <span class="built_in">which</span></div><div class="line">                                           every new session is initialized.</div><div class="line">                                           Properties might be overwritten by</div><div class="line">                                           session properties.</div><div class="line">     <span class="_">-e</span>,--environment &lt;environment file&gt;   The environment properties to be</div><div class="line">                                           imported into the session. It might</div><div class="line">                                           overwrite default environment</div><div class="line">                                           properties.</div><div class="line">     -h,--help                             Show the <span class="built_in">help</span> message with</div><div class="line">                                           descriptions of all options.</div><div class="line">     -j,--jar &lt;JAR file&gt;                   A JAR file to be imported into the</div><div class="line">                                           session. The file might contain</div><div class="line">                                           user-defined classes needed <span class="keyword">for</span> the</div><div class="line">                                           execution of statements such as</div><div class="line">                                           <span class="built_in">functions</span>, table sources, or sinks.</div><div class="line">                                           Can be used multiple times.</div><div class="line">     <span class="_">-l</span>,--library &lt;JAR directory&gt;          A JAR file directory with <span class="built_in">which</span> every</div><div class="line">                                           new session is initialized. The files</div><div class="line">                                           might contain user-defined classes</div><div class="line">                                           needed <span class="keyword">for</span> the execution of</div><div class="line">                                           statements such as <span class="built_in">functions</span>, table</div><div class="line">                                           sources, or sinks. Can be used</div><div class="line">                                           multiple times.</div><div class="line">     <span class="_">-s</span>,--session &lt;session identifier&gt;     The identifier <span class="keyword">for</span> a session.</div><div class="line">                                           <span class="string">'default'</span> is the default identifier.</div></pre></td></tr></table></figure>
<h3 id="2-运行"><a href="#2-运行" class="headerlink" title="2. 运行"></a>2. 运行</h3><p>安装完成后，我们需要真正将系统运行起来，运行系统的时候有两个地方要特别关注：<strong>命令行和配置文件</strong>，它们主要提供了两个非常关键的信息：</p>
<ol>
<li>系统具备哪些能力（提供哪些可配置化的参数，这些参数是做什么的以及不同的配置带来的影响是什么）；</li>
<li>系统将会如何运行。</li>
</ol>
<p>这些信息是我们窥视系统内部运行机制和原理的一扇窗口。</p>
<p>例如，下面 Flink 配置中一些配置参数（Flink 集群模式的安装和运行可以参考 <a href="http://wuchong.me/blog/2016/02/26/flink-docs-setup-cluster/" target="_blank" rel="external">Flink官方文档翻译：安装部署（集群模式）</a>），通过这几个启动时的配置参数，我们可以获取下面这些信息：</p>
<ul>
<li><code>jobmanager.rpc.address</code>：The external address of the JobManager, which is the master/coordinator of the distributed system (DEFAULT: localhost)；</li>
<li><code>jobmanager.rpc.port</code>：The port number of the JobManager (DEFAULT: 6123)；</li>
<li><code>jobmanager.heap.mb</code>：JVM heap size (in megabytes) for the JobManager. You may have to increase the heap size for the JobManager if you are running very large applications (with many operators), or if you are keeping a long history of them.</li>
<li><code>taskmanager.numberOfTaskSlots</code>： JVM heap size (in megabytes) for the TaskManagers, which are the parallel workers of the system；</li>
<li><code>taskmanager.numberOfTaskSlots</code>: The number of parallel operator or user function instances that a single TaskManager can run (DEFAULT: 1).</li>
<li><code>parallelism.default</code>：The default parallelism to use for programs that have no parallelism specified. (DEFAULT: 1).；</li>
</ul>
<p>通过上面这些配置参数，我们基本上可以看到 Flink 的 Master/Salve 模型，是分为 JobManager 和 TaskManager，而 TaskManager 中又有对应的 TaskSlot，系统也提供了相应配置参数进行设置，Flink 1.5.0 的配置信息可以参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.5/ops/config.html" target="_blank" rel="external">Flink 1.5.0 配置</a>，社区的文档对这些参数描述得非常清楚，如果之前有大数据系统的基础，比如了解 HDFS、YARN、Spark、Storm、Kafka 的架构，那么在看到这些参数时，其实并不会感觉到太陌生，分布式系统很多东西都是相通的。</p>
<p>在通常情况下，如果我们将每个命令行参数和配置项的作用和原理都全部掌握清楚了的话，基本上对系统已经很熟悉了。这里李运华老师介绍了一个他的经验，那么就是：<strong>不管三七二十一，先把所有的配置项全部研究一遍，包括配置项的原理、作用、影响，并且尝试去修改配置项然后看看系统会有什么变化</strong>。</p>
<h3 id="3-原理研究"><a href="#3-原理研究" class="headerlink" title="3. 原理研究"></a>3. 原理研究</h3><p>在完成前两个步骤后，我们对系统已经有了初步的感觉和理解，此时可以更进一步去研究其原理。其实在研究命令行和配置项的时候已经涉及一部分原理了，但是并不是很系统，因此我们要专门针对原理进行系统性的研究。这里的关键就是<strong>系统性</strong>三个字，怎么才算系统性呢？主要体现在如下几个方面：</p>
<h4 id="3-1-关键特性的基本实现原理"><a href="#3-1-关键特性的基本实现原理" class="headerlink" title="3.1. 关键特性的基本实现原理"></a>3.1. 关键特性的基本实现原理</h4><p>每个应用广泛的开源项目之所以能够受到大众的欢迎，肯定是有一些卖点的，有一些它们的应用场景，常见的有高性能、高可用、可扩展等特性，那到底这些项目是如何做到其所宣称的那么牛的呢？这些牛的地方就是我们需要深入学习的地方：</p>
<ol>
<li>Memcache 的高性能具体是怎么做到的呢？首先是基于 libevent 实现了高性能的网络模型，其次是内存管理 Slab Allocator 机制。为了彻底理解 Memcache 的高性能网络模型，我们需要掌握很多知识：多路复用、Linux epoll、Reactor 模型、多线程等，通过研究 Memcache 的高性能网络模型，我们能够学习一个具体的项目中如何将这些东西全部串起来实现了高性能。</li>
<li>再以 React 为例，Virtual DOM 的实现原理是什么、为何要实现 Virtual DOM、React 是如何构建 Virtual DOM 树、Virtual DOM 与 DOM 什么关系等，通过研究学习 Virtual DOM，即使不使用 React，我们也能够学习如何写出高性能的前端的代码。</li>
<li>这里再以 Kafka 为例，Kafka 作为在大数据领域应用非常广泛的消息队列，它是如何实现它宣称的高性能的、高可靠？以及在 0.11.0 之后的版本它是如何实现幂等性、事务性的？在 2.0 之后是如何实现可以支撑千台机器、百万 partition 规模的？通过深入学习一些，能够让我们学学习到大数据存储系统的可靠性、高性能实现方案，以及分布式一致性（事务性）的实现；</li>
<li>最后以 Flink 为例，Flink 最开始的卖点是 Exactly once 和低延迟，现在的话再加上流式系统 SQL 的支持，那么它与 Storm、Spark streaming 相比，Flink 的 Exactly once 是怎么实现的？为什么 Storm 在现有机制上（不含 Trident）无法实现 Exactly once？Spark Streaming 微批处理模型延迟消耗主要在什么地方？为什么 Flink 可以做到低延迟？Flink  怎么实现窗口计算以及 Flink SQL 是怎么实现的，以及 Flink SQL 现在面对的问题是什么？</li>
</ol>
<h4 id="3-2-优缺点对比分析"><a href="#3-2-优缺点对比分析" class="headerlink" title="3.2. 优缺点对比分析"></a>3.2. 优缺点对比分析</h4><p>这是我想特别强调的一点，<strong>只有清楚掌握技术方案的优缺点后才算真正的掌握这门技术，也只有掌握了技术方案的优缺点后才能在架构设计的时候做出合理的选择。优缺点主要通过对比来分析，即：我们将两个类似的系统进行对比，看看它们的实现差异，以及不同的实现优缺点都是什么</strong>。</p>
<ol>
<li>典型的对比有 Memcache 和 Redis，例如（仅举例说明，实际上对比的点很多），Memcache 用多线程，Redis 用单进程，各有什么优缺点？Memcache 和 Redis 的集群方式，各有什么优缺点？</li>
<li>即使是 Redis 自身，我们也可以对比 RDB 和 AOF 两种模式的优缺点。</li>
</ol>
<h4 id="3-3-如何系统性学习一个开源项目"><a href="#3-3-如何系统性学习一个开源项目" class="headerlink" title="3.3. 如何系统性学习一个开源项目"></a>3.3. 如何系统性学习一个开源项目</h4><p>在你了解了什么是【系统性】后，我来介绍一下原理研究的手段，主要有三种：</p>
<ol>
<li>通读项目的设计文档：例如 Kafka 的设计文档，基本涵盖了消息队列设计的关键决策部分；Disruptor 的设计白皮书，详细的阐述了 Java 单机高性能的设计技巧（官方文档是学习一个项目的必须资料）。</li>
<li>阅读网上已有的分析文档：通常情况下比较热门的开源项目，都已经有非常多的分析文档了，我们可以站在前人的基础上，避免大量的重复投入。但需要注意的是，由于经验、水平、关注点等差异，不同的人分析的结论可能有差异，甚至有的是错误的，因此不能完全参照。一个比较好的方式就是多方对照，也就是说看很多篇分析文档，比较它们的内容共同点和差异点（网上分析文档很多，但是要知道如何区分这些分析文档，多对比一些，同一个东西，每个人的理解并不一定相同）。</li>
<li>Demo 验证：如果有些技术点难以查到资料，自己又不确定，则可以真正去写 Demo 进行验证，通过打印一些日志或者调试，能清晰的理解具体的细节。例如，写一个简单的分配内存程序，然后通过日志和命令行（jmap、jstat、jstack 等）来查看 Java 虚拟机垃圾回收时的具体表现（开源项目一般都会有一些实例供我们学习参考，这也是我们学习一个项目的重要资料，先去看如何使用，再去看不同使用方式背后的原理）。</li>
</ol>
<h3 id="4-测试"><a href="#4-测试" class="headerlink" title="4. 测试"></a>4. 测试</h3><p>通常情况下，如果你真的准备在实际项目中使用某个开源项目的话，必须进行测试。有的同学可能会说，网上的分析和测试文档很多，直接找一篇看就可以了？如果只是自己学习和研究，这样做是可以的，因为构建完整的测试用例既需要耗费较多时间，又需要较多机器资源，如果每个项目都这么做的话，投入成本有点大；但如果是要在实践项目中使用，必须自己进行测试，因为网上搜的测试结果，不一定与自己的业务场景很契合，如果简单参考别人的测试结果，很可能会得出错误的结论。例如，开源系统的版本不同，测试结果可能差异较大。同样是 K-V 存储，别人测试的 value 是 128 字节，而你的场景 value 都达到了 128k 字节，两者的测试结果也差异很大，不能简单照搬（在实际真正应用前，需要足够的性能测试，而且要能分析出测试结论背后的理论原因，如果找不到理论做为支撑，这样的测试并不是可信的，因为网络中环境有很大的随机性）。</p>
<p>测试阶段需要特别强调的一点就是：测试一定要在原理研究之后做，不能安装完成立马就测试！原因在于如果对系统不熟悉，很可能出现命令行、配置参数没用对，或者运行模式选择不对，导致没有根据业务的特点搭建正确的环境、没有设计合理的测试用例，从而使得最终的测试结果得出了错误结论，误导了设计决策。曾经有团队安装完成 MySQL 5.1 后就进行性能测试，测试结果出来让人大跌眼镜，经过定位才发现 <code>innodb_buffer_pool_size</code> 使用的是默认值 8M。</p>
<h3 id="5-源码研究"><a href="#5-源码研究" class="headerlink" title="5. 源码研究"></a>5. 源码研究</h3><p>源码研究的主要目的是<strong>学习原理背后的具体编码如何实现，通过学习这些技巧来提升我们自己的技术能力</strong>。例如 Redis 的 RDB 快照、Nginx 的多 Reactor 模型、Disruptor 如何使用 volatile 以及 CAS 来做无锁设计、Netty 的 Zero-Copy 等，这些技巧都很精巧，掌握后能够大大提升自己的编码能力。</p>
<p>通常情况下，不建议通读所有源码，因为想掌握每行代码的含义和作用还是非常耗费时间的，尤其是 MySQL、Nginx 这种规模的项目，即使是他们的开发人员，都不一定每个人都掌握了所有代码。带着明确目的去研究源码，做到有的放矢，才能事半功倍，这也是源码研究要放在最后的原因。</p>
<h3 id="时间分配"><a href="#时间分配" class="headerlink" title="时间分配"></a>时间分配</h3><p>前面介绍的【自顶向下】五个步骤，完整执行下来需要花费较长时间，而时间又是大部分技术人员比较稀缺的资源。很多人在学习技术的时候都会反馈说时间不够，版本进度很紧，很难有大量的时间进行学习，但如果不学习感觉自己又很难提升？面对这种两难问题，具体该如何做呢？</p>
<p>通常情况下，以上 5 个步骤的前 3 个步骤，不管是已经成为架构师的技术人员，还是立志成为架构师的技术人员，在研究开源项目的时候都必不可少；第四步可以在准备采用开源项目的时候才实施，第五步可以根据你的时间来进行灵活安排。这里的“灵活安排”不是说省略不去做，而是在自己有一定时间和精力的时候做，因为只有这样才能真正理解和学到具体的技术。</p>
<p>如果感觉自己时间和精力不够，与其蜻蜓点水每个开源项目都去简单了解一下，还不如集中精力将一个开源项目研究通透，就算是每个季度只学习一个开源项目，积累几年后这个数量也是很客观的；而且一旦你将一个项目研究透以后，再去研究其他类似项目，你会发现自己学习的非常快，因为共性的部分你已经都掌握了，只需要掌握新项目差异的部分即可。</p>
<p>这里个人的感想是，对于初中级工程师，最好还是要有2-3项目或者2-3个方向需要走到第五步，毕竟我们是靠这个吃饭的，对于其他的项目（目前业界统一的解决方案，比如 hdfs、hbase、spark 等），至少需要走到第四步，这样与这方面的专业人士沟通交流时，至少让自己不至于处在懵逼状态。当然对于这些项目的核心代码，也是可以深入学习，比如 spark 的 shuffle 在代码上是如何实现的等。在一个项目上深入之后，再去看同一个领域的其他项目时，当看到其他的架构时，其实我们基本上就可以清楚这架构设计的原因、要解决的问题以及这种设计带来的其他问题，每种设计都有其应用场景，比如对 Kafka 有了深入了解后，再看 RocketMQ、phxqueue 时，看到它们的架构方案基本上就明白要解决的问题以及其特定的应用场景，当然对一些独特的特性，还是需要深入到代码层面去学习的，比如 RocketMQ 的延迟队列实现。这是一些个人的感想，并不一定对，大家可以共同交流，总之，是感觉李运华老师这篇文章是值得总结分享的，希望自己在后面学习开源项目时，能够静下心、认真坚持学下去。</p>
<hr>
<p>最后，非常推荐极客时间李运华老师的《从0开始学架构》的专栏，里面涉猎的内容很多，讲得也都比较深入，如果能结合这个专栏去学习，是非常能开阔自己的技术视野，需要的同学可以通过下面的连接购买，会有一定的优惠。</p>
<p><img src="/images/share/learn-the-design-of-architecture.jpeg" alt="Apache BookKeeper 架构图"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章的方法论内容基本来自订阅的极客时间-李运华老师的《从0开始学架构》中的一篇文章，会结合自己的学习经验、加上以 Flink 为例来做一个总结，也为了让自己再学习其他开源项目时能够按照这样的一个方法论高效的深入学习。先简单说一下开源项目，开源项目最早从上个世纪开始，我知
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="思考" scheme="http://matt33.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>JVM 之 ParNew 和 CMS 日志分析</title>
    <link href="http://matt33.com/2018/07/28/jvm-cms/"/>
    <id>http://matt33.com/2018/07/28/jvm-cms/</id>
    <published>2018-07-28T11:53:04.000Z</published>
    <updated>2018-07-29T04:20:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>在两年前的文章 <a href="http://matt33.com/2016/09/18/jvm-basic2/">JVM 学习——垃圾收集器与内存分配策略</a> 中，已经对 GC 算法的原理以及常用的垃圾收集器做了相应的总结。今天这篇文章主要是对生产环境中（Java7）常用的两种垃圾收集器（ParNew：年轻代，CMS：老年代）从日志信息上进行分析，做一下总结，这样当我们在排查相应的问题时，看到 GC 的日志信息，不会再那么陌生，能清楚地知道这些日志是什么意思，GC 线程当前处在哪个阶段，正在做什么事情等。</p>
<h2 id="ParNew-收集器"><a href="#ParNew-收集器" class="headerlink" title="ParNew 收集器"></a>ParNew 收集器</h2><p><a href="http://matt33.com/2016/09/18/jvm-basic2/#ParNew-%E6%94%B6%E9%9B%86%E5%99%A8">ParNew 收集器</a>是年轻代常用的垃圾收集器，它采用的是复制算法，youngGC 时一个典型的日志信息如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.134+0800: 15578.050: [GC2018-04-12T13:48:26.135+0800: 15578.050: [ParNew: 3412467K-&gt;59681K(3774912K), 0.0971990 secs] 9702786K-&gt;6354533K(24746432K), 0.0974940 secs] [Times: user=0.95 sys=0.00, real=0.09 secs]</div></pre></td></tr></table></figure>
<p>依次分析一下上面日志信息的含义：</p>
<ul>
<li><code>2018-04-12T13:48:26.134+0800</code>：Mirror GC 发生的时间；</li>
<li><code>15578.050</code>：GC 开始时，相对 JVM 启动的相对时间，单位时秒，这里是4h+；</li>
<li><code>ParNew</code>：收集器名称，这里是 ParNew 收集器，它使用的是并行的 mark-copy 算法，GC 过程也会 Stop the World；</li>
<li><code>3412467K-&gt;59681K</code>：收集前后年轻代的使用情况，这里是 3.25G-&gt;58.28M；</li>
<li><code>3774912K</code>：整个年轻代的容量，这里是 3.6G；</li>
<li><code>0.0971990 secs</code>：Duration for the collection w/o final cleanup.</li>
<li><code>9702786K-&gt;6354533K</code>：收集前后整个堆的使用情况，这里是 9.25G-&gt;6.06G;</li>
<li><code>24746432K</code>：整个堆的容量，这里是 23.6G；</li>
<li><code>0.0974940 secs</code>：ParNew 收集器标记和复制年轻代活着的对象所花费的时间（包括和老年代通信的开销、对象晋升到老年代开销、垃圾收集周期结束一些最后的清理对象等的花销）；</li>
</ul>
<p>对于 <code>[Times: user=0.95 sys=0.00, real=0.09 secs]</code>，这里面涉及到三种时间类型，含义如下：</p>
<ul>
<li>user：GC 线程在垃圾收集期间所使用的 CPU 总时间；</li>
<li>sys：系统调用或者等待系统事件花费的时间；</li>
<li>real：应用被暂停的时钟时间，由于 GC 线程是多线程的，导致了 real 小于 (user+real)，如果是 gc 线程是单线程的话，real 是接近于 (user+real) 时间。</li>
</ul>
<h2 id="CMS-收集器"><a href="#CMS-收集器" class="headerlink" title="CMS 收集器"></a>CMS 收集器</h2><p><a href="http://matt33.com/2016/09/18/jvm-basic2/#CMS-%E6%94%B6%E9%9B%86%E5%99%A8">CMS 收集器</a>是老年代经常使用的收集器，它采用的是标记-清楚算法，应用程序在发生一次 Full GC 时，典型的 GC 日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.233+0800: 15578.148: [GC [1 CMS-initial-mark: 6294851K(20971520K)] 6354687K(24746432K), 0.0466580 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]</div><div class="line">2018-04-12T13:48:26.280+0800: 15578.195: [CMS-concurrent-mark-start]</div><div class="line">2018-04-12T13:48:26.418+0800: 15578.333: [CMS-concurrent-mark: 0.138/0.138 secs] [Times: user=1.01 sys=0.21, real=0.14 secs]</div><div class="line">2018-04-12T13:48:26.418+0800: 15578.334: [CMS-concurrent-preclean-start]</div><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-preclean: 0.056/0.057 secs] [Times: user=0.20 sys=0.12, real=0.06 secs]</div><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-abortable-preclean-start]</div><div class="line">2018-04-12T13:48:29.989+0800: 15581.905: [CMS-concurrent-abortable-preclean: 3.506/3.514 secs] [Times: user=11.93 sys=6.77, real=3.51 secs]</div><div class="line">2018-04-12T13:48:29.991+0800: 15581.906: [GC[YG occupancy: 1805641 K (3774912 K)]2018-04-12T13:48:29.991+0800: 15581.906: [GC2018-04-12T13:48:29.991+0800: 15581.906: [ParNew: 1805641K-&gt;48395K(3774912K), 0.0826620 secs] 8100493K-&gt;6348225K(24746432K), 0.0829480 secs] [Times: user=0.81 sys=0.00, real=0.09 secs]2018-04-12T13:48:30.074+0800: 15581.989: [Rescan (parallel) , 0.0429390 secs]2018-04-12T13:48:30.117+0800: 15582.032: [weak refs processing, 0.0027800 secs]2018-04-12T13:48:30.119+0800: 15582.035: [class unloading, 0.0033120 secs]2018-04-12T13:48:30.123+0800: 15582.038: [scrub symbol table, 0.0016780 secs]2018-04-12T13:48:30.124+0800: 15582.040: [scrub string table, 0.0004780 secs] [1 CMS-remark: 6299829K(20971520K)] 6348225K(24746432K), 0.1365130 secs] [Times: user=1.24 sys=0.00, real=0.14 secs]</div><div class="line">2018-04-12T13:48:30.128+0800: 15582.043: [CMS-concurrent-sweep-start]</div><div class="line">2018-04-12T13:48:36.638+0800: 15588.553: [GC2018-04-12T13:48:36.638+0800: 15588.554: [ParNew: 3403915K-&gt;52142K(3774912K), 0.0874610 secs] 4836483K-&gt;1489601K(24746432K), 0.0877490 secs] [Times: user=0.84 sys=0.00, real=0.09 secs]</div><div class="line">2018-04-12T13:48:38.412+0800: 15590.327: [CMS-concurrent-sweep: 8.193/8.284 secs] [Times: user=30.34 sys=16.44, real=8.28 secs]</div><div class="line">2018-04-12T13:48:38.419+0800: 15590.334: [CMS-concurrent-reset-start]</div><div class="line">2018-04-12T13:48:38.462+0800: 15590.377: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.15 sys=0.10, real=0.04 secs]</div></pre></td></tr></table></figure>
<p>CMS Full GC 拆分开来，涉及的阶段比较多，下面分别来介绍各个阶段的情况。</p>
<h3 id="阶段1：Initial-Mark"><a href="#阶段1：Initial-Mark" class="headerlink" title="阶段1：Initial Mark"></a>阶段1：Initial Mark</h3><p>这个是 CMS 两次 stop-the-wolrd 事件的其中一次，这个阶段的目标是：标记那些直接被 GC root 引用或者被年轻代存活对象所引用的所有对象，标记后示例如下所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>）：</p>
<p><img src="/images/java/cms-1.png" alt="CMS 初始标记阶段"></p>
<p>上述例子对应的日志信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.233+0800: 15578.148: [GC [1 CMS-initial-mark: 6294851K(20971520K)] 6354687K(24746432K), 0.0466580 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]</div></pre></td></tr></table></figure>
<p>逐行介绍上面日志的含义：</p>
<ol>
<li><code>2018-04-12T13:48:26.233+0800: 15578.148</code>：GC 开始的时间，以及相对于 JVM 启动的相对时间（单位是秒，这里大概是4.33h），与前面 ParNew 类似，下面的分析中就直接跳过这个了；</li>
<li><code>CMS-initial-mark</code>：初始标记阶段，它会收集所有 GC Roots 以及其直接引用的对象；</li>
<li><code>6294851K</code>：当前老年代使用的容量，这里是 6G；</li>
<li><code>(20971520K)</code>：老年代可用的最大容量，这里是 20G；</li>
<li><code>6354687K</code>：整个堆目前使用的容量，这里是 6.06G；</li>
<li><code>(24746432K)</code>：堆可用的容量，这里是 23.6G；</li>
<li><code>0.0466580 secs</code>：这个阶段的持续时间；</li>
<li><code>[Times: user=0.04 sys=0.00, real=0.04 secs]</code>：与前面的类似，这里是相应 user、system and real 的时间统计。</li>
</ol>
<h3 id="阶段2：并发标记"><a href="#阶段2：并发标记" class="headerlink" title="阶段2：并发标记"></a>阶段2：并发标记</h3><p>在这个阶段 Garbage Collector 会遍历老年代，然后标记所有存活的对象，它会根据上个阶段找到的 GC Roots 遍历查找。并发标记阶段，它会与用户的应用程序并发运行。并不是老年代所有的存活对象都会被标记，因为在标记期间用户的程序可能会改变一些引用，如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>）：</p>
<p><img src="/images/java/cms-2.png" alt="CMS 并发标记阶段"></p>
<p>在上面的图中，与阶段1的图进行对比，就会发现有一个对象的引用已经发生了变化，这个阶段相应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.280+0800: 15578.195: [CMS-concurrent-mark-start]</div><div class="line">2018-04-12T13:48:26.418+0800: 15578.333: [CMS-concurrent-mark: 0.138/0.138 secs] [Times: user=1.01 sys=0.21, real=0.14 secs]</div></pre></td></tr></table></figure>
<p>这里详细对上面的日志解释，如下所示：</p>
<ol>
<li><code>CMS-concurrent-mark</code>：并发收集阶段，这个阶段会遍历老年代，并标记所有存活的对象；</li>
<li><code>0.138/0.138 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=1.01 sys=0.21, real=0.14 secs]</code>：如前面所示，但是这部的时间，其实意义不大，因为它是从并发标记的开始时间开始计算，这期间因为是并发进行，不仅仅包含 GC 线程的工作。</li>
</ol>
<h3 id="阶段3：Concurrent-Preclean"><a href="#阶段3：Concurrent-Preclean" class="headerlink" title="阶段3：Concurrent Preclean"></a>阶段3：Concurrent Preclean</h3><p>Concurrent Preclean：这也是一个并发阶段，与应用的线程并发运行，并不会 stop 应用的线程。在并发运行的过程中，一些对象的引用可能会发生变化，但是这种情况发生时，JVM 会将包含这个对象的区域（Card）标记为 Dirty，这也就是 Card Marking。如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：</p>
<p><img src="/images/java/cms-3.png" alt="Concurrent Preclean 1"></p>
<p>在pre-clean阶段，那些能够从 Dirty 对象到达的对象也会被标记，这个标记做完之后，dirty card 标记就会被清除了，如下（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：</p>
<p><img src="/images/java/cms-4.png" alt="Concurrent Preclean 2"></p>
<p>这个阶段相应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.418+0800: 15578.334: [CMS-concurrent-preclean-start]</div><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-preclean: 0.056/0.057 secs] [Times: user=0.20 sys=0.12, real=0.06 secs]</div></pre></td></tr></table></figure>
<p>其含义为：</p>
<ol>
<li><code>CMS-concurrent-preclean</code>：Concurrent Preclean 阶段，对在前面并发标记阶段中引用发生变化的对象进行标记；</li>
<li><code>0.056/0.057 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=0.20 sys=0.12, real=0.06 secs]</code>：同并发标记阶段中的含义。</li>
</ol>
<h3 id="阶段4：Concurrent-Abortable-Preclean"><a href="#阶段4：Concurrent-Abortable-Preclean" class="headerlink" title="阶段4：Concurrent Abortable Preclean"></a>阶段4：Concurrent Abortable Preclean</h3><p>这也是一个并发阶段，但是同样不会影响影响用户的应用线程，这个阶段是为了尽量承担 STW（stop-the-world）中最终标记阶段的工作。这个阶段持续时间依赖于很多的因素，由于这个阶段是在重复做很多相同的工作，直接满足一些条件（比如：重复迭代的次数、完成的工作量或者时钟时间等）。这个阶段的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-abortable-preclean-start]</div><div class="line">2018-04-12T13:48:29.989+0800: 15581.905: [CMS-concurrent-abortable-preclean: 3.506/3.514 secs] [Times: user=11.93 sys=6.77, real=3.51 secs]</div></pre></td></tr></table></figure>
<ol>
<li><code>CMS-concurrent-abortable-preclean</code>：Concurrent Abortable Preclean 阶段；</li>
<li><code>3.506/3.514 secs</code>：这个阶段的持续时间与时钟时间，本质上，这里的 gc 线程会在 STW 之前做更多的工作，通常会持续 5s 左右；</li>
<li><code>[Times: user=11.93 sys=6.77, real=3.51 secs]</code>：同前面。</li>
</ol>
<h3 id="阶段5：Final-Remark"><a href="#阶段5：Final-Remark" class="headerlink" title="阶段5：Final Remark"></a>阶段5：Final Remark</h3><p>这是第二个 STW 阶段，也是 CMS 中的最后一个，这个阶段的目标是标记所有老年代所有的存活对象，由于之前的阶段是并发执行的，gc 线程可能跟不上应用程序的变化，为了完成标记老年代所有存活对象的目标，STW 就非常有必要了。</p>
<p>通常 CMS 的 Final Remark 阶段会在年轻代尽可能干净的时候运行，目的是为了减少连续 STW 发生的可能性（年轻代存活对象过多的话，也会导致老年代涉及的存活对象会很多）。这个阶段会比前面的几个阶段更复杂一些，相关日志如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:29.991+0800: 15581.906: [GC[YG occupancy: 1805641 K (3774912 K)]2018-04-12T13:48:29.991+0800: 15581.906: [GC2018-04-12T13:48:29.991+0800: 15581.906: [ParNew: 1805641K-&gt;48395K(3774912K), 0.0826620 secs] 8100493K-&gt;6348225K(24746432K), 0.0829480 secs] [Times: user=0.81 sys=0.00, real=0.09 secs]2018-04-12T13:48:30.074+0800: 15581.989: [Rescan (parallel) , 0.0429390 secs]2018-04-12T13:48:30.117+0800: 15582.032: [weak refs processing, 0.0027800 secs]2018-04-12T13:48:30.119+0800: 15582.035: [class unloading, 0.0033120 secs]2018-04-12T13:48:30.123+0800: 15582.038: [scrub symbol table, 0.0016780 secs]2018-04-12T13:48:30.124+0800: 15582.040: [scrub string table, 0.0004780 secs] [1 CMS-remark: 6299829K(20971520K)] 6348225K(24746432K), 0.1365130 secs] [Times: user=1.24 sys=0.00, real=0.14 secs]</div></pre></td></tr></table></figure>
<p>对上面的日志进行分析：</p>
<ol>
<li><code>YG occupancy: 1805641 K (3774912 K)</code>：年轻代当前占用量及容量，这里分别是 1.71G 和 3.6G；</li>
<li><code>ParNew:...</code>：触发了一次 young GC，这里触发的原因是为了减少年轻代的存活对象，尽量使年轻代更干净一些；</li>
<li><code>[Rescan (parallel) , 0.0429390 secs]</code>：这个 Rescan 是当应用暂停的情况下完成对所有存活对象的标记，这个阶段是并行处理的，这里花费了  0.0429390s；</li>
<li><code>[weak refs processing, 0.0027800 secs]</code>：第一个子阶段，它的工作是处理弱引用；</li>
<li><code>[class unloading, 0.0033120 secs]</code>：第二个子阶段，它的工作是：unloading the unused classes；</li>
<li><code>[scrub symbol table, 0.0016780 secs] ... [scrub string table, 0.0004780 secs]</code>：最后一个子阶段，它的目的是：cleaning up symbol and string tables which hold class-level metadata and internalized string respectively，时钟的暂停也包含在这里；</li>
<li><code>6299829K(20971520K)</code>：这个阶段之后，老年代的使用量与总量，这里分别是 6G 和 20G；</li>
<li><code>6348225K(24746432K)</code>：这个阶段之后，堆的使用量与总量（包括年轻代，年轻代在前面发生过 GC），这里分别是 6.05G 和 23.6G；</li>
<li><code>0.1365130 secs</code>：这个阶段的持续时间；</li>
<li><code>[Times: user=1.24 sys=0.00, real=0.14 secs]</code>：对应的时间信息。</li>
</ol>
<p>经历过这五个阶段之后，老年代所有存活的对象都被标记过了，现在可以通过清除算法去清理那些老年代不再使用的对象。</p>
<h3 id="阶段6：Concurrent-Sweep"><a href="#阶段6：Concurrent-Sweep" class="headerlink" title="阶段6：Concurrent Sweep"></a>阶段6：Concurrent Sweep</h3><p>这里不需要 STW，它是与用户的应用程序并发运行，这个阶段是：清除那些不再使用的对象，回收它们的占用空间为将来使用。如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：<br>）：</p>
<p><img src="/images/java/cms-5.png" alt="CMS Concurrent Sweep 阶段"></p>
<p>这个阶段对应的日志信息如下（这中间又发生了一次 Young GC）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:30.128+0800: 15582.043: [CMS-concurrent-sweep-start]</div><div class="line">2018-04-12T13:48:36.638+0800: 15588.553: [GC2018-04-12T13:48:36.638+0800: 15588.554: [ParNew: 3403915K-&gt;52142K(3774912K), 0.0874610 secs] 4836483K-&gt;1489601K(24746432K), 0.0877490 secs] [Times: user=0.84 sys=0.00, real=0.09 secs]</div><div class="line">2018-04-12T13:48:38.412+0800: 15590.327: [CMS-concurrent-sweep: 8.193/8.284 secs] [Times: user=30.34 sys=16.44, real=8.28 secs]</div></pre></td></tr></table></figure>
<p>分别介绍一下：</p>
<ol>
<li><code>CMS-concurrent-sweep</code>：这个阶段主要是清除那些没有被标记的对象，回收它们的占用空间；</li>
<li><code>8.193/8.284 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=30.34 sys=16.44, real=8.28 secs]</code>：同前面；</li>
</ol>
<h3 id="阶段7：Concurrent-Reset"><a href="#阶段7：Concurrent-Reset" class="headerlink" title="阶段7：Concurrent Reset."></a>阶段7：Concurrent Reset.</h3><p>这个阶段也是并发执行的，它会重设 CMS 内部的数据结构，为下次的 GC 做准备，对应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:38.419+0800: 15590.334: [CMS-concurrent-reset-start]</div><div class="line">2018-04-12T13:48:38.462+0800: 15590.377: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.15 sys=0.10, real=0.04 secs]</div></pre></td></tr></table></figure>
<p>日志详情分别如下：</p>
<ol>
<li><code>CMS-concurrent-reset</code>：这个阶段的开始，目的如前面所述；</li>
<li><code>0.044/0.044 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=0.15 sys=0.10, real=0.04 secs]</code>：同前面。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CMS 通过将大量工作分散到并发处理阶段来在减少 STW 时间，在这块做得非常优秀，但是 CMS 也有一些其他的问题：</p>
<ol>
<li>CMS 收集器无法处理浮动垃圾（ Floating Garbage），可能出现 “Concurrnet Mode Failure” 失败而导致另一次 Full GC 的产生，可能引发串行 Full GC；</li>
<li>空间碎片，导致无法分配大对象，CMS 收集器提供了一个 <code>-XX:+UseCMSCompactAtFullCollection</code> 开关参数（默认就是开启的），用于在 CMS 收集器顶不住要进行 Full GC 时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间不得不变长；</li>
<li>对于堆比较大的应用上，GC 的时间难以预估。</li>
</ol>
<p>CMS 的一些缺陷也是 G1 收集器兴起的原因。</p>
<p>参考：</p>
<ul>
<li><a href="https://t.hao0.me/jvm/2016/03/15/jvm-gc-log.html" target="_blank" rel="external">JVM各类GC日志剖析</a>；</li>
<li><a href="http://www.cnblogs.com/zhangxiaoguang/p/5792468.html" target="_blank" rel="external">GC之详解CMS收集过程和日志分析</a>；</li>
<li><a href="http://www.oracle.com/technetwork/tutorials/tutorials-1876574.html" target="_blank" rel="external">Getting Started with the G1 Garbage Collector</a>；</li>
<li><a href="http://ifeve.com/useful-jvm-flags-part-7-cms-collector/" target="_blank" rel="external">JVM实用参数（七）CMS收集器</a>；</li>
<li><a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在两年前的文章 &lt;a href=&quot;http://matt33.com/2016/09/18/jvm-basic2/&quot;&gt;JVM 学习——垃圾收集器与内存分配策略&lt;/a&gt; 中，已经对 GC 算法的原理以及常用的垃圾收集器做了相应的总结。今天这篇文章主要是对生产环境中（Java7
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="jvm" scheme="http://matt33.com/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>HDFS 架构学习总结</title>
    <link href="http://matt33.com/2018/07/15/hdfs-architecture-learn/"/>
    <id>http://matt33.com/2018/07/15/hdfs-architecture-learn/</id>
    <published>2018-07-15T15:46:45.000Z</published>
    <updated>2018-09-01T15:35:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS（Hadoop Distributed File System）是一个分布式文件存储系统，几乎是离线存储领域的标准解决方案（有能力自研的大厂列外），业内应用非常广泛。近段抽时间，看一下 HDFS 的架构设计，虽然研究生也学习过相关内容，但是现在基本忘得差不多了，今天抽空对这块做了一个简单的总结，也算是再温习了一下这块的内容，这样后续再看 HDFS 方面的文章时，不至于处于懵逼状态。</p>
<h2 id="HDFS-1-0-架构"><a href="#HDFS-1-0-架构" class="headerlink" title="HDFS 1.0 架构"></a>HDFS 1.0 架构</h2><p>HDFS 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点，如下图所示（这个图是 HDFS1.0的架构图，经典的架构图）：</p>
<p><img src="/images/hadoop/hdfs.jpg" alt="HDFS 1.0 架构图"></p>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>NameNode 负责管理整个分布式系统的元数据，主要包括：</p>
<ul>
<li>目录树结构；</li>
<li>文件到数据库 Block 的映射关系；</li>
<li>Block 副本及其存储位置等管理数据；</li>
<li>DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。</li>
</ul>
<p>这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。</p>
<ul>
<li>fsimage：是内存命名空间元数据在外存的镜像文件；</li>
<li>editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。</li>
</ul>
<p>这两个文件相结合可以构造完整的内存数据。</p>
<h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><p>Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。</p>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。</p>
<h3 id="文件写入过程"><a href="#文件写入过程" class="headerlink" title="文件写入过程"></a>文件写入过程</h3><p>Client 向 HDFS 文件写入的过程可以参考<a href="http://shiyanjun.cn/archives/942.html" target="_blank" rel="external">HDFS写文件过程分析</a>，整体过程如下图（这个图比较经典，最开始来自<a href="https://book.douban.com/subject/3220004/" target="_blank" rel="external">《Hadoop：The Definitive Guide》</a>）所示：</p>
<p><img src="/images/hadoop/hdfs-write-flow.png" alt="HDFS 文件写入过程"></p>
<p>具体过程如下：</p>
<ol>
<li>Client 调用 DistributedFileSystem 对象的 <code>create</code> 方法，创建一个文件输出流（FSDataOutputStream）对象；</li>
<li>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</li>
<li>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</li>
<li>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</li>
<li>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</li>
<li>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 <code>close</code> 方法，完成文件写入；</li>
<li>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</li>
</ol>
<h3 id="文件读取过程"><a href="#文件读取过程" class="headerlink" title="文件读取过程"></a>文件读取过程</h3><p>相对于文件写入，文件的读取就简单一些，流程如下图所示：</p>
<p><img src="/images/hadoop/hdfs-read-flow.png" alt="HDFS 文件读取过程"></p>
<p>其具体过程总结如下（简单总结一下）：</p>
<ol>
<li>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</li>
<li>NameNode 返回存储的每个块的 DataNode 列表；</li>
<li>Client 将连接到列表中最近的 DataNode；</li>
<li>Client 开始从 DataNode 并行读取数据；</li>
<li>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</li>
</ol>
<p>在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。</p>
<h2 id="HDFS-1-0-的问题"><a href="#HDFS-1-0-的问题" class="headerlink" title="HDFS 1.0 的问题"></a>HDFS 1.0 的问题</h2><p>在前面的介绍中，关于 HDFS1.0 的架构，首先都会看到 NameNode 的单点问题，这个在生产环境中是非常要命的问题，早期的 HDFS 由于规模较小，有些问题就被隐藏了，但自从进入了移动互联网时代，很多公司都开始进入了 PB 级的大数据时代，HDFS 1.0的设计缺陷已经无法满足生产的需求，最致命的问题有以下两点：</p>
<ul>
<li>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</li>
<li>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。</li>
</ul>
<p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p>
<ol>
<li>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</li>
<li>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。</li>
</ol>
<h2 id="HDFS-2-0-的-HA-实现"><a href="#HDFS-2-0-的-HA-实现" class="headerlink" title="HDFS 2.0 的 HA 实现"></a>HDFS 2.0 的 HA 实现</h2><p>关于 HDFS 高可用方案，非常推荐这篇文章：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a>，IBM 博客的质量确实很高，这部分我这里也是主要根据这篇文章做一个总结，这里会从问题的原因、如何解决的角度去总结，并不会深入源码的实现细节，想有更深入了解还是推荐上面文章。</p>
<p>这里先看下 HDFS 高可用解决方案的架构设计，如下图（下图来自上面的文章）所示：</p>
<p><img src="/images/hadoop/hdfs-ha.png" alt="HDFS HA 架构实现"></p>
<p>这里与前面 1.0 的架构已经有很大变化，简单介绍一下上面的组件：</p>
<ol>
<li>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li>
<li>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；</li>
<li>Zookeeper 集群：为主备切换控制器提供主备选举支持；</li>
<li>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在<strong>确认元数据完全同步之后才能继续对外提供服务</strong>。</li>
<li>DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li>
</ol>
<h3 id="FailoverController"><a href="#FailoverController" class="headerlink" title="FailoverController"></a>FailoverController</h3><p>FC 最初的目的是为了实现 SNN 和 ANN 之间故障自动切换，FC 是独立与 NN 之外的故障切换控制器，ZKFC 作为 NameNode 机器上一个独立的进程启动 ，它启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，其中：</p>
<ol>
<li>HealthMonitor：主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举；</li>
<li>ActiveStandbyElector：主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。</li>
</ol>
<h3 id="自动触发主备选举"><a href="#自动触发主备选举" class="headerlink" title="自动触发主备选举"></a>自动触发主备选举</h3><p>NameNode 在选举成功后，会在 zk 上创建了一个 <code>/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock</code> 节点，而没有选举成功的备 NameNode 会监控这个节点，通过 Watcher 来监听这个节点的状态变化事件，ZKFC 的 ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件（这部分实现跟 Kafka 中 Controller 的选举一样）。</p>
<p>如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点 /hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建 /hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock  节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。</p>
<p>当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。</p>
<h3 id="HDFS-脑裂问题"><a href="#HDFS-脑裂问题" class="headerlink" title="HDFS 脑裂问题"></a>HDFS 脑裂问题</h3><p>在实际中，NameNode 可能会出现这种情况，NameNode 在垃圾回收（GC）时，可能会在长时间内整个系统无响应，因此，也就无法向 zk 写入心跳信息，这样的话可能会导致临时节点掉线，备 NameNode 会切换到 Active 状态，这种情况，可能会导致整个集群会有同时有两个 NameNode，这就是脑裂问题。</p>
<p>脑裂问题的解决方案是隔离（Fencing），主要是在以下三处采用隔离措施：</p>
<ol>
<li>第三方共享存储：任一时刻，只有一个 NN 可以写入；</li>
<li>DataNode：需要保证只有一个 NN 发出与管理数据副本有关的删除命令；</li>
<li>Client：需要保证同一时刻只有一个 NN 能够对 Client 的请求发出正确的响应。</li>
</ol>
<p>关于这个问题目前解决方案的实现如下：</p>
<ol>
<li>ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为 <strong>/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb</strong> 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息；</li>
<li>Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候，会一起删除这个持久节点；</li>
<li>但如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于 /hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来，后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing。</li>
</ol>
<p>在进行 fencing 的时候，会执行以下的操作：</p>
<ol>
<li>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 <code>transitionToStandby</code> 方法，看能不能把它转换为 Standby 状态；</li>
<li>如果 <code>transitionToStandby</code> 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施。</li>
</ol>
<p>Hadoop 目前主要提供两种隔离措施，通常会选择第一种：</p>
<ol>
<li>sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；</li>
<li>shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离。</li>
</ol>
<p>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 <code>becomeActive</code> 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。</p>
<p>NameNode 选举的实现机制与 Kafka 的 Controller 类似，那么 Kafka 是如何避免脑裂问题的呢？</p>
<ol>
<li>Controller 给 Broker 发送的请求中，都会携带 controller epoch 信息，如果 broker 发现当前请求的 epoch 小于缓存中的值，那么就证明这是来自旧 Controller 的请求，就会决绝这个请求，正常情况下是没什么问题的；</li>
<li>但是异常情况下呢？如果 Broker 先收到异常 Controller 的请求进行处理呢？现在看 Kafka 在这一部分并没有适合的方案；</li>
<li>正常情况下，Kafka 新的 Controller 选举出来之后，Controller 会向全局所有 broker 发送一个 metadata 请求，这样全局所有 Broker 都可以知道当前最新的 controller epoch，但是并不能保证可以完全避免上面这个问题，还是有出现这个问题的几率的，只不过非常小，而且即使出现了由于 Kafka 的高可靠架构，影响也非常有限，至少从目前看，这个问题并不是严重的问题。</li>
</ol>
<h3 id="第三方存储（共享存储）"><a href="#第三方存储（共享存储）" class="headerlink" title="第三方存储（共享存储）"></a>第三方存储（共享存储）</h3><p>上述 HA 方案还有一个明显缺点，那就是第三方存储节点有可能失效，之前有很多共享存储的实现方案，目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。</p>
<p>QJM（Quorum Journal Manager）本质上是利用 Paxos 协议来实现的，QJM 在 <code>2F+1</code>  个 JournalNode 上存储 NN 的 editlog，每次写入操作都通过 Paxos 保证写入的一致性，它最多可以允许有 F 个 JournalNode 节点同时故障，其实现如下（图片来自：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a> ）：</p>
<p><img src="/images/hadoop/hdfs-ha-qjm.png" alt="基于 QJM 的共享存储的数据同步机制"></p>
<p>Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog。</p>
<p>还有一点需要注意的是，在 2.0 中不再有 SNN 这个角色了，NameNode 在启动后，会先加载 FSImage 文件和共享目录上的 EditLog Segment 文件，之后 NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式，其中：</p>
<ol>
<li>EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog；</li>
<li>StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。</li>
</ol>
<h2 id="HDFS-2-0-Federation-实现"><a href="#HDFS-2-0-Federation-实现" class="headerlink" title="HDFS 2.0 Federation 实现"></a>HDFS 2.0 Federation 实现</h2><p>在 1.0 中，HDFS 的架构设计有以下缺点：</p>
<ol>
<li>namespace 扩展性差：在单一的 NN 情况下，因为所有 namespace 数据都需要加载到内存，所以物理机内存的大小限制了整个 HDFS 能够容纳文件的最大个数（namespace 指的是 HDFS 中树形目录和文件结构以及文件对应的 block 信息）；</li>
<li>性能可扩展性差：由于所有请求都需要经过 NN，单一 NN 导致所有请求都由一台机器进行处理，很容易达到单台机器的吞吐；</li>
<li>隔离性差：多租户的情况下，单一 NN 的架构无法在租户间进行隔离，会造成不可避免的相互影响。</li>
</ol>
<p>而 Federation 的设计就是为了解决这些问题，采用 Federation 的最主要原因是设计实现简单，而且还能解决问题。</p>
<h3 id="Federation-架构"><a href="#Federation-架构" class="headerlink" title="Federation 架构"></a>Federation 架构</h3><p>Federation 的架构设计如下图所示（图片来自 <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">HDFS Federation</a>）：</p>
<p><img src="/images/hadoop/hdfs-federation.gif" alt="HDFS Federation 架构实现"></p>
<h3 id="Federation-的核心设计思想"><a href="#Federation-的核心设计思想" class="headerlink" title="Federation 的核心设计思想"></a>Federation 的核心设计思想</h3><p>Federation 的核心思想是将一个大的 namespace 划分多个子 namespace，并且每个 namespace 分别由单独的 NameNode 负责，这些 NameNode 之间互相独立，不会影响，不需要做任何协调工作（其实跟拆集群有一些相似），集群的所有 DataNode 会被多个 NameNode 共享。</p>
<p>其中，每个子 namespace 和 DataNode 之间会由数据块管理层作为中介建立映射关系，数据块管理层由若干数据块池（Pool）构成，每个数据块只会唯一属于某个固定的数据块池，而一个子 namespace 可以对应多个数据块池。每个 DataNode 需要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告，并执行来自所有 NameNode 的命令。</p>
<ul>
<li>一个 block pool 由属于同一个 namespace 的数据块组成，每个 DataNode 可能会存储集群中所有 block pool 的数据块；</li>
<li>每个 block pool 内部自治，也就是说各自管理各自的 block，不会与其他 block pool 交流，如果一个 NameNode 挂掉了，不会影响其他 NameNode;</li>
<li>某个 NameNode 上的 namespace 和它对应的 block pool 一起被称为 namespace volume，它是管理的基本单位。当一个 NameNode/namespace 被删除后，其所有 DataNode 上对应的 block pool 也会被删除，当集群升级时，每个 namespace volume 可以作为一个基本单元进行升级。</li>
</ul>
<p>到这里，基本对 HDFS 这部分总结完了，虽然文章的内容基本都来自下面的参考资料，但是自己在总结的过程中，也对 HDFS 的基本架构有一定的了解，后续结合公司 HDFS 团队的 CaseStudy 深入学习这部分的内容，工作中，也慢慢感觉到分布式系统，很多的设计实现与问题解决方案都很类似，只不过因为面对业务场景的不同而采用了不同的实现。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="external">HDFS Architecture</a>;</li>
<li><a href="http://shiyanjun.cn/archives/942.html" target="_blank" rel="external">HDFS 写文件过程分析</a>;</li>
<li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSRouterFederation.html" target="_blank" rel="external">HDFS Router-based Federation</a>；</li>
<li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="external">HDFS High Availability Using the Quorum Journal Manager</a>；</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="external">HDFS Commands Guide</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">HDFS Federation</a>；</li>
<li><a href="http://dongxicheng.org/mapreduce/hdfs-federation-introduction/" target="_blank" rel="external">HDFS Federation设计动机与基本原理</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://tech.meituan.com/namenode-restart-optimization.html" target="_blank" rel="external">HDFS NameNode重启优化</a>；</li>
<li><a href="https://tech.meituan.com/hdfs-federation.html" target="_blank" rel="external">HDFS Federation在美团点评的应用与改进</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS（Hadoop Distributed File System）是一个分布式文件存储系统，几乎是离线存储领域的标准解决方案（有能力自研的大厂列外），业内应用非常广泛。近段抽时间，看一下 HDFS 的架构设计，虽然研究生也学习过相关内容，但是现在基本忘得差不多了，今天
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hadoop" scheme="http://matt33.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Controller Redesign 方案</title>
    <link href="http://matt33.com/2018/07/14/kafka-controller-redesign/"/>
    <id>http://matt33.com/2018/07/14/kafka-controller-redesign/</id>
    <published>2018-07-14T15:13:56.000Z</published>
    <updated>2018-07-15T04:17:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是一次重构，因此，社区准备在新版的系统里对 Controller 做一些相应的优化（0.11.0及以后的版本），相应的设计方案见：<a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>，本文的内容就是结合这篇文章做一个简单的总结。</p>
<h2 id="Controller-功能"><a href="#Controller-功能" class="headerlink" title="Controller 功能"></a>Controller 功能</h2><p>在一个 Kafka 中，Controller 要处理的事情总结如下表所示：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>详情</th>
</tr>
</thead>
<tbody>
<tr>
<td>cluster metadata updates</td>
<td>producer 或 consumer 可以通过 MetadataRequest 请求从集群任何一台 broker 上查询到某个 Partition 的 metadata 信息，如果一个 Partition 的 leader 或 isr 等信息变化，Controller 会广播到集群的所有 broker 上，这样每台 Broker 都会有该 Partition 的最新 Metadata 信息</td>
</tr>
<tr>
<td>topic creation</td>
<td>用户可以通过多种方式创建一个 topic，最终的结果都是在 zk 的 <code>/brokers/topics</code> 目录下新建一个 topic 节点信息，controller 通过监控这个目录来判断是否有新的 topic 需要创建</td>
</tr>
<tr>
<td>topic deletion</td>
<td>Controller 通过监控 zk 的 <code>/admin/delete_topics</code> 节点来触发 topic 删除操作</td>
</tr>
<tr>
<td>partition reassignment</td>
<td>Controller 通过监控 zk 的 <code>/admin/reassign_partitions</code> 节点来触发 Partition 的副本迁移操作</td>
</tr>
<tr>
<td>preferred replica leader election</td>
<td>Controller 通过监控 zk 的 <code>/admin/preferred_replica_election</code> 节点来触发最优 leader 选举操作，该操作的目的选举 Partition 的第一个 replica 作为 leader</td>
</tr>
<tr>
<td>topic partition expansion</td>
<td>Controller 通过监控 zk 的 <code>/brokers/topics/&lt;topic&gt;</code> 数据内容的变化，来触发 Topic 的 Partition 扩容操作</td>
</tr>
<tr>
<td>broker join</td>
<td>Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 是最新加入的，进而触发 broker 的上线操作</td>
</tr>
<tr>
<td>broker failure</td>
<td>同样，Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 掉线了，进而触发 broker 的下线操作</td>
<td></td>
</tr>
<tr>
<td>controlled shutdown</td>
<td>Controller 通过处理 ControlledShudownRequest 请求来优雅地关闭一个 broker 节点，主动关闭与直接 kill 的区别，它可以减少 Partition 的不可用时间，因为一个 broker 的 zk 临时节点消失是需要一定时间的</td>
</tr>
<tr>
<td>controller leader election</td>
<td>集群中所有 broker 会监听 zk 的 <code>/controller</code> 节点，如果该节点消失，所有的 broker 都回去抢占 controller 节点，抢占成功的，就成了最新的 controller</td>
</tr>
</tbody>
</table>
<h2 id="Controller-目前存在的问题"><a href="#Controller-目前存在的问题" class="headerlink" title="Controller 目前存在的问题"></a>Controller 目前存在的问题</h2><p>之所以要重新设计 Controller，是因为现在的 Controller 积累了一些比较难解决的问题，这些问题解决起来，代码改动量都是巨大的，甚至需要改变 controller 部门的设计，基本就跟重构差不多了，下面我们先来了看一下 controller 之前（主要是 0.11.0 之前的版本）存在的一些问题。</p>
<p>目前遇到的比较大的问题有以下几个：</p>
<ol>
<li>Partition 级别同步 zk 写；</li>
<li>sequential per-partition controller-to-broker requests；</li>
<li>Controller 复杂的并发语义；</li>
<li>代码组织混乱；</li>
<li>控制类请求与数据类请求未分离；</li>
<li>Controller 给 broker 的请求中没有 broker 的 generation信息；</li>
<li>ZkClient 阻碍 Client 的状态管理。</li>
</ol>
<h3 id="Partition-级别同步-zk-写"><a href="#Partition-级别同步-zk-写" class="headerlink" title="Partition 级别同步 zk 写"></a>Partition 级别同步 zk 写</h3><p>zookeeper 的同步写意味着在下次写之前需要等待前面整个过程的结束，而且由于它们都是 partition 粒度的（一个 Partition 一个 Partition 的去执行写操作），对于 Partition 非常多的集群来说，需要等待的时间会更长，Controller 通常会在下面这两个地方做 Partition 级别 zookeeper 同步写操作：</p>
<ol>
<li>PartitionStateMachine 在进行触发 leader 选举（partition 目的状态是 OnlinePartition），将会触发上面的操作；</li>
<li>ReplicaStateMachine 更新 LeaderAndIsr 信息到 zk（replica 状态转变为 OfflineReplica），这种情况也触发这种情况，它既阻碍了 Controller 进程，也有可能会 zk 造成压力。</li>
</ol>
<h3 id="sequential-per-partition-controller-to-broker-requests"><a href="#sequential-per-partition-controller-to-broker-requests" class="headerlink" title="sequential per-partition controller-to-broker requests"></a>sequential per-partition controller-to-broker requests</h3><p>Controller 在向 Broker 发送请求，有些情况下也是 Partition 粒度去发送的，效率非常低，比如在 Controller 处理 broker shutdown 请求时，这里是按 Partition 级别处理，每处理一个 Partition 都会执行 Partition、Replica 状态变化以及 Metadata 更新，并且调用 <code>sendRequestsToBrokers()</code> 向 broker 发送请求，这样的话，效率将变得非常低。</p>
<h3 id="Controller-复杂的并发语义"><a href="#Controller-复杂的并发语义" class="headerlink" title="Controller 复杂的并发语义"></a>Controller 复杂的并发语义</h3><p>Controller 需要在多个线程之间共享状态信息，这些线程有：</p>
<ol>
<li>IO threads handling controlled shutdown requests</li>
<li>The ZkClient org.I0Itec.zkclient.ZkEventThread processing zookeeper callbacks sequentially；</li>
<li>The TopicDeletionManager kafka.controller.DeleteTopicsThread；</li>
<li>Per-broker RequestSendThread within ControllerChannelManager.</li>
</ol>
<p>所有这些线程都需要访问或修改状态信息（ControllerContext），现在它们是通过 ControllerContext 的 controllerLock（排它锁）实现的，Controller 的并发变得虚弱无力。</p>
<h3 id="代码组织混乱"><a href="#代码组织混乱" class="headerlink" title="代码组织混乱"></a>代码组织混乱</h3><p>KafkaController 部分的代码组织（KafkaController、PartitionStateMachine 和 ReplicaStateMachine）不是很清晰，比如，下面的问题就很难回答：</p>
<ol>
<li>where and when does zookeeper get updated?</li>
<li>where and when does a controller-to-broker request get formed?</li>
<li>what impact does a failing zookeeper update or controller-to-broker request have on the cluster state?</li>
</ol>
<p>这也导致了这部分很多开发者不敢轻易去改动。</p>
<h3 id="控制类请求与数据类请求未分离"><a href="#控制类请求与数据类请求未分离" class="headerlink" title="控制类请求与数据类请求未分离"></a>控制类请求与数据类请求未分离</h3><p>现在 broker 收到的请求，有来自 client、broker 和 controller 的请求，这些请求都会被放到同一个 requestQueue 中，它们有着同样的优先级，所以来自 client 的请求很可能会影响来自 controller 请求的处理（如果是 leader 变动的请求，ack 设置的不是 all，这种情况有可能会导致数据丢失）。</p>
<h3 id="Controller-给-broker-的请求中没有-broker-的-generation信息"><a href="#Controller-给-broker-的请求中没有-broker-的-generation信息" class="headerlink" title="Controller 给 broker 的请求中没有 broker 的 generation信息"></a>Controller 给 broker 的请求中没有 broker 的 generation信息</h3><p>这里的 Broker generation 代表着一个标识，每当它重新加入集群时，这个标识都会变化。如果 Controller 的请求没有这个信息的话，可能会导致一个重启的 Broker 收到之前的请求，让 Broker 进入到一个错误的状态。</p>
<p>比如，Broker 收到之前的 StopReplica 请求，可能会导致副本同步线程退出。</p>
<h3 id="ZkClient-阻碍-Client-的状态管理"><a href="#ZkClient-阻碍-Client-的状态管理" class="headerlink" title="ZkClient 阻碍 Client 的状态管理"></a>ZkClient 阻碍 Client 的状态管理</h3><p>这里的状态管理指的是当 Client 发生重连或会话过期时，Client 可以监控这种状态变化，并做出一些处理，因为开源版的 ZKClient 在处理 notification 时，是线性处理的，一些 notification 会被先放到 ZkEventThread’s queue 中，这样会导致一些最新的 notification 不能及时被处理，特别是与 zk 连接断开重连的情况。</p>
<h2 id="Controller-改进方案"><a href="#Controller-改进方案" class="headerlink" title="Controller 改进方案"></a>Controller 改进方案</h2><p>关于上述问题，Kafka 提出了一些改进方案，有些已经在最新版的系统中实现，有的还在规划中。</p>
<h3 id="使用异步的-zk-api"><a href="#使用异步的-zk-api" class="headerlink" title="使用异步的 zk api"></a>使用异步的 zk api</h3><p>Zookeeper 的 client 提供三种执行请求的方式：</p>
<ol>
<li>同步调用，意味着下次请求需要等待当前当前请求的完成；</li>
<li>异步调用，意味着不需要等待当前请求的完成就可以开始下次请求的执行，并且我们可以通过回调机制去处理请求返回的结果；</li>
<li>单请求的 batch 调用，意味着 batch 内的所有请求都会在一次事务处理中完成，这里需要关注的是 zookeeper 的 server 对单请求的大小是有限制的（jute.maxbuffer）。</li>
</ol>
<p>文章中给出了三种请求的测试结果，Kafka 最后选取的是异步处理机制，因为对于单请求处理，异步处理更加简洁，并且相比于同步处理还可以保持一个更好的写性能。</p>
<h3 id="improve-controller-to-broker-request-batching"><a href="#improve-controller-to-broker-request-batching" class="headerlink" title="improve controller-to-broker request batching"></a>improve controller-to-broker request batching</h3><p>这个在设计文档还是 TODO 状态，具体的方案还没确定，不过基本可以猜测一下，因为目的是提高 batch 发送能力，那么只能是在调用对每个 broker 的 RequestSenderThread 线程发送请求之前，做一下检测，而不是来一个请求立马就发送，这是一个性能与时间的权衡，如果不是立马发送请求，那么可能会带来 broker 短时 metadata 信息的不一致，这个不一致时间不同的应用场景要求是不一样的。</p>
<h3 id="单线程的事件处理模型"><a href="#单线程的事件处理模型" class="headerlink" title="单线程的事件处理模型"></a>单线程的事件处理模型</h3><p>采用单线程的时间处理模型将极大简化 Controller 的并发实现，只允许这个线程访问和修改 Controller 的本地状态信息，因此在 Controller 部分也就不需要到处加锁来保证线程安全了。</p>
<p>目前 1.1.0 的实现中，Controller 使用了一个 ControllerEventThread 线程来处理所有的 event，目前可以支持13种不同类型事件：</p>
<ol>
<li>Idle：代表当前 ControllerEventThread 处理空闲状态；</li>
<li>ControllerChange：Controller 切换处理；</li>
<li>BrokerChange：Broker 变动处理，broker 可能有上线或掉线；</li>
<li>TopicChange：Topic 新增处理；</li>
<li>TopicDeletion：Topic 删除处理；</li>
<li>PartitionReassignment：Partition 副本迁移处理；</li>
<li>AutoLeaderBalance：自动 rebalance 处理；</li>
<li>ManualLeaderBalance：最优 leader 选举处理，这里叫做手动 rebalance，手动去切流量；</li>
<li>ControlledShutdown：优雅关闭 broker；</li>
<li>IsrChange：Isr 变动处理；</li>
<li>LeaderAndIsrResponseReceived；</li>
<li>LogDirChange：Broker 某个目录失败后的处理（比如磁盘坏掉等）；</li>
<li>ControllerShutdown：ControllerEventThread 处理这个事件时，会关闭当前线程。</li>
</ol>
<h3 id="重构集群状态管理"><a href="#重构集群状态管理" class="headerlink" title="重构集群状态管理"></a>重构集群状态管理</h3><p>这部分的改动，目前社区也没有一个很好的解决思路，重构这部分的目的是希望 Partition、Replica 的状态管理变得更清晰一些，让我们从代码中可以清楚地明白状态是在什么时间、什么地方、什么条件下被触发的。这个优化其实是跟上面那个有很大关联，采用单线程的事件处理模型，可以让状态管理也变得更清晰。</p>
<h4 id="prioritize-controller-requests"><a href="#prioritize-controller-requests" class="headerlink" title="prioritize controller requests"></a>prioritize controller requests</h4><p>我们想要把控制类请求与数据类请求分开，提高 controller 请求的优先级，这样的话即使 Broker 中请求有堆积，Broker 也会优先处理控制类的请求。</p>
<p>这部分的优化可以在网络层的 RequestChannel 中做，RequestChannel 可以根据请求的 id 信息把请求分为正常的和优先的，如果请求是 UpdateMetadataRequest、LeaderAndIsrRequest 或者 StopReplicaRequest，那么这个请求的优先级应该提高。实现方案有以下两种：</p>
<ol>
<li>在请求队列中增加一个优先级队列，优先级高的请求放到 the prioritized request queue 中，优先级低的放到普通请求队列中，但是无论使用一个定时拉取（poll）还是2个定时拉取，都会带来其他的问题，要么是增大普通请求的处理延迟，要么是增大了优先级高请求的延迟；</li>
<li>直接使用优先级队列代替现在的普通队列，设计上更倾向与这一种。</li>
</ol>
<p>目前这部分在1.1.0中还未实现。</p>
<h3 id="Controller-发送请求中添加-broker-的-generation-信息"><a href="#Controller-发送请求中添加-broker-的-generation-信息" class="headerlink" title="Controller 发送请求中添加 broker 的 generation 信息"></a>Controller 发送请求中添加 broker 的 generation 信息</h3><p>generation 信息是用来标识当前 broker 加入集群 epoch 信息，每当 broker 重新加入集群中，该 broker.id 对应的 generation 都应该变化（要求递增），目前有两种实现方案：</p>
<ol>
<li>为 broker 分配的一个全局唯一的 id，由 controller 广播给其他 broker；</li>
<li>直接使用 zookeeper 的 zxid 信息（broker.id 注册时的 zxid）。</li>
</ol>
<h3 id="直接使用原生的-Zookeeper-client"><a href="#直接使用原生的-Zookeeper-client" class="headerlink" title="直接使用原生的 Zookeeper client"></a>直接使用原生的 Zookeeper client</h3><p>Client 端的状态管理意味着当 Client 端发生状态变化（像连接中断或回话超时）时，我们有能力做一些操作。其中，zookeeper client 有效的状态（目前的 client 比下面又多了几种状态，这里先不深入）是:</p>
<ul>
<li>NOT_CONNECTED： the initial state of the client；</li>
<li>CONNECTING： the client is establishing a connection to zookeeper；</li>
<li>CONNECTED： the client has established a connection and session to zookeeper；</li>
<li>CLOSED： the session has closed or expired。</li>
</ul>
<p>有效的状态转移是：</p>
<ul>
<li>NOT_CONNECTED &gt; CONNECTING</li>
<li>CONNECTING &gt; CONNECTED</li>
<li>CONNECTING &gt; CLOSED</li>
<li>CONNECTED &gt; CONNECTING</li>
<li>CONNECTED &gt; CLOSED</li>
</ul>
<p>最开始的设想是直接使用原生 Client 的异步调用方式，这样的话依然可以通过回调方法监控到状态的变化（像连接中断或回话超时），同样，在每次事件处理时，可以通过检查状态信息来监控到 Client 状态的变化，及时做一些处理。</p>
<p>当一个 Client 接收到连接中断的 notification（Client 状态变成了 CONNECTING 状态），它意味着 Client 不能再从 zookeeper 接收到任何 notification 了。如果断开连接，对于 Controller 而言，无论它现在正在做什么它都应该先暂停，因为可能集群的 Controller 已经切换到其他机器上了，只是它还没接收到通知，它如果还在工作，可能会导致集群状态不一致。当连接断开后，Client 可以重新建立连接（re-establish，状态变为 CONNECTED）或者会话过期（状态变为 CLOSED，会话过期是由 zookeeper Server 来决定的）。如果变成了 CONNECTED 状态，Controller 应该重新开始这些暂停的操作，而如果状态变成了 CLOSED 状态，旧的 Controller 就会知道它不再是 controller，应该丢弃掉这些任务。</p>
<p>参考：</p>
<ul>
<li><a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>；</li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6980045.html" target="_blank" rel="external">Kafka controller重设计</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统的一致性协议之 2PC 和 3PC</title>
    <link href="http://matt33.com/2018/07/08/distribute-system-consistency-protocol/"/>
    <id>http://matt33.com/2018/07/08/distribute-system-consistency-protocol/</id>
    <published>2018-07-08T15:21:34.000Z</published>
    <updated>2018-07-08T16:02:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，最多只能满足其中两个。但在实际中，由于网络环境是不可信的，所以分区容忍性几乎是必不可选的，设计者基本就是在一致性和可用性之间做选择，当然大部分情况下，大家都会选择牺牲一部分的一致性来保证可用性（可用性较差的系统非常影响用户体验的，但是对另一些场景，比如支付场景，强一致性是必须要满足）。但是分布式系统又无法彻底放弃一致性（Consistency），如果真的放弃一致性，那么就说明这个系统中的数据根本不可信，数据也就没有意义，那么这个系统也就没有任何价值可言。</p>
<h2 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a>CAP 理论</h2><p>CAP 理论三个特性的详细含义如下：</p>
<ol>
<li>一致性（Consistency）：每次读取要么是最新的数据，要么是一个错误；</li>
<li>可用性（Availability）：client 在任何时刻的读写操作都能在限定的延迟内完成的，即每次请求都能获得一个响应（非错误），但不保证是最新的数据；</li>
<li>分区容忍性（Partition tolerance）：在大规模分布式系统中，网络分区现象，即分区间的机器无法进行网络通信的情况是必然会发生的，系统应该能保证在这种情况下可以正常工作。</li>
</ol>
<h3 id="分区容忍性"><a href="#分区容忍性" class="headerlink" title="分区容忍性"></a>分区容忍性</h3><p>很多人可能对分区容忍性不太理解，知乎有一个回答对这个解释的比较清楚（<a href="https://www.zhihu.com/question/54105974" target="_blank" rel="external">CAP理论中的P到底是个什么意思？</a>），这里引用一下：</p>
<ul>
<li>一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。</li>
<li>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</li>
<li>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里，容忍性就提高了。</li>
<li>然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。</li>
<li>要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</li>
<li>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</li>
</ul>
<h3 id="CAP-如何选择"><a href="#CAP-如何选择" class="headerlink" title="CAP 如何选择"></a>CAP 如何选择</h3><p>CAP 理论一个经典原理如下所示：</p>
<p><img src="/images/distribute/CAP.png" alt="CAP 理论原理"></p>
<p>CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。但是，对于大多数互联网应用来说，因为规模比较大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。但是对于一些金融相关行业，它有很多场景需要确保一致性，这种情况通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p>
<p>在一个分布式系统中，对于这三个特性，我们只能三选二，无法同时满足这三个特性，三选二的组合以及这样系统的特点总结如下（来自<a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>）：</p>
<ul>
<li>CA (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li>
<li>CP (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p>对于分布式系统分区容忍性是天然具备的要求，否则一旦出现网络分区，系统就拒绝所有写入只允许可读，这对大部分的场景是不可接收的，因此，在设计分布式系统时，更多的情况下是选举 CP 还是 AP，要么选择强一致性弱可用性，要么选择高可用性容忍弱一致性。</p>
<h3 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h3><p>关于分布式系统的一致性模型有以下几种：</p>
<h4 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h4><p>当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值，直到这个数据被其他数据更新为止。</p>
<p>但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。</p>
<h4 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a>弱一致性</h4><p>系统并不保证进程或者线程的访问都会返回最新更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。</p>
<h4 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h4><p>最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一个时间，在这个时间之后可以保证这一点，而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为「不一致窗口」。不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。</p>
<p>最终一致性在实际应用中又有多种变种：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>因果一致性</td>
<td>如果 A 进程在更新之后向 B 进程通知更新的完成，那么 B 的访问操作将会返回更新的值。而没有因果关系的 C 进程将会遵循最终一致性的规则（C 在不一致窗口内还是看到是旧值）。</td>
</tr>
<tr>
<td>读你所写一致性</td>
<td>因果一致性的特定形式。一个进程进行数据更新后，会给自己发送一条通知，该进程后续的操作都会以最新值作为基础，而其他的进程还是只能在不一致窗口之后才能看到最新值。</td>
</tr>
<tr>
<td>会话一致性</td>
<td>读你所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程可以读取到最新之，但如果会话终止，重新连接后，如果此时还在不一致窗口内，还是可嫩读取到旧值。</td>
</tr>
<tr>
<td>单调读一致性</td>
<td>如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。</td>
</tr>
<tr>
<td>单调写一致性</td>
<td>系统保证对同一个进程的写操作串行化。</td>
</tr>
</tbody>
</table>
<p>它们的关系又如下图所示（图来自 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>）：</p>
<p><img src="/images/distribute/consistency.png" alt="一致性模型之间关系"></p>
<h2 id="分布式一致性协议"><a href="#分布式一致性协议" class="headerlink" title="分布式一致性协议"></a>分布式一致性协议</h2><p>为了解决分布式系统的一致性问题，在长期的研究探索过程中，业内涌现出了一大批经典的一致性协议和算法，其中比较著名的有二阶段提交协议（2PC），三阶段提交协议（3PC）和 Paxos 算法（本文暂时先不介绍）。</p>
<p>Google 2009年 在<a href="https://snarfed.org/transactions_across_datacenters_io.html" target="_blank" rel="external">Transaction Across DataCenter</a> 的分享中，对一致性协议在业内的实践做了一简单的总结，如下图所示，这是 CAP 理论在工业界应用的实践经验。</p>
<p><img src="/images/distribute/cap-sumarry.png" alt="CAP 理论在工业界的实践"></p>
<p>其中，第一行表头代表了分布式系统中通用的一致性方案，包括冷备、Master/Slave、Master/Master、两阶段提交以及基于 Paxos 算法的解决方案，第一列表头代表了分布式系统大家所关心的各项指标，包括一致性、事务支持程度、数据延迟、系统吞吐量、数据丢失可能性、故障自动恢复方式。</p>
<h2 id="两阶段提交协议（2PC）"><a href="#两阶段提交协议（2PC）" class="headerlink" title="两阶段提交协议（2PC）"></a>两阶段提交协议（2PC）</h2><p>二阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现 ACID 的原子性（A）。在数据一致性中，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来保证数据的强一致性。</p>
<p>2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为<strong>协调者</strong>的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</p>
<h3 id="2PC-过程"><a href="#2PC-过程" class="headerlink" title="2PC 过程"></a>2PC 过程</h3><p>关于两阶段提交的过程如下图所示：</p>
<p><img src="/images/distribute/2pc_process.png" alt="两阶段提交过程"></p>
<p>顾名思义，2PC 分为两个过程：</p>
<ol>
<li>表决阶段：此时 Coordinator （协调者）向所有的参与者发送一个 vote request，参与者在收到这请求后，如果准备好了就会向 Coordinator 发送一个 <code>VOTE_COMMIT</code> 消息作为回应，告知 Coordinator 自己已经做好了准备，否则会返回一个 <code>VOTE_ABORT</code> 消息；</li>
<li>提交阶段：Coordinator 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么 Coordinator 就会发送 <code>GLOBAL_COMMIT</code> 消息，否则发送 <code>GLOBAL_ABORT</code> 消息；对于参与者而言，如果收到 <code>GLOBAL_COMMIT</code> 消息，就会提交本地事务，否则就会取消本地事务。</li>
</ol>
<h3 id="2PC-一致性问题"><a href="#2PC-一致性问题" class="headerlink" title="2PC 一致性问题"></a>2PC 一致性问题</h3><p>这里先讨论一下，2PC 是否可以在任何情况下都可以解决一致性问题，在实际的网络生产中，各种情况都有可能发生，这里，我们先从理论上分析各种意外情况。</p>
<p>2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 挂了，参与者没挂</td>
<td>这种情况其实比较好解决，只要找一个 Coordinator 的替代者。当他成为新的 Coordinator 的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。</td>
</tr>
<tr>
<td>参与者挂了（无法恢复），Coordinator 没挂</td>
<td>如果挂了之后没有恢复，那么是不会导致数据一致性问题。</td>
</tr>
<tr>
<td>参与者挂了（后来恢复），Coordinator 没挂</td>
<td>恢复后参与者如果发现有未执行完的事务操作，直接取消，然后再询问 Coordinator 目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。</td>
</tr>
</tbody>
</table>
<p>还有一种情况是：参与者挂了，Coordinator 也挂了，需要再细分为几种类型来讨论：</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 和参与者在第一阶段挂了</td>
<td>由于这时还没有执行 commit 操作，新选出来的 Coordinator 可以询问各个参与者的情况，再决定是进行 commit 还是 roolback。因为还没有 commit，所以不会导致数据一致性问题。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，但是挂的这个参与者在挂之前还没有做相关操作</td>
<td>这种情况下，当新的 Coordinator 被选出来之后，他同样是询问所有参与者的情况。只要有机器执行了 abort（roolback）操作或者第一阶段返回的信息是 No 的话，那就直接执行 roolback 操作。如果没有人执行 abort 操作，但是有机器执行了 commit 操作，那么就直接执行 commit 操作。这样，当挂掉的参与者恢复之后，只要按照 Coordinator 的指示进行事务的 commit 还是 roolback 操作就可以了。因为挂掉的机器并没有做 commit 或者 roolback 操作，而没有挂掉的机器们和新的 Coordinator 又执行了同样的操作，那么这种情况不会导致数据不一致现象。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，挂的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。</td>
<td>这种情况下，新的 Coordinator 被选出来之后，如果他想负起 Coordinator 的责任的话他就只能按照之前那种情况来执行 commit 或者 roolback 操作。这样新的 Coordinator 和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了 commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他已经执行完了之前的事务，如果他执行的是 commit 那还好，和其他的机器保持一致了，万一他执行的是 roolback 操作呢？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和 Coordinator 通信，再想办法把数据搞成一致的，但是，这段时间内他的数据状态已经是不一致的了！</td>
</tr>
</tbody>
</table>
<p>所以，2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。为了解决这个问题，衍生除了3PC。</p>
<h3 id="2PC-优缺点"><a href="#2PC-优缺点" class="headerlink" title="2PC 优缺点"></a>2PC 优缺点</h3><p>简单总结一下 2PC 的优缺点：</p>
<ul>
<li>优点：原理简洁清晰、实现方便；</li>
<li>缺点：同步阻塞、单点问题、某些情况可能导致数据不一致。</li>
</ul>
<p>关于这几个缺点，在实际应用中，都是对2PC 做了相应的改造：</p>
<ol>
<li>同步阻塞：2PC 有几个过程（比如 Coordinator 等待所有参与者表决的过程中）都是同步阻塞的，在实际的应用中，这可能会导致长阻塞问题，这个问题是通过超时判断机制来解决的，但并不能完全解决同步阻塞问题；</li>
<li>Coordinator 单点问题：实际生产应用中，Coordinator 都会有相应的备选节点；</li>
<li>数据不一致：这个在前面已经讲述过了，如果在第二阶段，Coordinator 和参与者都出现挂掉的情况下，是有可能导致数据不一致的。</li>
</ol>
<h2 id="三阶段提交协议（3PC）"><a href="#三阶段提交协议（3PC）" class="headerlink" title="三阶段提交协议（3PC）"></a>三阶段提交协议（3PC）</h2><p>三阶段提交协议（Three-Phase Commit， 3PC）最关键要解决的就是 Coordinator 和参与者同时挂掉导致数据不一致的问题，所以 3PC 把在 2PC 中又添加一个阶段，这样三阶段提交就有：CanCommit、PreCommit 和 DoCommit 三个阶段。</p>
<h3 id="3PC-过程"><a href="#3PC-过程" class="headerlink" title="3PC 过程"></a>3PC 过程</h3><p>三阶段提交协议的过程如下图（图来自 <a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>）所示：</p>
<p><img src="/images/distribute/Three-phase_commit_diagram.png" alt="三节点提交过程"></p>
<p>3PC 的详细过程如下（这个过程步骤内容来自 <a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>）：</p>
<h4 id="阶段一-CanCommit"><a href="#阶段一-CanCommit" class="headerlink" title="阶段一 CanCommit"></a>阶段一 CanCommit</h4><ol>
<li>事务询问：Coordinator 向各参与者发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；</li>
<li>参与者向 Coordinator 反馈询问的响应：参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No。</li>
</ol>
<h4 id="阶段二-PreCommit"><a href="#阶段二-PreCommit" class="headerlink" title="阶段二 PreCommit"></a>阶段二 PreCommit</h4><p><strong>执行事务预提交</strong>：如果 Coordinator 接收到各参与者反馈都是Yes，那么执行事务预提交：</p>
<ol>
<li>发送预提交请求：Coordinator 向各参与者发送 preCommit 请求，并进入 prepared 阶段；</li>
<li>事务预提交：参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；</li>
<li>各参与者向 Coordinator 反馈事务执行的响应：如果各参与者都成功执行了事务操作，那么反馈给协调者 ACK 响应，同时等待最终指令，提交 commit 或者终止 abort，结束流程；</li>
</ol>
<p><strong>中断事务</strong>：如果任何一个参与者向 Coordinator 反馈了 No 响应，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务。</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者发送 abort 请求；</li>
<li>中断事务：无论是收到来自 Coordinator 的 abort 请求，还是等待超时，参与者都中断事务。</li>
</ol>
<h4 id="阶段三-doCommit"><a href="#阶段三-doCommit" class="headerlink" title="阶段三 doCommit"></a>阶段三 doCommit</h4><p><strong>执行提交</strong></p>
<ol>
<li>发送提交请求：假设 Coordinator 正常工作，接收到了所有参与者的 ack 响应，那么它将从预提交阶段进入提交状态，并向所有参与者发送 doCommit 请求；</li>
<li>事务提交：参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；</li>
<li>反馈事务提交结果：参与者完成事务提交后，向 Coordinator 发送 ACK 信息；</li>
<li>完成事务：Coordinator 接收到所有参与者 ack 信息，完成事务。</li>
</ol>
<p><strong>中断事务</strong>：假设 Coordinator 正常工作，并且有任一参与者反馈 No，或者在等待超时后无法接收所有参与者的反馈，都会中断事务</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者节点发送 abort 请求；</li>
<li>事务回滚：参与者接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资源；</li>
<li>反馈事务回滚结果：参与者在完成事务回滚之后，向 Coordinator 发送 ack 信息；</li>
<li>中断事务：Coordinator 接收到所有参与者反馈的 ack 信息后，中断事务。</li>
</ol>
<h3 id="3PC-分析"><a href="#3PC-分析" class="headerlink" title="3PC 分析"></a>3PC 分析</h3><p>3PC 虽然解决了 Coordinator 与参与者都异常情况下导致数据不一致的问题，3PC 依然带来其他问题：比如，网络分区问题，在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit。</p>
<p>而且由于3PC 的设计过于复杂，在解决2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。</p>
<p>参考：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external">维基百科：二阶段提交</a>；</li>
<li><a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>；</li>
<li><a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>；</li>
<li><a href="http://www.hollischuang.com/archives/663" target="_blank" rel="external">关于分布式一致性的探究</a>；</li>
<li><a href="http://www.hollischuang.com/archives/681" target="_blank" rel="external">关于分布式事务、两阶段提交协议、三阶提交协议</a>；</li>
<li><a href="http://www.hollischuang.com/archives/1580" target="_blank" rel="external">深入理解分布式系统的2PC和3PC</a>；</li>
<li><a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://coolshell.cn/articles/10910.html" target="_blank" rel="external">分布式系统的事务处理</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="分布式系统" scheme="http://matt33.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Java 守护线程</title>
    <link href="http://matt33.com/2018/07/07/java-daemon-thread/"/>
    <id>http://matt33.com/2018/07/07/java-daemon-thread/</id>
    <published>2018-07-07T13:43:21.000Z</published>
    <updated>2018-07-07T14:40:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 <code>setDaemon()</code> 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了<strong>守护线程(daemon thread)</strong>，那么什么是守护线程呢？或者说守护线程与非守护线程（普通线程）的区别在什么地方呢？这个就是本文主要讲述的内容。</p>
<h2 id="守护线程"><a href="#守护线程" class="headerlink" title="守护线程"></a>守护线程</h2><p>一般来说，Java 中的线程可以分为两种：守护线程和普通线程。在 JVM 刚启动时，它创建的所有线程，除了主线程（main thread）外，其他的线程都是守护线程（比如：垃圾收集器、以及其他执行辅助操作的线程）。</p>
<p>当创建一个新线程时，新线程将会继承它线程的守护状态，默认情况下，主线程创建的所有线程都是普通线程。</p>
<p>什么情况下会需要守护线程呢？一般情况下是，当我们希望创建一个线程来执行一些辅助的工作，但是又不希望这个线程阻碍 JVM 的关闭，在这种情况下，我们就需要使用守护线程了。</p>
<h2 id="守护线程的作用"><a href="#守护线程的作用" class="headerlink" title="守护线程的作用"></a>守护线程的作用</h2><p>守护线程与普通线程唯一的区别是：当线程退出时，JVM 会检查其他正在运行的线程，如果这些线程都是守护线程，那么 JVM 会正常退出操作，但是如果有普通线程还在运行，JVM 是不会执行退出操作的。当 JVM 退出时，所有仍然存在的守护线程都将被抛弃，既不会执行 finally 部分的代码，也不会执行 stack unwound 操作，JVM 会直接退出。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">When the JVM halts any remaining daemon threads are abandoned:</div><div class="line"></div><div class="line"> 1. finally blocks are not executed,</div><div class="line"> 2. stacks are not unwound - the JVM just exits.</div></pre></td></tr></table></figure>
<p>下面有个小示例，来自 <a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DaemonTest</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="keyword">new</span> WorkerThread().start();</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            Thread.sleep(<span class="number">7500</span>);</div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">            <span class="comment">// handle here exception</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(<span class="string">"Main Thread ending"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WorkerThread</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="comment">// When false, (i.e. when it's a user thread), the Worker thread continues to run.</span></div><div class="line">        <span class="comment">// When true, (i.e. when it's a daemon thread), the Worker thread terminates when the main thread terminates.</span></div><div class="line">        setDaemon(<span class="keyword">false</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            System.out.println(<span class="string">"Hello from Worker "</span> + count++);</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">                sleep(<span class="number">5000</span>);</div><div class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                <span class="comment">// handle exception here</span></div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当为普通线程时，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Hello from Worker 0</div><div class="line">Hello from Worker 1</div><div class="line">Main Thread ending</div><div class="line">Hello from Worker 2</div><div class="line">Hello from Worker 3</div><div class="line">Hello from Worker 4</div><div class="line">Hello from Worker 5</div><div class="line">....</div></pre></td></tr></table></figure>
<p>也就是说，此时即使主线程执行完了，JVM 也会等待 WorkerThread 执行完毕才会退出，而如果将该线程设置守护线程的话，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Hello from Worker 0</div><div class="line">Hello from Worker 1</div><div class="line">Main Thread ending</div></pre></td></tr></table></figure>
<p>在 main 线程执行完毕后，JVM 进程就退出了，不会 care WorkerThread 线程是否执行完毕。</p>
<p>参考：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>;</li>
<li><a href="http://www.javaconcurrencyinpractice.com/" target="_blank" rel="external">《Java 并发编程实战》</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 &lt;code&gt;setDaemon()&lt;/code&gt; 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了&lt;stron
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="java" scheme="http://matt33.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 1+N+M 网络处理模型（二十三）</title>
    <link href="http://matt33.com/2018/06/27/kafka-server-process-model/"/>
    <id>http://matt33.com/2018/06/27/kafka-server-process-model/</id>
    <published>2018-06-27T15:18:01.000Z</published>
    <updated>2018-06-27T15:43:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实现，如果还有印象，就会知道它们都是通过 KafkaApis 对象处理的，但是前面并没有详细讲述 Server 端是如何监听到相应的请求、请求是如何交给 KafkaApis 对象进行处理，以及处理后是如何返回给请求者（请求者可以是 client 也可以是 server），这些都属于 Server 的网络处理模型，也是本文讲述的主要内容。</p>
<h2 id="Server-网络模型整体流程"><a href="#Server-网络模型整体流程" class="headerlink" title="Server 网络模型整体流程"></a>Server 网络模型整体流程</h2><p>Kafka Server 启动后，会通过 KafkaServer 的 <code>startup()</code> 方法初始化涉及到网络模型的相关对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>()&#123;</div><div class="line">  <span class="comment">//note: socketServer</span></div><div class="line">  socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">  socketServer.startup()</div><div class="line">  <span class="comment">//<span class="doctag">NOTE:</span> 初始化 KafkaApis 实例,每个 Server 只会启动一个线程</span></div><div class="line">  apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">    kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">    clusterId, time)</div><div class="line"></div><div class="line">  requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">    config.numIoThreads)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Kafka Server 在启动时会初始化 SocketServer、KafkaApis 和 KafkaRequestHandlerPool 对象，这也是 Server 网络处理模型的主要组成部分。Kafka Server 的网络处理模型也是基于 Java NIO 机制实现的，实现模式与 Reactor 模式类似，其完整的处理流程图如下所示：</p>
<p><img src="/images/kafka/server-nio.png" alt="Kafka Server 1+N+M 网络处理模型"></p>
<p>上图如果现在不理解，并不要紧，这里先简单介绍一些，讲述一下整体的流程，本节下面会结合 Kafka 的代码详细来讲述图中的过程。上图的网络模型可以简要总结为以下三个重要组成部分：</p>
<ol>
<li>1 个 Acceptor 线程，负责监听 Socket 新的连接请求，注册了 <code>OP_ACCEPT</code> 事件，将新的连接按照 round robin 方式交给对应的 Processor 线程处理；</li>
<li>N 个 Processor 线程，其中每个 Processor 都有自己的 selector，它会向 Acceptor 分配的 SocketChannel 注册相应的 <code>OP_READ</code> 事件，N 的大小由 <code>num.networker.threads</code> 决定；</li>
<li>M 个 KafkaRequestHandler  线程处理请求，并将处理的结果返回给 Processor 线程对应的 response queue 中，由 Processor 将处理的结果返回给相应的请求发送者，M 的大小由 <code>num.io.threads</code> 来决定。</li>
</ol>
<p>上图展示的整体的处理流程如下所示：</p>
<ol>
<li>Acceptor 监听到来自请求者（请求者可以是来自 client，也可以来自 server）的新的连接，Acceptor 将这个请求者按照 round robin 的方式交给对对应的 Processor 进行处理；</li>
<li>Processor 注册这个 SocketChannel 的 <code>OP_READ</code> 的事件，如果有请求发送过来就可以被 Processor 的 Selector 选中；</li>
<li>Processor 将请求者发送的请求放入到一个 Request Queue 中，这是所有 Processor 共有的一个队列；</li>
<li>KafkaRequestHandler 从 Request Queue 中取出请求；</li>
<li>调用 KafkaApis 进行相应的处理；</li>
<li>处理的结果放入到该 Processor 对应的 Response Queue 中（每个 request 都标识它们来自哪个 Processor），Request Queue 的数量与 Processor 的数量保持一致；</li>
<li>Processor 从对应的 Response Queue 中取出 response；</li>
<li>Processor 将处理的结果返回给对应的请求者。</li>
</ol>
<p>上面是 Server 端网络处理的整体流程，下面我们开始详细讲述上面内容在 Kafka 中实现。</p>
<h2 id="SocketServer"><a href="#SocketServer" class="headerlink" title="SocketServer"></a>SocketServer</h2><p>SocketServer 是接收 Socket 连接、处理请求并返回处理结果的地方，Acceptor 及 Processor 的初始化、处理逻辑都是在这里实现的。在SocketServer 内有几个比较重要的变量，这里先来看下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SocketServer</span>(<span class="params">val config: <span class="type">KafkaConfig</span>, val metrics: <span class="type">Metrics</span>, val time: <span class="type">Time</span>, val credentialProvider: <span class="type">CredentialProvider</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> endpoints = config.listeners.map(l =&gt; l.listenerName -&gt; l).toMap <span class="comment">//note: broker 开放的端口数</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> numProcessorThreads = config.numNetworkThreads <span class="comment">//note: num.network.threads 默认为 3个，即 processor</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxQueuedRequests = config.queuedMaxRequests <span class="comment">//note:  queued.max.requests，request 队列中允许的最多请求数，默认是500</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> totalProcessorThreads = numProcessorThreads * endpoints.size <span class="comment">//note: 每个端口会对应 N 个 processor</span></div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIp = config.maxConnectionsPerIp <span class="comment">//note: 默认 2147483647</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIpOverrides = config.maxConnectionsPerIpOverrides</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Socket Server on Broker "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">  <span class="comment">//note: 请求队列</span></div><div class="line">  <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(totalProcessorThreads, maxQueuedRequests)</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> processors = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Processor</span>](totalProcessorThreads)</div><div class="line"></div><div class="line">  <span class="keyword">private</span>[network] <span class="keyword">val</span> acceptors = mutable.<span class="type">Map</span>[<span class="type">EndPoint</span>, <span class="type">Acceptor</span>]()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val numProcessors: <span class="type">Int</span>, val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> responseListeners: <span class="type">List</span>[(<span class="type">Int</span>) =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></div><div class="line">  <span class="comment">//note: 一个 requestQueue 队列,N 个 responseQueues 队列</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Request</span>](queueSize)</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> responseQueues = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]](numProcessors)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中</p>
<ol>
<li><code>numProcessorThreads</code>：决定了 Processor 的个数，默认是3个，也就是 1+N+M 的 N 的数值；</li>
<li><code>maxQueuedRequests</code>：决定了 request queue 中最多允许放入多少个请求（等待处理的请求），默认是 500；</li>
<li>在 <code>RequestChannel</code> 中初始化了一个 requestQueue 和 N 个 responseQueue。</li>
</ol>
<h3 id="SocketServer-初始化"><a href="#SocketServer-初始化" class="headerlink" title="SocketServer 初始化"></a>SocketServer 初始化</h3><p>在 SocketServer 初始化方法 <code>startup()</code> 中，会初始化 1 个 Acceptor 和 N 个 Processor 线程（每个 EndPoint 都会初始化这么多，一般来说一个 Server 只会设置一个端口），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">this</span>.synchronized &#123;</div><div class="line">    <span class="comment">//note: 一台 broker 一般只设置一个端口，当然这里也可以设置两个</span></div><div class="line">    config.listeners.foreach &#123; endpoint =&gt;</div><div class="line">      <span class="keyword">val</span> listenerName = endpoint.listenerName</div><div class="line">      <span class="keyword">val</span> securityProtocol = endpoint.securityProtocol</div><div class="line">      <span class="keyword">val</span> processorEndIndex = processorBeginIndex + numProcessorThreads</div><div class="line"></div><div class="line">      <span class="comment">//note: N 个 processor</span></div><div class="line">      <span class="keyword">for</span> (i &lt;- processorBeginIndex until processorEndIndex)</div><div class="line">        processors(i) = newProcessor(i, connectionQuotas, listenerName, securityProtocol)</div><div class="line"></div><div class="line">      <span class="comment">//note: 1个 Acceptor</span></div><div class="line">      <span class="keyword">val</span> acceptor = <span class="keyword">new</span> <span class="type">Acceptor</span>(endpoint, sendBufferSize, recvBufferSize, brokerId,</div><div class="line">        processors.slice(processorBeginIndex, processorEndIndex), connectionQuotas)</div><div class="line">      acceptors.put(endpoint, acceptor)</div><div class="line">      <span class="type">Utils</span>.newThread(<span class="string">s"kafka-socket-acceptor-<span class="subst">$listenerName</span>-<span class="subst">$securityProtocol</span>-<span class="subst">$&#123;endpoint.port&#125;</span>"</span>, acceptor, <span class="literal">false</span>).start()</div><div class="line">      acceptor.awaitStartup()</div><div class="line"></div><div class="line">      processorBeginIndex = processorEndIndex</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Acceptor-处理"><a href="#Acceptor-处理" class="headerlink" title="Acceptor 处理"></a>Acceptor 处理</h3><p>SocketServer 在初始化后 Acceptor 线程后，Acceptor 启动，会首先注册 <code>OP_ACCEPT</code> 事件，监听是否有新的连接，如果来了新的连接就将该 SocketChannel 交给对应的 Processor 进行处理，Processor 是通过 round robin 方法选择的，这样可以保证 Processor 的负载相差无几（至少可以保证监听的 SocketChannel 差不多），实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  serverChannel.register(nioSelector, <span class="type">SelectionKey</span>.<span class="type">OP_ACCEPT</span>)<span class="comment">//note: 注册 accept 事件</span></div><div class="line">  startupComplete()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">var</span> currentProcessor = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> (isRunning) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">val</span> ready = nioSelector.select(<span class="number">500</span>)</div><div class="line">        <span class="keyword">if</span> (ready &gt; <span class="number">0</span>) &#123;</div><div class="line">          <span class="keyword">val</span> keys = nioSelector.selectedKeys()</div><div class="line">          <span class="keyword">val</span> iter = keys.iterator()</div><div class="line">          <span class="keyword">while</span> (iter.hasNext &amp;&amp; isRunning) &#123;</div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">              <span class="keyword">val</span> key = iter.next</div><div class="line">              iter.remove()</div><div class="line">              <span class="keyword">if</span> (key.isAcceptable)</div><div class="line">                accept(key, processors(currentProcessor))<span class="comment">//note: 拿到一个socket 连接，轮询选择一个processor进行处理</span></div><div class="line">              <span class="keyword">else</span></div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Unrecognized key state for acceptor thread."</span>)</div><div class="line"></div><div class="line">              <span class="comment">//note: 轮询算法,使用 round robin</span></div><div class="line">              <span class="comment">// round robin to the next processor thread</span></div><div class="line">              currentProcessor = (currentProcessor + <span class="number">1</span>) % processors.length</div><div class="line">            &#125; <span class="keyword">catch</span> &#123;</div><div class="line">              <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while accepting connection"</span>, e)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="comment">// We catch all the throwables to prevent the acceptor thread from exiting on exceptions due</span></div><div class="line">        <span class="comment">// to a select operation on a specific channel or a bad request. We don't want</span></div><div class="line">        <span class="comment">// the broker to stop responding to requests from other clients in these scenarios.</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error occurred"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> &#123;</div><div class="line">    debug(<span class="string">"Closing server socket and selector."</span>)</div><div class="line">    swallowError(serverChannel.close())</div><div class="line">    swallowError(nioSelector.close())</div><div class="line">    shutdownComplete()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Acceptor 通过 <code>accept()</code> 将该新连接交给对应的 Processor，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理一个新的连接</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</div><div class="line">  <span class="comment">//note: accept 事件发生时，获取注册到 selector 上的 ServerSocketChannel</span></div><div class="line">  <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]</div><div class="line">  <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    connectionQuotas.inc(socketChannel.socket().getInetAddress)</div><div class="line">    socketChannel.configureBlocking(<span class="literal">false</span>)</div><div class="line">    socketChannel.socket().setTcpNoDelay(<span class="literal">true</span>)</div><div class="line">    socketChannel.socket().setKeepAlive(<span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (sendBufferSize != <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>)</div><div class="line">      socketChannel.socket().setSendBufferSize(sendBufferSize)</div><div class="line"></div><div class="line">    debug(<span class="string">"Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]"</span></div><div class="line">          .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id,</div><div class="line">                socketChannel.socket.getSendBufferSize, sendBufferSize,</div><div class="line">                socketChannel.socket.getReceiveBufferSize, recvBufferSize))</div><div class="line"></div><div class="line">    <span class="comment">//note: 轮询选择不同的 processor 进行处理</span></div><div class="line">    processor.accept(socketChannel)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">TooManyConnectionsException</span> =&gt;</div><div class="line">      info(<span class="string">"Rejected connection from %s, address already has the configured maximum of %d connections."</span>.format(e.ip, e.count))</div><div class="line">      close(socketChannel)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Processor-处理"><a href="#Processor-处理" class="headerlink" title="Processor 处理"></a>Processor 处理</h3><p>在前面，Acceptor 通过 <code>accept()</code> 将新的连接交给 Processor，Processor 实际上是将该 SocketChannel 添加到该 Processor 的 <code>newConnections</code> 队列中，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</div><div class="line">  newConnections.add(socketChannel)<span class="comment">//note: 添加到队列中</span></div><div class="line">  wakeup()<span class="comment">//note: 唤醒 Processor 的 selector（如果此时在阻塞的话）</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里详细看下 Processor 线程做了什么事情，其 <code>run()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  startupComplete()</div><div class="line">  <span class="keyword">while</span> (isRunning) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// setup any new connections that have been queued up</span></div><div class="line">      configureNewConnections()<span class="comment">//note: 对新的 socket 连接,并注册 READ 事件</span></div><div class="line">      <span class="comment">// register any new responses for writing</span></div><div class="line">      processNewResponses()<span class="comment">//note: 处理 response 队列中 response</span></div><div class="line">      poll() <span class="comment">//note: 监听所有的 socket channel，是否有新的请求发送过来</span></div><div class="line">      processCompletedReceives() <span class="comment">//note: 处理接收到请求，将其放入到 request queue 中</span></div><div class="line">      processCompletedSends() <span class="comment">//note: 处理已经完成的发送</span></div><div class="line">      processDisconnected() <span class="comment">//note: 处理断开的连接</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// We catch all the throwables here to prevent the processor thread from exiting. We do this because</span></div><div class="line">      <span class="comment">// letting a processor exit might cause a bigger impact on the broker. Usually the exceptions thrown would</span></div><div class="line">      <span class="comment">// be either associated with a specific socket channel or a bad request. We just ignore the bad socket channel</span></div><div class="line">      <span class="comment">// or request. This behavior might need to be reviewed if we see an exception that need the entire broker to stop.</span></div><div class="line">      <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Processor got uncaught exception."</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  debug(<span class="string">"Closing selector - processor "</span> + id)</div><div class="line">  swallowError(closeAll())</div><div class="line">  shutdownComplete()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Processor 在一次循环中，主要做的事情如下：</p>
<ol>
<li><code>configureNewConnections()</code>：对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，这里主要是 Processor 的 selector 注册该连接的 <code>OP_READ</code> 事件；</li>
<li><code>processNewResponses()</code>：从该 Processor 对应的 response queue 中取出一个 response，进行发送；</li>
<li><code>poll()</code>：调用 selector 的 <code>poll()</code> 方法，遍历注册的 SocketChannel，查看是否有事件准备就绪；</li>
<li><code>processCompletedReceives()</code>：将接收到请求添加到的 request queue 中；</li>
<li><code>processCompletedSends()</code>：处理已经完成的响应发送；</li>
<li><code>processDisconnected()</code>：处理断开的 SocketChannel。</li>
</ol>
<p>上面就是 Processor 线程处理的主要逻辑，先是向新的 SocketChannel 注册相应的事件，监控是否有请求发送过来，接着从 response queue 中取出处理完成的请求发送给对应的请求者，然后调用一下 selector 的 <code>poll()</code>，遍历一下注册的所有 SocketChannel，判断是否有事件就绪，然后做相应的处理。这里需要注意的是，request queue 是所有 Processor 公用的一个队列，而 response queue 则是与 Processor 一一对应的，因为每个 Processor 监听的 SocketChannel 并不是同一批的，如果公有一个 response queue，那么这个 N 个 Processor 的 selector 要去监听所有的 SocketChannel，而不是现在这种，只需要去关注分配给自己的 SocketChannel。</p>
<p>下面分别看下上面的这些方法的具体实现。</p>
<h4 id="configureNewConnections"><a href="#configureNewConnections" class="headerlink" title="configureNewConnections"></a>configureNewConnections</h4><p><code>configureNewConnections()</code> 对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，主要是 selector 注册相应的 <code>OP_READ</code> 事件，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果有新的连接过来，将该 Channel 的 OP_READ 事件注册到 selector 上</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">configureNewConnections</span></span>() &#123;</div><div class="line">  <span class="keyword">while</span> (!newConnections.isEmpty) &#123;</div><div class="line">    <span class="keyword">val</span> channel = newConnections.poll()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      debug(<span class="string">s"Processor <span class="subst">$id</span> listening to new connection from <span class="subst">$&#123;channel.socket.getRemoteSocketAddress&#125;</span>"</span>)</div><div class="line">      <span class="keyword">val</span> localHost = channel.socket().getLocalAddress.getHostAddress</div><div class="line">      <span class="keyword">val</span> localPort = channel.socket().getLocalPort</div><div class="line">      <span class="keyword">val</span> remoteHost = channel.socket().getInetAddress.getHostAddress</div><div class="line">      <span class="keyword">val</span> remotePort = channel.socket().getPort</div><div class="line">      <span class="keyword">val</span> connectionId = <span class="type">ConnectionId</span>(localHost, localPort, remoteHost, remotePort).toString</div><div class="line">      selector.register(connectionId, channel)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// We explicitly catch all non fatal exceptions and close the socket to avoid a socket leak. The other</span></div><div class="line">      <span class="comment">// throwables will be caught in processor and logged as uncaught exceptions.</span></div><div class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</div><div class="line">        <span class="keyword">val</span> remoteAddress = channel.getRemoteAddress</div><div class="line">        <span class="comment">// need to close the channel here to avoid a socket leak.</span></div><div class="line">        close(channel)</div><div class="line">        error(<span class="string">s"Processor <span class="subst">$id</span> closed connection from <span class="subst">$remoteAddress</span>"</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processNewResponses"><a href="#processNewResponses" class="headerlink" title="processNewResponses"></a>processNewResponses</h4><p><code>processNewResponses()</code> 方法是从该 Processor 对应的 response queue 中取出一个 response，Processor 是通过 RequestChannel 的 <code>receiveResponse()</code> 从该 Processor 对应的 response queue 中取出 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 response</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">receiveResponse</span></span>(processor: <span class="type">Int</span>): <span class="type">RequestChannel</span>.<span class="type">Response</span> = &#123;</div><div class="line">  <span class="keyword">val</span> response = responseQueues(processor).poll()</div><div class="line">  <span class="keyword">if</span> (response != <span class="literal">null</span>)</div><div class="line">    response.request.responseDequeueTimeMs = <span class="type">Time</span>.<span class="type">SYSTEM</span>.milliseconds</div><div class="line">  response</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>取到相应的 response 之后，会判断该 response 的类型，进行相应的操作，如果需要返回，那么会调用 <code>sendResponse()</code> 发送该 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理一个新的 response 响应</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processNewResponses</span></span>() &#123;</div><div class="line">  <span class="keyword">var</span> curr = requestChannel.receiveResponse(id)</div><div class="line">  <span class="keyword">while</span> (curr != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      curr.responseAction <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">NoOpAction</span> =&gt; <span class="comment">//note: 如果这个请求不需要返回 response，再次注册该监听事件</span></div><div class="line">          <span class="comment">// There is no response to send to the client, we need to read more pipelined requests</span></div><div class="line">          <span class="comment">// that are sitting in the server's socket buffer</span></div><div class="line">          curr.request.updateRequestMetrics</div><div class="line">          trace(<span class="string">"Socket server received empty response to send, registering for read: "</span> + curr)</div><div class="line">          <span class="keyword">val</span> channelId = curr.request.connectionId</div><div class="line">          <span class="keyword">if</span> (selector.channel(channelId) != <span class="literal">null</span> || selector.closingChannel(channelId) != <span class="literal">null</span>)</div><div class="line">              selector.unmute(channelId)</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">SendAction</span> =&gt; <span class="comment">//note: 需要发送的 response，那么进行发送</span></div><div class="line">          sendResponse(curr)</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">CloseConnectionAction</span> =&gt; <span class="comment">//note: 要关闭的 response</span></div><div class="line">          curr.request.updateRequestMetrics</div><div class="line">          trace(<span class="string">"Closing socket connection actively according to the response code."</span>)</div><div class="line">          close(selector, curr.request.connectionId)</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      curr = requestChannel.receiveResponse(id)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* `protected` for test usage */</span></div><div class="line"><span class="comment">//note: 发送的对应的 response</span></div><div class="line"><span class="keyword">protected</span>[network] <span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</div><div class="line">  trace(<span class="string">s"Socket server received response to send, registering for write and sending data: <span class="subst">$response</span>"</span>)</div><div class="line">  <span class="keyword">val</span> channel = selector.channel(response.responseSend.destination)</div><div class="line">  <span class="comment">// `channel` can be null if the selector closed the connection because it was idle for too long</span></div><div class="line">  <span class="keyword">if</span> (channel == <span class="literal">null</span>) &#123;</div><div class="line">    warn(<span class="string">s"Attempting to send response via channel for which there is no open connection, connection id <span class="subst">$id</span>"</span>)</div><div class="line">    response.request.updateRequestMetrics()</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    selector.send(response.responseSend) <span class="comment">//note: 发送该 response</span></div><div class="line">    inflightResponses += (response.request.connectionId -&gt; response) <span class="comment">//note: 添加到 inflinght 中</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processCompletedReceives"><a href="#processCompletedReceives" class="headerlink" title="processCompletedReceives"></a>processCompletedReceives</h4><p><code>processCompletedReceives()</code> 方法的主要作用是处理接收到请求，并将其放入到 request queue 中，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理接收到的所有请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedReceives</span></span>() &#123;</div><div class="line">  selector.completedReceives.asScala.foreach &#123; receive =&gt;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> openChannel = selector.channel(receive.source)</div><div class="line">      <span class="keyword">val</span> session = &#123;</div><div class="line">        <span class="comment">// Only methods that are safe to call on a disconnected channel should be invoked on 'channel'.</span></div><div class="line">        <span class="keyword">val</span> channel = <span class="keyword">if</span> (openChannel != <span class="literal">null</span>) openChannel <span class="keyword">else</span> selector.closingChannel(receive.source)</div><div class="line">        <span class="type">RequestChannel</span>.<span class="type">Session</span>(<span class="keyword">new</span> <span class="type">KafkaPrincipal</span>(<span class="type">KafkaPrincipal</span>.<span class="type">USER_TYPE</span>, channel.principal.getName), channel.socketAddress)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> req = <span class="type">RequestChannel</span>.<span class="type">Request</span>(processor = id, connectionId = receive.source, session = session,</div><div class="line">        buffer = receive.payload, startTimeMs = time.milliseconds, listenerName = listenerName,</div><div class="line">        securityProtocol = securityProtocol)</div><div class="line">      requestChannel.sendRequest(req) <span class="comment">//note: 添加到请求队列，如果队列满了，将会阻塞</span></div><div class="line">      selector.mute(receive.source) <span class="comment">//note: 移除该连接的 OP_READ 监听</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e @ (_: <span class="type">InvalidRequestException</span> | _: <span class="type">SchemaException</span>) =&gt;</div><div class="line">        <span class="comment">// note that even though we got an exception, we can assume that receive.source is valid. Issues with constructing a valid receive object were handled earlier</span></div><div class="line">        error(<span class="string">s"Closing socket for <span class="subst">$&#123;receive.source&#125;</span> because of error"</span>, e)</div><div class="line">        close(selector, receive.source)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processCompletedSends"><a href="#processCompletedSends" class="headerlink" title="processCompletedSends"></a>processCompletedSends</h4><p><code>processCompletedSends()</code> 方法是处理已经完成的发送，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedSends</span></span>() &#123;</div><div class="line">  selector.completedSends.asScala.foreach &#123; send =&gt;</div><div class="line">    <span class="comment">//note: response 发送完成，从正在发送的集合中移除</span></div><div class="line">    <span class="keyword">val</span> resp = inflightResponses.remove(send.destination).getOrElse &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Send for <span class="subst">$&#123;send.destination&#125;</span> completed, but not in `inflightResponses`"</span>)</div><div class="line">    &#125;</div><div class="line">    resp.request.updateRequestMetrics()</div><div class="line">    selector.unmute(send.destination) <span class="comment">//note: 完成这个请求之后再次监听 OP_READ 事件</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="KafkaRequestHandlerPool"><a href="#KafkaRequestHandlerPool" class="headerlink" title="KafkaRequestHandlerPool"></a>KafkaRequestHandlerPool</h2><p>上面主要是讲述 SocketServer 中 Acceptor 与 Processor 的处理内容，也就是 1+N+M 模型中 1+N 部分，下面开始讲述 M 部分，也就是 KafkaRequestHandler 的内容，其初始化实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></div><div class="line">                              val requestChannel: <span class="type">RequestChannel</span>,</div><div class="line">                              val apis: <span class="type">KafkaApis</span>,</div><div class="line">                              time: <span class="type">Time</span>,</div><div class="line">                              numThreads: <span class="type">Int</span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> &#123;</div><div class="line"></div><div class="line">  <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">"RequestHandlerAvgIdlePercent"</span>, <span class="string">"percent"</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Request Handler on Broker "</span> + brokerId + <span class="string">"], "</span></div><div class="line">  <span class="keyword">val</span> threads = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Thread</span>](numThreads)</div><div class="line">  <span class="keyword">val</span> runnables = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</div><div class="line">  <span class="comment">//note: 建立 M 个（numThreads）KafkaRequestHandler</span></div><div class="line">  <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numThreads) &#123;</div><div class="line">    <span class="comment">//note: requestChannel 是 Processor 存放 request 请求的地方,也是 Handler 处理完请求存放 response 的地方</span></div><div class="line">    runnables(i) = <span class="keyword">new</span> <span class="type">KafkaRequestHandler</span>(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis, time)</div><div class="line">    threads(i) = <span class="type">Utils</span>.daemonThread(<span class="string">"kafka-request-handler-"</span> + i, runnables(i))</div><div class="line">    threads(i).start()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shutdown</span></span>() &#123;</div><div class="line">    info(<span class="string">"shutting down"</span>)</div><div class="line">    <span class="keyword">for</span>(handler &lt;- runnables)</div><div class="line">      handler.shutdown</div><div class="line">    <span class="keyword">for</span>(thread &lt;- threads)</div><div class="line">      thread.join</div><div class="line">    info(<span class="string">"shut down completely"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如上面实现所示：</p>
<ol>
<li>KafkaRequestHandlerPool 会初始化 M 个 KafkaRequestHandler 线程，并启动该线程；</li>
<li>在初始化 KafkaRequestHandler 时，传入一个 requestChannel 变量，这个是 Processor 存放 request 的地方，KafkaRequestHandler 在处理请求时，会从这个 queue 中取出相应的 request。</li>
</ol>
<h3 id="KafkaRequestHandler"><a href="#KafkaRequestHandler" class="headerlink" title="KafkaRequestHandler"></a>KafkaRequestHandler</h3><p>KafkaRequestHandler 线程的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> req : <span class="type">RequestChannel</span>.<span class="type">Request</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">while</span> (req == <span class="literal">null</span>) &#123;</div><div class="line">        <span class="comment">// We use a single meter for aggregate idle percentage for the thread pool.</span></div><div class="line">        <span class="comment">// Since meter is calculated as total_recorded_value / time_window and</span></div><div class="line">        <span class="comment">// time_window is independent of the number of threads, each recorded idle</span></div><div class="line">        <span class="comment">// time should be discounted by # threads.</span></div><div class="line">        <span class="keyword">val</span> startSelectTime = time.nanoseconds</div><div class="line">        req = requestChannel.receiveRequest(<span class="number">300</span>) <span class="comment">//note: 从 request queue 中拿去 request</span></div><div class="line">        <span class="keyword">val</span> idleTime = time.nanoseconds - startSelectTime</div><div class="line">        aggregateIdleMeter.mark(idleTime / totalHandlerThreads)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(req eq <span class="type">RequestChannel</span>.<span class="type">AllDone</span>) &#123;</div><div class="line">        debug(<span class="string">"Kafka request handler %d on broker %d received shut down command"</span>.format(</div><div class="line">          id, brokerId))</div><div class="line">        <span class="keyword">return</span></div><div class="line">      &#125;</div><div class="line">      req.requestDequeueTimeMs = time.milliseconds</div><div class="line">      trace(<span class="string">"Kafka request handler %d on broker %d handling request %s"</span>.format(id, brokerId, req))</div><div class="line">      apis.handle(req) <span class="comment">//note: 处理请求,并将处理的结果通过 sendResponse 放入 response queue 中</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Exception when handling request"</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法的实现逻辑：</p>
<ol>
<li>从 RequestChannel 取出相应的 request；</li>
<li>KafkaApis 处理这个 request，并通过 <code>requestChannel.sendResponse()</code> 将处理的结果放入 requestChannel 的 response queue 中，如下所示：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 response 添加到对应的队列中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</div><div class="line">  responseQueues(response.processor).put(response)</div><div class="line">  <span class="keyword">for</span>(onResponse &lt;- responseListeners)</div><div class="line">    onResponse(response.processor) <span class="comment">//note: 调用对应 processor 的 wakeup 方法</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里为止，一个请求从 Processor 接收，到 KafkaRequestHandler 通过 KafkaApis 处理并放回该 Processor 对应的 response queue 这整个过程就完成了（建议阅读本文的时候结合最前面的流程图一起看）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 LeaderAndIsr 请求的处理（二十二）</title>
    <link href="http://matt33.com/2018/06/25/leaderAndIsr-process/"/>
    <id>http://matt33.com/2018/06/25/leaderAndIsr-process/</id>
    <published>2018-06-25T01:01:12.000Z</published>
    <updated>2018-06-25T04:11:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka Server 是如何处理 LeaderAndIsr 请求的。</p>
<h2 id="LeaderAndIsr-请求"><a href="#LeaderAndIsr-请求" class="headerlink" title="LeaderAndIsr 请求"></a>LeaderAndIsr 请求</h2><p>LeaderAndIsr 请求是在一个 Topic Partition 的 leader、isr、assignment replicas 变动时，Controller 向 Broker 发送的一种请求，有时候是向这个 Topic Partition 的所有副本发送，有时候是其中的某个副本，跟具体的触发情况有关系。在一个 LeaderAndIsr 请求中，会封装多个 Topic Partition 的信息，每个 Topic Partition 会对应一个 PartitionState 对象，这个对象主要成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionState</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> controllerEpoch;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leader;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leaderEpoch;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> List&lt;Integer&gt; isr;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> zkVersion;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> Set&lt;Integer&gt; replicas;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>由此可见，在 LeaderAndIsr 请求中，会包含一个 Partition 的以下信息：</p>
<ol>
<li>当前 Controller 的 epoch（Broker 收到这个请求后，如果发现是过期的 Controller 请求，就会拒绝这个请求）；</li>
<li>leader，Partition 的 leader 信息；</li>
<li>leader epoch，Partition leader epoch 信息（leader、isr、AR 变动时，这个 epoch 都会加1）；</li>
<li>isr 列表；</li>
<li>zkVersion，；</li>
<li>AR，所有的 replica 列表。</li>
</ol>
<h3 id="LeaderAndIsr-请求处理"><a href="#LeaderAndIsr-请求处理" class="headerlink" title="LeaderAndIsr 请求处理"></a>LeaderAndIsr 请求处理</h3><h3 id="处理整体流程"><a href="#处理整体流程" class="headerlink" title="处理整体流程"></a>处理整体流程</h3><p>LeaderAndIsr 请求可谓是包含了一个 Partition 的所有 metadata 信息，Server 在接收到 Controller 发送的这个请求后，其处理的逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//KafkaApis</span></div><div class="line"><span class="comment">//note: LeaderAndIsr 请求的处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleLeaderAndIsrRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></div><div class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></div><div class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> leaderAndIsrRequest = request.body.asInstanceOf[<span class="type">LeaderAndIsrRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</div><div class="line">      <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></div><div class="line">      <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></div><div class="line">      <span class="comment">// leadership changes</span></div><div class="line">      <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></div><div class="line">      updatedLeaders.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">          coordinator.handleGroupImmigration(partition.partitionId)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></div><div class="line">      updatedFollowers.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">          coordinator.handleGroupEmigration(partition.partitionId)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> leaderAndIsrResponse =</div><div class="line">      <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;<span class="comment">//note: 有权限的情况下</span></div><div class="line">        <span class="comment">//note: replicaManager 进行相应的处理</span></div><div class="line">        <span class="keyword">val</span> result = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest, metadataCache, onLeadershipChange)</div><div class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(result.errorCode, result.responseMap.mapValues(<span class="keyword">new</span> <span class="type">JShort</span>(_)).asJava)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> result = leaderAndIsrRequest.partitionStates.asScala.keys.map((_, <span class="keyword">new</span> <span class="type">JShort</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code))).toMap</div><div class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code, result.asJava)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, leaderAndIsrResponse))</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">"Disk error during leadership change."</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述处理逻辑分为以下两步：</p>
<ol>
<li>ReplicaManager 调用 <code>becomeLeaderOrFollower()</code> 方法对这个请求进行相应的处理；</li>
<li>如果请求中包含 <code>__consumer_offset</code> 的 Partition（对应两种情况：之前是 fllower 现在变成了 leader、之前是 leader 现在变成了 follower），那么还需要调用这个方法中定义的 <code>onLeadershipChange()</code> 方法进行相应的处理。</li>
</ol>
<p><code>becomeLeaderOrFollower()</code>  的整体处理流程如下：</p>
<p><img src="/images/kafka/leader-and-isr.png" alt="LeaderAndIsr 请求的处理"></p>
<h3 id="becomeLeaderOrFollower"><a href="#becomeLeaderOrFollower" class="headerlink" title="becomeLeaderOrFollower"></a>becomeLeaderOrFollower</h3><p>这里先看下 ReplicaManager 的 <code>becomeLeaderOrFollower()</code> 方法，它是 LeaderAndIsr 请求处理的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 LeaderAndIsr 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">becomeLeaderOrFollower</span></span>(correlationId: <span class="type">Int</span>,leaderAndISRRequest: <span class="type">LeaderAndIsrRequest</span>,</div><div class="line">                           metadataCache: <span class="type">MetadataCache</span>,</div><div class="line">                           onLeadershipChange: (<span class="type">Iterable</span>[<span class="type">Partition</span>], <span class="type">Iterable</span>[<span class="type">Partition</span>]) =&gt; <span class="type">Unit</span>): <span class="type">BecomeLeaderOrFollowerResult</span> = &#123;</div><div class="line">  leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</div><div class="line">    stateChangeLogger.trace(<span class="string">"Broker %d received LeaderAndIsr request %s correlation id %d from controller %d epoch %d for partition [%s,%d]"</span></div><div class="line">                              .format(localBrokerId, stateInfo, correlationId,</div><div class="line">                                      leaderAndISRRequest.controllerId, leaderAndISRRequest.controllerEpoch, topicPartition.topic, topicPartition.partition))</div><div class="line">  &#125;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> responseMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]</div><div class="line">    <span class="comment">//note: 1. 验证 controller 的 epoch，如果是来自旧的 controller，就拒绝这个请求</span></div><div class="line">    <span class="keyword">if</span> (leaderAndISRRequest.controllerEpoch &lt; controllerEpoch) &#123;</div><div class="line">      stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d since "</span> +</div><div class="line">        <span class="string">"its controller epoch %d is old. Latest known controller epoch is %d"</span>).format(localBrokerId, leaderAndISRRequest.controllerId,</div><div class="line">        correlationId, leaderAndISRRequest.controllerEpoch, controllerEpoch))</div><div class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 controller 的请求</span></div><div class="line">      <span class="keyword">val</span> controllerId = leaderAndISRRequest.controllerId</div><div class="line">      controllerEpoch = leaderAndISRRequest.controllerEpoch</div><div class="line"></div><div class="line">      <span class="comment">// First check partition's leader epoch</span></div><div class="line">      <span class="comment">//note: 2. 检查 leader epoch，得到一个 partitionState map，epoch 满足条件并且有副本在本地的集合</span></div><div class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>]()</div><div class="line">      leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</div><div class="line">        <span class="keyword">val</span> partition = getOrCreatePartition(topicPartition) <span class="comment">//note: 对应的 tp 如果没有 Partition 实例的话,就新建一个</span></div><div class="line">        <span class="keyword">val</span> partitionLeaderEpoch = partition.getLeaderEpoch <span class="comment">//note: 更新 leader epoch</span></div><div class="line">        <span class="comment">// If the leader epoch is valid record the epoch of the controller that made the leadership decision.</span></div><div class="line">        <span class="comment">// This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path</span></div><div class="line">        <span class="keyword">if</span> (partitionLeaderEpoch &lt; stateInfo.leaderEpoch) &#123;</div><div class="line">          <span class="keyword">if</span>(stateInfo.replicas.contains(localBrokerId))</div><div class="line">            partitionState.put(partition, stateInfo)  <span class="comment">//note: 更新 replica 的 stateInfo</span></div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</div><div class="line">              <span class="string">"epoch %d for partition [%s,%d] as itself is not in assigned replica list %s"</span>)</div><div class="line">              .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</div><div class="line">                topicPartition.topic, topicPartition.partition, stateInfo.replicas.asScala.mkString(<span class="string">","</span>)))</div><div class="line">            responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code)</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;  <span class="comment">//note: 忽略这个请求，因为请求的 leader epoch 小于缓存的 epoch</span></div><div class="line">          <span class="comment">// Otherwise record the error code in response</span></div><div class="line">          stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</div><div class="line">            <span class="string">"epoch %d for partition [%s,%d] since its associated leader epoch %d is not higher than the current leader epoch %d"</span>)</div><div class="line">            .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</div><div class="line">              topicPartition.topic, topicPartition.partition, stateInfo.leaderEpoch, partitionLeaderEpoch))</div><div class="line">          responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 3. 过滤出本地副本设置为 leader 的 Partition 列表</span></div><div class="line">      <span class="keyword">val</span> partitionsTobeLeader = partitionState.filter &#123; <span class="keyword">case</span> (_, stateInfo) =&gt;</div><div class="line">        stateInfo.leader == localBrokerId</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 4. 过滤出本地副本设置为 follower 的 Partition 列表</span></div><div class="line">      <span class="keyword">val</span> partitionsToBeFollower = partitionState -- partitionsTobeLeader.keys <span class="comment">//note: 这些 tp 设置为了 follower</span></div><div class="line"></div><div class="line">      <span class="comment">//note: 5. 将为 leader 的副本设置为 leader</span></div><div class="line">      <span class="keyword">val</span> partitionsBecomeLeader = <span class="keyword">if</span> (partitionsTobeLeader.nonEmpty)</div><div class="line">        makeLeaders(controllerId, controllerEpoch, partitionsTobeLeader, correlationId, responseMap)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</div><div class="line"></div><div class="line">      <span class="comment">//note: 6. 将为 follower 的副本设置为 follower</span></div><div class="line">      <span class="keyword">val</span> partitionsBecomeFollower = <span class="keyword">if</span> (partitionsToBeFollower.nonEmpty)</div><div class="line">        makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap, metadataCache)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</div><div class="line"></div><div class="line">      <span class="comment">//note: 7. 如果 hw checkpoint 的线程没有初始化，这里需要进行一次初始化</span></div><div class="line">      <span class="comment">// we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions</span></div><div class="line">      <span class="comment">// have been completely populated before starting the checkpointing there by avoiding weird race conditions</span></div><div class="line">      <span class="keyword">if</span> (!hwThreadInitialized) &#123;</div><div class="line">        startHighWaterMarksCheckPointThread()</div><div class="line">        hwThreadInitialized = <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 8. 检查 replica fetcher 是否需要关闭（有些副本需要关闭因为可能从 follower 变为 leader）</span></div><div class="line">      replicaFetcherManager.shutdownIdleFetcherThreads()</div><div class="line"></div><div class="line">      <span class="comment">//note: 9. 检查是否 __consumer_offset 的 Partition 的 leaderAndIsr 信息，有的话进行相应的操作</span></div><div class="line">      onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)</div><div class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述实现，其处理逻辑总结如下：</p>
<ol>
<li>检查 Controller 的 epoch，如果是来自旧的 Controller，那么就拒绝这个请求；</li>
<li>获取请求的 Partition 列表的 PartitionState 信息，在遍历的过程中，会进行一个检查，如果 leader epoch 小于缓存中的 epoch 值，那么就过滤掉这个 Partition 信息，如果这个 Partition 在本地不存在，那么会初始化这个 Partition 的对象（这时候并不会初始化本地副本）；</li>
<li>获取出本地副本为 leader 的 Partition 列表（partitionsTobeLeader）；</li>
<li>获取出本地副本为 follower 的 Partition 列表（partitionsToBeFollower）；</li>
<li>调用 <code>makeLeaders()</code> 方法将 leader 的副本设置为 leader；</li>
<li>调用 <code>makeFollowers()</code> 方法将 leader 的副本设置为 follower；</li>
<li>检查 HW checkpoint 的线程是否初始化，如果没有，这里需要进行一次初始化；</li>
<li>检查 ReplicaFetcherManager 是否有线程需要关闭（如果这个线程上没有分配要拉取的 Topic Partition，那么在这里这个线程就会被关闭，下次需要时会再次启动）；</li>
<li>检查是否有 <code>__consumer_offset</code> Partition 的 leaderAndIsr 信息，有的话进行相应的操作。</li>
</ol>
<p>这其中，比较复杂的部分是第 5、6、9步，也前面图中标出的 1、2、4步，文章下面接着分析这三部分。</p>
<h3 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h3><p>ReplicaManager 的 <code>makeLeaders()</code> 的作用是将指定的这批 Partition 列表设置为 Leader，并返回是新 leader 对应的 Partition 列表（之前不是 leader，现在选举为了 leader），其实实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 1. 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 2. 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>实现逻辑如下：</p>
<ol>
<li>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 方法移除这些 Partition 的副本同步线程；</li>
<li>遍历这些 Partition，通过 Partition 的 <code>makeLeader()</code> 方法将这个 Partition 设置为 Leader，如果设置成功（如果 leader 没有变化，证明这个 Partition 之前就是 leader，这个方法返回的是 false，这种情况下不会更新到缓存中），那么将 leader 信息更新到缓存中。</li>
</ol>
<p>下面来看下在 Partition 中是如何真正初始化一个 Partition 的 leader？其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将本地副本设置为 leader, 如果 leader 不变,向 ReplicaManager 返回 false</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeLeader</span></span>(controllerId: <span class="type">Int</span>, partitionStateInfo: <span class="type">PartitionState</span>, correlationId: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="keyword">val</span> (leaderHWIncremented, isNewLeader) = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="keyword">val</span> allReplicas = partitionStateInfo.replicas.asScala.map(_.toInt)</div><div class="line">    <span class="comment">// record the epoch of the controller that made the leadership decision. This is useful while updating the isr</span></div><div class="line">    <span class="comment">// to maintain the decision maker controller's epoch in the zookeeper path</span></div><div class="line">    controllerEpoch = partitionStateInfo.controllerEpoch</div><div class="line">    <span class="comment">// add replicas that are new</span></div><div class="line">    <span class="comment">//note: 为了新的 replica 创建副本实例</span></div><div class="line">    allReplicas.foreach(replica =&gt; getOrCreateReplica(replica))</div><div class="line">    <span class="comment">//note: 获取新的 isr 列表</span></div><div class="line">    <span class="keyword">val</span> newInSyncReplicas = partitionStateInfo.isr.asScala.map(r =&gt; getOrCreateReplica(r)).toSet</div><div class="line">    <span class="comment">// remove assigned replicas that have been removed by the controller</span></div><div class="line">    <span class="comment">//note: 将已经在不在 AR 中的副本移除</span></div><div class="line">    (assignedReplicas.map(_.brokerId) -- allReplicas).foreach(removeReplica)</div><div class="line">    inSyncReplicas = newInSyncReplicas</div><div class="line">    leaderEpoch = partitionStateInfo.leaderEpoch</div><div class="line">    zkVersion = partitionStateInfo.zkVersion</div><div class="line">    <span class="comment">//note: 判断是否是新的 leader</span></div><div class="line">    <span class="keyword">val</span> isNewLeader =</div><div class="line">      <span class="keyword">if</span> (leaderReplicaIdOpt.isDefined &amp;&amp; leaderReplicaIdOpt.get == localBrokerId) &#123;<span class="comment">//note: leader 没有更新</span></div><div class="line">        <span class="literal">false</span></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        leaderReplicaIdOpt = <span class="type">Some</span>(localBrokerId)</div><div class="line">        <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">    <span class="keyword">val</span> leaderReplica = getReplica().get <span class="comment">//note: 获取在当前上的副本,也就是 leader replica</span></div><div class="line">    <span class="keyword">val</span> curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset <span class="comment">//note: 获取 leader replica 的 the end offset</span></div><div class="line">    <span class="keyword">val</span> curTimeMs = time.milliseconds</div><div class="line">    <span class="comment">// initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.</span></div><div class="line">    (assignedReplicas - leaderReplica).foreach &#123; replica =&gt; <span class="comment">//note: 对于 isr 中的 replica,更新 LastCaughtUpTime</span></div><div class="line">      <span class="keyword">val</span> lastCaughtUpTimeMs = <span class="keyword">if</span> (inSyncReplicas.contains(replica)) curTimeMs <span class="keyword">else</span> <span class="number">0</span>L</div><div class="line">      replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line">    <span class="keyword">if</span> (isNewLeader) &#123;  <span class="comment">//note: 如果是新的 leader,那么需要</span></div><div class="line">      <span class="comment">// construct the high watermark metadata for the new leader replica</span></div><div class="line">      <span class="comment">//note: 为新的 leader 构造 replica 的 HW metadata</span></div><div class="line">      leaderReplica.convertHWToLocalOffsetMetadata()</div><div class="line">      <span class="comment">// reset log end offset for remote replicas</span></div><div class="line">      <span class="comment">//note: 更新远程副本的副本同步信息（设置为 unKnown）</span></div><div class="line">      assignedReplicas.filter(_.brokerId != localBrokerId).foreach(_.updateLogReadResult(<span class="type">LogReadResult</span>.<span class="type">UnknownLogReadResult</span>))</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 如果满足更新 isr 的条件,就更新 HW 信息</span></div><div class="line">    (maybeIncrementLeaderHW(leaderReplica), isNewLeader)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented) <span class="comment">//note: HW 更新的情况下</span></div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">  isNewLeader</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单总结一下上述的实现：</p>
<ol>
<li>首先更新这个 Partition 的相应信息，包括：isr、AR、leader epoch、zkVersion 等，并为每个副本创建一个 Replica 对象（如果不存在该对象的情况下才会创建，只有本地副本才会初始化相应的日志对象）；</li>
<li>如果这个 Partition 的 leader 本来就是本地副本，那么返回的结果设置为 false，证明这个 leader 并不是新的 leader；</li>
<li>对于 isr 中的所有 Replica，更新 LastCaughtUpTime 值，即最近一次赶得上 leader 的时间；</li>
<li>如果是新的 leader，那么为 leader 初始化相应的 HighWatermarkMetadata 对象，并将所有副本的副本同步信息更新为 UnknownLogReadResult；</li>
<li>检查一下是否需要更新 HW 值。</li>
</ol>
<p>如果这个本地副本是新选举的 leader，那么它所做的事情就是初始化 Leader 应该记录的相关信息。</p>
<h3 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h3><p>ReplicaManager 的 <code>makeFollowers()</code> 方法，是将哪些 Partition 设置为 Follower，返回的结果是那些新的 follower 对应的 Partition 列表（之前是 leader，现在变成了 follower），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 1. 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的 Partition</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: 2. 将 Partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 3. 移除这些 Partition 的副本同步线程,这样在 MakeFollower 期间,这些 Partition 就不会进行副本同步了</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 4. Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 5. 完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 6. 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单总结一下上述的逻辑过程：</p>
<ol>
<li>首先遍历所有的 Partition，获到那些 leader 可用、并且 Partition 可以成功设置为 Follower 的 Partition 列表（partitionsToMakeFollower）；</li>
<li>在上面遍历的过程中，会调用 Partition 的 <code>makeFollower()</code> 方法将 Partition 设置为 Follower（在这里，如果该 Partition 的本地副本不存在，会初始化相应的日志对象，如果该 Partition 的 leader 已经存在，并且没有变化，那么就返回 false，只有 leader 变化的 Partition，才会返回 true，才会加入到 partitionsToMakeFollower 集合中，这是因为 leader 没有变化的 Partition 是不需要变更副本同步线程的）；</li>
<li>移除这些 Partition 的副本同步线程，这样在 MakeFollower 期间，这些 Partition 就不会进行副本同步了；</li>
<li>Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset，因为前面已经移除了这个 Partition 的副本同步线程，所以这里在 checkpoint 后可以保证所有缓存的数据都可以刷新到磁盘；</li>
<li>完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）；</li>
<li>启动相应的副本同步线程。</li>
</ol>
<p>到这里 LeaderAndIsr 请求的大部分处理已经完成，但是有一个比较特殊的 topic（<code>__consumer_offset</code>），如果这 Partition 的 leader 发生变化，是需要一些额外的处理。</p>
<h2 id="consumer-offset-leader-切换处理"><a href="#consumer-offset-leader-切换处理" class="headerlink" title="__consumer_offset leader 切换处理"></a><code>__consumer_offset</code> leader 切换处理</h2><p><code>__consumer_offset</code> 这个 Topic 如果发生了 leader 切换，GroupCoordinator 需要进行相应的处理，其处理过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</div><div class="line">  <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></div><div class="line">  <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></div><div class="line">  <span class="comment">// leadership changes</span></div><div class="line">  <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></div><div class="line">  updatedLeaders.foreach &#123; partition =&gt;</div><div class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">      coordinator.handleGroupImmigration(partition.partitionId)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></div><div class="line">  updatedFollowers.foreach &#123; partition =&gt;</div><div class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">      coordinator.handleGroupEmigration(partition.partitionId)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="成为-leader"><a href="#成为-leader" class="headerlink" title="成为 leader"></a>成为 leader</h3><p>如果当前节点这个 <code>__consumer_offset</code> 有 Partition 成为 leader，GroupCoordinator 通过 <code>handleGroupImmigration()</code> 方法进行相应的处理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 加载这个 Partition 对应的 group offset 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupImmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</div><div class="line">  groupManager.loadGroupsForPartition(offsetTopicPartitionId, onGroupLoaded)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 异步地加载这个 offset Partition 的信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doLoadGroupsAndOffsets</span></span>() &#123;</div><div class="line">    info(<span class="string">s"Loading offsets and group metadata from <span class="subst">$topicPartition</span>"</span>)</div><div class="line"></div><div class="line">    <span class="comment">//note: 添加到  loadingPartitions 集合中</span></div><div class="line">    inLock(partitionLock) &#123;</div><div class="line">      <span class="keyword">if</span> (loadingPartitions.contains(offsetsPartition)) &#123;</div><div class="line">        info(<span class="string">s"Offset load from <span class="subst">$topicPartition</span> already in progress."</span>)</div><div class="line">        <span class="keyword">return</span></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        loadingPartitions.add(offsetsPartition)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 开始加载，加载成功的话，将该 Partition 从 loadingPartitions 集合中移除，添加到 ownedPartition 集合中</span></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      loadGroupsAndOffsets(topicPartition, onGroupLoaded)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt; error(<span class="string">s"Error loading offsets from <span class="subst">$topicPartition</span>"</span>, t)</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      inLock(partitionLock) &#123;</div><div class="line">        ownedPartitions.add(offsetsPartition)</div><div class="line">        loadingPartitions.remove(offsetsPartition)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  scheduler.schedule(topicPartition.toString, doLoadGroupsAndOffsets)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的做的事情是：</p>
<ol>
<li>将正在处理的 Partition 添加到 loadingPartitions 集合中，这个集合内都是当前正在加载的 Partition（特指 <code>__consumer_offset</code> Topic）；</li>
<li>通过 <code>loadGroupsAndOffsets()</code> 加载这个 Partition 的数据，处理完成后，该 Partition 从 loadingPartitions 中清除，并添加到 ownedPartitions 集合中。</li>
</ol>
<p><code>loadGroupsAndOffsets()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 读取该 group offset Partition 数据</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="function"><span class="keyword">def</span> <span class="title">loadGroupsAndOffsets</span></span>(topicPartition: <span class="type">TopicPartition</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="comment">//note: 这个必然有本地副本，现获取 hw（如果本地是 leader 的情况，否则返回-1）</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">highWaterMark</span> </span>= replicaManager.getHighWatermark(topicPartition).getOrElse(<span class="number">-1</span>L)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds()</div><div class="line">  replicaManager.getLog(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      warn(<span class="string">s"Attempted to load offsets and group metadata from <span class="subst">$topicPartition</span>, but found no log"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</div><div class="line">      <span class="keyword">var</span> currOffset = log.logStartOffset <span class="comment">//note: 这副本最起始的 offset</span></div><div class="line">      <span class="keyword">val</span> buffer = <span class="type">ByteBuffer</span>.allocate(config.loadBufferSize) <span class="comment">//note: 默认5MB</span></div><div class="line">      <span class="comment">// loop breaks if leader changes at any time during the load, since getHighWatermark is -1</span></div><div class="line">      <span class="comment">//note: group 与 offset 的对应关系</span></div><div class="line">      <span class="keyword">val</span> loadedOffsets = mutable.<span class="type">Map</span>[<span class="type">GroupTopicPartition</span>, <span class="type">OffsetAndMetadata</span>]()</div><div class="line">      <span class="keyword">val</span> removedOffsets = mutable.<span class="type">Set</span>[<span class="type">GroupTopicPartition</span>]()</div><div class="line">      <span class="comment">//note: Group 对应的 meta 信息</span></div><div class="line">      <span class="keyword">val</span> loadedGroups = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">GroupMetadata</span>]()</div><div class="line">      <span class="keyword">val</span> removedGroups = mutable.<span class="type">Set</span>[<span class="type">String</span>]()</div><div class="line"></div><div class="line">      <span class="keyword">while</span> (currOffset &lt; highWaterMark &amp;&amp; !shuttingDown.get()) &#123; <span class="comment">//note: 直到读取到 hw 位置，或服务关闭</span></div><div class="line">        buffer.clear()</div><div class="line">        <span class="keyword">val</span> fileRecords = log.read(currOffset, config.loadBufferSize, maxOffset = <span class="type">None</span>, minOneMessage = <span class="literal">true</span>)</div><div class="line">          .records.asInstanceOf[<span class="type">FileRecords</span>]</div><div class="line">        <span class="keyword">val</span> bufferRead = fileRecords.readInto(buffer, <span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="type">MemoryRecords</span>.readableRecords(bufferRead).deepEntries.asScala.foreach &#123; entry =&gt;</div><div class="line">          <span class="keyword">val</span> record = entry.record</div><div class="line">          require(record.hasKey, <span class="string">"Group metadata/offset entry key should not be null"</span>)</div><div class="line"></div><div class="line">          <span class="type">GroupMetadataManager</span>.readMessageKey(record.key) <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> offsetKey: <span class="type">OffsetKey</span> =&gt; <span class="comment">//note: GroupTopicPartition，有 group 和 topic-partition</span></div><div class="line">              <span class="comment">// load offset</span></div><div class="line">              <span class="comment">//note: 加载 offset 信息</span></div><div class="line">              <span class="keyword">val</span> key = offsetKey.key</div><div class="line">              <span class="keyword">if</span> (record.hasNullValue) &#123; <span class="comment">//note: value 为空</span></div><div class="line">                loadedOffsets.remove(key)</div><div class="line">                removedOffsets.add(key)</div><div class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 有 commit offset 信息</span></div><div class="line">                <span class="keyword">val</span> value = <span class="type">GroupMetadataManager</span>.readOffsetMessageValue(record.value)</div><div class="line">                loadedOffsets.put(key, value)</div><div class="line">                removedOffsets.remove(key)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> groupMetadataKey: <span class="type">GroupMetadataKey</span> =&gt;</div><div class="line">              <span class="comment">// load group metadata</span></div><div class="line">              <span class="comment">//note: 加载 group metadata 信息</span></div><div class="line">              <span class="keyword">val</span> groupId = groupMetadataKey.key</div><div class="line">              <span class="keyword">val</span> groupMetadata = <span class="type">GroupMetadataManager</span>.readGroupMessageValue(groupId, record.value)</div><div class="line">              <span class="keyword">if</span> (groupMetadata != <span class="literal">null</span>) &#123;</div><div class="line">                trace(<span class="string">s"Loaded group metadata for group <span class="subst">$groupId</span> with generation <span class="subst">$&#123;groupMetadata.generationId&#125;</span>"</span>)</div><div class="line">                removedGroups.remove(groupId)</div><div class="line">                loadedGroups.put(groupId, groupMetadata)</div><div class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 更新最新的信息</span></div><div class="line">                loadedGroups.remove(groupId)</div><div class="line">                removedGroups.add(groupId)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> unknownKey =&gt;</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected message key <span class="subst">$unknownKey</span> while loading offsets and group metadata"</span>)</div><div class="line">          &#125;</div><div class="line"></div><div class="line">          currOffset = entry.nextOffset</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">val</span> (groupOffsets, emptyGroupOffsets) = loadedOffsets</div><div class="line">        .groupBy(_._1.group)</div><div class="line">        .mapValues(_.map &#123; <span class="keyword">case</span> (groupTopicPartition, offset) =&gt; (groupTopicPartition.topicPartition, offset)&#125; )</div><div class="line">        .partition &#123; <span class="keyword">case</span> (group, _) =&gt; loadedGroups.contains(group) &#125; <span class="comment">//note: 把集合根据条件分两个部分</span></div><div class="line"></div><div class="line">      loadedGroups.values.foreach &#123; group =&gt;</div><div class="line">        <span class="keyword">val</span> offsets = groupOffsets.getOrElse(group.groupId, <span class="type">Map</span>.empty[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>])</div><div class="line">        loadGroup(group, offsets) <span class="comment">//note: 在缓存中添加 group 和初始化 offset 信息</span></div><div class="line">        onGroupLoaded(group) <span class="comment">//note: 设置 group 下一次心跳超时时间</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// load groups which store offsets in kafka, but which have no active members and thus no group</span></div><div class="line">      <span class="comment">// metadata stored in the log</span></div><div class="line">      <span class="comment">//note: 加载哪些有 offset 信息但是当前没有活跃的 member 信息的 group</span></div><div class="line">      emptyGroupOffsets.foreach &#123; <span class="keyword">case</span> (groupId, offsets) =&gt;</div><div class="line">        <span class="keyword">val</span> group = <span class="keyword">new</span> <span class="type">GroupMetadata</span>(groupId)</div><div class="line">        loadGroup(group, offsets)</div><div class="line">        onGroupLoaded(group)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      removedGroups.foreach &#123; groupId =&gt;</div><div class="line">        <span class="comment">// if the cache already contains a group which should be removed, raise an error. Note that it</span></div><div class="line">        <span class="comment">// is possible (however unlikely) for a consumer group to be removed, and then to be used only for</span></div><div class="line">        <span class="comment">// offset storage (i.e. by "simple" consumers)</span></div><div class="line">        <span class="keyword">if</span> (groupMetadataCache.contains(groupId) &amp;&amp; !emptyGroupOffsets.contains(groupId))</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected unload of active group <span class="subst">$groupId</span> while "</span> +</div><div class="line">            <span class="string">s"loading partition <span class="subst">$topicPartition</span>"</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (!shuttingDown.get())</div><div class="line">        info(<span class="string">"Finished loading offsets from %s in %d milliseconds."</span></div><div class="line">          .format(topicPartition, time.milliseconds() - startMs))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面方法的实现虽然比较长，但是处理逻辑还是比较简单的，实现结果如下：</p>
<ol>
<li>获取这个 Partition 的 HW 值（如果 leader 不在本地，那么返回-1）；</li>
<li>初始化 loadedOffsets 和 removedOffsets、loadedGroups 和 removedGroups 集合，它们就是 group offset 信息以及 consumer member 信息；</li>
<li>从这个 Partition 第一条数据开始读取，直到读取到 HW 位置，加载相应的 commit offset、consumer member 信息，因为是顺序读取的，所以会新的值会覆盖前面的值；</li>
<li>通过 <code>loadGroup()</code> 加载到 GroupCoordinator 的缓存中。</li>
</ol>
<p>经过上面这些步骤，这个 Partition 的数据就被完整加载缓存中了。</p>
<h3 id="变成-follower"><a href="#变成-follower" class="headerlink" title="变成 follower"></a>变成 follower</h3><p>如果 <code>__consumer_offset</code> 有 Partition 变成了 follower（之前是 leader，如果之前不是 leader，不会走到这一步的），GroupCoordinator 通过 <code>handleGroupEmigration()</code> 移除这个 Partition 相应的缓存信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 移除这个 Partition 对应的 group offset 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupEmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</div><div class="line">  groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>removeGroupsForPartition()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当一个 broker 变成一个 follower 时，清空这个 partition 的相关缓存信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>,</div><div class="line">                             onGroupUnloaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</div><div class="line">  scheduler.schedule(topicPartition.toString, removeGroupsAndOffsets)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">removeGroupsAndOffsets</span></span>() &#123;</div><div class="line">    <span class="keyword">var</span> numOffsetsRemoved = <span class="number">0</span></div><div class="line">    <span class="keyword">var</span> numGroupsRemoved = <span class="number">0</span></div><div class="line"></div><div class="line">    inLock(partitionLock) &#123;</div><div class="line">      <span class="comment">// we need to guard the group removal in cache in the loading partition lock</span></div><div class="line">      <span class="comment">// to prevent coordinator's check-and-get-group race condition</span></div><div class="line">      ownedPartitions.remove(offsetsPartition)</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (group &lt;- groupMetadataCache.values) &#123;</div><div class="line">        <span class="keyword">if</span> (partitionFor(group.groupId) == offsetsPartition) &#123;</div><div class="line">          onGroupUnloaded(group) <span class="comment">//note: 将 group 状态转移成 dead</span></div><div class="line">          groupMetadataCache.remove(group.groupId, group) <span class="comment">//note: 清空 group 的信息</span></div><div class="line">          numGroupsRemoved += <span class="number">1</span></div><div class="line">          numOffsetsRemoved += group.numOffsets</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (numOffsetsRemoved &gt; <span class="number">0</span>)</div><div class="line">      info(<span class="string">s"Removed <span class="subst">$numOffsetsRemoved</span> cached offsets for <span class="subst">$topicPartition</span> on follower transition."</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (numGroupsRemoved &gt; <span class="number">0</span>)</div><div class="line">      info(<span class="string">s"Removed <span class="subst">$numGroupsRemoved</span> cached groups for <span class="subst">$topicPartition</span> on follower transition."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onGroupUnloaded</span></span>(group: <span class="type">GroupMetadata</span>) &#123;</div><div class="line">  group synchronized &#123;</div><div class="line">    info(<span class="string">s"Unloading group metadata for <span class="subst">$&#123;group.groupId&#125;</span> with generation <span class="subst">$&#123;group.generationId&#125;</span>"</span>)</div><div class="line">    <span class="keyword">val</span> previousState = group.currentState</div><div class="line">    group.transitionTo(<span class="type">Dead</span>) <span class="comment">//note: 状态转移成 dead</span></div><div class="line"></div><div class="line">    previousState <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Empty</span> | <span class="type">Dead</span> =&gt;</div><div class="line">      <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt;</div><div class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息返回异常</span></div><div class="line">          <span class="keyword">if</span> (member.awaitingJoinCallback != <span class="literal">null</span>) &#123;</div><div class="line">            member.awaitingJoinCallback(joinError(member.memberId, <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code))</div><div class="line">            member.awaitingJoinCallback = <span class="literal">null</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt;</div><div class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息，返回异常</span></div><div class="line">          <span class="keyword">if</span> (member.awaitingSyncCallback != <span class="literal">null</span>) &#123;</div><div class="line">            member.awaitingSyncCallback(<span class="type">Array</span>.empty[<span class="type">Byte</span>], <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">            member.awaitingSyncCallback = <span class="literal">null</span></div><div class="line">          &#125;</div><div class="line">          heartbeatPurgatory.checkAndComplete(<span class="type">MemberKey</span>(member.groupId, member.memberId))</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于在这个 Partition 上的所有 Group，会按下面的步骤执行：</p>
<ol>
<li>通过 <code>onGroupUnloaded()</code> 方法先将这个 Group 的状态转换为 dead，如果 Group 处在 PreparingRebalance/Stable/AwaitingSync 状态，并且设置了相应的回调函数，那么就在回调函数中返回带有 NOT_COORDINATOR_FOR_GROUP 异常信息的响应，consumer 在收到这个异常信息会重新加入 group；</li>
<li>从缓存中移除这个 Group 的信息。</li>
</ol>
<p>这个遍历执行完成之后，这个 Topic Partition 就从 Leader 变成了 follower 状态。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 发送模型（二十一）</title>
    <link href="http://matt33.com/2018/06/23/controller-request-model/"/>
    <id>http://matt33.com/2018/06/23/controller-request-model/</id>
    <published>2018-06-23T05:26:38.000Z</published>
    <updated>2018-06-23T05:42:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 部分的一个补充，在这篇文章中，将会看到 Controller 在处理 leader 切换、ShutDown 请求时如何向 Broker 发送相应的请求。</p>
<p>Kafka Controller 向 Broker 发送的请求类型主要分为三种：LeaderAndIsr、UpdateMetadata、StopReplica 请求，正如  <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 这里介绍的，Controller 会为每台 Broker 初始化为一个 ControllerBrokerStateInfo 对象，该对象主要包含以下四个内容：</p>
<ol>
<li>NetworkClient：与 Broker 的网络连接对象；</li>
<li>Node：Broker 的节点信息；</li>
<li>MessageQueue：每个 Broker 对应的请求队列，Controller 向 Broker 发送的请求会想放在这个队列里；</li>
<li>RequestSendThread：每台 Broker 对应的请求发送线程。</li>
</ol>
<h2 id="Controller-的请求发送模型"><a href="#Controller-的请求发送模型" class="headerlink" title="Controller 的请求发送模型"></a>Controller 的请求发送模型</h2><p>在讲述 Controller 发送模型之前，先看下 Controller 是如何向 Broker 发送请求的，这里以发送 metadata 更新请求为例，简略的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建新的批量请求</span></div><div class="line">brokerRequestBatch.newBatch()</div><div class="line"><span class="comment">//note: 为目标 Broker 添加相应的请求</span></div><div class="line">brokerRequestBatch.addUpdateMetadataRequestForBrokers(brokers, partitions)</div><div class="line"><span class="comment">//note: 发送请求，实际上只是把请求添加发送线程的 request queue 中</span></div><div class="line">brokerRequestBatch.sendRequestsToBrokers(epoch)</div></pre></td></tr></table></figure>
<p>这里有一个比较重要的对象，就是 ControllerBrokerRequestBatch 对象，可以认为它是一个专门用于批量请求发送的对象，在这个对象中有几个重要成员变量：</p>
<ol>
<li>leaderAndIsrRequestMap：记录每个 broker 与要发送的 LeaderAndIsr 请求集合的 map；</li>
<li>stopReplicaRequestMap：记录每个 broker 与要发送的 StopReplica 集合的 map；</li>
<li>updateMetadataRequestBrokerSet：记录要发送的 update-metadata 请求的 broker 集合；</li>
<li>updateMetadataRequestPartitionInfoMap：记录 update-metadata 请求要更新的 Topic Partition 集合。</li>
</ol>
<p>Controller 可以通过下面这三方法向这些集合添加相应的请求：</p>
<ol>
<li><code>addLeaderAndIsrRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 LeaderAndIsr 请求；</li>
<li><code>addStopReplicaRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 StopReplica 请求；</li>
<li><code>addUpdateMetadataRequestForBrokers()</code>：向给定的 Broker 发送某一批 Partitions 的 UpdateMetadata 请求。</li>
</ol>
<p>Controller 整体的请求模型概况如下图所示：</p>
<p><img src="/images/kafka/controller-request-model.png" alt="Controller 的请求发送模型"></p>
<p>上述三个方法将相应的请求添加到对应的集合中后，然后通过 <code>sendRequestsToBrokers()</code> 方法将该请求添加到该 Broker 对应的请求队列中，接着再由该 Broker 对应的 RequestSendThread 去发送相应的请求。</p>
<h2 id="ControllerBrokerRequestBatch"><a href="#ControllerBrokerRequestBatch" class="headerlink" title="ControllerBrokerRequestBatch"></a>ControllerBrokerRequestBatch</h2><p>这节详细讲述一下关于 ControllerBrokerRequestBatch 的一些方法实现。</p>
<h3 id="newBatch-方法"><a href="#newBatch-方法" class="headerlink" title="newBatch 方法"></a>newBatch 方法</h3><p>Controller 在添加请求前，都会先调用 <code>newBatch()</code> 方法，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建新的请求前,确保前一批请求全部发送完毕</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">newBatch</span></span>() &#123;</div><div class="line">  <span class="comment">// raise error if the previous batch is not empty</span></div><div class="line">  <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating "</span> +</div><div class="line">      <span class="string">"a new one. Some LeaderAndIsr state changes %s might be lost "</span>.format(leaderAndIsrRequestMap.toString()))</div><div class="line">  <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</div><div class="line">      <span class="string">"new one. Some StopReplica state changes %s might be lost "</span>.format(stopReplicaRequestMap.toString()))</div><div class="line">  <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</div><div class="line">      <span class="string">"new one. Some UpdateMetadata state changes to brokers %s with partition info %s might be lost "</span>.format(</div><div class="line">        updateMetadataRequestBrokerSet.toString(), updateMetadataRequestPartitionInfoMap.toString()))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的主要作用是检查上一波的 LeaderAndIsr、UpdateMetadata、StopReplica 请求是否已经发送，正常情况下，Controller 在调用 <code>sendRequestsToBrokers()</code> 方法之后，这些集合中的请求都会被发送，发送之后，会将相应的请求集合清空，当然在异常情况可能会导致部分集合没有被清空，导致无法 <code>newBatch()</code>，这种情况下，通常策略是重启 controller，因为现在 Controller 的设计还是有些复杂，在某些情况下还是可能会导致异常发生，并且有些异常还是无法恢复的。</p>
<h3 id="添加-LeaderAndIsr-请求"><a href="#添加-LeaderAndIsr-请求" class="headerlink" title="添加 LeaderAndIsr 请求"></a>添加 LeaderAndIsr 请求</h3><p>Controller 可以通过 <code>addLeaderAndIsrRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 LeaderAndIsr 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 LeaderAndIsr 添加到对应的 broker 中,还未开始发送数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addLeaderAndIsrRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</div><div class="line">                                     leaderIsrAndControllerEpoch: <span class="type">LeaderIsrAndControllerEpoch</span>,</div><div class="line">                                     replicas: <span class="type">Seq</span>[<span class="type">Int</span>], callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition)</div><div class="line"></div><div class="line">  <span class="comment">//note: 将请求添加到对应的 broker 上</span></div><div class="line">  brokerIds.filter(_ &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</div><div class="line">    <span class="keyword">val</span> result = leaderAndIsrRequestMap.getOrElseUpdate(brokerId, mutable.<span class="type">Map</span>.empty)</div><div class="line">    result.put(topicPartition, <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas.toSet))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 在更新 LeaderAndIsr 信息时,主题的 metadata 相当于也进行了更新,需要发送这个 topic 的 metadata 给所有存活的 broker</span></div><div class="line">  addUpdateMetadataRequestForBrokers(controllerContext.liveOrShuttingDownBrokerIds.toSeq,</div><div class="line">                                     <span class="type">Set</span>(<span class="type">TopicAndPartition</span>(topic, partition)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的处理流程如下：</p>
<ol>
<li>向对应的 Broker 添加 LeaderAndIsr 请求，请求会被添加到 leaderAndIsrRequestMap 集合中；</li>
<li>并通过 <code>addUpdateMetadataRequestForBrokers()</code> 方法向所有的 Broker 添加这个 Topic-Partition 的 UpdateMatedata 请求，leader 或 isr 变动时，会向所有 broker 同步这个 Partition 的 metadata 信息，这样可以保证每台 Broker 上都有最新的 metadata 信息。</li>
</ol>
<h3 id="添加-UpdateMetadata-请求"><a href="#添加-UpdateMetadata-请求" class="headerlink" title="添加 UpdateMetadata 请求"></a>添加 UpdateMetadata 请求</h3><p>Controller 可以通过 <code>addUpdateMetadataRequestForBrokers()</code> 向指定 Broker 列表添加某批 Partitions 的 UpdateMetadata 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向给行的 Broker 发送 UpdateMetadataRequest 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addUpdateMetadataRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>],</div><div class="line">                                       partitions: collection.<span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = <span class="type">Set</span>.empty[<span class="type">TopicAndPartition</span>],</div><div class="line">                                       callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  <span class="comment">//note: 将 Topic-Partition 添加到对应的 map 中</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateMetadataRequestPartitionInfo</span></span>(partition: <span class="type">TopicAndPartition</span>, beingDeleted: <span class="type">Boolean</span>) &#123;</div><div class="line">    <span class="keyword">val</span> leaderIsrAndControllerEpochOpt = controllerContext.partitionLeadershipInfo.get(partition)</div><div class="line">    leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">        <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(partition).toSet</div><div class="line">        <span class="keyword">val</span> partitionStateInfo = <span class="keyword">if</span> (beingDeleted) &#123; <span class="comment">//note: 正在删除的 Partition,设置 leader 为-2</span></div><div class="line">          <span class="keyword">val</span> leaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(<span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>, leaderIsrAndControllerEpoch.leaderAndIsr.isr)</div><div class="line">          <span class="type">PartitionStateInfo</span>(<span class="type">LeaderIsrAndControllerEpoch</span>(leaderAndIsr, leaderIsrAndControllerEpoch.controllerEpoch), replicas)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: 添加到对应的 request map 中</span></div><div class="line">        updateMetadataRequestPartitionInfoMap.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(partition.topic, partition.partition), partitionStateInfo)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        info(<span class="string">"Leader not yet assigned for partition %s. Skip sending UpdateMetadataRequest."</span>.format(partition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note:过滤出要发送的 partition</span></div><div class="line">  <span class="keyword">val</span> filteredPartitions = &#123;</div><div class="line">    <span class="keyword">val</span> givenPartitions = <span class="keyword">if</span> (partitions.isEmpty)</div><div class="line">      controllerContext.partitionLeadershipInfo.keySet <span class="comment">//note: Partitions 为空时，就过滤出所有的 topic</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      partitions</div><div class="line">    <span class="keyword">if</span> (controller.deleteTopicManager.partitionsToBeDeleted.isEmpty)</div><div class="line">      givenPartitions</div><div class="line">    <span class="keyword">else</span></div><div class="line">      givenPartitions -- controller.deleteTopicManager.partitionsToBeDeleted <span class="comment">//note: 将要删除的 topic 过滤掉</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  updateMetadataRequestBrokerSet ++= brokerIds.filter(_ &gt;= <span class="number">0</span>) <span class="comment">//note: 将 broker 列表更新到要发送的集合中</span></div><div class="line">  <span class="comment">//note: 对于要更新 metadata 的 Partition,设置 beingDeleted 为 False</span></div><div class="line">  filteredPartitions.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">false</span>))</div><div class="line">  <span class="comment">//note: 要删除的 Partition 设置 BeingDeleted 为 True</span></div><div class="line">  controller.deleteTopicManager.partitionsToBeDeleted.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">true</span>))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的实现逻辑如下：</p>
<ol>
<li>首先过滤出要发送的 Partition 列表，如果没有指定要发送 partitions 列表，那么默认就是发送全局的 metadata 信息；</li>
<li>接着将已经标记为删除的 Partition 从上面的列表中移除；</li>
<li>将要发送的 Broker 列表添加到 updateMetadataRequestBrokerSet 集合中；</li>
<li>将前面过滤的 Partition 列表对应的 metadata 信息添加到对应的 updateMetadataRequestPartitionInfoMap 集合中;</li>
<li>将当前设置为删除的所有 Partition 的 metadata 信息也添加到 updateMetadataRequestPartitionInfoMap 集合中，添加前会把其 leader 设置为-2，这样 Broker 收到这个 Partition 的 metadata 信息之后就会知道这个 Partition 是设置删除标志。</li>
</ol>
<h3 id="添加-StopReplica-请求"><a href="#添加-StopReplica-请求" class="headerlink" title="添加 StopReplica 请求"></a>添加 StopReplica 请求</h3><p>Controller 可以通过 <code>addStopReplicaRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 StopReplica 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 StopReplica 添加到对应的 Broker 中,还未开始发送数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addStopReplicaRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>, deletePartition: <span class="type">Boolean</span>,</div><div class="line">                                    callback: (<span class="type">AbstractResponse</span>, <span class="type">Int</span>) =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  brokerIds.filter(b =&gt; b &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</div><div class="line">    stopReplicaRequestMap.getOrElseUpdate(brokerId, <span class="type">Seq</span>.empty[<span class="type">StopReplicaRequestInfo</span>])</div><div class="line">    <span class="keyword">val</span> v = stopReplicaRequestMap(brokerId)</div><div class="line">    <span class="keyword">if</span>(callback != <span class="literal">null</span>)</div><div class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</div><div class="line">        deletePartition, (r: <span class="type">AbstractResponse</span>) =&gt; callback(r, brokerId))</div><div class="line">    <span class="keyword">else</span></div><div class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</div><div class="line">        deletePartition)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的实现逻辑比较简单，直接将 StopReplica 添加到 stopReplicaRequestMap 中。</p>
<h3 id="向-Broker-发送请求"><a href="#向-Broker-发送请求" class="headerlink" title="向 Broker 发送请求"></a>向 Broker 发送请求</h3><p>Controller 在添加完相应的请求后，最后一步都会去调用 <code>sendRequestsToBrokers()</code> 方法构造相应的请求，并把请求添加到 Broker 对应的 RequestQueue 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送请求给 broker（只是将对应处理后放入到对应的 queue 中）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequestsToBrokers</span></span>(controllerEpoch: <span class="type">Int</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">//note: LeaderAndIsr 请求</span></div><div class="line">    leaderAndIsrRequestMap.foreach &#123; <span class="keyword">case</span> (broker, partitionStateInfos) =&gt;</div><div class="line">      partitionStateInfos.foreach &#123; <span class="keyword">case</span> (topicPartition, state) =&gt;</div><div class="line">        <span class="keyword">val</span> typeOfRequest = <span class="keyword">if</span> (broker == state.leaderIsrAndControllerEpoch.leaderAndIsr.leader) <span class="string">"become-leader"</span> <span class="keyword">else</span> <span class="string">"become-follower"</span></div><div class="line">        stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending %s LeaderAndIsr request %s to broker %d "</span> +</div><div class="line">                                 <span class="string">"for partition [%s,%d]"</span>).format(controllerId, controllerEpoch, typeOfRequest,</div><div class="line">                                                                 state.leaderIsrAndControllerEpoch, broker,</div><div class="line">                                                                 topicPartition.topic, topicPartition.partition))</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: leader id 集合</span></div><div class="line">      <span class="keyword">val</span> leaderIds = partitionStateInfos.map(_._2.leaderIsrAndControllerEpoch.leaderAndIsr.leader).toSet</div><div class="line">      <span class="keyword">val</span> leaders = controllerContext.liveOrShuttingDownBrokers.filter(b =&gt; leaderIds.contains(b.id)).map &#123;</div><div class="line">        _.getNode(controller.config.interBrokerListenerName)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: requests.PartitionState</span></div><div class="line">      <span class="keyword">val</span> partitionStates = partitionStateInfos.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</div><div class="line">        <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</div><div class="line">        <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</div><div class="line">          leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</div><div class="line">          partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</div><div class="line">        topicPartition -&gt; partitionState</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 构造 LeaderAndIsr 请求,并添加到对应的 queue 中</span></div><div class="line">      <span class="keyword">val</span> leaderAndIsrRequest = <span class="keyword">new</span> <span class="type">LeaderAndIsrRequest</span>.</div><div class="line">          <span class="type">Builder</span>(controllerId, controllerEpoch, partitionStates.asJava, leaders.asJava)</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">LEADER_AND_ISR</span>, leaderAndIsrRequest, <span class="literal">null</span>)</div><div class="line">    &#125;</div><div class="line">    leaderAndIsrRequestMap.clear() <span class="comment">//note: 清空 leaderAndIsr 集合</span></div><div class="line"></div><div class="line">    <span class="comment">//note: update-metadata 请求</span></div><div class="line">    updateMetadataRequestPartitionInfoMap.foreach(p =&gt; stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending UpdateMetadata request %s "</span> +</div><div class="line">      <span class="string">"to brokers %s for partition %s"</span>).format(controllerId, controllerEpoch, p._2.leaderIsrAndControllerEpoch,</div><div class="line">      updateMetadataRequestBrokerSet.toString(), p._1)))</div><div class="line">    <span class="keyword">val</span> partitionStates = updateMetadataRequestPartitionInfoMap.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</div><div class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</div><div class="line">        leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</div><div class="line">        partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</div><div class="line">      topicPartition -&gt; partitionState</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> version: <span class="type">Short</span> =</div><div class="line">      <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_2_IV0</span>) <span class="number">3</span></div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_0_IV1</span>) <span class="number">2</span></div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_9_0</span>) <span class="number">1</span></div><div class="line">      <span class="keyword">else</span> <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment">//note: 构造 update-metadata 请求</span></div><div class="line">    <span class="keyword">val</span> updateMetadataRequest = &#123;</div><div class="line">      <span class="keyword">val</span> liveBrokers = <span class="keyword">if</span> (version == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">// Version 0 of UpdateMetadataRequest only supports PLAINTEXT.</span></div><div class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</div><div class="line">          <span class="keyword">val</span> securityProtocol = <span class="type">SecurityProtocol</span>.<span class="type">PLAINTEXT</span></div><div class="line">          <span class="keyword">val</span> listenerName = <span class="type">ListenerName</span>.forSecurityProtocol(securityProtocol)</div><div class="line">          <span class="keyword">val</span> node = broker.getNode(listenerName)</div><div class="line">          <span class="keyword">val</span> endPoints = <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">EndPoint</span>(node.host, node.port, securityProtocol, listenerName))</div><div class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</div><div class="line">          <span class="keyword">val</span> endPoints = broker.endPoints.map &#123; endPoint =&gt;</div><div class="line">            <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">EndPoint</span>(endPoint.host, endPoint.port, endPoint.securityProtocol, endPoint.listenerName)</div><div class="line">          &#125;</div><div class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Builder</span>(</div><div class="line">        controllerId, controllerEpoch, partitionStates.asJava, liveBrokers.asJava).</div><div class="line">        setVersion(version)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 将请求添加到对应的 queue</span></div><div class="line">    updateMetadataRequestBrokerSet.foreach &#123; broker =&gt;</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">UPDATE_METADATA_KEY</span>, updateMetadataRequest, <span class="literal">null</span>)</div><div class="line">    &#125;</div><div class="line">    updateMetadataRequestBrokerSet.clear() <span class="comment">//note: 清空对应的请求记录</span></div><div class="line">    updateMetadataRequestPartitionInfoMap.clear()</div><div class="line"></div><div class="line">    <span class="comment">//note: StopReplica 请求的处理</span></div><div class="line">    stopReplicaRequestMap.foreach &#123; <span class="keyword">case</span> (broker, replicaInfoList) =&gt;</div><div class="line">      <span class="keyword">val</span> stopReplicaWithDelete = replicaInfoList.filter(_.deletePartition).map(_.replica).toSet</div><div class="line">      <span class="keyword">val</span> stopReplicaWithoutDelete = replicaInfoList.filterNot(_.deletePartition).map(_.replica).toSet</div><div class="line">      debug(<span class="string">"The stop replica request (delete = true) sent to broker %d is %s"</span></div><div class="line">        .format(broker, stopReplicaWithDelete.mkString(<span class="string">","</span>)))</div><div class="line">      debug(<span class="string">"The stop replica request (delete = false) sent to broker %d is %s"</span></div><div class="line">        .format(broker, stopReplicaWithoutDelete.mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">      <span class="keyword">val</span> (replicasToGroup, replicasToNotGroup) = replicaInfoList.partition(r =&gt; !r.deletePartition &amp;&amp; r.callback == <span class="literal">null</span>)</div><div class="line"></div><div class="line">      <span class="comment">// Send one StopReplicaRequest for all partitions that require neither delete nor callback. This potentially</span></div><div class="line">      <span class="comment">// changes the order in which the requests are sent for the same partitions, but that's OK.</span></div><div class="line">      <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(controllerId, controllerEpoch, <span class="literal">false</span>,</div><div class="line">        replicasToGroup.map(r =&gt; <span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).toSet.asJava)</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest)</div><div class="line"></div><div class="line">      replicasToNotGroup.foreach &#123; r =&gt;</div><div class="line">        <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(</div><div class="line">            controllerId, controllerEpoch, r.deletePartition,</div><div class="line">            <span class="type">Set</span>(<span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).asJava)</div><div class="line">        controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest, r.callback)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    stopReplicaRequestMap.clear()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty) &#123;</div><div class="line">        error(<span class="string">"Haven't been able to send leader and isr requests, current state of "</span> +</div><div class="line">            <span class="string">s"the map is <span class="subst">$leaderAndIsrRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty) &#123;</div><div class="line">        error(<span class="string">s"Haven't been able to send metadata update requests to brokers <span class="subst">$updateMetadataRequestBrokerSet</span>, "</span> +</div><div class="line">              <span class="string">s"current state of the partition info is <span class="subst">$updateMetadataRequestPartitionInfoMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty) &#123;</div><div class="line">        error(<span class="string">"Haven't been able to send stop replica requests, current state of "</span> +</div><div class="line">            <span class="string">s"the map is <span class="subst">$stopReplicaRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面这个方法看着很复杂，其实做的事情很明确，就是将三个集合中的请求发送对应 Broker 的请求队列中，这里简单作一个总结：</p>
<ol>
<li>从 leaderAndIsrRequestMap 集合中构造相应的 LeaderAndIsr 请求，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 leaderAndIsrRequestMap 集合；</li>
<li>从 updateMetadataRequestPartitionInfoMap 集合中构造相应的 UpdateMetadata 请求，，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 updateMetadataRequestBrokerSet 和 updateMetadataRequestPartitionInfoMap 集合；</li>
<li>从 stopReplicaRequestMap 集合中构造相应的 StopReplica 请求，在构造时会根据是否设置删除标志将要涉及的 Partition 分成两类，构造对应的请求，对于要删除数据的 StopReplica 会设置相应的回调函数，然后通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 stopReplicaRequestMap 集合。</li>
</ol>
<p>走到这一步，Controller 要发送的请求算是都添加到对应 Broker 的 MessageQueue 中，后台的 RequestSendThread 线程会从这个请求队列中遍历相应的请求，发送给对应的 Broker。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 &lt;a href=&quot;http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager&quot;&gt;Contro
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Topic 的新建/扩容/删除（二十）</title>
    <link href="http://matt33.com/2018/06/18/topic-create-alter-delete/"/>
    <id>http://matt33.com/2018/06/18/topic-create-alter-delete/</id>
    <published>2018-06-18T09:24:21.000Z</published>
    <updated>2018-06-18T09:30:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 <a href="http://matt33.com/2017/07/21/kafka-topic-create/">Kafka 源码解析之 topic 创建过程（三）</a> 中已经讲述过了，本篇与前面不同的是，本篇主要是从 Controller 角度来讲述，而且是把新建、扩容、删除这三个 Topic 级别的操作放在一起做一个总结。</p>
<h2 id="Topic-新建与扩容"><a href="#Topic-新建与扩容" class="headerlink" title="Topic 新建与扩容"></a>Topic 新建与扩容</h2><p>这里把 Topic 新建与扩容放在一起讲解，主要是因为无论 Topic 是新建还是扩容，在 Kafka 内部其实都是 Partition 的新建，底层的实现机制是一样的，Topic 的新建与扩容的整体流程如下图所示：</p>
<p><img src="/images/kafka/topic-create-alter.png" alt="Topic 新建与扩容流程"></p>
<p>Topic 新建与扩容触发条件的不同如下所示：</p>
<ol>
<li>对于 Topic 扩容，监控的节点是 <code>/brokers/topics/TOPIC_NAME</code>，监控的是具体的 Topic 节点，通过 PartitionStateMachine 的 <code>registerPartitionChangeListener(topic)</code> 方法注册的相应 listener；</li>
<li>对于 Topic 新建，监控的节点是 <code>/brokers/topics</code>，监控的是 Topic 列表，通过 PartitionStateMachine 的 <code>registerTopicChangeListener()</code> 方法注册的相应 listener。</li>
</ol>
<p>下面开始详细讲述这两种情况。</p>
<h3 id="Topic-扩容"><a href="#Topic-扩容" class="headerlink" title="Topic 扩容"></a>Topic 扩容</h3><p>Kafka 提供了 Topic 扩容工具，假设一个 Topic（topic_test）只有一个 partition，这时候我们想把它扩容到两个 Partition，可以通过下面两个命令来实现：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --partitions 2</div><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --replica-assignment 1:2,2:1 --partitions 2</div></pre></td></tr></table></figure>
<p>这两种方法的区别是：第二种方法直接指定了要扩容的 Partition 2 的副本需要分配到哪台机器上，这样的话我们可以精确控制到哪些 Topic 放下哪些机器上。</p>
<p>无论是使用哪种方案，上面两条命令产生的结果只有一个，将 Topic 各个 Partition 的副本写入到 ZK 对应的节点上，这样的话 <code>/brokers/topics/topic_test</code> 节点的内容就会发生变化，PartitionModificationsListener 监听器就会被触发，该监听器的处理流程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Partition change 监听器,主要是用于 Partition 扩容的监听</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionModificationsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"AddPartitionsListener"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        info(<span class="string">s"Partition modification triggered <span class="subst">$data</span> for path <span class="subst">$dataPath</span>"</span>)</div><div class="line">        <span class="keyword">val</span> partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(<span class="type">List</span>(topic))</div><div class="line">        <span class="comment">//note: 获取新增的 partition 列表及其对应的分配副本列表</span></div><div class="line">        <span class="keyword">val</span> partitionsToBeAdded = partitionReplicaAssignment.filter(p =&gt;</div><div class="line">          !controllerContext.partitionReplicaAssignment.contains(p._1))</div><div class="line">        <span class="comment">//note: 如果该 topic 被标记为删除,那么直接跳过,不再处理,否则创建该 Partition</span></div><div class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(topic))</div><div class="line">          error(<span class="string">"Skipping adding partitions %s for topic %s since it is currently being deleted"</span></div><div class="line">                .format(partitionsToBeAdded.map(_._1.partition).mkString(<span class="string">","</span>), topic))</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">if</span> (partitionsToBeAdded.nonEmpty) &#123;</div><div class="line">            info(<span class="string">"New partitions to be added %s"</span>.format(partitionsToBeAdded))</div><div class="line">            controllerContext.partitionReplicaAssignment.++=(partitionsToBeAdded)</div><div class="line">            controller.onNewPartitionCreation(partitionsToBeAdded.keySet)<span class="comment">//note: 创建新的 partition</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling add partitions for data path "</span> + dataPath, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// this is not implemented for partition change</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(parentPath: <span class="type">String</span>): <span class="type">Unit</span> = &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其 <code>doHandleDataChange()</code> 方法的处理流程如下：</p>
<ol>
<li>首先获取该 Topic 在 ZK 的 Partition 副本列表，跟本地的缓存做对比，获取新增的 Partition 列表；</li>
<li>检查这个 Topic 是否被标记为删除，如果被标记了，那么直接跳过，不再处理这个 Partition 扩容的请求；</li>
<li>调用 KafkaController 的 <code>onNewPartitionCreation()</code> 新建该 Partition。</li>
</ol>
<p>下面我们看下 <code>onNewPartitionCreation()</code> 方法，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 用于 Topic Partition 的新建</span></div><div class="line"><span class="comment">//note: 1. 将新创建的 partition 状态置为 NewPartition 状态;</span></div><div class="line"><span class="comment">//note: 2. 将新创建的 Replica 状态置为 NewReplica 状态;</span></div><div class="line"><span class="comment">//note: 3. 将该 Partition 从 NewPartition 改为 OnlinePartition 状态,这期间会 为该 Partition 选举 leader 和 isr，更新到 zk 和 controller的缓存中</span></div><div class="line"><span class="comment">//note: 4. 将副本状态从 NewReplica 改为 OnlineReplica 状态。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewPartitionCreation</span></span>(newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"New partition creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">NewPartition</span>)</div><div class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">NewReplica</span>)</div><div class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">OnlinePartition</span>, offlinePartitionSelector)</div><div class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">OnlineReplica</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Partition 的新建，总共分了以下四步：</p>
<ol>
<li>将新创建的 Partition 状态置为 NewPartition 状态，此时 Partition 刚刚创建，只是分配了相应的 Replica 但是还没有 leader 和 isr，不能正常工作;</li>
<li>将该 Partition 对应的 Replica 列表状态设置为 NewReplica 状态，这部分只是将 Replica 的状态设置为了 NewReplica，并没有做其他的处理;</li>
<li>将该 Partition 的状态从 NewPartition 改为 OnlinePartition 状态，这期间会为该 Partition 选举 leader 和 isr，并将结果更新到 ZK 和 Controller 的缓存中，并向该 Partition 的所有副本发送对应的 LeaderAndIsr 信息（发送 LeaderAndIsr 请求的同时也会向所有 Broker 发送该 Topic 的 leader、isr metadata 信息）；</li>
<li>将副本状态从 NewReplica 转移为 OnlineReplica 状态。</li>
</ol>
<p>经过上面几个阶段，一个 Partition 算是真正创建出来，可以正常进行读写工作了，当然上面只是讲述了 Controller 端做的内容，Partition 副本所在节点对 LeaderAndIsr 请求会做更多的工作，这部分会在后面关于 LeaderAndIsr 请求的处理中只能够详细讲述。</p>
<h3 id="Topic-新建"><a href="#Topic-新建" class="headerlink" title="Topic 新建"></a>Topic 新建</h3><p>Kafka 也提供了 Topic 创建的工具，假设我们要创建一个名叫 topic_test，Partition 数为2的 Topic，创建的命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --partitions 2 --replication-factor 2</div><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --replica-assignment 1:2,2:1 --partitions 2</div></pre></td></tr></table></figure>
<p>跟前面的类似，方法二是可以精确控制新建 Topic 每个 Partition 副本所在位置，Topic 创建的本质上是在 <code>/brokers/topics</code> 下新建一个节点信息，并将 Topic 的分区详情写入进去，当 <code>/brokers/topics</code> 有了新增的 Topic 节点后，会触发 TopicChangeListener 监听器，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 监控 zk 上 Topic 子节点的变化 ,KafkaController 会进行相应的处理</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"TopicChangeListener"</span></div><div class="line"></div><div class="line">  <span class="comment">//note: 当 zk 上 topic 节点上有变更时,这个方法就会调用</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> currentChildren = &#123;</div><div class="line">            debug(<span class="string">"Topic change listener fired for path %s with children %s"</span>.format(parentPath, children.mkString(<span class="string">","</span>)))</div><div class="line">            children.toSet</div><div class="line">          &#125;</div><div class="line">          <span class="comment">//note: 新创建的 topic 列表</span></div><div class="line">          <span class="keyword">val</span> newTopics = currentChildren -- controllerContext.allTopics</div><div class="line">          <span class="comment">//note: 已经删除的 topic 列表</span></div><div class="line">          <span class="keyword">val</span> deletedTopics = controllerContext.allTopics -- currentChildren</div><div class="line">          controllerContext.allTopics = currentChildren</div><div class="line"></div><div class="line">          <span class="comment">//note: 新创建 topic 对应的 partition 列表及副本列表添加到 Controller 的缓存中</span></div><div class="line">          <span class="keyword">val</span> addedPartitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(newTopics.toSeq)</div><div class="line">          <span class="comment">//note: Controller 从缓存中把已经删除 partition 过滤掉</span></div><div class="line">          controllerContext.partitionReplicaAssignment = controllerContext.partitionReplicaAssignment.filter(p =&gt;</div><div class="line">            !deletedTopics.contains(p._1.topic))</div><div class="line">          controllerContext.partitionReplicaAssignment.++=(addedPartitionReplicaAssignment)<span class="comment">//note: 将新增的 tp-replicas 更新到缓存中</span></div><div class="line">          info(<span class="string">"New topics: [%s], deleted topics: [%s], new partition replica assignment [%s]"</span>.format(newTopics,</div><div class="line">            deletedTopics, addedPartitionReplicaAssignment))</div><div class="line">          <span class="keyword">if</span> (newTopics.nonEmpty)<span class="comment">//note: 处理新建的 topic</span></div><div class="line">            controller.onNewTopicCreation(newTopics, addedPartitionReplicaAssignment.keySet)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling new topic"</span>, e)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>只要 <code>/brokers/topics</code> 下子节点信息有变化（topic 新增或者删除），TopicChangeListener 都会被触发，其 <code>doHandleChildChange()</code> 方法的处理流程如下：</p>
<ol>
<li>获取 ZK 当前的所有 Topic 列表，根据本地缓存的 Topic 列表记录，可以得到新增的 Topic 记录与已经删除的 Topic 列表；</li>
<li>将新增 Topic 的相信信息更新到 Controller 的缓存中，将已经删除的 Topic 从 Controller 的副本缓存中移除；</li>
<li>调用 KafkaController 的 <code>onNewTopicCreation()</code> 方法创建该 topic。</li>
</ol>
<p>接着看下 <code>onNewTopicCreation()</code> 方法实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 partition state machine 监控到有新 topic 或 partition 时,这个方法将会被调用</span></div><div class="line"><span class="comment">//note: 1. 注册 partition change listener, 监听 Parition 变化;</span></div><div class="line"><span class="comment">//note: 2. 触发 the new partition, 也即是 onNewPartitionCreation()</span></div><div class="line"><span class="comment">//note: 3. 发送 metadata 请求给所有的 Broker</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewTopicCreation</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>], newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"New topic creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">// subscribe to partition changes</span></div><div class="line">  topics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">  onNewPartitionCreation(newPartitions)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法主要做了两件事：</p>
<ol>
<li>注册这个 topic 的 PartitionModificationsListener 监听器；</li>
<li>通过 <code>onNewPartitionCreation()</code> 创建该 Topic 的所有 Partition。</li>
</ol>
<p><code>onNewPartitionCreation()</code> 的实现在前面 Topic 扩容部分已经讲述过，这里不再重复，最好参考前面流程图来梳理 Topic 扩容和新建的整个过程。</p>
<h2 id="Topic-删除"><a href="#Topic-删除" class="headerlink" title="Topic 删除"></a>Topic 删除</h2><p>Kafka Topic 删除这部分的逻辑是一个单独线程去做的，这个线程是在 Controller 启动时初始化和启动的。</p>
<h3 id="TopicDeletionManager-初始化"><a href="#TopicDeletionManager-初始化" class="headerlink" title="TopicDeletionManager 初始化"></a>TopicDeletionManager 初始化</h3><p>TopicDeletionManager 启动实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Invoked at the end of new controller initiation</div><div class="line"> */</div><div class="line"><span class="comment">//note: Controller 初始化完成,触发这个操作,删除 topic 线程启动</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span> (isDeleteTopicEnabled) &#123;</div><div class="line">    deleteTopicsThread = <span class="keyword">new</span> <span class="type">DeleteTopicsThread</span>()</div><div class="line">    <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty)</div><div class="line">      deleteTopicStateChanged.set(<span class="literal">true</span>)</div><div class="line">    deleteTopicsThread.start() <span class="comment">//note: 启动 DeleteTopicsThread</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>TopicDeletionManager 启动时只是初始化了一个 DeleteTopicsThread 线程，并启动该线程。TopicDeletionManager 这个类从名字上去看，它是 Topic 删除的管理器，它是如何实现 Topic 删除管理呢，这里先看下该类的几个重要的成员变量：</p>
<ol>
<li>topicsToBeDeleted：需要删除的 Topic 列表，每当有新的 topic 需要删除时，Controller 就通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到这个列表中，而 DeleteTopicsThread 线程则会从列表拿到需要进行删除的 Topic 信息；</li>
<li>partitionsToBeDeleted：需要删除的 Partition 列表，跟上面的 Topic 列表保持一致，只不过纬度不同；</li>
<li>topicsIneligibleForDeletion：非法删除的 Topic 列表，当一个 Topic 正在进行副本迁移、leader 选举或者有副本 dead 的情况下，该 Topic 都会设置被非法删除状态，只有恢复正常后，这个状态才会解除，处在这个状态的 Topic 是无法删除的。</li>
</ol>
<h3 id="Topic-删除整体流程"><a href="#Topic-删除整体流程" class="headerlink" title="Topic 删除整体流程"></a>Topic 删除整体流程</h3><p>前面一小节，简单介绍了 TopicDeletionManager、DeleteTopicsThread 的启动以及它们之间的关系，这里我们看下一个 Topic 被设置删除后，其处理的整理流程，简单做了一个小图，如下所示：</p>
<p><img src="/images/kafka/topic-delete.png" alt="Topic 删除整理流程"></p>
<p>这里先简单讲述上面的流程，当一个 Topic 设置为删除后：</p>
<ol>
<li>首先 DeleteTopicsListener 会被触发，然后通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表中；</li>
<li>DeleteTopicsThread 这个线程会不断调用 <code>doWork()</code> 方法，这个方法被调用时，它会遍历 <code>topicsToBeDeleted</code> 中的所有 Topic 列表；</li>
<li>对于之前没有处理过的 Topic（之前还没有开始删除），会通过 TopicDeletionManager 的 <code>onTopicDeletion()</code> 方法执行删除操作；</li>
<li>如果 Topic 删除完成（所有 Replica 的状态都变为 ReplicaDeletionSuccessful 状态），那么就执行 TopicDeletionManager 的 <code>completeDeleteTopic()</code> 完成删除流程，即更新状态信息，并将 Topic 的 meta 信息从缓存和 ZK 中清除。</li>
</ol>
<h3 id="Topic-删除详细实现"><a href="#Topic-删除详细实现" class="headerlink" title="Topic 删除详细实现"></a>Topic 删除详细实现</h3><p>先看下 DeleteTopicsListener 的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除 Topic 包括以下操作:</span></div><div class="line"><span class="comment">//note: 1. 如果要删除的 topic 存在,将 Topic 添加到 Topic 将要删除的缓存中;</span></div><div class="line"><span class="comment">//note: 2. 如果有 Topic 将要被删除,那么将触发 Topic 删除线程</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"DeleteTopicsListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when a topic is being deleted</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当 topic 需要被删除时,才会触发</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">var</span> topicsToBeDeleted = children.toSet</div><div class="line">      debug(<span class="string">"Delete topics listener fired for topics %s to be deleted"</span>.format(topicsToBeDeleted.mkString(<span class="string">","</span>)))</div><div class="line">      <span class="comment">//note: 不存在的、需要删除的 topic, 直接清除 zk 上的记录</span></div><div class="line">      <span class="keyword">val</span> nonExistentTopics = topicsToBeDeleted -- controllerContext.allTopics</div><div class="line">      <span class="keyword">if</span> (nonExistentTopics.nonEmpty) &#123;</div><div class="line">        warn(<span class="string">"Ignoring request to delete non-existing topics "</span> + nonExistentTopics.mkString(<span class="string">","</span>))</div><div class="line">        nonExistentTopics.foreach(topic =&gt; zkUtils.deletePathRecursive(getDeleteTopicPath(topic)))</div><div class="line">      &#125;</div><div class="line">      topicsToBeDeleted --= nonExistentTopics</div><div class="line">      <span class="keyword">if</span> (controller.config.deleteTopicEnable) &#123; <span class="comment">//note: 如果允许 topic 删除</span></div><div class="line">        <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 有 Topic 需要删除</span></div><div class="line">          info(<span class="string">"Starting topic deletion for topics "</span> + topicsToBeDeleted.mkString(<span class="string">","</span>))</div><div class="line">          <span class="comment">// mark topic ineligible for deletion if other state changes are in progress</span></div><div class="line">          topicsToBeDeleted.foreach &#123; topic =&gt; <span class="comment">//note: 如果 topic 正在最优 leader 选举或正在迁移,那么将 topic 标记为非法删除状态</span></div><div class="line">            <span class="keyword">val</span> preferredReplicaElectionInProgress =</div><div class="line">              controllerContext.partitionsUndergoingPreferredReplicaElection.map(_.topic).contains(topic)</div><div class="line">            <span class="keyword">val</span> partitionReassignmentInProgress =</div><div class="line">              controllerContext.partitionsBeingReassigned.keySet.map(_.topic).contains(topic)</div><div class="line">            <span class="keyword">if</span> (preferredReplicaElectionInProgress || partitionReassignmentInProgress)</div><div class="line">              controller.deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">          &#125;</div><div class="line">          <span class="comment">// add topic to deletion list</span></div><div class="line">          <span class="comment">//note: 将要删除的 topic 添加到待删除的 topic</span></div><div class="line">          controller.deleteTopicManager.enqueueTopicsForDeletion(topicsToBeDeleted)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// If delete topic is disabled remove entries under zookeeper path : /admin/delete_topics</span></div><div class="line">        <span class="keyword">for</span> (topic &lt;- topicsToBeDeleted) &#123;</div><div class="line">          info(<span class="string">"Removing "</span> + getDeleteTopicPath(topic) + <span class="string">" since delete topic is disabled"</span>)</div><div class="line">          zkUtils.zkClient.delete(getDeleteTopicPath(topic))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其 <code>doHandleChildChange()</code> 的实现逻辑如下：</p>
<ol>
<li>根据要删除的 Topic 列表，过滤出那些不存在的 Topic 列表，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）；</li>
<li>如果集群不允许 Topic 删除，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）这些 Topic 列表，结束流程；</li>
<li>如果这个列表中有正在进行副本迁移或 leader 选举的 Topic，那么先将这些 Topic 加入到 <code>topicsIneligibleForDeletion</code> 中，即标记为非法删除；</li>
<li>通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表（<code>topicsToBeDeleted</code>）、将 Partition 添加到要删除的 Partition 列表中（<code>partitionsToBeDeleted</code>）。</li>
</ol>
<p>接下来，看下 Topic 删除线程 DeleteTopicsThread 的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">/note: topic 删除线程</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsThread</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">ShutdownableThread</span>(<span class="params">name = "delete-topics-thread-" + controller.config.brokerId, isInterruptible = false</span>) </span>&#123;</div><div class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">    awaitTopicDeletionNotification()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (!isRunning.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="comment">//note: 要删除的 topic 列表</span></div><div class="line">      <span class="keyword">val</span> topicsQueuedForDeletion = <span class="type">Set</span>.empty[<span class="type">String</span>] ++ topicsToBeDeleted</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(topicsQueuedForDeletion.nonEmpty)</div><div class="line">        info(<span class="string">"Handling deletion for topics "</span> + topicsQueuedForDeletion.mkString(<span class="string">","</span>))</div><div class="line"></div><div class="line">      topicsQueuedForDeletion.foreach &#123; topic =&gt;</div><div class="line">      <span class="comment">// if all replicas are marked as deleted successfully, then topic deletion is done</span></div><div class="line">        <span class="keyword">if</span>(controller.replicaStateMachine.areAllReplicasForTopicDeleted(topic)) &#123;<span class="comment">//note: 如果 Topic 所有副本都删除成功的情况下</span></div><div class="line">          <span class="comment">// clear up all state for this topic from controller cache and zookeeper</span></div><div class="line">          <span class="comment">//note: 从 controller 的缓存和 zk 中清除这个 topic 的所有记录,这个 topic 彻底删除成功了</span></div><div class="line">          completeDeleteTopic(topic)</div><div class="line">          info(<span class="string">"Deletion of topic %s successfully completed"</span>.format(topic))</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">if</span>(controller.replicaStateMachine.isAtLeastOneReplicaInDeletionStartedState(topic)) &#123;</div><div class="line">            <span class="comment">//note: Topic 的副本至少有一个状态为 ReplicaDeletionStarted 时</span></div><div class="line">            <span class="comment">// ignore since topic deletion is in progress</span></div><div class="line">            <span class="comment">//note: 过滤出 Topic 中副本状态为 ReplicaDeletionStarted 的 Partition 列表</span></div><div class="line">            <span class="keyword">val</span> replicasInDeletionStartedState = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionStarted</span>)</div><div class="line">            <span class="comment">//note: 表明了上面这些副本正在删除中</span></div><div class="line">            <span class="keyword">val</span> replicaIds = replicasInDeletionStartedState.map(_.replica)</div><div class="line">            <span class="keyword">val</span> partitions = replicasInDeletionStartedState.map(r =&gt; <span class="type">TopicAndPartition</span>(r.topic, r.partition))</div><div class="line">            info(<span class="string">"Deletion for replicas %s for partition %s of topic %s in progress"</span>.format(replicaIds.mkString(<span class="string">","</span>),</div><div class="line">              partitions.mkString(<span class="string">","</span>), topic))</div><div class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note:副本既没有全部删除完成、也没有一个副本是在删除过程中，证明这个 topic 还没有开始删除或者删除完成但是至少一个副本删除失败</span></div><div class="line">            <span class="comment">// if you come here, then no replica is in TopicDeletionStarted and all replicas are not in</span></div><div class="line">            <span class="comment">// TopicDeletionSuccessful. That means, that either given topic haven't initiated deletion</span></div><div class="line">            <span class="comment">// or there is at least one failed replica (which means topic deletion should be retried).</span></div><div class="line">            <span class="keyword">if</span>(controller.replicaStateMachine.isAnyReplicaInState(topic, <span class="type">ReplicaDeletionIneligible</span>)) &#123;</div><div class="line">              <span class="comment">//note: 如果有副本删除失败,那么进行重试操作</span></div><div class="line">              <span class="comment">// mark topic for deletion retry</span></div><div class="line">              markTopicForDeletionRetry(topic)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// Try delete topic if it is eligible for deletion.</span></div><div class="line">        <span class="keyword">if</span>(isTopicEligibleForDeletion(topic)) &#123; <span class="comment">//note: 如果 topic 可以被删除</span></div><div class="line">          info(<span class="string">"Deletion of topic %s (re)started"</span>.format(topic))</div><div class="line">          <span class="comment">// topic deletion will be kicked off</span></div><div class="line">          <span class="comment">//note: 开始删除 topic</span></div><div class="line">          onTopicDeletion(<span class="type">Set</span>(topic))</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(isTopicIneligibleForDeletion(topic)) &#123;</div><div class="line">          info(<span class="string">"Not retrying deletion of topic %s at this time since it is marked ineligible for deletion"</span>.format(topic))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>doWork()</code> 方法处理逻辑如下：</p>
<ol>
<li>遍历所有要删除的 Topic，进行如下处理；</li>
<li>如果该 Topic 的所有副本都下线成功（状态为 ReplicaDeletionSuccessful）时，那么执行 <code>completeDeleteTopic()</code> 方法完成 Topic 的删除；</li>
<li>否则，如果 Topic 在删除过程有失败的副本（状态为 ReplicaDeletionIneligible），那么执行 <code>markTopicForDeletionRetry()</code> 将失败的 Replica 状态设置为 OfflineReplica；</li>
<li>判断 Topic 是否允许删除（不在非法删除的集合中就代表运允许），调用 <code>onTopicDeletion()</code> 执行 Topic 删除。</li>
</ol>
<p>先看下 <code>onTopicDeletion()</code> 方法，这是 Topic 最开始删除时的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Topic 删除</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onTopicDeletion</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>]) &#123;</div><div class="line">  info(<span class="string">"Topic deletion callback for %s"</span>.format(topics.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">// send update metadata so that brokers stop serving data for topics to be deleted</span></div><div class="line">  <span class="keyword">val</span> partitions = topics.flatMap(controllerContext.partitionsForTopic) <span class="comment">//note: topic 的所有 Partition</span></div><div class="line">  controller.sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, partitions) <span class="comment">//note: 更新meta</span></div><div class="line">  <span class="keyword">val</span> partitionReplicaAssignmentByTopic = controllerContext.partitionReplicaAssignment.groupBy(p =&gt; p._1.topic)</div><div class="line">  topics.foreach &#123; topic =&gt; <span class="comment">//note:  删除 topic 的每一个 Partition</span></div><div class="line">    onPartitionDeletion(partitionReplicaAssignmentByTopic(topic).keySet)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 这个方法是用于 delete-topic, 用于删除 topic 的所有 partition</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onPartitionDeletion</span></span>(partitionsToBeDeleted: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"Partition deletion callback for %s"</span>.format(partitionsToBeDeleted.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="keyword">val</span> replicasPerPartition = controllerContext.replicasForPartition(partitionsToBeDeleted)</div><div class="line">  startReplicaDeletion(replicasPerPartition)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Topic 的删除的真正实现方法还是在 <code>startReplicaDeletion()</code> 方法中，Topic 删除时，会先调用 <code>onPartitionDeletion()</code> 方法删除所有的 Partition，然后在 Partition 删除时，执行 <code>startReplicaDeletion()</code> 方法删除该 Partition 的副本，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 被 onPartitionDeletion 方法触发,删除副本具体的实现的地方</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startReplicaDeletion</span></span>(replicasForTopicsToBeDeleted: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>]) &#123;</div><div class="line">  replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach &#123; topic =&gt;</div><div class="line">    <span class="comment">//note: topic 所有存活的 replica</span></div><div class="line">    <span class="keyword">val</span> aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p =&gt; p.topic == topic)</div><div class="line">    <span class="comment">//note: topic 的 dead replica</span></div><div class="line">    <span class="keyword">val</span> deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic</div><div class="line">    <span class="comment">//note: topic 中已经处于 ReplicaDeletionSuccessful 状态的副本</span></div><div class="line">    <span class="keyword">val</span> successfullyDeletedReplicas = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">    <span class="comment">//note: 还没有成功删除的、存活的副本</span></div><div class="line">    <span class="keyword">val</span> replicasForDeletionRetry = aliveReplicasForTopic -- successfullyDeletedReplicas</div><div class="line">    <span class="comment">// move dead replicas directly to failed state</span></div><div class="line">    <span class="comment">//note: 将 dead replica 设置为 ReplicaDeletionIneligible（删除无效的状态）</span></div><div class="line">    replicaStateMachine.handleStateChanges(deadReplicasForTopic, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">    <span class="comment">// send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader</span></div><div class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 OfflineReplica（发送 StopReplica 请求）</span></div><div class="line">    replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">OfflineReplica</span>)</div><div class="line">    debug(<span class="string">"Deletion started for replicas %s"</span>.format(replicasForDeletionRetry.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 ReplicaDeletionStarted 状态</span></div><div class="line">    controller.replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">ReplicaDeletionStarted</span>,</div><div class="line">      <span class="keyword">new</span> <span class="type">Callbacks</span>.<span class="type">CallbackBuilder</span>().stopReplicaCallback(deleteTopicStopReplicaCallback).build)</div><div class="line">    <span class="keyword">if</span>(deadReplicasForTopic.nonEmpty) &#123; <span class="comment">//note: 将 topic 标记为不能删除</span></div><div class="line">      debug(<span class="string">"Dead Replicas (%s) found for topic %s"</span>.format(deadReplicasForTopic.mkString(<span class="string">","</span>), topic))</div><div class="line">      markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>该方法的执行逻辑如下：</p>
<ol>
<li>首先获取当前集群所有存活的 broker 信息，根据这个信息可以知道 Topic 哪些副本所在节点是处于 dead 状态；</li>
<li>找到那些已经成功删除的 Replica 列表（状态为 ReplicaDeletionSuccessful），进而可以得到那些还没有成功删除、并且存活的 Replica 列表（<code>replicasForDeletionRetry</code>）；</li>
<li>将处于 dead 节点上的 Replica 的状态设置为 ReplicaDeletionIneligible 状态；</li>
<li>然后重新删除 replicasForDeletionRetry 列表中的副本，先将其状态转移为 OfflineReplica，再转移为 ReplicaDeletionStarted 状态（真正从发送 StopReplica +从物理上删除数据）；</li>
<li>如果有 Replica 所在的机器处于 dead 状态，那么将 Topic 设置为非法删除状态。</li>
</ol>
<p>在将副本状态从 OfflineReplica 转移成 ReplicaDeletionStarted 时，会设置一个回调方法 <code>deleteTopicStopReplicaCallback()</code>，该方法会将删除成功的 Replica 设置为 ReplicaDeletionSuccessful 状态，删除失败的 Replica 设置为 ReplicaDeletionIneligible 状态（需要根据 StopReplica 请求处理的过程，看下哪些情况下 Replica 会删除失败，这个会在后面讲解）。</p>
<p>下面看下这个方法 <code>completeDeleteTopic()</code>，当一个 Topic 的所有 Replica 都删除成功时，即其状态都在 ReplicaDeletionSuccessful 时，会调用这个方法，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: topic 删除后,从 controller 缓存、状态机以及 zk 移除这个 topic 相关记录</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">completeDeleteTopic</span></span>(topic: <span class="type">String</span>) &#123;</div><div class="line">  <span class="comment">// deregister partition change listener on the deleted topic. This is to prevent the partition change listener</span></div><div class="line">  <span class="comment">// firing before the new topic listener when a deleted topic gets auto created</span></div><div class="line">  <span class="comment">//note: 1. 取消 zk 对这个 topic 的 partition-modify-listener</span></div><div class="line">  partitionStateMachine.deregisterPartitionChangeListener(topic)</div><div class="line">  <span class="comment">//note: 2. 过滤出副本状态为 ReplicaDeletionSuccessful 的副本列表</span></div><div class="line">  <span class="keyword">val</span> replicasForDeletedTopic = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">  <span class="comment">// controller will remove this replica from the state machine as well as its partition assignment cache</span></div><div class="line">  <span class="comment">//note: controller 将会从副本状态机移除这些副本</span></div><div class="line">  replicaStateMachine.handleStateChanges(replicasForDeletedTopic, <span class="type">NonExistentReplica</span>)</div><div class="line">  <span class="keyword">val</span> partitionsForDeletedTopic = controllerContext.partitionsForTopic(topic)</div><div class="line">  <span class="comment">// move respective partition to OfflinePartition and NonExistentPartition state</span></div><div class="line">  <span class="comment">//note: 3. 从分区状态机中下线并移除这个 topic 的分区</span></div><div class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">OfflinePartition</span>)</div><div class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">NonExistentPartition</span>)</div><div class="line">  topicsToBeDeleted -= topic <span class="comment">//note: 删除成功,从删除 topic 列表中移除</span></div><div class="line">  partitionsToBeDeleted.retain(_.topic != topic) <span class="comment">//note: 从 partitionsToBeDeleted 移除这个 topic</span></div><div class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line">  <span class="comment">//note: 4. 删除 zk 上关于这个 topic 的相关记录</span></div><div class="line">  zkUtils.zkClient.deleteRecursive(getTopicPath(topic))</div><div class="line">  zkUtils.zkClient.deleteRecursive(getEntityConfigPath(<span class="type">ConfigType</span>.<span class="type">Topic</span>, topic))</div><div class="line">  zkUtils.zkClient.delete(getDeleteTopicPath(topic))</div><div class="line">  <span class="comment">//note: 5. 从 controller 的所有缓存中再次移除关于这个 topic 的信息</span></div><div class="line">  controllerContext.removeTopic(topic)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当一个 Topic 所有副本都删除后，会进行如下处理：</p>
<ol>
<li>取消对该 Topic 的 partition-modify-listener 监听器；</li>
<li>将状态为 ReplicaDeletionSuccessful 的副本状态都转移成 NonExistentReplica；</li>
<li>将该 Topic Partition 状态先后转移成 OfflinePartition、NonExistentPartition 状态，正式下线了该 Partition；</li>
<li>从分区状态机和副本状态机中移除这个 Topic 记录；</li>
<li>从 Controller 缓存和 ZK 中清除这个 Topic 的相关记录。</li>
</ol>
<p>至此，一个 Topic 算是真正删除完成。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 &lt;a href=&quot;ht
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Broker 上线下线（十九）</title>
    <link href="http://matt33.com/2018/06/17/broker-online-offline/"/>
    <id>http://matt33.com/2018/06/17/broker-online-offline/</id>
    <published>2018-06-17T07:04:21.000Z</published>
    <updated>2018-06-17T07:56:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 <code>/brokers/ids</code> 下注册一个节点，节点名字就是 broker id，这个节点是临时节点，该节点内部会有这个 Broker 的详细节点信息。Controller 会监听 <code>/brokers/ids</code> 这个路径下的所有子节点，如果有新的节点出现，那么就代表有新的 Broker 上线，如果有节点消失，就代表有 broker 下线，Controller 会进行相应的处理，Kafka 就是利用 ZK 的这种 watch 机制及临时节点的特性来完成集群 Broker 的上下线，本文将会深入讲解这一过程。</p>
<h2 id="BrokerChangeListener"><a href="#BrokerChangeListener" class="headerlink" title="BrokerChangeListener"></a>BrokerChangeListener</h2><p>KafkaController 在启动时，会通过副本状态机注册一个监控 broker 上下线的监听器，通过 ReplicaStateMachine 的 <code>registerListeners()</code> 方法实现的，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// register ZK listeners of the replica state machine</span></div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">registerListeners</span></span>() &#123;</div><div class="line">   <span class="comment">// register broker change listener</span></div><div class="line">   registerBrokerChangeListener() <span class="comment">//note: 监听【/brokers/ids】，broker 的上线下线</span></div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerBrokerChangeListener</span></span>() = &#123;</div><div class="line">  zkUtils.zkClient.subscribeChildChanges(<span class="type">ZkUtils</span>.<span class="type">BrokerIdsPath</span>, brokerChangeListener)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>BrokerChangeListener 是监听 <code>/brokers/ids</code> 节点的监听器，当该节点有变化时会触发 <code>doHandleChildChange()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果 【/brokers/ids】 目录下子节点有变化将会触发这个操作</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrokerChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"BrokerChangeListener"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, currentBrokerList: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    info(<span class="string">"Broker change listener fired for path %s with children %s"</span>.format(parentPath, currentBrokerList.sorted.mkString(<span class="string">","</span>)))</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">        <span class="type">ControllerStats</span>.leaderElectionTimer.time &#123;</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            <span class="comment">//note: 当前 zk 的 broker 列表</span></div><div class="line">            <span class="keyword">val</span> curBrokers = currentBrokerList.map(_.toInt).toSet.flatMap(zkUtils.getBrokerInfo)</div><div class="line">            <span class="comment">//note: ZK 中的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> curBrokerIds = curBrokers.map(_.id)</div><div class="line">            <span class="comment">//note: Controller 缓存中的 broker 列表</span></div><div class="line">            <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</div><div class="line">            <span class="comment">//note: 新上线的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokerIds = curBrokerIds -- liveOrShuttingDownBrokerIds</div><div class="line">            <span class="comment">//note: 掉线的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> deadBrokerIds = liveOrShuttingDownBrokerIds -- curBrokerIds</div><div class="line">            <span class="comment">//note: 新上线的 Broker 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokers = curBrokers.filter(broker =&gt; newBrokerIds(broker.id))</div><div class="line">            controllerContext.liveBrokers = curBrokers <span class="comment">//note: 更新缓存中当前 broker 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokerIdsSorted = newBrokerIds.toSeq.sorted</div><div class="line">            <span class="keyword">val</span> deadBrokerIdsSorted = deadBrokerIds.toSeq.sorted</div><div class="line">            <span class="keyword">val</span> liveBrokerIdsSorted = curBrokerIds.toSeq.sorted</div><div class="line">            info(<span class="string">"Newly added brokers: %s, deleted brokers: %s, all live brokers: %s"</span></div><div class="line">              .format(newBrokerIdsSorted.mkString(<span class="string">","</span>), deadBrokerIdsSorted.mkString(<span class="string">","</span>), liveBrokerIdsSorted.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="comment">//note: Broker 上线, 在 Controller Channel Manager 中添加该 broker</span></div><div class="line">            newBrokers.foreach(controllerContext.controllerChannelManager.addBroker)</div><div class="line">            <span class="comment">//note: Broker 下线处理, 在 Controller Channel Manager 移除该 broker</span></div><div class="line">            deadBrokerIds.foreach(controllerContext.controllerChannelManager.removeBroker)</div><div class="line">            <span class="keyword">if</span>(newBrokerIds.nonEmpty) <span class="comment">//note: 启动该 Broker</span></div><div class="line">              controller.onBrokerStartup(newBrokerIdsSorted)</div><div class="line">            <span class="keyword">if</span>(deadBrokerIds.nonEmpty) <span class="comment">//note: broker 掉线后开始 leader 选举</span></div><div class="line">              controller.onBrokerFailure(deadBrokerIdsSorted)</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling broker changes"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里需要重点关注 <code>doHandleChildChange()</code> 方法的实现，该方法处理逻辑如下：</p>
<ol>
<li>从 ZK 获取当前的 Broker 列表（<code>curBrokers</code>）及 broker id 的列表（<code>curBrokerIds</code>）；</li>
<li>获取当前 Controller 中缓存的 broker id 列表（<code>liveOrShuttingDownBrokerIds</code>）；</li>
<li>获取新上线 broker id 列表：<code>newBrokerIds</code> = <code>curBrokerIds</code> – <code>liveOrShuttingDownBrokerIds</code>；</li>
<li>获取掉线的 broker id 列表：<code>deadBrokerIds</code> = <code>liveOrShuttingDownBrokerIds</code> – <code>curBrokerIds</code>；</li>
<li>对于新上线的 broker，先在 ControllerChannelManager 中添加该 broker（即建立与该 Broker 的连接、初始化相应的发送线程和请求队列），最后 Controller 调用 <code>onBrokerStartup()</code> 上线该 Broker；</li>
<li>对于掉线的 broker，先在 ControllerChannelManager 中移除该 broker（即关闭与 Broker 的连接、关闭相应的发送线程和清空请求队列），最后 Controller 调用 <code>onBrokerFailure()</code> 下线该 Broker。</li>
</ol>
<p>整体的处理流程如下图所示：</p>
<p><img src="/images/kafka/broker_online_offline.png" alt="Broker 上线下线处理过程"></p>
<h2 id="Broker-上线"><a href="#Broker-上线" class="headerlink" title="Broker 上线"></a>Broker 上线</h2><p>本节主要讲述一台 Broker 上线的过程，如前面图中所示，一台 Broker 上线主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">controllerContext.controllerChannelManager.addBroker</div><div class="line">controller.onBrokerStartup(newBrokerIdsSorted)</div></pre></td></tr></table></figure>
<ol>
<li>在 Controller Channel Manager 中添加该 Broker 节点，主要的内容是：Controller 建立与该 Broker 的连接、初始化相应的请求发送线程与请求队列；</li>
<li>调用 Controller 的 <code>onBrokerStartup()</code> 方法上线该节点。</li>
</ol>
<p>Controller Channel Manager 添加 Broker 的实现如下，这里就不重复讲述了，前面讲述 Controller 服务初始化的文章（ <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager </a>）已经讲述过这部分的内容。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</div><div class="line">  <span class="comment">// be careful here. Maybe the startup() API has already started the request send thread</span></div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(!brokerStateInfo.contains(broker.id)) &#123;</div><div class="line">      addNewBroker(broker)</div><div class="line">      startRequestSendThread(broker.id)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面再看下 Controller 如何在 <code>onBrokerStartup()</code> 方法中实现 Broker 上线操作的，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个是被 副本状态机触发的</span></div><div class="line"><span class="comment">//note: 1. 发送 update-metadata 请求给所有存活的 broker;</span></div><div class="line"><span class="comment">//note: 2. 对于所有 new/offline partition 触发选主操作, 选举成功的, Partition 状态设置为 Online</span></div><div class="line"><span class="comment">//note: 3. 检查是否有分区的重新副本分配分配到了这个台机器上, 如果有, 就进行相应的操作</span></div><div class="line"><span class="comment">//note: 4. 检查这台机器上是否有 Topic 被设置为了删除标志, 如果是, 那么机器启动完成后, 重新尝试删除操作</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerStartup</span></span>(newBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</div><div class="line">  info(<span class="string">"New broker startup callback for %s"</span>.format(newBrokers.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="keyword">val</span> newBrokersSet = newBrokers.toSet <span class="comment">//note: 新启动的 broker</span></div><div class="line">  <span class="comment">// send update metadata request to all live and shutting down brokers. Old brokers will get to know of the new</span></div><div class="line">  <span class="comment">// broker via this update.</span></div><div class="line">  <span class="comment">// In cases of controlled shutdown leaders will not be elected when a new broker comes up. So at least in the</span></div><div class="line">  <span class="comment">// common controlled shutdown case, the metadata will reach the new brokers faster</span></div><div class="line">  <span class="comment">//note: 发送 metadata 更新给所有的 broker, 这样的话旧的 broker 将会知道有机器新上线了</span></div><div class="line">  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line">  <span class="comment">// the very first thing to do when a new broker comes up is send it the entire list of partitions that it is</span></div><div class="line">  <span class="comment">// supposed to host. Based on that the broker starts the high watermark threads for the input list of partitions</span></div><div class="line">  <span class="comment">//note:  获取这个机器上的所有 replica 请求</span></div><div class="line">  <span class="keyword">val</span> allReplicasOnNewBrokers = controllerContext.replicasOnBrokers(newBrokersSet)</div><div class="line">  <span class="comment">//note: 将这些副本的状态设置为 OnlineReplica</span></div><div class="line">  replicaStateMachine.handleStateChanges(allReplicasOnNewBrokers, <span class="type">OnlineReplica</span>)</div><div class="line">  <span class="comment">// when a new broker comes up, the controller needs to trigger leader election for all new and offline partitions</span></div><div class="line">  <span class="comment">// to see if these brokers can become leaders for some/all of those</span></div><div class="line">  <span class="comment">//note: 新的 broker 上线也会触发所有处于 new/offline 的 partition 进行 leader 选举</span></div><div class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</div><div class="line">  <span class="comment">// check if reassignment of some partitions need to be restarted</span></div><div class="line">  <span class="comment">//note: 检查是否副本的重新分配分配到了这台机器上</span></div><div class="line">  <span class="keyword">val</span> partitionsWithReplicasOnNewBrokers = controllerContext.partitionsBeingReassigned.filter &#123;</div><div class="line">    <span class="keyword">case</span> (_, reassignmentContext) =&gt; reassignmentContext.newReplicas.exists(newBrokersSet.contains(_))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 如果需要副本进行迁移的话,就执行副本迁移操作</span></div><div class="line">  partitionsWithReplicasOnNewBrokers.foreach(p =&gt; onPartitionReassignment(p._1, p._2))</div><div class="line">  <span class="comment">// check if topic deletion needs to be resumed. If at least one replica that belongs to the topic being deleted exists</span></div><div class="line">  <span class="comment">// on the newly restarted brokers, there is a chance that topic deletion can resume</span></div><div class="line">  <span class="comment">//note: 检查 topic 删除操作是否需要重新启动</span></div><div class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnNewBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123;</div><div class="line">    info((<span class="string">"Some replicas %s for topics scheduled for deletion %s are on the newly restarted brokers %s. "</span> +</div><div class="line">      <span class="string">"Signaling restart of topic deletion for these topics"</span>).format(replicasForTopicsToBeDeleted.mkString(<span class="string">","</span>),</div><div class="line">      deleteTopicManager.topicsToBeDeleted.mkString(<span class="string">","</span>), newBrokers.mkString(<span class="string">","</span>)))</div><div class="line">    deleteTopicManager.resumeDeletionForTopics(replicasForTopicsToBeDeleted.map(_.topic))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>onBrokerStartup()</code> 方法在实现的逻辑上分为以下几步：</p>
<ol>
<li>调用 <code>sendUpdateMetadataRequest()</code> 方法向当前集群所有存活的 Broker 发送 Update Metadata 请求，这样的话其他的节点就会知道当前的 Broker 已经上线了；</li>
<li>获取当前节点分配的所有的 Replica 列表，并将其状态转移为 OnlineReplica 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 leader 选举，如果 leader 选举成功，那么该 Partition 的状态就会转移到 OnlinePartition 状态，否则状态转移失败；</li>
<li>如果副本迁移中有新的 Replica 落在这台新上线的节点上，那么开始执行副本迁移操作（见<a href="http://matt33.com/2018/06/16/partition-reassignment/">Kafka 源码解析之 Partition 副本迁移实现</a>）;</li>
<li>如果之前由于这个 Topic 设置为删除标志，但是由于其中有 Replica 掉线而导致无法删除，这里在节点启动后，尝试重新执行删除操作。</li>
</ol>
<p>到此为止，一台 Broker 算是真正加入到了 Kafka 的集群中，在上述过程中，涉及到 leader 选举的操作，都会触发 LeaderAndIsr 请求及 Metadata 请求的发送。</p>
<h2 id="Broker-掉线"><a href="#Broker-掉线" class="headerlink" title="Broker 掉线"></a>Broker 掉线</h2><p>本节主要讲述一台 Broker 掉线后的处理过程，正如前面图中所示，一台 Broker 掉线后主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">controllerContext.controllerChannelManager.removeBroker</div><div class="line">controller.onBrokerFailure(deadBrokerIdsSorted)</div></pre></td></tr></table></figure>
<ol>
<li>首先在 Controller Channel Manager 中移除该 Broker 节点，主要的内容是：关闭 Controller  与 Broker 的连接和相应的请求发送线程，并清空请求队列；</li>
<li>调用 Controller 的 <code>onBrokerFailure()</code> 方法下线该节点。</li>
</ol>
<p>Controller Channel Manager 下线 Broker 的处理如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeBroker</span></span>(brokerId: <span class="type">Int</span>) &#123;</div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    removeExistingBroker(brokerStateInfo(brokerId))</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 移除旧的 broker（关闭网络连接、关闭请求发送线程）</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">removeExistingBroker</span></span>(brokerState: <span class="type">ControllerBrokerStateInfo</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    brokerState.networkClient.close()</div><div class="line">    brokerState.messageQueue.clear()</div><div class="line">    brokerState.requestSendThread.shutdown()</div><div class="line">    brokerStateInfo.remove(brokerState.brokerNode.id)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while removing broker by the controller"</span>, e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 Controller Channel Manager 处理完掉线的 Broker 节点后，下面 KafkaController 将会调用 <code>onBrokerFailure()</code> 进行相应的处理，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法会被副本状态机调用（进行 broker 节点下线操作）</span></div><div class="line"><span class="comment">//note: 1. 将 leader 在这台机器上的分区设置为 Offline</span></div><div class="line"><span class="comment">//note: 2. 通过 OfflinePartitionLeaderSelector 为 new/offline partition 选举新的 leader</span></div><div class="line"><span class="comment">//note: 3. leader 选举后,发送 LeaderAndIsr 请求给该分区所有存活的副本;</span></div><div class="line"><span class="comment">//note: 4. 分区选举 leader 后,状态更新为 Online</span></div><div class="line"><span class="comment">//note: 5. 要下线的 broker 上的所有 replica 改为 Offline 状态</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerFailure</span></span>(deadBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</div><div class="line">  info(<span class="string">"Broker failure callback for %s"</span>.format(deadBrokers.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">//note: 从正在下线的 broker 集合中移除已经下线的机器</span></div><div class="line">  <span class="keyword">val</span> deadBrokersThatWereShuttingDown =</div><div class="line">    deadBrokers.filter(id =&gt; controllerContext.shuttingDownBrokerIds.remove(id))</div><div class="line">  info(<span class="string">"Removed %s from list of shutting down brokers."</span>.format(deadBrokersThatWereShuttingDown))</div><div class="line">  <span class="keyword">val</span> deadBrokersSet = deadBrokers.toSet</div><div class="line">  <span class="comment">// trigger OfflinePartition state for all partitions whose current leader is one amongst the dead brokers</span></div><div class="line">  <span class="comment">//note: 1. 将 leader 在这台机器上的、并且未设置删除的分区状态设置为 Offline</span></div><div class="line">  <span class="keyword">val</span> partitionsWithoutLeader = controllerContext.partitionLeadershipInfo.filter(partitionAndLeader =&gt;</div><div class="line">    deadBrokersSet.contains(partitionAndLeader._2.leaderAndIsr.leader) &amp;&amp;</div><div class="line">      !deleteTopicManager.isTopicQueuedUpForDeletion(partitionAndLeader._1.topic)).keySet</div><div class="line">  partitionStateMachine.handleStateChanges(partitionsWithoutLeader, <span class="type">OfflinePartition</span>)</div><div class="line">  <span class="comment">// trigger OnlinePartition state changes for offline or new partitions</span></div><div class="line">  <span class="comment">//note: 2. 选举 leader, 选举成功后设置为 Online 状态</span></div><div class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</div><div class="line">  <span class="comment">// filter out the replicas that belong to topics that are being deleted</span></div><div class="line">  <span class="comment">//note: 过滤出 replica 在这个机器上、并且没有被设置为删除的 topic 列表</span></div><div class="line">  <span class="keyword">var</span> allReplicasOnDeadBrokers = controllerContext.replicasOnBrokers(deadBrokersSet)</div><div class="line">  <span class="keyword">val</span> activeReplicasOnDeadBrokers = allReplicasOnDeadBrokers.filterNot(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="comment">// handle dead replicas</span></div><div class="line">  <span class="comment">//note: 将这些 replica 状态转为 Offline</span></div><div class="line">  replicaStateMachine.handleStateChanges(activeReplicasOnDeadBrokers, <span class="type">OfflineReplica</span>)</div><div class="line">  <span class="comment">// check if topic deletion state for the dead replicas needs to be updated</span></div><div class="line">  <span class="comment">//note: 过滤设置为删除的 replica</span></div><div class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnDeadBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 将上面这个 topic 列表的 topic 标记为删除失败</span></div><div class="line">    <span class="comment">// it is required to mark the respective replicas in TopicDeletionFailed state since the replica cannot be</span></div><div class="line">    <span class="comment">// deleted when the broker is down. This will prevent the replica from being in TopicDeletionStarted state indefinitely</span></div><div class="line">    <span class="comment">// since topic deletion cannot be retried until at least one replica is in TopicDeletionStarted state</span></div><div class="line">    deleteTopicManager.failReplicaDeletion(replicasForTopicsToBeDeleted)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// If broker failure did not require leader re-election, inform brokers of failed broker</span></div><div class="line">  <span class="comment">// Note that during leader re-election, brokers update their metadata</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithoutLeader.isEmpty) &#123;</div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 对于掉线 Broker 的处理过程主要有以下几步：</p>
<ol>
<li>首先找到 Leader 在该 Broker 上所有 Partition 列表，然后将这些 Partition 的状态全部转移为 OfflinePartition 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 Leader 选举，如果 Leader 选举成功，那么该 Partition 的状态就会迁移到 OnlinePartition 状态，否则状态转移失败（Broker 上线/掉线、Controller 初始化时都会触发这个方法）；</li>
<li>获取在该 Broker 上的所有 Replica 列表，将其状态转移成 OfflineReplica 状态；</li>
<li>过滤出设置为删除、并且有副本在该节点上的 Topic 列表，先将该 Replica 的转移成 ReplicaDeletionIneligible 状态，然后再将该 Topic 标记为非法删除，即因为有 Replica 掉线导致该 Topic 无法删除；</li>
<li>如果 leader 在该 Broker 上所有 Partition 列表不为空，证明有 Partition 的 leader 需要选举，在最后一步会触发全局 metadata 信息的更新。</li>
</ol>
<p>到这里，一台掉线的 Broker 算是真正下线完成了。</p>
<h2 id="Broker-优雅下线"><a href="#Broker-优雅下线" class="headerlink" title="Broker 优雅下线"></a>Broker 优雅下线</h2><p>前面部分是关于通过监听节点变化来实现对 Broker 的上下线，这也是 Kafka 上下线 Broker 的主要流程，但是还有一种情况是：主动关闭 Kafka 服务，这种情况又被称为 Broker 的优雅关闭。</p>
<p>优雅关闭的节点会向 Controller 发送 ControlledShutdownRequest 请求，Controller 在收到这个情况会进行相应的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleControlledShutdownRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></div><div class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></div><div class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></div><div class="line">  <span class="keyword">val</span> controlledShutdownRequest = request.requestObj.asInstanceOf[<span class="type">ControlledShutdownRequest</span>]</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断该连接是否经过认证</span></div><div class="line">  authorizeClusterAction(request)</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理该请求</span></div><div class="line">  <span class="keyword">val</span> partitionsRemaining = controller.shutdownBroker(controlledShutdownRequest.brokerId)</div><div class="line">  <span class="comment">//note: 返回的 response</span></div><div class="line">  <span class="keyword">val</span> controlledShutdownResponse = <span class="keyword">new</span> <span class="type">ControlledShutdownResponse</span>(controlledShutdownRequest.correlationId,</div><div class="line">    <span class="type">Errors</span>.<span class="type">NONE</span>.code, partitionsRemaining)</div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, <span class="keyword">new</span> <span class="type">RequestOrResponseSend</span>(request.connectionId, controlledShutdownResponse)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 在接收这个关闭服务的请求，通过 <code>shutdownBroker()</code> 方法进行处理，实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 优雅地关闭 Broker</span></div><div class="line"><span class="comment">//note: controller 首先决定将这个 broker 上的 leader 迁移到其他可用的机器上</span></div><div class="line"><span class="comment">//note: 返回还没有 leader 的迁移的 TopicPartition 集合</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownBroker</span></span>(id: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (!isActive) &#123;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(<span class="string">"Controller moved to another broker. Aborting controlled shutdown"</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  controllerContext.brokerShutdownLock synchronized &#123; <span class="comment">//note: 拿到 broker shutdown 的唯一锁</span></div><div class="line">    info(<span class="string">"Shutting down broker "</span> + id)</div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 拿到 controllerLock 的排它锁</span></div><div class="line">      <span class="keyword">if</span> (!controllerContext.liveOrShuttingDownBrokerIds.contains(id))</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BrokerNotAvailableException</span>(<span class="string">"Broker id %d does not exist."</span>.format(id))</div><div class="line"></div><div class="line">      controllerContext.shuttingDownBrokerIds.add(id) <span class="comment">//note: 将 broker id 添加到正在关闭的 broker 列表中</span></div><div class="line">      debug(<span class="string">"All shutting down brokers: "</span> + controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>))</div><div class="line">      debug(<span class="string">"Live brokers: "</span> + controllerContext.liveBrokerIds.mkString(<span class="string">","</span>))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 获取这个 broker 上所有 Partition 与副本数的 map</span></div><div class="line">    <span class="keyword">val</span> allPartitionsAndReplicationFactorOnBroker: <span class="type">Set</span>[(<span class="type">TopicAndPartition</span>, <span class="type">Int</span>)] =</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        controllerContext.partitionsOnBroker(id)</div><div class="line">          .map(topicAndPartition =&gt; (topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition).size))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 处理这些 TopicPartition，更新 Partition 或 Replica 的状态，必要时进行 leader 选举</span></div><div class="line">    allPartitionsAndReplicationFactorOnBroker.foreach &#123;</div><div class="line">      <span class="keyword">case</span>(topicAndPartition, replicationFactor) =&gt;</div><div class="line">        <span class="comment">// Move leadership serially to relinquish lock.</span></div><div class="line">        inLock(controllerContext.controllerLock) &#123;</div><div class="line">          controllerContext.partitionLeadershipInfo.get(topicAndPartition).foreach &#123; currLeaderIsrAndControllerEpoch =&gt;</div><div class="line">            <span class="keyword">if</span> (replicationFactor &gt; <span class="number">1</span>) &#123; <span class="comment">//note: 副本数大于1</span></div><div class="line">              <span class="keyword">if</span> (currLeaderIsrAndControllerEpoch.leaderAndIsr.leader == id) &#123; <span class="comment">//note: leader 正好是下线的节点</span></div><div class="line">                <span class="comment">// If the broker leads the topic partition, transition the leader and update isr. Updates zk and</span></div><div class="line">                <span class="comment">// notifies all affected brokers</span></div><div class="line">                <span class="comment">//todo: 这种情况下 Replica 的状态不需要修改么？（Replica 的处理还是通过监听器还实现的,这里只是在服务关闭前进行 leader 切换和停止副本同步）</span></div><div class="line">                <span class="comment">//note: 状态变化（变为 OnlinePartition，并且进行 leader 选举，使用 controlledShutdownPartitionLeaderSelector 算法）</span></div><div class="line">                partitionStateMachine.handleStateChanges(<span class="type">Set</span>(topicAndPartition), <span class="type">OnlinePartition</span>,</div><div class="line">                  controlledShutdownPartitionLeaderSelector)</div><div class="line">              &#125; <span class="keyword">else</span> &#123;</div><div class="line">                <span class="comment">// Stop the replica first. The state change below initiates ZK changes which should take some time</span></div><div class="line">                <span class="comment">// before which the stop replica request should be completed (in most cases)</span></div><div class="line">                <span class="keyword">try</span> &#123; <span class="comment">//note: 要下线的机器停止副本迁移，发送 StopReplica 请求</span></div><div class="line">                  brokerRequestBatch.newBatch()</div><div class="line">                  brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">Seq</span>(id), topicAndPartition.topic,</div><div class="line">                    topicAndPartition.partition, deletePartition = <span class="literal">false</span>)</div><div class="line">                  brokerRequestBatch.sendRequestsToBrokers(epoch)</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> e : <span class="type">IllegalStateException</span> =&gt; &#123;</div><div class="line">                    <span class="comment">// Resign if the controller is in an illegal state</span></div><div class="line">                    error(<span class="string">"Forcing the controller to resign"</span>)</div><div class="line">                    brokerRequestBatch.clear()</div><div class="line">                    controllerElector.resign()</div><div class="line"></div><div class="line">                    <span class="keyword">throw</span> e</div><div class="line">                  &#125;</div><div class="line">                &#125;</div><div class="line">                <span class="comment">// If the broker is a follower, updates the isr in ZK and notifies the current leader</span></div><div class="line">                <span class="comment">//note: 更新这个副本的状态，变为 OfflineReplica</span></div><div class="line">                replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="type">PartitionAndReplica</span>(topicAndPartition.topic,</div><div class="line">                  topicAndPartition.partition, id)), <span class="type">OfflineReplica</span>)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 返回 leader 在这个要下线节点上并且副本数大于 1 的 TopicPartition 集合</span></div><div class="line">    <span class="comment">//note: 在已经进行前面 leader 迁移后</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replicatedPartitionsBrokerLeads</span></span>() = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      trace(<span class="string">"All leaders = "</span> + controllerContext.partitionLeadershipInfo.mkString(<span class="string">","</span>))</div><div class="line">      controllerContext.partitionLeadershipInfo.filter &#123;</div><div class="line">        <span class="keyword">case</span> (topicAndPartition, leaderIsrAndControllerEpoch) =&gt;</div><div class="line">          leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &amp;&amp; controllerContext.partitionReplicaAssignment(topicAndPartition).size &gt; <span class="number">1</span></div><div class="line">      &#125;.keys</div><div class="line">    &#125;</div><div class="line">    replicatedPartitionsBrokerLeads().toSet</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法的处理逻辑如下：</p>
<ol>
<li>先将要下线的 Broker 添加到 shuttingDownBrokerIds 集合中，该集合记录了当前正在进行关闭的 broker 列表；</li>
<li>获取副本在该节点上的所有 Partition 的列表集合；</li>
<li>遍历上述 Partition 列表进行处理：如果该 Partition 的 leader 是要下线的节点，那么通过 PartitionStateMachine 进行状态转移（OnlinePartition –&gt; OnlinePartition）触发 leader 选举，使用的 leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#ControlledShutdownLeaderSelector">ControlledShutdownLeaderSelector</a>，它会选举 isr 中第一个没有正在关闭的 Replica 作为 leader，否则抛出 StateChangeFailedException 异常；</li>
<li>否则的话，即要下线的节点不是 leader，那么就向要下线的节点发送 StopReplica 请求停止副本同步，并将该副本设置为 OfflineReplica 状态，这里对 Replica 进行处理的原因是为了让要下线的机器关闭副本同步流程，这样 Kafka 服务才能正常关闭。</li>
</ol>
<p>我在看这部分的代码是有一个疑问的，那就是如果要下线的节点是 Partition leader 的情况下，并没有对 Replica 进行相应的处理，这里的原因是，这部分 Replica 的处理可以放在 <code>onBrokerFailure()</code> 方法中处理，即使通过优雅下线的方法下线了 Broker，但是监听 ZK 的 BrokerChangeListener 监听器还是会被触发的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 &lt;code&gt;/brokers/ids&lt;/code&gt; 下注册一个节点，节点名字就是 brok
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Partition 副本迁移实现（十八）</title>
    <link href="http://matt33.com/2018/06/16/partition-reassignment/"/>
    <id>http://matt33.com/2018/06/16/partition-reassignment/</id>
    <published>2018-06-16T15:40:00.000Z</published>
    <updated>2018-06-18T05:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Controller 在初始化时，会利用 ZK 的 watch 机制注册很多不同类型的监听器，当监听的事件被触发时，Controller 就会触发相应的操作。</p>
<p>Controller 在初始化时，会注册多种类型的监听器，主要有以下6种：</p>
<ol>
<li>监听 <code>/admin/reassign_partitions</code> 节点，用于分区副本迁移的监听；</li>
<li>监听 <code>/isr_change_notification</code> 节点，用于 Partition Isr 变动的监听，；</li>
<li>监听 <code>/admin/preferred_replica_election</code> 节点，用于需要进行 Partition 最优 leader 选举的监听；</li>
<li>监听 <code>/brokers/topics</code> 节点，用于 Topic 新建的监听；</li>
<li>监听 <code>/brokers/topics/TOPIC_NAME</code> 节点，用于 Topic Partition 扩容的监听；</li>
<li>监听 <code>/admin/delete_topics</code> 节点，用于 Topic 删除的监听；</li>
<li>监听 <code>/brokers/ids</code> 节点，用于 Broker 上下线的监听。</li>
</ol>
<p>本文主要讲解第一部分，也就是 Controller 对 Partition 副本迁移的处理，后续会单独一篇文章讲述 Topic 的新建、扩容和删除，再单独一篇文章讲述 Broker 的上下线，另外两部分将会在对 LeaderAndIsr 请求处理的文章中讲述。</p>
<h2 id="Partition-副本迁移整体流程"><a href="#Partition-副本迁移整体流程" class="headerlink" title="Partition 副本迁移整体流程"></a>Partition 副本迁移整体流程</h2><p>Partition 的副本迁移实际上就是将分区的副本重新分配到不同的代理节点上，如果 zk 中新副本的集合与 Partition 原来的副本集合相同，那么这个副本就不需要重新分配了。</p>
<p>Partition 的副本迁移是通过监听 zk 的 <code>/admin/reassign_partitions</code> 节点触发的，Kafka 也向用户提供相应的脚本工具进行副本迁移，副本迁移的脚本使用方法如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-reassign-partitions.sh --zookeeper XXX --reassignment-json-file XXX.json --execute</div></pre></td></tr></table></figure>
<p>其中 XXX.json 为要进行 Partition 副本迁移的 json 文件，json 文件的格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"version"</span>:<span class="number">1</span>,</div><div class="line">    <span class="attr">"partitions"</span>:[</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">19</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">9</span>,</div><div class="line">                <span class="number">2</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">26</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">2</span>,</div><div class="line">                <span class="number">6</span>,</div><div class="line">                <span class="number">4</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">27</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">5</span>,</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">8</span></div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个 json 文件的意思是将 Topic <code>__consumer_offsets</code> Partition 19 的副本迁移到 {3, 2, 9} 上，Partition 26 的副本迁移到 {6, 2, 4} 上，Partition 27 的副本迁移到 {5, 3, 8} 上。</p>
<p>在调用脚本向 zk 提交 Partition 的迁移计划时，迁移计划更新到 zk 前需要进行一步判断，如果该节点（写入迁移计划的节点）已经存在，即副本迁移还在进行，那么本次副本迁移计划是无法提交的，实现的逻辑如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">executeAssignment</span></span>(zkUtils: <span class="type">ZkUtils</span>, reassignmentJsonString: <span class="type">String</span>, throttle: <span class="type">Long</span> = <span class="number">-1</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsToBeReassigned = parseAndValidate(zkUtils, reassignmentJsonString)</div><div class="line">  <span class="keyword">val</span> reassignPartitionsCommand = <span class="keyword">new</span> <span class="type">ReassignPartitionsCommand</span>(zkUtils, partitionsToBeReassigned.toMap)</div><div class="line"></div><div class="line">  <span class="comment">// If there is an existing rebalance running, attempt to change its throttle</span></div><div class="line">  <span class="comment">//note: 如果副本迁移正在进行,那么这次的副本迁移计划是无法提交的</span></div><div class="line">  <span class="keyword">if</span> (zkUtils.pathExists(<span class="type">ZkUtils</span>.<span class="type">ReassignPartitionsPath</span>)) &#123;</div><div class="line">    println(<span class="string">"There is an existing assignment running."</span>)</div><div class="line">    reassignPartitionsCommand.maybeLimit(throttle)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    printCurrentAssignment(zkUtils, partitionsToBeReassigned)</div><div class="line">    <span class="keyword">if</span> (throttle &gt;= <span class="number">0</span>)</div><div class="line">      println(<span class="type">String</span>.format(<span class="string">"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value."</span>))</div><div class="line">    <span class="comment">//note: 将迁移计划更新到 zk 上</span></div><div class="line">    <span class="keyword">if</span> (reassignPartitionsCommand.reassignPartitions(throttle)) &#123;</div><div class="line">      println(<span class="string">"Successfully started reassignment of partitions."</span>)</div><div class="line">    &#125; <span class="keyword">else</span></div><div class="line">      println(<span class="string">"Failed to reassign partitions %s"</span>.format(partitionsToBeReassigned))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在迁移计划提交到 zk 之后，Controller 的 PartitionsReassignedListener 就会被触发，Controller 开始 Partition 的副本迁移，触发之后 Controller 的处理流程大体如下图所示：</p>
<p><img src="/images/kafka/partition_reassignment.png" alt="Partition 迁移过程"></p>
<h2 id="PartitionsReassignedListener-副本迁移处理"><a href="#PartitionsReassignedListener-副本迁移处理" class="headerlink" title="PartitionsReassignedListener 副本迁移处理"></a>PartitionsReassignedListener 副本迁移处理</h2><p>在 zk 的 <code>/admin/reassign_partitions</code> 节点数据有变化时，就会触发 PartitionsReassignedListener 的 <code>doHandleDataChange()</code> 方法，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 开始进行 partition reassignment 除非这三种情况发生:</span></div><div class="line"><span class="comment">//note: 1. 这个 partition 的 reassignment 之前已经存在, 即正在迁移中;</span></div><div class="line"><span class="comment">//note: 2. new replica 与已经存在的 replicas 相同;</span></div><div class="line"><span class="comment">//note: 3. Partition 所有新分配 replica 都已经 dead;</span></div><div class="line"><span class="comment">//note: 这种情况发生时,会输出一条日志,并从 zk 移除该 Partition 的迁移计划。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionsReassignedListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"PartitionsReassignedListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when some partitions are reassigned by the admin command</div><div class="line">   *</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当一些分区需要进行迁移时</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    debug(<span class="string">"Partitions reassigned listener fired for path %s. Record partitions to be reassigned %s"</span></div><div class="line">      .format(dataPath, data))</div><div class="line">    <span class="keyword">val</span> partitionsReassignmentData = <span class="type">ZkUtils</span>.parsePartitionReassignmentData(data.toString)</div><div class="line">    <span class="keyword">val</span> partitionsToBeReassigned = inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 需要迁移的新副本</span></div><div class="line">      <span class="comment">//note: 过滤掉正在迁移的副本,如果 Partition 正在迁移,这一波迁移完之前不允许再次迁移</span></div><div class="line">      partitionsReassignmentData.filterNot(p =&gt; controllerContext.partitionsBeingReassigned.contains(p._1))</div><div class="line">    &#125;</div><div class="line">    partitionsToBeReassigned.foreach &#123; partitionToBeReassigned =&gt;</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(partitionToBeReassigned._1.topic)) &#123;</div><div class="line">          <span class="comment">//note: 如果这个 topic 已经设置了删除，那么就不会进行迁移了（从需要副本迁移的集合中移除）</span></div><div class="line">          error(<span class="string">"Skipping reassignment of partition %s for topic %s since it is currently being deleted"</span></div><div class="line">            .format(partitionToBeReassigned._1, partitionToBeReassigned._1.topic))</div><div class="line">          controller.removePartitionFromReassignedPartitions(partitionToBeReassigned._1)</div><div class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 添加到需要迁移的副本集合中</span></div><div class="line">          <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">ReassignedPartitionsContext</span>(partitionToBeReassigned._2)</div><div class="line">          controller.initiateReassignReplicasForTopicPartition(partitionToBeReassigned._1, context)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果 Partition 出现下面的情况，将不会进行副本迁移，直接将 Partition 的迁移计划从 ZK 移除：</p>
<ol>
<li>这个 Partition 的 reassignment 之前已经存在, 即正在迁移中;</li>
<li>这个 Partition 新分配的 replica 与之前的 replicas 相同;</li>
<li>这个 Partition 所有新分配 replica 都已经 dead;</li>
<li>这个 Partition 已经被设置了删除标志。</li>
</ol>
<p>对于可以进行副本迁移的 Partition 集合，这里将会调用 Kafka Controller 的 <code>initiateReassignReplicasForTopicPartition()</code> 方法对每个 Partition 进行处理。</p>
<h2 id="副本迁移初始化"><a href="#副本迁移初始化" class="headerlink" title="副本迁移初始化"></a>副本迁移初始化</h2><p>进行了前面的判断后，这个 Partition 满足了可以迁移的条件，Controller 会首先初始化副本迁移的流程，实现如下所示：</p>
<blockquote>
<p>如果 Partition 新分配的 replica 与之前的 replicas 相同，那么不会进行副本迁移，这部分的判断实际上是在这里实现的，前面只是为了更好地讲述。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化 Topic-Partition 的副本迁移</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initiateReassignReplicasForTopicPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>,</div><div class="line">                                      reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</div><div class="line">  <span class="comment">//note: 要迁移的 topic-partition，及新的副本</span></div><div class="line">  <span class="keyword">val</span> newReplicas = reassignedPartitionContext.newReplicas</div><div class="line">  <span class="keyword">val</span> topic = topicAndPartition.topic</div><div class="line">  <span class="keyword">val</span> partition = topicAndPartition.partition</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> assignedReplicasOpt = controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="comment">//note: partition 的 AR</span></div><div class="line">    assignedReplicasOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</div><div class="line">        <span class="keyword">if</span> (assignedReplicas == newReplicas) &#123; <span class="comment">//note: 不需要迁移</span></div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Partition %s to be reassigned is already assigned to replicas"</span>.format(topicAndPartition) +</div><div class="line">            <span class="string">" %s. Ignoring request for partition reassignment"</span>.format(newReplicas.mkString(<span class="string">","</span>)))</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          info(<span class="string">"Handling reassignment of partition %s to new replicas %s"</span>.format(topicAndPartition, newReplicas.mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// first register ISR change listener</span></div><div class="line">          <span class="comment">//note: 首先注册 ISR 监听的变化</span></div><div class="line">          watchIsrChangesForReassignedPartition(topic, partition, reassignedPartitionContext)</div><div class="line">          <span class="comment">//note: 正在迁移 Partition 添加到缓存中</span></div><div class="line">          controllerContext.partitionsBeingReassigned.put(topicAndPartition, reassignedPartitionContext)</div><div class="line">          <span class="comment">// mark topic ineligible for deletion for the partitions being reassigned</span></div><div class="line">          <span class="comment">//note: 设置正在迁移的副本为不能删除</span></div><div class="line">          deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">          <span class="comment">//note: 进行副本迁移</span></div><div class="line">          onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</div><div class="line">        &#125;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Attempt to reassign partition %s that doesn't exist"</span></div><div class="line">        .format(topicAndPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error completing reassignment of partition %s"</span>.format(topicAndPartition), e)</div><div class="line">    <span class="comment">// remove the partition from the admin path to unblock the admin client</span></div><div class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于副本迁移流程初始化如下：</p>
<ol>
<li>通过 <code>watchIsrChangesForReassignedPartition()</code> 方法监控这个 Partition 的 LeaderAndIsr 变化，如果有新的副本数据同步完成，那么 leader 会将其加到 isr 中更新到 zk 中，这时候 Controller 是可以接收到相关的信息通知的；</li>
<li>将正在迁移的 Partition 添加到 partitionsBeingReassigned 中，它会记录当前正在迁移的 Partition 列表；</li>
<li>将要迁移的 Topic 设置为非法删除删除状态，在这个状态的 Topic 是无法进行删除的；</li>
<li>调用 <code>onPartitionReassignment()</code>，进行副本迁移。</li>
</ol>
<p>在第一步中，会向这个 Partition 注册一个额外的监听器，监听其 LeaderAndIsr 信息变化，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: ISR 变动的监听器（这个不是由 leader 主动触发的，而是 controller 自己触发的，主要用于 partition 迁移时，isr 变动的监听处理）</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionsIsrChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</span></span></div><div class="line">                                            reassignedReplicas: <span class="type">Set</span>[<span class="type">Int</span>]) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> &#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controller.controllerContext.zkUtils</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"ReassignedPartitionsIsrChangeListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when some partitions need to move leader to preferred replica</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      debug(<span class="string">"Reassigned partitions isr change listener fired for path %s with children %s"</span>.format(dataPath, data))</div><div class="line">      <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// check if this partition is still being reassigned or not</span></div><div class="line">        <span class="comment">//note: 检查这个副本是不是还在迁移中（这个方法只用于副本迁移中）</span></div><div class="line">        controllerContext.partitionsBeingReassigned.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(reassignedPartitionContext) =&gt;</div><div class="line">            <span class="comment">// need to re-read leader and isr from zookeeper since the zkclient callback doesn't return the Stat object</span></div><div class="line">            <span class="comment">//note: 从 zk 获取最新的 leader 和 isr 信息</span></div><div class="line">            <span class="keyword">val</span> newLeaderAndIsrOpt = zkUtils.getLeaderAndIsrForPartition(topic, partition)</div><div class="line">            newLeaderAndIsrOpt <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Some</span>(leaderAndIsr) =&gt; <span class="comment">// check if new replicas have joined ISR</span></div><div class="line">                <span class="keyword">val</span> caughtUpReplicas = reassignedReplicas &amp; leaderAndIsr.isr.toSet</div><div class="line">                <span class="keyword">if</span>(caughtUpReplicas == reassignedReplicas) &#123; <span class="comment">//note: 新分配的副本已经全部在 isr 中了</span></div><div class="line">                  <span class="comment">// resume the partition reassignment process</span></div><div class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></div><div class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</div><div class="line">                    <span class="string">"Resuming partition reassignment"</span>)</div><div class="line">                  <span class="comment">//note: 再次触发 onPartitionReassignment 方法,副本已经迁移完成</span></div><div class="line">                  controller.onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span> &#123;  <span class="comment">//note: 否则不进行任何处理</span></div><div class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></div><div class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</div><div class="line">                    <span class="string">"Replica(s) %s still need to catch up"</span>.format((reassignedReplicas -- leaderAndIsr.isr.toSet).mkString(<span class="string">","</span>)))</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">None</span> =&gt; error(<span class="string">"Error handling reassignment of partition %s to replicas %s as it was never created"</span></div><div class="line">                .format(topicAndPartition, reassignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">            &#125;</div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling partition reassignment"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果该 Partition 的 LeaderAndIsr 信息有变动，那么就会触发这个 listener 的 <code>doHandleDataChange()</code> 方法：</p>
<ol>
<li>首先检查这个 Partition 是否在还在迁移中，不在的话直接结束流程，因为这个监听器本来就是为了 Partition 副本迁移而服务的；</li>
<li>从 zk 获取最新的 leader 和 isr 信息，如果新分配的副本全部都在 isr 中，那么就再次触发 controller 的 <code>onPartitionReassignment()</code> 方法，再次调用时实际上已经证明了这个 Partition 的副本迁移已经完成，否则的话就会不进行任何处理，等待新分配的所有副本迁移完成。</li>
</ol>
<h2 id="副本迁移"><a href="#副本迁移" class="headerlink" title="副本迁移"></a>副本迁移</h2><p>Partition 副本迁移真正实际处理是在 Controller 的 <code>onPartitionReassignment()</code> 方法完成的，在看这个方法之前，先介绍几个基本的概念（假设一个 Partition 原来的 replica 是 {1、2、3}，新分配的副本列表是：{2、3、4}）：</p>
<ul>
<li>RAR = Reassigned replicas，即新分配的副本列表，也就是 {2、3、4}；</li>
<li>OAR = Original list of replicas for partition，即这个 Partition 原来的副本列表，也就是 {1、2、3}；</li>
<li>AR = current assigned replicas，该 Partition 当前的副本列表，这个会随着阶段的不同而变化；</li>
<li>RAR-OAR：需要创建、数据同步的新副本，也就是 {4}；</li>
<li>OAR-RAR：不需要创建、数据同步的副本，也就是{2、3}</li>
</ul>
<p>这个方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个回调方法被 reassigned partitions listener 触发,当需要进行分区副本迁移时,会在【/admin/reassign_partitions】下创建一个节点来触发操作</span></div><div class="line"><span class="comment">//note: RAR: 重新分配的副本, OAR: 这个分区原来的副本列表, AR: 当前的分配的副本</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPartitionReassignment</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</div><div class="line">  <span class="keyword">val</span> reassignedReplicas = reassignedPartitionContext.newReplicas</div><div class="line">  <span class="keyword">if</span> (!areReplicasInIsr(topicAndPartition.topic, topicAndPartition.partition, reassignedReplicas)) &#123;</div><div class="line">    <span class="comment">//note: 新分配的并没有权限在 isr 中</span></div><div class="line">    info(<span class="string">"New replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</div><div class="line">      <span class="string">"reassigned not yet caught up with the leader"</span>)</div><div class="line">    <span class="comment">//note: RAR-OAR</span></div><div class="line">    <span class="keyword">val</span> newReplicasNotInOldReplicaList = reassignedReplicas.toSet -- controllerContext.partitionReplicaAssignment(topicAndPartition).toSet</div><div class="line">    <span class="comment">//note: RAR+OAR</span></div><div class="line">    <span class="keyword">val</span> newAndOldReplicas = (reassignedPartitionContext.newReplicas ++ controllerContext.partitionReplicaAssignment(topicAndPartition)).toSet</div><div class="line">    <span class="comment">//1. Update AR in ZK with OAR + RAR.</span></div><div class="line">    updateAssignedReplicasForPartition(topicAndPartition, newAndOldReplicas.toSeq)</div><div class="line">    <span class="comment">//2. Send LeaderAndIsr request to every replica in OAR + RAR (with AR as OAR + RAR).</span></div><div class="line">    updateLeaderEpochAndSendRequest(topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition),</div><div class="line">      newAndOldReplicas.toSeq)</div><div class="line">    <span class="comment">//3. replicas in RAR - OAR -&gt; NewReplica</span></div><div class="line">    <span class="comment">//note: 新分配的副本状态更新为 NewReplica（在第二步中发送 LeaderAndIsr 请求时,新的副本会开始创建并且同步数据）</span></div><div class="line">    startNewReplicasForReassignedPartition(topicAndPartition, reassignedPartitionContext, newReplicasNotInOldReplicaList)</div><div class="line">    info(<span class="string">"Waiting for new replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</div><div class="line">      <span class="string">"reassigned to catch up with the leader"</span>)</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 新副本全在 isr 中了</span></div><div class="line">    <span class="comment">//4. Wait until all replicas in RAR are in sync with the leader.</span></div><div class="line">   <span class="comment">//note: 【OAR-RAR】</span></div><div class="line">    <span class="keyword">val</span> oldReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).toSet -- reassignedReplicas.toSet</div><div class="line">    <span class="comment">//5. replicas in RAR -&gt; OnlineReplica</span></div><div class="line">    <span class="comment">//note: RAR 中的副本都在 isr 中了,将副本状态设置为 OnlineReplica</span></div><div class="line">    reassignedReplicas.foreach &#123; replica =&gt;</div><div class="line">      replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="keyword">new</span> <span class="type">PartitionAndReplica</span>(topicAndPartition.topic, topicAndPartition.partition,</div><div class="line">        replica)), <span class="type">OnlineReplica</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//6. Set AR to RAR in memory.</span></div><div class="line">    <span class="comment">//7. Send LeaderAndIsr request with a potential new leader (if current leader not in RAR) and</span></div><div class="line">    <span class="comment">//   a new AR (using RAR) and same isr to every broker in RAR</span></div><div class="line">    <span class="comment">//note: 到这一步,新加入的 replica 已经同步完成,leader和isr都更新到最新的结果</span></div><div class="line">    moveReassignedPartitionLeaderIfRequired(topicAndPartition, reassignedPartitionContext)</div><div class="line">    <span class="comment">//8. replicas in OAR - RAR -&gt; Offline (force those replicas out of isr)</span></div><div class="line">    <span class="comment">//9. replicas in OAR - RAR -&gt; NonExistentReplica (force those replicas to be deleted)</span></div><div class="line">    <span class="comment">//note: 下线旧的副本</span></div><div class="line">    stopOldReplicasOfReassignedPartition(topicAndPartition, reassignedPartitionContext, oldReplicas)</div><div class="line">    <span class="comment">//10. Update AR in ZK with RAR.</span></div><div class="line">    updateAssignedReplicasForPartition(topicAndPartition, reassignedReplicas)</div><div class="line">    <span class="comment">//11. Update the /admin/reassign_partitions path in ZK to remove this partition.</span></div><div class="line">    <span class="comment">//note: partition 迁移完成,从待迁移的集合中移除该 Partition</span></div><div class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</div><div class="line">    info(<span class="string">"Removed partition %s from the list of reassigned partitions in zookeeper"</span>.format(topicAndPartition))</div><div class="line">    controllerContext.partitionsBeingReassigned.remove(topicAndPartition)</div><div class="line">    <span class="comment">//12. After electing leader, the replicas and isr information changes, so resend the update metadata request to every broker</span></div><div class="line">    <span class="comment">//note: 发送 metadata 更新请求给所有存活的 broker</span></div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, <span class="type">Set</span>(topicAndPartition))</div><div class="line">    <span class="comment">// signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed</span></div><div class="line">    <span class="comment">//note: topic 删除恢复（如果当前 topic 设置了删除,之前由于无法删除）</span></div><div class="line">    deleteTopicManager.resumeDeletionForTopics(<span class="type">Set</span>(topicAndPartition.topic))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法整体分为以下12个步骤：</p>
<ol>
<li>把 AR = OAR+RAR （{1、2、3、4}）更新到 zk 及本地 Controller 缓存中;</li>
<li>发送 LeaderAndIsr 给 AR 中每一个副本,并且会强制更新 zk 中 leader 的 epoch;</li>
<li>创建需要新建的副本（【RAR-OAR】，即 {4}）,将其状态设置为 NewReplica；</li>
<li>等待直到 RAR（{2、3、4}） 中的所有副本都在 ISR 中;</li>
<li>把 RAR（{2、3、4}） 中的所有副本设置为 OnReplica 状态;</li>
<li>将缓存中 AR 更新为 RAR（重新分配的副本列表，即 {2、3、4}）;</li>
<li>如果 leader 不在 RAR 中, 就从 RAR 选择对应的 leader, 然后发送 LeaderAndIsr 请求；如果不需要，那么只会更新 leader epoch，然后发送 LeaderAndIsr 请求; 在发送 LeaderAndIsr 请求前设置了 AR=RAR, 这将确保了 leader 在 isr 中不会添加任何 【RAR-OAR】中的副本（old replica，即 {1}）；</li>
<li>将【OAR-RAR】（{1}）中的副本设置为 OfflineReplica 状态，OfflineReplica 状态的变化，将会从 ISR 中删除【OAR-RAR】的副本，更新到 zk 中并发送 LeaderAndIsr 请求给 leader，通知 leader isr 变动。之后再发送 StopReplica 请求（delete=false）给【OAR-RAR】中的副本；</li>
<li>将【OAR-RAR】中的副本设置为 NonExistentReplica 状态。这将发送 StopReplica 请求（delete=true）给【OAR-RAR】中的副本，这些副本将会从本地上删除数据；</li>
<li>在 zk 中更新 AR 为 RAR；</li>
<li>更新 zk 中路径 【/admin/reassign_partitions】信息，移除已经成功迁移的 Partition；</li>
<li>leader 选举之后，这个 replica 和 isr 信息将会变动，发送 metadata 更新给所有的 broker。</li>
</ol>
<p>上面的流程简单来说，就是先创建新的 replica，开始同步数据，等待所有新的分配都加入到了 isr 中后，开始进行 leader 选举（需要的情况下），下线不需要的副本（OAR-RAR），下线完成后将 Partition 的最新 AR （即 RAR）信息更新到 zk 中，最后发送相应的请求给 broker，到这里一个 Partition 的副本迁移算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Co
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本状态机与分区状态机（十七）</title>
    <link href="http://matt33.com/2018/06/16/controller-state-machine/"/>
    <id>http://matt33.com/2018/06/16/controller-state-machine/</id>
    <published>2018-06-16T03:04:14.000Z</published>
    <updated>2018-08-18T05:49:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副本状态机则是记录着当前集群所有 Replica 的状态信息以及如何对 Replica 状态转变进行相应的处理。</p>
<h2 id="ReplicaStateMachine"><a href="#ReplicaStateMachine" class="headerlink" title="ReplicaStateMachine"></a>ReplicaStateMachine</h2><p>ReplicaStateMachine 记录着集群所有 Replica 的状态信息，它决定着一个 replica 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中副本的状态总共有以下七种类型：</p>
<ol>
<li>NewReplica：这种状态下 Controller 可以创建这个 Replica，这种状态下该 Replica 只能作为 follower，它可以是 Replica 删除后的一个临时状态，它有效的前置状态是 NonExistentReplica；</li>
<li>OnlineReplica：一旦这个 Replica 被分配到指定的 Partition 上，并且 Replica 创建完成，那么它将会被置为这个状态，在这个状态下，这个 Replica 既可以作为 leader 也可以作为 follower，它有效的前置状态是  NewReplica、OnlineReplica 或 OfflineReplica；</li>
<li>OfflineReplica：如果一个 Replica 挂掉（所在的节点宕机或者其他情况），该 Replica 将会被转换到这个状态，它有的效前置状态是 NewReplica、OfflineReplica 或者 OnlineReplica；</li>
<li>ReplicaDeletionStarted：Replica 开始删除时被置为的状态，它有效的前置状态是 OfflineReplica；</li>
<li>ReplicaDeletionSuccessful：如果 Replica 在删除时没有遇到任何错误信息，它将被置为这个状态，这个状态代表该 Replica 的数据已经从节点上清除了，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>ReplicaDeletionIneligible：如果 Replica 删除失败，它将会转移到这个状态，这个状态意思是非法删除，也就是删除是无法成功的，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>NonExistentReplica：如果 Replica 删除成功，它将被转移到这个状态，它有效的前置状态是：ReplicaDeletionSuccessful。</li>
</ol>
<p>上面的状态中其中后面4是专门为 Replica 删除而服务的，副本状态机转移图如下所示：</p>
<p><img src="/images/kafka/replica_state.png" alt="副本状态机"></p>
<p>这张图是副本状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 ReplicaStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="ReplicaStateMachine-初始化"><a href="#ReplicaStateMachine-初始化" class="headerlink" title="ReplicaStateMachine 初始化"></a>ReplicaStateMachine 初始化</h3><p>副本状态机初始化的过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 重新选举后触发的操作</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// initialize replica state</span></div><div class="line">  <span class="comment">//note: 初始化 zk 上所有的 Replica 状态信息（replica 存活的话设置为 Online,不存活的设置为 ReplicaDeletionIneligible）</span></div><div class="line">  initializeReplicaState()</div><div class="line">  <span class="comment">// set started flag</span></div><div class="line">  hasStarted.set(<span class="literal">true</span>)</div><div class="line">  <span class="comment">// move all Online replicas to Online</span></div><div class="line">  <span class="comment">//note: 将存活的副本状态转变为 OnlineReplica</span></div><div class="line">  handleStateChanges(controllerContext.allLiveReplicas(), <span class="type">OnlineReplica</span>)</div><div class="line"></div><div class="line">  info(<span class="string">"Started replica state machine with initial state -&gt; "</span> + replicaState.toString())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，ReplicaStateMachine 先调用 <code>initializeReplicaState()</code> 方法初始化集群中所有 Replica 的状态信息，如果 Replica 所在机器是 alive 的，那么将其状态设置为 OnlineReplica，否则设置为 ReplicaDeletionIneligible 状态，这里只是将 Replica 的状态信息更新副本状态机的缓存 <code>replicaState</code> 中，并没有真正进行状态转移的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化所有副本的状态信息</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeReplicaState</span></span>() &#123;</div><div class="line">  <span class="keyword">for</span>((topicPartition, assignedReplicas) &lt;- controllerContext.partitionReplicaAssignment) &#123;</div><div class="line">    <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">    <span class="keyword">val</span> partition = topicPartition.partition</div><div class="line">    assignedReplicas.foreach &#123; replicaId =&gt;</div><div class="line">      <span class="keyword">val</span> partitionAndReplica = <span class="type">PartitionAndReplica</span>(topic, partition, replicaId)</div><div class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(replicaId)) <span class="comment">//note: 如果副本是存活,那么将状态都设置为 OnlineReplica</span></div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">// mark replicas on dead brokers as failed for topic deletion, if they belong to a topic to be deleted.</span></div><div class="line">        <span class="comment">// This is required during controller failover since during controller failover a broker can go down,</span></div><div class="line">        <span class="comment">// so the replicas on that broker should be moved to ReplicaDeletionIneligible to be on the safer side.</span></div><div class="line">        <span class="comment">//note: 将不存活的副本状态设置为 ReplicaDeletionIneligible</span></div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接着第二步调用 <code>handleStateChanges()</code> 将所有存活的副本状态转移为 OnlineReplica 状态，这里才是真正进行状态转移的地方，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 用于处理 Replica 状态的变化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</div><div class="line">                       callbacks: <span class="type">Callbacks</span> = (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build) &#123;</div><div class="line">  <span class="keyword">if</span>(replicas.nonEmpty) &#123;</div><div class="line">    info(<span class="string">"Invoking state change to %s for replicas %s"</span>.format(targetState, replicas.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      brokerRequestBatch.newBatch()</div><div class="line">      <span class="comment">//note: 状态转变</span></div><div class="line">      replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks))</div><div class="line">      <span class="comment">//note: 向 broker 发送相应请求</span></div><div class="line">      brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">    &#125;<span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some replicas to %s state"</span>.format(targetState), e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里是副本状态机 <code>startup()</code> 方法的最后一步，它的目的是将所有 alive 的 Replica 状态转移到 OnlineReplica 状态，由于前面已经这些 alive replica 的状态设置成了 OnlineReplica，所以这里 Replica 的状态转移情况是：<strong>OnlineReplica –&gt; OnlineReplica</strong>，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="副本的状态转移"><a href="#副本的状态转移" class="headerlink" title="副本的状态转移"></a>副本的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewReplica、ReplicaDeletionStarted、ReplicaDeletionIneligible、ReplicaDeletionSuccessful、NonExistentReplica、OnlineReplica 或者 OfflineReplica 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewReplica"><a href="#TargetState-NewReplica" class="headerlink" title="TargetState: NewReplica"></a>TargetState: NewReplica</h4><p>NewReplica 这个状态是 Replica 准备开始创建是的一个状态，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> currState = replicaState.getOrElseUpdate(partitionAndReplica, <span class="type">NonExistentReplica</span>)<span class="comment">//note: Replica 不存在的话,状态初始化为 NonExistentReplica</span></div><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">NonExistentReplica</span>), targetState)<span class="comment">//note: 验证</span></div><div class="line"><span class="comment">// start replica as a follower to the current leader for its partition</span></div><div class="line"><span class="comment">//note: 从 zk 获取 Partition 的 leaderAndIsr 信息</span></div><div class="line"><span class="keyword">val</span> leaderIsrAndControllerEpochOpt = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topic, partition)</div><div class="line">leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">    <span class="keyword">if</span>(leaderIsrAndControllerEpoch.leaderAndIsr.leader == replicaId)<span class="comment">//note: 这个状态的 Replica 不能作为 leader</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Replica %d for partition %s cannot be moved to NewReplica"</span></div><div class="line">        .format(replicaId, topicAndPartition) + <span class="string">"state as it is being requested to become leader"</span>)</div><div class="line">    <span class="comment">//note: 向该 replicaId 发送 LeaderAndIsr 请求,这个方法同时也会向所有的 broker 发送 updateMeta 请求</span></div><div class="line">    brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId),</div><div class="line">                                                        topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">                                                        replicaAssignment)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// new leader request will be sent to this replica when one gets elected</span></div><div class="line">&#125;</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">NewReplica</span>)<span class="comment">//note: 缓存这个 replica 对象的状态</span></div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                  targetState))</div></pre></td></tr></table></figure>
<p>当想要把 Replica 的状态转移为 NewReplica 时，副本状态机的处理逻辑如下：</p>
<ol>
<li>校验 Replica 的前置状态，只有处于 NonExistentReplica 状态的副本才能转移到 NewReplica 状态；</li>
<li>从 zk 中获取该 Topic-Partition 的 LeaderIsrAndControllerEpoch 信息；</li>
<li>如果获取不到上述信息，直接将该 Replica 的状态转移成 NewReplica，然后结束流程（对与新建的 Partition，处于这个状态时，该 Partition 是没有相应的 LeaderAndIsr 信息的）；</li>
<li>获取到 Partition 的 LeaderIsrAndControllerEpoch 信息，如果发现该 Partition 的 leader 是当前副本，那么就抛出 StateChangeFailedException 异常，因为处在这个状态的 Replica 是不能被选举为 leader 的；</li>
<li>获取到了 Partition 的 LeaderIsrAndControllerEpoch 信息，并且该 Partition 的 leader 不是当前 replica，那么向该 Partition 的所有 Replica 添加一个 LeaderAndIsr 请求（添加 LeaderAndIsr 请求时，实际上也会向所有的 Broker 都添加一个 Update-Metadata 请求）；</li>
<li>最后将该 Replica 的状态转移成 NewReplica，然后结束流程。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionStarted"><a href="#TargetState-ReplicaDeletionStarted" class="headerlink" title="TargetState: ReplicaDeletionStarted"></a>TargetState: ReplicaDeletionStarted</h4><p>这是 Replica 开始删除时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">OfflineReplica</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionStarted</span>)</div><div class="line"><span class="comment">// send stop replica command</span></div><div class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,并设置 deletePartition=true</span></div><div class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">true</span>,</div><div class="line">  callbacks.stopReplicaResponseCallback)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>这部分的实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 OfflineReplica 的情况下才能转移到这种状态；</li>
<li>更新向该 Replica 的状态为 ReplicaDeletionStarted；</li>
<li>向该 replica 发送 StopReplica 请求（deletePartition = true），收到这请求后，broker 会从物理存储上删除这个 Replica 的数据内容；</li>
<li>如果请求返回的话会触发其回调函数（这部分会在 topic 删除部分讲解）。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionIneligible"><a href="#TargetState-ReplicaDeletionIneligible" class="headerlink" title="TargetState: ReplicaDeletionIneligible"></a>TargetState: ReplicaDeletionIneligible</h4><p>ReplicaDeletionIneligible 是副本删除失败时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionIneligible。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionSuccessful"><a href="#TargetState-ReplicaDeletionSuccessful" class="headerlink" title="TargetState: ReplicaDeletionSuccessful"></a>TargetState: ReplicaDeletionSuccessful</h4><p>ReplicaDeletionSuccessful 是副本删除成功时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionSuccessful。</li>
</ol>
<h4 id="TargetState-NonExistentReplica"><a href="#TargetState-NonExistentReplica" class="headerlink" title="TargetState: NonExistentReplica"></a>TargetState: NonExistentReplica</h4><p>NonExistentReplica 是副本完全删除、不存在这个副本的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionSuccessful</span>), targetState)</div><div class="line"><span class="comment">// remove this replica from the assigned replicas list for its partition</span></div><div class="line"><span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line"><span class="comment">//note: 从 controller 和副本状态机的缓存中清除这个 Replica 的记录西溪</span></div><div class="line">controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas.filterNot(_ == replicaId))</div><div class="line">replicaState.remove(partitionAndReplica)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionSuccessful 下才能转移这种状态；</li>
<li>在 controller 的 partitionReplicaAssignment 删除这个 Partition 对应的 replica 信息；</li>
<li>从 Controller 和副本状态机中将这个 Topic 从缓存中删除。</li>
</ol>
<h4 id="TargetState-OnlineReplica"><a href="#TargetState-OnlineReplica" class="headerlink" title="TargetState: OnlineReplica"></a>TargetState: OnlineReplica</h4><p>OnlineReplica 是副本正常工作时的状态，此时的 Replica 既可以作为 leader 也可以作为 follower，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica,</div><div class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line">replicaState(partitionAndReplica) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NewReplica</span> =&gt; <span class="comment">//note: NewReplica --&gt; OnlineReplica</span></div><div class="line">    <span class="comment">// add this replica to the assigned replicas list for its partition</span></div><div class="line">    <span class="comment">//note: 向 the assigned replicas list 添加这个 replica（正常情况下这些 replicas 已经更新到 list 中了）</span></div><div class="line">    <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">if</span>(!currentAssignedReplicas.contains(replicaId))</div><div class="line">      controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas :+ replicaId)</div><div class="line">    stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                              .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                      targetState))</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">//note: OnlineReplica/OfflineReplica/ReplicaDeletionIneligible --&gt; OnlineReplica</span></div><div class="line">    <span class="comment">// check if the leader for this partition ever existed</span></div><div class="line">    <span class="comment">//note: 如果该 Partition 的 LeaderIsrAndControllerEpoch 信息存在,那么就更新副本的状态,并发送相应的请求</span></div><div class="line">    controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">        brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">          replicaAssignment)</div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div><div class="line">        stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// that means the partition was never in OnlinePartition state, this means the broker never</span></div><div class="line">        <span class="comment">// started a log for that partition and does not have a high watermark value for this partition</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div></pre></td></tr></table></figure>
<p>从前面的状态转移图中可以看出，当 Replica 处在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，Replica 是可以转移到 OnlineReplica 状态的，下面分两种情况讲述：</p>
<p><strong>NewReplica –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionReplicaAssignment 中获取这个 Partition 的 AR；</li>
<li>如果 Replica 不在 AR 中的话，那么就将其添加到 Partition 的 AR 中；</li>
<li>最后将 Replica 的状态设置为 OnlineReplica 状态。</li>
</ol>
<p><strong>OnlineReplica/OfflineReplica/ReplicaDeletionIneligible –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionLeadershipInfo 中获取 Partition 的 LeaderAndIsr 信息；</li>
<li>如果该信息存在，那么就向这个 Replica 所在 broker 添加这个 Partition 的 LeaderAndIsr 请求，并将 Replica 的状态设置为 OnlineReplica 状态；</li>
<li>否则不做任务处理；</li>
<li>最后更新R Replica 的状态为 OnlineReplica。</li>
</ol>
<h4 id="TargetState-OfflineReplica"><a href="#TargetState-OfflineReplica" class="headerlink" title="TargetState: OfflineReplica"></a>TargetState: OfflineReplica</h4><p>OfflineReplica 是 Replica 所在 Broker 掉线时 Replica 的状态，转移到这种状态的处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica,</div><div class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line"><span class="comment">// send stop replica command to the replica so that it stops fetching from the leader</span></div><div class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,先停止副本同步</span></div><div class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">false</span>)</div><div class="line"><span class="comment">// As an optimization, the controller removes dead replicas from the ISR</span></div><div class="line"><span class="keyword">val</span> leaderAndIsrIsEmpty: <span class="type">Boolean</span> =</div><div class="line">  controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</div><div class="line">      controller.removeReplicaFromIsr(topic, partition, replicaId) <span class="keyword">match</span> &#123; <span class="comment">//note: 从 isr 中移除这个副本（前提是 ISR 有其他有效副本）</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(updatedLeaderIsrAndControllerEpoch) =&gt;</div><div class="line">          <span class="comment">// send the shrunk ISR state change request to all the remaining alive replicas of the partition.</span></div><div class="line">          <span class="comment">//note: 发送 LeaderAndIsr 请求给剩余的其他副本,因为 ISR 变动了</span></div><div class="line">          <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">          <span class="keyword">if</span> (!controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition)) &#123;</div><div class="line">            brokerRequestBatch.addLeaderAndIsrRequestForBrokers(currentAssignedReplicas.filterNot(_ == replicaId),</div><div class="line">              topic, partition, updatedLeaderIsrAndControllerEpoch, replicaAssignment)</div><div class="line">          &#125;</div><div class="line">          replicaState.put(partitionAndReplica, <span class="type">OfflineReplica</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">            .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div><div class="line">          <span class="literal">false</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="literal">true</span></div><div class="line">  &#125;</div><div class="line"><span class="keyword">if</span> (leaderAndIsrIsEmpty &amp;&amp; !controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition))</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(</div><div class="line">    <span class="string">"Failed to change state of replica %d for partition %s since the leader and isr path in zookeeper is empty"</span></div><div class="line">    .format(replicaId, topicAndPartition))</div></pre></td></tr></table></figure>
<p>处理逻辑如下：</p>
<ol>
<li>校验其前置状态，只有 Replica 在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，才能转移到这种状态；</li>
<li>向该 Replica 所在节点发送 StopReplica 请求（deletePartition = false）；</li>
<li>调用 Controller 的 <code>removeReplicaFromIsr()</code> 方法将该 replica 从 Partition 的 isr 移除这个 replica（前提 isr 中还有其他有效副本），然后向该 Partition 的其他副本发送 LeaderAndIsr 请求；</li>
<li>更新这个 Replica 的状态为 OfflineReplica。</li>
</ol>
<h3 id="状态转移触发的条件"><a href="#状态转移触发的条件" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Replica 各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onBrokerStartup()</td>
<td>Broker 启动时，目的是将在该节点的 Replica 状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>新建 Partition 时，Replica 初始化及 Partition 状态变成 OnlinePartition 后，新创建的 Replica 状态也变为 OnlineReplica；</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onPartitionReassignment()</td>
<td>副本迁移完成后，RAR 中的副本设置为 OnlineReplica 状态</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>ReplicaStateMachine 的 startup()</td>
<td>副本状态机刚初始化启动时，将存活的副本状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的  markTopicForDeletionRetry()</td>
<td>将删除失败的 Replica 设置为 OfflineReplica，重新进行删除</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的 startReplicaDeletion()</td>
<td>开始副本删除时，先将副本设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 shutdownBroker() 方法</td>
<td>优雅关闭 broker 时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 onBrokerFailure()</td>
<td>broker 掉线时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>Partition 新建时，当 Partition 状态变为 NewPartition 后，副本的状态变为 NewReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 startNewReplicasForReassignedPartition()</td>
<td>Partition 副本迁移时，将新分配的副本状态设置为 NewReplica；</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>下线副本时，将成功设置为 OfflineReplica 的 Replica 设置为 ReplicaDeletionStarted 状态，开始物理上删除副本数据（也是发送 StopReplica）</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，目的是下线那些 old replica，新的 replica 已经迁移到新分配的副本上了</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>物理将数据成功删除的 Replica 状态会变为这个</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，在下线那些旧 Replica 时的一个状态，删除成功</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>开始副本删除时，删除失败的副本会设置成这个状态</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 副本迁移时，在下线那些旧的 Replica 时的一个状态，删除失败</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>副本删除成功后（状态为 ReplicaDeletionSuccessful），从状态机和 Controller 的缓存中清除该副本的记录；</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本成功迁移、旧副本成功删除后，从状态机和 Controller 的缓存中清除旧副本的记录</td>
</tr>
</tbody>
</table>
<h2 id="PartitionStateMachine"><a href="#PartitionStateMachine" class="headerlink" title="PartitionStateMachine"></a>PartitionStateMachine</h2><p>PartitionStateMachine 记录着集群所有 Partition 的状态信息，它决定着一个 Partition 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中 Partition 的状态总共有以下四种类型：</p>
<ol>
<li>NonExistentPartition：这个代表着这个 Partition 之前没有被创建过或者之前创建了现在又被删除了，它有效的前置状态是 OfflinePartition；</li>
<li>NewPartition：Partition 创建后，它将处于这个状态，这个状态的 Partition 还没有 leader 和 isr，它有效的前置状态是 NonExistentPartition；</li>
<li>OnlinePartition：一旦这个 Partition 的 leader 被选举出来了，它将处于这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition；</li>
<li>OfflinePartition：如果这个 Partition 的 leader 掉线，这个 Partition 将被转移到这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition。</li>
</ol>
<p>分区状态机转移图如下所示：</p>
<p><img src="/images/kafka/partition_state.png" alt="分区状态机"></p>
<p>这张图是分区状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 PartitionStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="PartitionStateMachine-初始化"><a href="#PartitionStateMachine-初始化" class="headerlink" title="PartitionStateMachine 初始化"></a>PartitionStateMachine 初始化</h3><p>PartitionStateMachine 的初始化方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 启动时触发</span></div><div class="line"><span class="comment">//note: 初始化所有 Partition 的状态（从 zk 获取）, 然后对于 new/offline Partition 触发选主（选主成功的话,变为 OnlinePartition）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// initialize partition state</span></div><div class="line">  <span class="comment">//note: 初始化 partition 的状态,如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></div><div class="line">  initializePartitionState()</div><div class="line">  <span class="comment">// set started flag</span></div><div class="line">  hasStarted.set(<span class="literal">true</span>)</div><div class="line">  <span class="comment">// try to move partitions to online state</span></div><div class="line">  <span class="comment">//note: 为所有处理 NewPartition 或 OnlinePartition 状态 Partition 选举 leader</span></div><div class="line">  triggerOnlinePartitionStateChange()</div><div class="line"></div><div class="line">  info(<span class="string">"Started partition state machine with initial state -&gt; "</span> + partitionState.toString())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，PartitionStateMachine 先调用 <code>initializePartitionState()</code> 方法初始化集群中所有 Partition 的状态信息：</p>
<ol>
<li>如果该 Partition 有 LeaderAndIsr 信息，那么如果 Partition leader 所在的机器是 alive 的，那么将其状态设置为 OnlinePartition，否则设置为 OfflinePartition 状态；</li>
<li>如果该 Partition 没有 LeaderAndIsr 信息，那么将其状态设置为 NewPartition。</li>
</ol>
<p>这里只是将 Partition 的状态信息更新分区状态机的缓存 <code>partitionState</code> 中，并没有真正进行状态的转移。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 根据从 zk 获取的所有 Partition,进行状态初始化</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializePartitionState</span></span>() &#123;</div><div class="line">  <span class="keyword">for</span> (topicPartition &lt;- controllerContext.partitionReplicaAssignment.keys) &#123;</div><div class="line">    <span class="comment">// check if leader and isr path exists for partition. If not, then it is in NEW state</span></div><div class="line">    controllerContext.partitionLeadershipInfo.get(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(currentLeaderIsrAndEpoch) =&gt;</div><div class="line">        <span class="comment">// else, check if the leader for partition is alive. If yes, it is in Online state, else it is in Offline state</span></div><div class="line">        <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(currentLeaderIsrAndEpoch.leaderAndIsr.leader))</div><div class="line">          <span class="comment">// leader is alive</span></div><div class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,并且 leader 存活,设置为 OnlinePartition 状态</span></div><div class="line">          partitionState.put(topicPartition, <span class="type">OnlinePartition</span>)</div><div class="line">        <span class="keyword">else</span></div><div class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,但是 leader 不存活,设置为 OfflinePartition 状态</span></div><div class="line">          partitionState.put(topicPartition, <span class="type">OfflinePartition</span>)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">//note: 没有 LeaderAndIsr 信息,设置为 NewPartition 状态（这个 Partition 还没有）</span></div><div class="line">        partitionState.put(topicPartition, <span class="type">NewPartition</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在初始化的第二步，将会调用 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有的状态为 NewPartition/OnlinePartition 的 Partition 进行 leader 选举，选举成功后的话，其状态将会设置为 OnlinePartition，调用的 Leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#OfflinePartitionLeaderSelector">OfflinePartitionLeaderSelector</a>（具体实现参考链接）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是在 controller 选举后或 broker 上线或下线时时触发的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">triggerOnlinePartitionStateChange</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    brokerRequestBatch.newBatch()</div><div class="line">    <span class="comment">// try to move all partitions in NewPartition or OfflinePartition state to OnlinePartition state except partitions</span></div><div class="line">    <span class="comment">// that belong to topics to be deleted</span></div><div class="line">    <span class="comment">//note: 开始为所有状态在 NewPartition or OfflinePartition 状态的 partition 更新状态（除去将要被删除的 topic）</span></div><div class="line">    <span class="keyword">for</span>((topicAndPartition, partitionState) &lt;- partitionState</div><div class="line">        <span class="keyword">if</span> !controller.deleteTopicManager.isTopicQueuedUpForDeletion(topicAndPartition.topic)) &#123;</div><div class="line">      <span class="keyword">if</span>(partitionState.equals(<span class="type">OfflinePartition</span>) || partitionState.equals(<span class="type">NewPartition</span>))</div><div class="line">        <span class="comment">//note: 尝试为处在 OfflinePartition 或 NewPartition 状态的 Partition 选主,成功后转换为 OnlinePartition</span></div><div class="line">        handleStateChange(topicAndPartition.topic, topicAndPartition.partition, <span class="type">OnlinePartition</span>, controller.offlinePartitionSelector,</div><div class="line">                          (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 发送请求给所有的 broker,包括 LeaderAndIsr 请求和 UpdateMetadata 请求（这里只是添加到 Broker 对应的 RequestQueue 中,后台有线程去发送）</span></div><div class="line">    brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some partitions to the online state"</span>, e)</div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> It is not enough to bail out and log an error, it is important to trigger leader election for those partitions</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面方法的目的是为尝试将所有的状态为 NewPartition/OnlinePartition 的 Partition 状态转移到 OnlinePartition，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="分区的状态转移"><a href="#分区的状态转移" class="headerlink" title="分区的状态转移"></a>分区的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewPartition、OfflinePartition、NonExistentPartition 或者 OnlinePartition 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewPartition"><a href="#TargetState-NewPartition" class="headerlink" title="TargetState: NewPartition"></a>TargetState: NewPartition</h4><p>NewPartition 是 Partition 刚创建时的一个状态，其处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果该 Partition 的状态不存在,默认为 NonExistentPartition</span></div><div class="line"><span class="keyword">val</span> currState = partitionState.getOrElseUpdate(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line"><span class="comment">// pre: partition did not exist before this</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</div><div class="line">partitionState.put(topicAndPartition, <span class="type">NewPartition</span>) <span class="comment">//note: 缓存 partition 的状态</span></div><div class="line"><span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</div><div class="line">                                  assignedReplicas))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NonExistentPartition；</li>
<li>将该 Partition 的状态转移为 NewPartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-OnlinePartition"><a href="#TargetState-OnlinePartition" class="headerlink" title="TargetState: OnlinePartition"></a>TargetState: OnlinePartition</h4><p>OnlinePartition 是一个 Partition 正常工作时的状态，这个状态下的 Partition 已经成功选举出了 leader 和 isr 信息，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 判断 Partition 之前的状态是否可以转换为目的状态</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</div><div class="line">partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt; <span class="comment">//note: 新建的 Partition</span></div><div class="line">    <span class="comment">//note: 选举 leader 和 isr,更新到 zk 和 controller 中,如果没有存活的 replica,抛出异常</span></div><div class="line">    <span class="comment">// initialize leader and isr path for new partition</span></div><div class="line">    initializeLeaderAndIsrForPartition(topicAndPartition)</div><div class="line">  <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt; <span class="comment">//note: leader 挂掉的 Partition</span></div><div class="line">    <span class="comment">//note: 进行 leader 选举,更新到 zk 及 controller 缓存中,失败的抛出异常</span></div><div class="line">    electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">  <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></div><div class="line">    <span class="comment">//note:这种只有在 leader 需要重新选举时才会触发</span></div><div class="line">    electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></div><div class="line">&#125;</div><div class="line">partitionState.put(topicAndPartition, <span class="type">OnlinePartition</span>)</div><div class="line"><span class="keyword">val</span> leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s from %s to %s with leader %d"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验这个 Partition 的前置状态，有效的前置状态是：NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>如果前置状态是 NewPartition，那么为该 Partition 选举 leader 和 isr，更新到 zk 和 controller 的缓存中，如果副本没有处于 alive 状态的话，就抛出异常；</li>
<li>如果前置状态是 OnlinePartition，那么只是触发 leader 选举，在 OnlinePartition –&gt; OnlinePartition 这种状态转移时，需要传入 leader 选举的方法，触发该 Partition 的 leader 选举；</li>
<li>如果前置状态是 OfflinePartition，同上，也是触发 leader 选举。</li>
<li>更新 Partition 的状态为 OnlinePartition。</li>
</ol>
<p>对于以上这几种情况，无论前置状态是什么，最后都会触发这个 Partition 的 leader 选举，leader 成功后，都会触发向这个 Partition 的所有 replica 发送 LeaderAndIsr 请求。</p>
<h4 id="TargetState-OfflinePartition"><a href="#TargetState-OfflinePartition" class="headerlink" title="TargetState: OfflinePartition"></a>TargetState: OfflinePartition</h4><p>OfflinePartition 是这个 Partition 的 leader 挂掉时转移的一个状态，如果 Partition 转移到这个状态，那么就意味着这个 Partition 没有了可用 leader。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// pre: partition should be in New or Online state</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OfflinePartition</span>)</div><div class="line"><span class="comment">// should be called when the leader for a partition is no longer alive</span></div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">partitionState.put(topicAndPartition, <span class="type">OfflinePartition</span>)</div><div class="line"><span class="comment">// post: partition has no alive leader</span></div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 OfflinePartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-NonExistentPartition"><a href="#TargetState-NonExistentPartition" class="headerlink" title="TargetState: NonExistentPartition"></a>TargetState: NonExistentPartition</h4><p>NonExistentPartition 代表了已经处于 OfflinePartition 状态的 Partition 已经从 metadata 和 zk 中删除后进入的状态。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// pre: partition should be in Offline state</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">OfflinePartition</span>), <span class="type">NonExistentPartition</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">partitionState.put(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line"><span class="comment">// post: partition state is deleted from all brokers and zookeeper</span></div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 NonExistentPartition 状态，并且更新到缓存中。</li>
</ol>
<h3 id="状态转移触发的条件-1"><a href="#状态转移触发的条件-1" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Partition   各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 shutdownBroker()</td>
<td>优雅关闭 Broker 时调用，因为要下线的节点是 leader，所以需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 新建时，这个是在 Replica 已经变为 NewPartition 状态后进行的，为新建的 Partition 初始化 leader 和 isr</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 onPreferredReplicaElection()</td>
<td>对 Partition 进行最优 leader 选举，目的是触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 moveReassignedPartitionLeaderIfRequired()</td>
<td>分区副本迁移完成后，1. 当前的 leader 不在 RAR 中，需要触发 leader 选举；2. 当前 leader 在 RAR 但是掉线了，也需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 triggerOnlinePartitionStateChange()</td>
<td>当 Controller 重新选举出来或 broker 有变化时，目的为了那些状态为 NewPartition/OfflinePartition 的 Partition 重新选举 leader，选举成功后状态变为 OnlinePartition</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 initializePartitionState()</td>
<td>Controller 初始化时，遍历 zk 的所有的分区，如果有 LeaderAndIsr 信息并且 leader 在 alive broker 上，那么就将状态转为 OnlinePartition。</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>controller 的 onBrokerFailure()</td>
<td>当有 broker 掉线时，将 leader 在这个机器上的 Partition 设置为 OfflinePartition</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，中间会将该 Partition 的状态先转变为 OfflinePartition</td>
</tr>
<tr>
<td>NonExistentPartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，最后会将该 Partition 的状态转移为 NonExistentPartition</td>
</tr>
<tr>
<td>NewPartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 刚创建时的一个中间状态 ，此时还没选举 leader 和设置 isr 信息</td>
</tr>
</tbody>
</table>
<p>上面就是副本状态机与分区状态机的所有内容，这里只是单纯地讲述了一下这两种状态机，后续文章会开始介绍 Controller 一些其他内容，包括 Partition 迁移、Topic 新建、Topic 下线等，这些内容都会用到这篇文章讲述的内容。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 选举及服务启动流程（十六）</title>
    <link href="http://matt33.com/2018/06/15/kafka-controller-start/"/>
    <id>http://matt33.com/2018/06/15/kafka-controller-start/</id>
    <published>2018-06-15T03:04:14.000Z</published>
    <updated>2018-06-23T03:28:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>从本篇文章开始，Kafka 源码解析就正式进入了 Controller 部分，Controller 作为 Kafka Server 端一个重要的组件，它的角色类似于其他分布式系统 Master 的角色，跟其他系统不一样的是，Kafka 集群的任何一台 Broker 都可以作为 Controller，但是在一个集群中同时只会有一个 Controller 是 alive 状态。Controller 在集群中负责的事务很多，比如：集群 meta 信息的一致性保证、Partition leader 的选举、broker 上下线等都是由 Controller 来具体负责。Controller 部分的内容还是比较多的，计划分5篇左右的文章讲述，本文先来看下 Controller 的简介、Controller 的选举、Controller 选举后服务的启动流程以及 Controller 的四种不同 leader 选举机制。分区状态机、副本副本状态机以及对各种 listener 的处理将在后续的文章中展开。</p>
<h2 id="Controller-简介"><a href="#Controller-简介" class="headerlink" title="Controller 简介"></a>Controller 简介</h2><p>在于分布式系统中，总会有一个地方需要对全局 meta 做一个统一的维护，Kafka 的 Controller 就是充当这个角色的。Kafka 简单的框架图如下所示</p>
<p><img src="/images/kafka/kafka-framwoker.png" alt="Kafka架构简图"></p>
<p>Controller 是运行在 Broker 上的，任何一台 Broker 都可以作为 Controller，但是一个集群同时只能存在一个 Controller，也就意味着 Controller 与数据节点是在一起的，Controller 做的主要事情如下：</p>
<ol>
<li>Broker 的上线、下线处理；</li>
<li>新创建的 topic 或已有 topic 的分区扩容，处理分区副本的分配、leader 选举；</li>
<li>管理所有副本的状态机和分区的状态机，处理状态机的变化事件；</li>
<li>topic 删除、副本迁移、leader 切换等处理。</li>
</ol>
<h2 id="Controller-选举过程"><a href="#Controller-选举过程" class="headerlink" title="Controller 选举过程"></a>Controller 选举过程</h2><p>Kafka 的每台 Broker 在启动过程中，都会启动 Controller 服务，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  info(<span class="string">"starting"</span>)</div><div class="line">  <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">  <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">    <span class="comment">/* start kafka controller */</span></div><div class="line">    <span class="comment">//note: 启动 controller</span></div><div class="line">    kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">    kafkaController.startup()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Controller-启动"><a href="#Controller-启动" class="headerlink" title="Controller 启动"></a>Controller 启动</h3><p>Kafka Server 在启动的过程中，都会去启动 Controller 服务，Controller 启动方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 当 broker 的 controller 模块启动时触发,它比并不保证当前 broker 是 controller,它仅仅是注册 registerSessionExpirationListener 和启动 controllerElector</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() = &#123;</div><div class="line">  inLock(controllerContext.controllerLock) &#123;</div><div class="line">    info(<span class="string">"Controller starting up"</span>)</div><div class="line">    registerSessionExpirationListener() <span class="comment">// note: 注册回话失效的监听器</span></div><div class="line">    isRunning = <span class="literal">true</span></div><div class="line">    controllerElector.startup <span class="comment">//note: 启动选举过程</span></div><div class="line">    info(<span class="string">"Controller startup complete"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 在 <code>startup()</code> 方法中主要实现以下两部分功能：</p>
<ol>
<li><code>registerSessionExpirationListener()</code> 方法注册连接 zk 的超时监听器；</li>
<li><code>controllerElector.startup()</code> 方法，监听 zk 上 controller 节点的变化，并触发 controller 选举方法。</li>
</ol>
<h3 id="Controller-选举"><a href="#Controller-选举" class="headerlink" title="Controller 选举"></a>Controller 选举</h3><p>Controller 在启动时，会初始化 ZookeeperLeaderElector 对象，并调用其 <code>startup()</code> 启动相应的流程，具体过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span> </span>&#123;</div><div class="line">  inLock(controllerContext.controllerLock) &#123;</div><div class="line">    controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)</div><div class="line">    elect</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>startup()</code> 方法中，主要做了下面两件事情：</p>
<ol>
<li>监听 zk 的 <code>/controller</code> 节点的数据变化，一旦节点有变化，立刻通过 LeaderChangeListener 的方法进行相应的处理；</li>
<li><code>elect</code> 在 controller 不存在的情况下选举 controller，存在的话，就是从 zk 获取当前的 controller 节点信息。</li>
</ol>
<h4 id="Controller-选举方法-elect"><a href="#Controller-选举方法-elect" class="headerlink" title="Controller 选举方法 elect"></a>Controller 选举方法 elect</h4><p>ZookeeperLeaderElector 的 <code>elect</code> 方法实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 从 zk 获取当前的 controller 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getControllerID</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  controllerContext.zkUtils.readDataMaybeNull(electionPath)._1 <span class="keyword">match</span> &#123;</div><div class="line">     <span class="keyword">case</span> <span class="type">Some</span>(controller) =&gt; <span class="type">KafkaController</span>.parseControllerId(controller)</div><div class="line">     <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="number">-1</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 进行 controller 选举</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">elect</span></span>: <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="keyword">val</span> timestamp = time.milliseconds.toString</div><div class="line">  <span class="keyword">val</span> electString = <span class="type">Json</span>.encode(<span class="type">Map</span>(<span class="string">"version"</span> -&gt; <span class="number">1</span>, <span class="string">"brokerid"</span> -&gt; brokerId, <span class="string">"timestamp"</span> -&gt; timestamp))</div><div class="line"></div><div class="line"> leaderId = getControllerID</div><div class="line">  <span class="comment">/*</span></div><div class="line">   * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,</div><div class="line">   * it's possible that the controller has already been elected when we get here. This check will prevent the following</div><div class="line">   * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.</div><div class="line">   */</div><div class="line">  <span class="keyword">if</span>(leaderId != <span class="number">-1</span>) &#123;</div><div class="line">     debug(<span class="string">"Broker %d has been elected as leader, so stopping the election process."</span>.format(leaderId))</div><div class="line">     <span class="keyword">return</span> amILeader</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,</div><div class="line">                                                    electString,</div><div class="line">                                                    controllerContext.zkUtils.zkConnection.getZookeeper,</div><div class="line">                                                    <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">    zkCheckedEphemeral.create() <span class="comment">//note: 没有异常的话就是创建成功了</span></div><div class="line">    info(brokerId + <span class="string">" successfully elected as leader"</span>)</div><div class="line">    leaderId = brokerId</div><div class="line">    onBecomingLeader() <span class="comment">//note: 成为了 controller</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt; <span class="comment">//note: 在创建时,发现已经有 broker 提前注册成功</span></div><div class="line">      <span class="comment">// If someone else has written the path, then</span></div><div class="line">      leaderId = getControllerID</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (leaderId != <span class="number">-1</span>)</div><div class="line">        debug(<span class="string">"Broker %d was elected as leader instead of broker %d"</span>.format(leaderId, brokerId))</div><div class="line">      <span class="keyword">else</span></div><div class="line">        warn(<span class="string">"A leader has been elected but just resigned, this will result in another round of election"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> e2: <span class="type">Throwable</span> =&gt; <span class="comment">//note: 抛出了其他异常，那么重新选举 controller</span></div><div class="line">      error(<span class="string">"Error while electing or becoming leader on broker %d"</span>.format(brokerId), e2)</div><div class="line">      resign()</div><div class="line">  &#125;</div><div class="line">  amILeader</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">amILeader</span> </span>: <span class="type">Boolean</span> = leaderId == brokerId</div></pre></td></tr></table></figure>
<p>其实现逻辑如下：</p>
<ol>
<li>先获取 zk 的 <code>/cotroller</code> 节点的信息，获取 controller 的 broker id，如果该节点不存在（比如集群刚创建时），那么获取的 controller id 为-1；</li>
<li>如果 controller id 不为-1，即 controller 已经存在，直接结束流程；</li>
<li>如果 controller id 为-1，证明 controller 还不存在，这时候当前 broker 开始在 zk 注册 controller；</li>
<li>如果注册成功，那么当前 broker 就成为了 controller，这时候开始调用 <code>onBecomingLeader()</code> 方法，正式初始化 controller（注意：<strong>controller 节点是临时节点</strong>，如果当前 controller 与 zk 的 session 断开，那么 controller 的临时节点会消失，会触发 controller 的重新选举）；</li>
<li>如果注册失败（刚好 controller 被其他 broker 创建了、抛出异常等），那么直接返回。</li>
</ol>
<p>在这里 controller 算是成功被选举出来了，controller 选举过程实际上就是各个 Broker 抢占式注册该节点，注册成功的便为 Controller。</p>
<h4 id="controller-节点监听-LeaderChangeListener"><a href="#controller-节点监听-LeaderChangeListener" class="headerlink" title="controller 节点监听 LeaderChangeListener"></a>controller 节点监听 LeaderChangeListener</h4><p>LeaderChangeListener 主要是监听 zk 上的 Controller 节点变化，如果该节点内容变化或者节点被删除，那么会触发 <code>handleDataChange()</code> 和 <code>handleDataDeleted()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 监控 controller 内容的变化</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeaderChangeListener</span> <span class="keyword">extends</span> <span class="title">IZkDataListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Called when the leader information stored in zookeeper has changed. Record the new leader in memory</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">Object</span>) &#123;</div><div class="line">    <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">val</span> amILeaderBeforeDataChange = amILeader</div><div class="line">      leaderId = <span class="type">KafkaController</span>.parseControllerId(data.toString)</div><div class="line">      info(<span class="string">"New leader is %d"</span>.format(leaderId))</div><div class="line">      <span class="comment">// The old leader needs to resign leadership if it is no longer the leader</span></div><div class="line">      amILeaderBeforeDataChange &amp;&amp; !amILeader</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 之前是 controller,现在不是了</span></div><div class="line">    <span class="keyword">if</span> (shouldResign)</div><div class="line">      onResigningAsLeader() <span class="comment">//note: 关闭 controller 服务</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader</div><div class="line">   * @throws Exception</div><div class="line">   *             On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 如果之前是 controller,现在这个节点被删除了,那么首先退出 controller 进程,然后开始重新选举 controller</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;</div><div class="line">    <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      debug(<span class="string">"%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"</span></div><div class="line">        .format(brokerId, dataPath))</div><div class="line">      amILeader</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (shouldResign)</div><div class="line">      onResigningAsLeader()</div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      elect</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>处理过程如下：</p>
<ol>
<li>如果 <code>/controller</code> 节点内容变化，那么更新一下 controller 最新的节点信息，如果该节点刚好之前是 controller，现在不是了，那么需要执行 controller 关闭操作，即 <code>onResigningAsLeader()</code> 方法；</li>
<li>如果 <code>/controller</code> 节点被删除，如果该节点刚好之前是 controller，那么需要执行 controller 关闭操作，即 <code>onResigningAsLeader()</code> 方法，然后再执行 <code>elect</code> 方法重新去选举 controller；</li>
</ol>
<h2 id="Controller-服务启动流程"><a href="#Controller-服务启动流程" class="headerlink" title="Controller 服务启动流程"></a>Controller 服务启动流程</h2><p>Controller 节点选举出来之后，ZookeeperLeaderElector 就会调用 <code>onBecomingLeader()</code> 方法初始化 KafkaController 的相关内容，在 KafkaController 对 ZookeeperLeaderElector 的初始化中可以看到 <code>onBecomingLeader()</code> 这个方法实际上是 KafkaController 的 <code>onControllerFailover()</code> 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaController</span></span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controllerElector = <span class="keyword">new</span> <span class="type">ZookeeperLeaderElector</span>(controllerContext, <span class="type">ZkUtils</span>.<span class="type">ControllerPath</span>, onControllerFailover,</div><div class="line">                                                               onControllerResignation, config.brokerId, time) <span class="comment">//note: controller 通过 zk 选举</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: controller 临时节点监控及 controller 选举</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZookeeperLeaderElector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>,</span></span></div><div class="line">                             electionPath: <span class="type">String</span>, //note: 路径是 /controller</div><div class="line">                             onBecomingLeader: () <span class="title">=&gt;</span> <span class="title">Unit</span>, <span class="title">//note</span>: onControllerFailover() 方法</div><div class="line">                             onResigningAsLeader: () =&gt; <span class="type">Unit</span>, <span class="comment">//note: onControllerResignation() 方法</span></div><div class="line">                             brokerId: <span class="type">Int</span>,</div><div class="line">                             time: <span class="type">Time</span>)</div></pre></td></tr></table></figure>
<h3 id="onControllerFailover-启动及初始化"><a href="#onControllerFailover-启动及初始化" class="headerlink" title="onControllerFailover 启动及初始化"></a>onControllerFailover 启动及初始化</h3><p>下面开始进入 KafkaController 正式初始化的讲解过程中，<code>onControllerFailover()</code> 方法实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果当前 Broker 被选为 controller 时, 当被选为 controller,它将会做以下操作</span></div><div class="line"><span class="comment">//note: 1. 注册 controller epoch changed listener;</span></div><div class="line"><span class="comment">//note: 2. controller epoch 自增加1;</span></div><div class="line"><span class="comment">//note: 3. 初始化 KafkaController 的上下文信息 ControllerContext,它包含了当前的 topic、存活的 broker 以及已经存在的 partition 的 leader;</span></div><div class="line"><span class="comment">//note: 4. 启动 controller 的 channel 管理: 建立与其他 broker 的连接的,负责与其他 broker 之间的通信;</span></div><div class="line"><span class="comment">//note: 5. 启动 ReplicaStateMachine（副本状态机,管理副本的状态）;</span></div><div class="line"><span class="comment">//note: 6. 启动 PartitionStateMachine（分区状态机,管理分区的状态）;</span></div><div class="line"><span class="comment">//note: 如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onControllerFailover</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span>(isRunning) &#123;</div><div class="line">    info(<span class="string">"Broker %d starting become controller state transition"</span>.format(config.brokerId))</div><div class="line">    readControllerEpochFromZookeeper() <span class="comment">//note: 从 zk 获取 controllrt 的 epoch 和 zkVersion 值</span></div><div class="line">    incrementControllerEpoch(zkUtils.zkClient) <span class="comment">//note: 更新 Controller 的 epoch 和 zkVersion 值，可能会抛出异常</span></div><div class="line"></div><div class="line">    <span class="comment">// before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks</span></div><div class="line">    <span class="comment">//note: 再从 zk 获取数据初始化前，注册一些关于 broker/topic 的回调监听器</span></div><div class="line">    registerReassignedPartitionsListener() <span class="comment">//note: 监控路径【/admin/reassign_partitions】，分区迁移监听</span></div><div class="line">    registerIsrChangeNotificationListener() <span class="comment">//note: 监控路径【/isr_change_notification】，isr 变动监听</span></div><div class="line">    registerPreferredReplicaElectionListener() <span class="comment">//note: 监听路径【/admin/preferred_replica_election】，最优 leader 选举</span></div><div class="line">    partitionStateMachine.registerListeners()<span class="comment">//note: 监听 Topic 的创建与删除</span></div><div class="line">    replicaStateMachine.registerListeners() <span class="comment">//note: 监听 broker 的上下线</span></div><div class="line"></div><div class="line">    <span class="comment">//note: 初始化 controller 相关的变量信息:包括 alive broker 列表、partition 的详细信息等</span></div><div class="line">    initializeControllerContext() <span class="comment">//note: 初始化 controller 相关的变量信息</span></div><div class="line"></div><div class="line">    <span class="comment">// We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines</span></div><div class="line">    <span class="comment">// are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before</span></div><div class="line">    <span class="comment">// they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and</span></div><div class="line">    <span class="comment">// partitionStateMachine.startup().</span></div><div class="line">    <span class="comment">//note: 在 controller contest 初始化之后,我们需要发送 UpdateMetadata 请求在状态机启动之前,这是因为 broker 需要从 UpdateMetadata 请求</span></div><div class="line">    <span class="comment">//note: 获取当前存活的 broker list, 因为它们需要处理来自副本状态机或分区状态机启动发送的 LeaderAndIsr 请求</span></div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line"></div><div class="line">    <span class="comment">//note: 初始化 replica 的状态信息: replica 是存活状态时是 OnlineReplica, 否则是 ReplicaDeletionIneligible</span></div><div class="line">    replicaStateMachine.startup() <span class="comment">//note: 初始化 replica 的状态信息</span></div><div class="line">    <span class="comment">//note: 初始化 partition 的状态信息:如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></div><div class="line">    <span class="comment">//note: 并状态为 OfflinePartition 的 topic 选举 leader</span></div><div class="line">    partitionStateMachine.startup() <span class="comment">//note: 初始化 partition 的状态信息</span></div><div class="line"></div><div class="line">    <span class="comment">// register the partition change listeners for all existing topics on failover</span></div><div class="line">    <span class="comment">//note: 为所有的 topic 注册 partition change 监听器</span></div><div class="line">    controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">    info(<span class="string">"Broker %d is ready to serve as the new controller with epoch %d"</span>.format(config.brokerId, epoch))</div><div class="line">    maybeTriggerPartitionReassignment() <span class="comment">//note: 触发一次分区副本迁移的操作</span></div><div class="line">    maybeTriggerPreferredReplicaElection() <span class="comment">//note: 触发一次分区的最优 leader 选举操作</span></div><div class="line">    <span class="keyword">if</span> (config.autoLeaderRebalanceEnable) &#123; <span class="comment">//note: 如果开启自动均衡</span></div><div class="line">      info(<span class="string">"starting the partition rebalance scheduler"</span>)</div><div class="line">      autoRebalanceScheduler.startup()</div><div class="line">      autoRebalanceScheduler.schedule(<span class="string">"partition-rebalance-thread"</span>, checkAndTriggerPartitionRebalance,</div><div class="line">        <span class="number">5</span>, config.leaderImbalanceCheckIntervalSeconds.toLong, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">//note: 发送最新的 meta 信息</span></div><div class="line">    &#125;</div><div class="line">    deleteTopicManager.start() <span class="comment">//note: topic 删除线程启动</span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span></div><div class="line">    info(<span class="string">"Controller has been shut down, aborting startup/failover"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>onControllerFailover()</code> 所做的事情如下：</p>
<ol>
<li><code>readControllerEpochFromZookeeper()</code> 方法更新 controller 的 epoch 及 zkVersion 信息，<code>incrementControllerEpoch()</code> 方法将 controller 的 epoch 字增加1，并更新到 zk 中；</li>
<li>在控制器中注册相关的监听器，主要有6类类型，如下面表格中所列；</li>
<li>通过 <code>initializeControllerContext()</code> 方法初始化 Controller 的上下文信息，更新 Controller 的相关缓存信息、并启动 ControllerChannelManager 等；</li>
<li>向所有 alive 的 broker 发送 Update-Metadata 请求，broker 通过这个请求获取当前集群中 alive 的 broker 列表；</li>
<li>启动副本状态机，初始化所有 Replica 的状态信息，如果 Replica 所在节点是 alive 的，那么状态更新为 OnlineReplica, 否则更新为 ReplicaDeletionIneligible；</li>
<li>启动分区状态机，初始化所有 Partition 的状态信息，如果 leader 所在 broker 是 alive 的，那么状态更新为 OnlinePartition，否则更新为 OfflinePartition；</li>
<li>为当前所有 topic 注册一个 PartitionModificationsListener 监听器，监听所有 Topic 分区数的变化；</li>
<li>KafkaController 初始化完成，正式启动；</li>
<li>KafkaController 启动后，触发一次副本迁移，如果需要的情况下；</li>
<li>KafkaController 启动后，触发一次最优 leader 选举操作，如果需要的情况下；</li>
<li>KafkaController 启动后，如果开启了自动 leader 均衡，启动自动 leader 均衡线程，它会根据配置的信息定期运行。</li>
</ol>
<p>KafkaController 需要监听的 zk 节点、触发的监听方法及作用如下：</p>
<table>
<thead>
<tr>
<th>监听方法</th>
<th>监听路径</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>registerReassignedPartitionsListener</td>
<td>/admin/reassign_partitions</td>
<td>用于分区副本迁移</td>
</tr>
<tr>
<td>registerIsrChangeNotificationListener</td>
<td>/isr_change_notification</td>
<td>用于 Partition ISR 变动</td>
</tr>
<tr>
<td>registerPreferredReplicaElectionListener</td>
<td>/admin/preferred_replica_election</td>
<td>用于 Partition 最优 leader 选举</td>
</tr>
<tr>
<td>partitionStateMachine.registerTopicChangeListener()</td>
<td>/brokers/topics</td>
<td>用于 Topic 新建的监听</td>
</tr>
<tr>
<td>partitionStateMachine.registerDeleteTopicListener()</td>
<td>/admin/delete_topics</td>
<td>用于 Topic 删除的监听</td>
</tr>
<tr>
<td>replicaStateMachine.registerBrokerChangeListener()</td>
<td>/brokers/ids</td>
<td>用于 broker 上下线的监听</td>
</tr>
<tr>
<td>partitionStateMachine.registerPartitionChangeListener(topic)</td>
<td>/brokers/topics/TOPIC_NAME</td>
<td>用于 Topic Partition 扩容的监听</td>
</tr>
</tbody>
</table>
<p>在 KafkaController 中</p>
<ul>
<li>有两个状态机：分区状态机和副本状态机；</li>
<li>一个管理器：Channel 管理器，负责管理所有的 Broker 通信；</li>
<li>相关缓存：Partition 信息、Topic 信息、broker id 信息等；</li>
<li>四种 leader 选举机制：分别是用 leader offline、broker 掉线、partition reassign、最优 leader 选举时触发；</li>
</ul>
<p>如下图所示：</p>
<p><img src="/images/kafka/controller-cache.png" alt="Kafka Controller 的重要内容"></p>
<h3 id="initializeControllerContext-初始化-Controller-上下文信息"><a href="#initializeControllerContext-初始化-Controller-上下文信息" class="headerlink" title="initializeControllerContext 初始化 Controller 上下文信息"></a>initializeControllerContext 初始化 Controller 上下文信息</h3><p>在 <code>initializeControllerContext()</code> 初始化 KafkaController 上下文信息的方法中，主要做了以下事情：</p>
<ol>
<li>从 zk 获取所有 alive broker 列表，记录到 <code>liveBrokers</code>；</li>
<li>从 zk 获取所有的 topic 列表，记录到 <code>allTopic</code> 中；</li>
<li>从 zk 获取所有 Partition 的 replica 信息，更新到 <code>partitionReplicaAssignment</code> 中；</li>
<li>从 zk 获取所有 Partition 的 LeaderAndIsr 信息，更新到 <code>partitionLeadershipInfo</code> 中；</li>
<li>调用 <code>startChannelManager()</code> 启动 Controller 的 Channel Manager；</li>
<li>通过 <code>initializePreferredReplicaElection()</code> 初始化需要最优 leader 选举的 Partition 列表，记录到 <code>partitionsUndergoingPreferredReplicaElection</code> 中；</li>
<li>通过 <code>initializePartitionReassignment()</code> 方法初始化需要进行副本迁移的 Partition 列表，记录到 <code>partitionsBeingReassigned</code> 中；</li>
<li>通过 <code>initializeTopicDeletion()</code> 方法初始化需要删除的 topic 列表及 TopicDeletionManager 对象；</li>
</ol>
<p>综上，这个方法最主要的作用就是相关的 meta 信息及启动 Channel 管理器，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化 KafkaController 的上下文数据</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeControllerContext</span></span>() &#123;</div><div class="line">  <span class="comment">// update controller cache with delete topic information</span></div><div class="line">  controllerContext.liveBrokers = zkUtils.getAllBrokersInCluster().toSet <span class="comment">//note: 初始化 zk 的 broker_list 信息</span></div><div class="line">  controllerContext.allTopics = zkUtils.getAllTopics().toSet <span class="comment">//note: 初始化所有的 topic 信息</span></div><div class="line">  <span class="comment">//note: 初始化所有 topic 的所有 partition 的 replica 分配</span></div><div class="line">  controllerContext.partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(controllerContext.allTopics.toSeq)</div><div class="line">  <span class="comment">//note: 下面两个都是新创建的空集合</span></div><div class="line">  controllerContext.partitionLeadershipInfo = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicAndPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>]</div><div class="line">  controllerContext.shuttingDownBrokerIds = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</div><div class="line">  <span class="comment">// update the leader and isr cache for all existing partitions from Zookeeper</span></div><div class="line">  updateLeaderAndIsrCache() <span class="comment">//note: 获取 topic-partition 的详细信息,更新到 partitionLeadershipInfo 中</span></div><div class="line">  <span class="comment">// start the channel manager</span></div><div class="line">  startChannelManager() <span class="comment">//note: 启动连接所有的 broker 的线程, 根据 broker/ids 的临时去判断要连接哪些 broker</span></div><div class="line">  initializePreferredReplicaElection() <span class="comment">//note: 初始化需要进行最优 leader 选举的 partition</span></div><div class="line">  initializePartitionReassignment() <span class="comment">//note: 初始化需要进行分区副本迁移的 partition</span></div><div class="line">  initializeTopicDeletion() <span class="comment">//note: 初始化要删除的 topic 及后台的 topic 删除线程,还有不能删除的 topic 集合</span></div><div class="line">  info(<span class="string">"Currently active brokers in the cluster: %s"</span>.format(controllerContext.liveBrokerIds))</div><div class="line">  info(<span class="string">"Currently shutting brokers in the cluster: %s"</span>.format(controllerContext.shuttingDownBrokerIds))</div><div class="line">  info(<span class="string">"Current list of topics in the cluster: %s"</span>.format(controllerContext.allTopics))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>最优 leader 选举：就是默认选择 Replica 分配中第一个 replica 作为 leader，为什么叫做最优 leader 选举呢？因为 Kafka 在给每个 Partition 分配副本时，它会保证分区的主副本会均匀分布在所有的 broker 上，这样的话只要保证第一个 replica 被选举为 leader，读写流量就会均匀分布在所有的 Broker 上，当然这是有一个前提的，那就是每个 Partition 的读写流量相差不多，但是在实际的生产环境，这是不太可能的，所以一般情况下，大集群是不建议开自动 leader 均衡的，可以通过额外的算法计算、手动去触发最优 leader 选举。</p>
</blockquote>
<h3 id="Controller-Channel-Manager"><a href="#Controller-Channel-Manager" class="headerlink" title="Controller Channel Manager"></a>Controller Channel Manager</h3><p><code>initializeControllerContext()</code> 方法会通过 <code>startChannelManager()</code> 方法初始化 ControllerChannelManager 对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 启动 ChannelManager 线程</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startChannelManager</span></span>() &#123;</div><div class="line">  controllerContext.controllerChannelManager = <span class="keyword">new</span> <span class="type">ControllerChannelManager</span>(controllerContext, config, time, metrics, threadNamePrefix)</div><div class="line">  controllerContext.controllerChannelManager.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ControllerChannelManager 在初始化时，会为集群中的每个节点初始化一个 ControllerBrokerStateInfo 对象，该对象包含四个部分：</p>
<ol>
<li>NetworkClient：网络连接对象；</li>
<li>Node：节点信息；</li>
<li>BlockingQueue：请求队列；</li>
<li>RequestSendThread：请求的发送线程。</li>
</ol>
<p>其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 控制所有已经存活 broker 的网络连接</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControllerChannelManager</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>, config: <span class="type">KafkaConfig</span>, time: <span class="type">Time</span>, metrics: <span class="type">Metrics</span>, threadNamePrefix: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">protected</span> <span class="keyword">val</span> brokerStateInfo = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ControllerBrokerStateInfo</span>]</div><div class="line">  controllerContext.liveBrokers.foreach(addNewBroker) <span class="comment">//note: 获取目前已经存活的所有 broker</span></div><div class="line">  <span class="comment">//note: 添加一个新的 broker（初始化时,这个方法相当于连接当前存活的所有 broker）</span></div><div class="line">  <span class="comment">//note: 建立网络连接、启动请求发送线程</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addNewBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</div><div class="line">    <span class="keyword">val</span> messageQueue = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">QueueItem</span>]</div><div class="line">    debug(<span class="string">"Controller %d trying to connect to broker %d"</span>.format(config.brokerId, broker.id))</div><div class="line">    <span class="keyword">val</span> brokerEndPoint = broker.getBrokerEndPoint(config.interBrokerListenerName)</div><div class="line">    <span class="keyword">val</span> brokerNode = <span class="keyword">new</span> <span class="type">Node</span>(broker.id, brokerEndPoint.host, brokerEndPoint.port)</div><div class="line">    <span class="keyword">val</span> networkClient = &#123; <span class="comment">//note: 初始化 NetworkClient</span></div><div class="line">      <span class="keyword">val</span> channelBuilder = <span class="type">ChannelBuilders</span>.clientChannelBuilder(</div><div class="line">        config.interBrokerSecurityProtocol,</div><div class="line">        <span class="type">LoginType</span>.<span class="type">SERVER</span>,</div><div class="line">        config.values,</div><div class="line">        config.saslMechanismInterBrokerProtocol,</div><div class="line">        config.saslInterBrokerHandshakeRequestEnable</div><div class="line">      )</div><div class="line">      <span class="keyword">val</span> selector = <span class="keyword">new</span> <span class="type">Selector</span>(</div><div class="line">        <span class="type">NetworkReceive</span>.<span class="type">UNLIMITED</span>,</div><div class="line">        <span class="type">Selector</span>.<span class="type">NO_IDLE_TIMEOUT_MS</span>,</div><div class="line">        metrics,</div><div class="line">        time,</div><div class="line">        <span class="string">"controller-channel"</span>,</div><div class="line">        <span class="type">Map</span>(<span class="string">"broker-id"</span> -&gt; broker.id.toString).asJava,</div><div class="line">        <span class="literal">false</span>,</div><div class="line">        channelBuilder</div><div class="line">      )</div><div class="line">      <span class="keyword">new</span> <span class="type">NetworkClient</span>(</div><div class="line">        selector,</div><div class="line">        <span class="keyword">new</span> <span class="type">ManualMetadataUpdater</span>(<span class="type">Seq</span>(brokerNode).asJava),</div><div class="line">        config.brokerId.toString,</div><div class="line">        <span class="number">1</span>,</div><div class="line">        <span class="number">0</span>,</div><div class="line">        <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>,</div><div class="line">        <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>,</div><div class="line">        config.requestTimeoutMs,</div><div class="line">        time,</div><div class="line">        <span class="literal">false</span></div><div class="line">      )</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="string">"Controller-%d-to-broker-%d-send-thread"</span>.format(config.brokerId, broker.id)</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(name) =&gt; <span class="string">"%s:Controller-%d-to-broker-%d-send-thread"</span>.format(name, config.brokerId, broker.id)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> requestThread = <span class="keyword">new</span> <span class="type">RequestSendThread</span>(config.brokerId, controllerContext, messageQueue, networkClient,</div><div class="line">      brokerNode, config, time, threadName) <span class="comment">//note: 初始化 requestThread</span></div><div class="line">    requestThread.setDaemon(<span class="literal">false</span>) <span class="comment">//note: 非守护进程</span></div><div class="line">    brokerStateInfo.put(broker.id, <span class="keyword">new</span> <span class="type">ControllerBrokerStateInfo</span>(networkClient, brokerNode, messageQueue, requestThread))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清楚了上面的逻辑，再来看 KafkaController 部分是如何向 Broker 发送请求的？</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sendRequest(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</div><div class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) = &#123;</div><div class="line">  controllerContext.controllerChannelManager.sendRequest(brokerId, apiKey, request, callback)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>KafkaController 实际上是调用的 ControllerChannelManager 的 <code>sendRequest()</code> 方法向 Broker 发送请求信息，其实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向 broker 发送请求（并没有真正发送,只是添加到对应的 queue 中）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</div><div class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> stateInfoOpt = brokerStateInfo.get(brokerId)</div><div class="line">    stateInfoOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(stateInfo) =&gt;</div><div class="line">        stateInfo.messageQueue.put(<span class="type">QueueItem</span>(apiKey, request, callback))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"Not sending request %s to broker %d, since it is offline."</span>.format(request, brokerId))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它实际上只是把对应的请求添加到该 Broker 对应的 MessageQueue 中，并没有真正的去发送请求，请求的的发送是在 每台 Broker 对应的 RequestSendThread 中处理的。</p>
<h2 id="Controller-原生的四种-leader-选举机制"><a href="#Controller-原生的四种-leader-选举机制" class="headerlink" title="Controller 原生的四种 leader 选举机制"></a>Controller 原生的四种 leader 选举机制</h2><p>KafkaController 在初始化时，也会初始化四种不同的 leader 选举机制，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: partition leader 挂掉时，选举 leader</span></div><div class="line"><span class="keyword">val</span> offlinePartitionSelector = <span class="keyword">new</span> <span class="type">OfflinePartitionLeaderSelector</span>(controllerContext, config)</div><div class="line"><span class="comment">//note: 重新分配分区时，leader 选举</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> reassignedPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ReassignedPartitionLeaderSelector</span>(controllerContext)</div><div class="line"><span class="comment">//note: 使用最优的副本作为 leader</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">PreferredReplicaPartitionLeaderSelector</span>(controllerContext)</div><div class="line"><span class="comment">//note: broker 掉线时，重新选举 leader</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> controlledShutdownPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ControlledShutdownLeaderSelector</span>(controllerContext)</div></pre></td></tr></table></figure>
<p>四种 leader 选举实现类及对应触发条件如下所示：</p>
<table>
<thead>
<tr>
<th>实现</th>
<th>触发条件</th>
</tr>
</thead>
<tbody>
<tr>
<td>OfflinePartitionLeaderSelector</td>
<td>leader 掉线时触发</td>
</tr>
<tr>
<td>ReassignedPartitionLeaderSelector</td>
<td>分区的副本重新分配数据同步完成后触发的</td>
</tr>
<tr>
<td>PreferredReplicaPartitionLeaderSelector</td>
<td>最优 leader 选举，手动触发或自动 leader 均衡调度时触发</td>
</tr>
<tr>
<td>ControlledShutdownLeaderSelector</td>
<td>broker 发送 ShutDown 请求主动关闭服务时触发</td>
</tr>
</tbody>
</table>
<h3 id="OfflinePartitionLeaderSelector"><a href="#OfflinePartitionLeaderSelector" class="headerlink" title="OfflinePartitionLeaderSelector"></a>OfflinePartitionLeaderSelector</h3><p>OfflinePartitionLeaderSelector Partition leader 选举的逻辑是：</p>
<ol>
<li>如果 isr 中至少有一个副本是存活的，那么从该 Partition 存活的 isr 中选举第一个副本作为新的 leader，存活的 isr 作为新的 isr；</li>
<li>否则，如果脏选举（unclear elect）是禁止的，那么就抛出 NoReplicaOnlineException 异常；</li>
<li>否则，即允许脏选举的情况下，从存活的、所分配的副本（不在 isr 中的副本）中选出一个副本作为新的 leader 和新的 isr 集合；</li>
<li>否则，即是 Partition 分配的副本没有存活的，抛出 NoReplicaOnlineException 异常；</li>
</ol>
<p>一旦 leader 被成功注册到 zk 中，它将会更新到 KafkaController 缓存中的 allLeaders 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于 LeaderAndIsrRequest， 选举一个新的 leader、isr 和 receiving replicas</span></div><div class="line"><span class="comment">//note: 1.如果 isr 中至少有一个副本是存活的，那么存活的 isr 中选举一个副本作为新的 leader，存活的 isr 作为新的 isr；</span></div><div class="line"><span class="comment">//note: 2.否则，如果脏选举（unclear elect）是禁止的，那么就抛出 NoReplicaOnlineException 异常；</span></div><div class="line"><span class="comment">//note: 3.否则，从存活的、所分配的副本中选出一个副本作为新的 leader 和新的 isr 集合；</span></div><div class="line"><span class="comment">//note: 4.否则，partition 分配的副本没有存活的，抛出 NoReplicaOnlineException 异常；</span></div><div class="line"><span class="comment">//note: 一旦 leader 被成功注册到 zk 中，它将更新缓存中的 allLeaders。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">OfflinePartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>, config: <span class="type">KafkaConfig</span></span>)</span></div><div class="line">  <span class="keyword">extends</span> <span class="type">PartitionLeaderSelector</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[OfflinePartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="comment">//note: leader 选举，过程如上面所述</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</div><div class="line">        <span class="comment">//note: AR 中还存活的副本</span></div><div class="line">        <span class="keyword">val</span> liveAssignedReplicas = assignedReplicas.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">        <span class="comment">//note: 当前 isr 中还存活的副本</span></div><div class="line">        <span class="keyword">val</span> liveBrokersInIsr = currentLeaderAndIsr.isr.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">        <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch <span class="comment">//note: epoch</span></div><div class="line">        <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion <span class="comment">//note: zkVersion</span></div><div class="line">        <span class="comment">//note: 选取新的 leader 和 isr</span></div><div class="line">        <span class="keyword">val</span> newLeaderAndIsr =</div><div class="line">          <span class="keyword">if</span> (liveBrokersInIsr.isEmpty) &#123; <span class="comment">//note: 当前 isr 中副本都挂了</span></div><div class="line">            <span class="comment">// Prior to electing an unclean (i.e. non-ISR) leader, ensure that doing so is not disallowed by the configuration</span></div><div class="line">            <span class="comment">// for unclean leader election.</span></div><div class="line">            <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(config.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(controllerContext.zkUtils,</div><div class="line">              <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicAndPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许脏选举的话，抛异常</span></div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>((<span class="string">"No broker in ISR for partition "</span> +</div><div class="line">                <span class="string">"%s is alive. Live brokers are: [%s],"</span>.format(topicAndPartition, controllerContext.liveBrokerIds)) +</div><div class="line">                <span class="string">" ISR brokers are: [%s]"</span>.format(currentLeaderAndIsr.isr.mkString(<span class="string">","</span>)))</div><div class="line">            &#125;</div><div class="line">            debug(<span class="string">"No broker in ISR is alive for %s. Pick the leader from the alive assigned replicas: %s"</span></div><div class="line">              .format(topicAndPartition, liveAssignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="keyword">if</span> (liveAssignedReplicas.isEmpty) &#123; <span class="comment">//note: 副本全挂了，抛异常</span></div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>((<span class="string">"No replica for partition "</span> +</div><div class="line">                <span class="string">"%s is alive. Live brokers are: [%s],"</span>.format(topicAndPartition, controllerContext.liveBrokerIds)) +</div><div class="line">                <span class="string">" Assigned replicas are: [%s]"</span>.format(assignedReplicas))</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 从存活的副本中选举 leader（不能保证选举的是 LEO 最大的副本），并将该副本作为 isr</span></div><div class="line">              <span class="type">ControllerStats</span>.uncleanLeaderElectionRate.mark()</div><div class="line">              <span class="keyword">val</span> newLeader = liveAssignedReplicas.head <span class="comment">//note: 选择第一个作为 leader</span></div><div class="line">              warn(<span class="string">"No broker in ISR is alive for %s. Elect leader %d from live brokers %s. There's potential data loss."</span></div><div class="line">                .format(topicAndPartition, newLeader, liveAssignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">              <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, <span class="type">List</span>(newLeader), currentLeaderIsrZkPathVersion + <span class="number">1</span>)</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 isr 中还有副本存活</span></div><div class="line">            <span class="keyword">val</span> liveReplicasInIsr = liveAssignedReplicas.filter(r =&gt; liveBrokersInIsr.contains(r))</div><div class="line">            <span class="keyword">val</span> newLeader = liveReplicasInIsr.head <span class="comment">//note: 第一个作为 leader</span></div><div class="line">            debug(<span class="string">"Some broker in ISR is alive for %s. Select %d from ISR %s to be the leader."</span></div><div class="line">              .format(topicAndPartition, newLeader, liveBrokersInIsr.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, liveBrokersInIsr.toList, currentLeaderIsrZkPathVersion + <span class="number">1</span>)</div><div class="line">          &#125;</div><div class="line">        info(<span class="string">"Selected new leader and ISR %s for offline partition %s"</span>.format(newLeaderAndIsr.toString(), topicAndPartition))</div><div class="line">        (newLeaderAndIsr, liveAssignedReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"Partition %s doesn't have replicas assigned to it"</span>.format(topicAndPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">`</div></pre></td></tr></table></figure>
<h3 id="ReassignedPartitionLeaderSelector"><a href="#ReassignedPartitionLeaderSelector" class="headerlink" title="ReassignedPartitionLeaderSelector"></a>ReassignedPartitionLeaderSelector</h3><p>ReassignedPartitionLeaderSelector 是在 Partition 副本迁移后，副本同步完成（RAR 都处在 isr 中，RAR 指的是该 Partition 新分配的副本）后触发的，其 leader 选举逻辑如下：</p>
<ol>
<li>leader 选择存活的 RAR 中的第一个副本，此时 RAR 都在 isr 中了；</li>
<li>new isr 是所有存活的 RAR 副本列表；</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 重新分配分区时，partition 的 leader 选举策略</span></div><div class="line"><span class="comment">//note: new leader = 新分配并且在 isr 中的一个副本</span></div><div class="line"><span class="comment">//note: new isr = 当前的 isr</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的副本 = reassigned replicas</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>) <span class="keyword">extends</span> <span class="title">PartitionLeaderSelector</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[ReassignedPartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * The reassigned replicas are already in the ISR when selectLeader is called.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当这个方法被调用时，要求新分配的副本已经在 isr 中了</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="comment">//note: 新分配的 replica 列表</span></div><div class="line">    <span class="keyword">val</span> reassignedInSyncReplicas = controllerContext.partitionsBeingReassigned(topicAndPartition).newReplicas</div><div class="line">    <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch</div><div class="line">    <span class="comment">//note: 当前的 zk version</span></div><div class="line">    <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion</div><div class="line">    <span class="comment">//note: 新分配的 replica 列表，并且其 broker 存活、且在 isr 中</span></div><div class="line">    <span class="keyword">val</span> aliveReassignedInSyncReplicas = reassignedInSyncReplicas.filter(r =&gt; controllerContext.liveBrokerIds.contains(r) &amp;&amp;</div><div class="line">                                                                             currentLeaderAndIsr.isr.contains(r))</div><div class="line">    <span class="comment">//note: 选择第一个作为新的 leader</span></div><div class="line">    <span class="keyword">val</span> newLeaderOpt = aliveReassignedInSyncReplicas.headOption</div><div class="line">    newLeaderOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(newLeader) =&gt; (<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, currentLeaderAndIsr.isr,</div><div class="line">        currentLeaderIsrZkPathVersion + <span class="number">1</span>), reassignedInSyncReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        reassignedInSyncReplicas.size <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"List of reassigned replicas for partition "</span> +</div><div class="line">              <span class="string">" %s is empty. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">          <span class="keyword">case</span> _ =&gt;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"None of the reassigned replicas for partition "</span> +</div><div class="line">              <span class="string">"%s are in-sync with the leader. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="PreferredReplicaPartitionLeaderSelector"><a href="#PreferredReplicaPartitionLeaderSelector" class="headerlink" title="PreferredReplicaPartitionLeaderSelector"></a>PreferredReplicaPartitionLeaderSelector</h3><p>PreferredReplicaPartitionLeaderSelector 是最优 leader 选举，选择 AR（assign replica）中的第一个副本作为 leader，前提是该 replica 在是存活的、并且在 isr 中，否则会抛出 StateChangeFailedException 的异常。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 最优的 leader 选举策略（主要用于自动 leader 均衡，选择 AR 中第一个为 leader，前提是它在 isr 中，这样整个集群的 leader 是均衡的,否则抛出异常）</span></div><div class="line"><span class="comment">//note: new leader = 第一个 replica（alive and in isr）</span></div><div class="line"><span class="comment">//note: new isr = 当前 isr</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的 replica = AR</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PreferredReplicaPartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>) <span class="keyword">extends</span> <span class="title">PartitionLeaderSelector</span></span></div><div class="line"><span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[PreferredReplicaPartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="comment">//note: Partition 的 AR</span></div><div class="line">    <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="comment">//note: preferredReplica，第一个 replica</span></div><div class="line">    <span class="keyword">val</span> preferredReplica = assignedReplicas.head</div><div class="line">    <span class="comment">// check if preferred replica is the current leader</span></div><div class="line">    <span class="comment">//note: 当前的 leader</span></div><div class="line">    <span class="keyword">val</span> currentLeader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">    <span class="keyword">if</span> (currentLeader == preferredReplica) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">LeaderElectionNotNeededException</span>(<span class="string">"Preferred replica %d is already the current leader for partition %s"</span></div><div class="line">                                                   .format(preferredReplica, topicAndPartition))</div><div class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 leader 不是 preferredReplica 的情况</span></div><div class="line">      info(<span class="string">"Current leader %d for partition %s is not the preferred replica."</span>.format(currentLeader, topicAndPartition) +</div><div class="line">        <span class="string">" Triggering preferred replica leader election"</span>)</div><div class="line">      <span class="comment">// check if preferred replica is not the current leader and is alive and in the isr</span></div><div class="line">      <span class="comment">//note: preferredReplica 是 alive 并且在 isr 中</span></div><div class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(preferredReplica) &amp;&amp; currentLeaderAndIsr.isr.contains(preferredReplica)) &#123;</div><div class="line">        (<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(preferredReplica, currentLeaderAndIsr.leaderEpoch + <span class="number">1</span>, currentLeaderAndIsr.isr,</div><div class="line">          currentLeaderAndIsr.zkVersion + <span class="number">1</span>), assignedReplicas)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Preferred replica %d for partition "</span>.format(preferredReplica) +</div><div class="line">          <span class="string">"%s is either not alive or not in the isr. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ControlledShutdownLeaderSelector"><a href="#ControlledShutdownLeaderSelector" class="headerlink" title="ControlledShutdownLeaderSelector"></a>ControlledShutdownLeaderSelector</h3><p>ControlledShutdownLeaderSelector 是在处理 broker 下线时调用的 leader 选举方法，它会选举 isr 中第一个没有正在关闭的 replica 作为 leader，否则抛出 StateChangeFailedException 异常。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Broker 掉线时，重新选举 leader 调用的 leader 选举方法</span></div><div class="line"><span class="comment">//note: new leader = 在 isr 中，并且没有正在 shutdown 的 replica</span></div><div class="line"><span class="comment">//note: new isr = 当前 isr 除去关闭的 replica</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的 replica = 存活的 AR</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControlledShutdownLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>)</span></div><div class="line">        <span class="keyword">extends</span> <span class="type">PartitionLeaderSelector</span></div><div class="line">        <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[ControlledShutdownLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch</div><div class="line">    <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion</div><div class="line"></div><div class="line">    <span class="keyword">val</span> currentLeader = currentLeaderAndIsr.leader</div><div class="line"></div><div class="line">    <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</div><div class="line">    <span class="comment">//note: 存活的 AR</span></div><div class="line">    <span class="keyword">val</span> liveAssignedReplicas = assignedReplicas.filter(r =&gt; liveOrShuttingDownBrokerIds.contains(r))</div><div class="line"></div><div class="line">    <span class="comment">//note: 从当前 isr 中过滤掉正在 shutdown 的 broker</span></div><div class="line">    <span class="keyword">val</span> newIsr = currentLeaderAndIsr.isr.filter(brokerId =&gt; !controllerContext.shuttingDownBrokerIds.contains(brokerId))</div><div class="line">    liveAssignedReplicas.find(newIsr.contains) <span class="keyword">match</span> &#123; <span class="comment">//note: find 方法返回的是第一满足条件的元素，AR 中第一个在 newIsr 集合中的元素被选为 leader</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(newLeader) =&gt;</div><div class="line">        debug(<span class="string">"Partition %s : current leader = %d, new leader = %d"</span>.format(topicAndPartition, currentLeader, newLeader))</div><div class="line">        (<span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, newIsr, currentLeaderIsrZkPathVersion + <span class="number">1</span>), liveAssignedReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>((<span class="string">"No other replicas in ISR %s for %s besides"</span> +</div><div class="line">          <span class="string">" shutting down brokers %s"</span>).format(currentLeaderAndIsr.isr.mkString(<span class="string">","</span>), topicAndPartition, controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>)))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从本篇文章开始，Kafka 源码解析就正式进入了 Controller 部分，Controller 作为 Kafka Server 端一个重要的组件，它的角色类似于其他分布式系统 Master 的角色，跟其他系统不一样的是，Kafka 集群的任何一台 Broker 都可以作
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 ReplicaManager 详解（十五）</title>
    <link href="http://matt33.com/2018/05/01/kafka-replica-manager/"/>
    <id>http://matt33.com/2018/05/01/kafka-replica-manager/</id>
    <published>2018-05-01T04:07:01.000Z</published>
    <updated>2018-05-01T04:35:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类为入口，通过对 ReplicaManager 的详解，顺便再把 Kafka 存储层的内容做一个简单的总结。</p>
<h2 id="ReplicaManager-简介"><a href="#ReplicaManager-简介" class="headerlink" title="ReplicaManager 简介"></a>ReplicaManager 简介</h2><p>前面三篇文章，关于 Produce 请求、Fetch 请求以及副本同步流程的启动都是由 ReplicaManager 来控制的，ReplicaManager 可以说是 Server 端重要的组成部分，回头再仔细看下 KafkaApi 这个类，就会发现 Server 端要处理的多种类型的请求都是 ReplicaManager 来处理的，ReplicaManager 需要处理的请求的有以下六种：</p>
<ol>
<li>LeaderAndIsr 请求；</li>
<li>StopReplica 请求；</li>
<li>UpdateMetadata 请求；</li>
<li>Produce 请求；</li>
<li>Fetch 请求；</li>
<li>ListOffset 请求；</li>
</ol>
<p>其中后面三个已经在前面的文章中介绍过，前面三个都是 Controller 发送的请求，虽然是由 ReplicaManager 中处理的，也会在 Controller 部分展开详细的介绍。</p>
<p>这里先看下面这张图，这张图把 ReplicaManager、Partition、Replica、LogManager、Log、logSegment 这几个抽象的类之间的调用关系简单地串了起来，也算是对存储层这几个重要的部分简单总结了一下：</p>
<p><img src="/images/kafka/replica-manager.png" alt="存储层各个类之间关系"></p>
<p>对着上面的图，简单总结一下：</p>
<ol>
<li>ReplicaManager 是最外层暴露的一个实例，前面说的那几种类型的请求都是由这个实例来处理的；</li>
<li>LogManager 负责管理本节点上所有的日志（Log）实例，它作为 ReplicaManager 的变量传入到了 ReplicaManager 中，ReplicaManager 通过 LogManager 可以对相应的日志实例进行操作；</li>
<li>在 ReplicaManager 中有一个变量：allPartitions，它负责管理本节点所有的 Partition 实例（只要本节点有这个 partition 的日志实例，就会有一个对应的 Partition 对对象实例）；</li>
<li>在创建 Partition 实例时，ReplicaManager 也会作为成员变量传入到 Partition 实例中，Partition 通过 ReplicaManager 可以获得 LogManager 实例、brokerId 等；</li>
<li>Partition 会为它的每一个副本创建一个 Replica 对象实例，但只会为那个在本地副本创建 Log 对象实例（LogManager 不存在这个 Log 对象的情况下，有的话直接引用），这样的话，本地的 Replica 也就与 Log 实例建立了一个联系。</li>
</ol>
<p>关于 ReplicaManager 的 allPartitions 变量可以看下面这张图（假设 Partition 设置的是3副本）：</p>
<p><img src="/images/kafka/all-partition.png" alt="ReplicaManager 的 allPartitions 变量"></p>
<p>allPartitions 管理的 Partition 实例，因为是 3 副本，所以每个 Partition 实例又会管理着三个 Replica，其中只有本地副本（对于上图，就是值 replica.id = 1 的副本）才有对应的 Log 实例对象（HW 和 LEO 的介绍参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Offset 那些事</a>）。</p>
<h2 id="ReplicaManager-启动"><a href="#ReplicaManager-启动" class="headerlink" title="ReplicaManager 启动"></a>ReplicaManager 启动</h2><p>KafkaServer 在启动时，就初始化了 ReplicaManager 实例，如下所示，KafkaServer 在初始化 logManager 后，将 logManager 作为参数传递给了 ReplicaManager。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(isShuttingDown.get)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(startupComplete.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">      brokerState.newState(<span class="type">Starting</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* start scheduler */</span></div><div class="line">      kafkaScheduler.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* setup zookeeper */</span></div><div class="line">      zkUtils = initZk()</div><div class="line"></div><div class="line">      <span class="comment">/* Get or create cluster_id */</span></div><div class="line">      _clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">      info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* generate brokerId */</span></div><div class="line">      config.brokerId =  getBrokerId</div><div class="line">      <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">      <span class="comment">/* start log manager */</span></div><div class="line">      <span class="comment">//note: 启动日志管理线程</span></div><div class="line">      logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">      logManager.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* start replica manager */</span></div><div class="line">      <span class="comment">//note: 启动 replica manager</span></div><div class="line">      replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">        isShuttingDown, quotaManagers.follower)</div><div class="line">      replicaManager.startup()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">      isStartingUp.set(<span class="literal">false</span>)</div><div class="line">      shutdown()</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">    &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h3 id="startup"><a href="#startup" class="headerlink" title="startup"></a>startup</h3><p>ReplicaManager <code>startup()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// start ISR expiration thread</span></div><div class="line">  <span class="comment">// A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR</span></div><div class="line">  <span class="comment">//note: 周期性检查 isr 是否有 replica 过期需要从 isr 中移除</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-expiration"</span>, maybeShrinkIsr, period = config.replicaLagTimeMaxMs / <span class="number">2</span>, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  <span class="comment">//note: 周期性检查是不是有 topic-partition 的 isr 需要变动,如果需要,就更新到 zk 上,来触发 controller</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-change-propagation"</span>, maybePropagateIsrChanges, period = <span class="number">2500</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法与 LogManager 的 <code>startup()</code> 方法类似，也是启动了相应的定时任务，这里，ReplicaManger 启动了两个周期性的任务：</p>
<ol>
<li>maybeShrinkIsr: 判断 topic-partition 的 isr 是否有 replica 因为延迟或 hang 住需要从 isr 中移除；</li>
<li>maybePropagateIsrChanges：判断是不是需要对 isr 进行更新，如果有 topic-partition 的 isr 发生了变动需要进行更新，那么这个方法就会被调用，它会触发 zk 的相应节点，进而触发 controller 进行相应的操作。</li>
</ol>
<p>关于 ReplicaManager 这两个方法的处理过程及 topic-partition isr 变动情况的触发，下面这张流程图做了简单的说明，如下所示：</p>
<p><img src="/images/kafka/replica-manager-startup.png" alt="ReplicaManager 的 Startup 方法启动两个周期性任务及 isr 扩充的情况"></p>
<h3 id="maybeShrinkIsr"><a href="#maybeShrinkIsr" class="headerlink" title="maybeShrinkIsr"></a>maybeShrinkIsr</h3><p>如前面流程图所示， ReplicaManager 的 <code>maybeShrinkIsr()</code> 实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code>  会遍历本节点所有的 Partition 实例，来检查它们 isr 中的 replica 是否需要从 isr 中移除，Partition 中这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查这个 isr 中的每个 replcia</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(replicaMaxLagTimeMs: <span class="type">Long</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123; <span class="comment">//note: 只有本地副本是 leader, 才会做这个操作</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="comment">//note: 检查当前 isr 的副本是否需要从 isr 中移除</span></div><div class="line">        <span class="keyword">val</span> outOfSyncReplicas = getOutOfSyncReplicas(leaderReplica, replicaMaxLagTimeMs)</div><div class="line">        <span class="keyword">if</span>(outOfSyncReplicas.nonEmpty) &#123;</div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas <span class="comment">//note: new isr</span></div><div class="line">          assert(newInSyncReplicas.nonEmpty)</div><div class="line">          info(<span class="string">"Shrinking ISR for partition [%s,%d] from %s to %s"</span>.format(topic, partitionId,</div><div class="line">            inSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>), newInSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// update ISR in zk and in cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新 isr 到 zk</span></div><div class="line">          <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line"></div><div class="line">          replicaManager.isrShrinkRate.mark() <span class="comment">//note: 更新 metrics</span></div><div class="line">          maybeIncrementLeaderHW(leaderReplica) <span class="comment">//note: isr 变动了,判断是否需要更新 partition 的 hw</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="literal">false</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// do nothing if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 检查 isr 中的副本是否需要从 isr 中移除</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOutOfSyncReplicas</span></span>(leaderReplica: <span class="type">Replica</span>, maxLagMs: <span class="type">Long</span>): <span class="type">Set</span>[<span class="type">Replica</span>] = &#123;</div><div class="line">  <span class="comment">//note: 获取那些不应该咋 isr 中副本的列表</span></div><div class="line">  <span class="comment">//note: 1. hang 住的 replica: replica 的 LEO 超过 maxLagMs 没有更新, 那么这个 replica 将会被从 isr 中移除;</span></div><div class="line">  <span class="comment">//note: 2. 数据同步慢的 replica: 副本在 maxLagMs 内没有追上 leader 当前的 LEO, 那么这个 replica 讲会从 ist 中移除;</span></div><div class="line">  <span class="comment">//note: 都是通过 lastCaughtUpTimeMs 来判断的</span></div><div class="line">  <span class="keyword">val</span> candidateReplicas = inSyncReplicas - leaderReplica</div><div class="line"></div><div class="line">  <span class="keyword">val</span> laggingReplicas = candidateReplicas.filter(r =&gt; (time.milliseconds - r.lastCaughtUpTimeMs) &gt; maxLagMs)</div><div class="line">  <span class="keyword">if</span> (laggingReplicas.nonEmpty)</div><div class="line">    debug(<span class="string">"Lagging replicas for partition %s are %s"</span>.format(topicPartition, laggingReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">  laggingReplicas</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code> 这个方法的实现可以简单总结为以下几步：</p>
<ol>
<li>先判断本地副本是不是这个 partition 的 leader，<strong>这个操作只会在 leader 上进行</strong>，如果不是 leader 直接跳过；</li>
<li>通过 <code>getOutOfSyncReplicas()</code> 方法遍历除 leader 外 isr 的所有 replica，找到那些满足条件（<strong>落后超过 maxLagMs 时间的副本</strong>）需要从 isr 中移除的 replica；</li>
<li>得到了新的 isr 列表，调用 <code>updateIsr()</code> 将新的 isr 更新到 zk 上，并且这个方法内部又调用了 ReplicaManager 的 <code>recordIsrChange()</code> 方法来告诉 ReplicaManager 当前这个 topic-partition 的 isr 发生了变化（<strong>可以看出，zk 上这个 topic-partition 的 isr 信息虽然变化了，但是实际上 controller 还是无法感知的</strong>）；</li>
<li>因为 isr 发生了变动，所以这里会通过 <code>maybeIncrementLeaderHW()</code> 方法来检查一下这个 partition 的 HW 是否需要更新。</li>
</ol>
<p><code>updateIsr()</code> 和 <code>maybeIncrementLeaderHW()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查是否需要更新 partition 的 HW,这个方法将在两种情况下触发:</span></div><div class="line"><span class="comment">//note: 1.Partition ISR 变动; 2. 任何副本的 LEO 改变;</span></div><div class="line"><span class="comment">//note: 在获取 HW 时,是从 isr 和认为能追得上的副本中选择最小的 LEO,之所以也要从能追得上的副本中选择,是为了等待 follower 追上 HW,否则可能没机会追上了</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeIncrementLeaderHW</span></span>(leaderReplica: <span class="type">Replica</span>, curTime: <span class="type">Long</span> = time.milliseconds): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取 isr 以及能够追上 isr （认为最近一次 fetch 的时间在 replica.lag.time.max.time 之内） 副本的 LEO 信息。</span></div><div class="line">  <span class="keyword">val</span> allLogEndOffsets = assignedReplicas.filter &#123; replica =&gt;</div><div class="line">    curTime - replica.lastCaughtUpTimeMs &lt;= replicaManager.config.replicaLagTimeMaxMs || inSyncReplicas.contains(replica)</div><div class="line">  &#125;.map(_.logEndOffset)</div><div class="line">  <span class="keyword">val</span> newHighWatermark = allLogEndOffsets.min(<span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>.<span class="type">OffsetOrdering</span>) <span class="comment">//note: 新的 HW</span></div><div class="line">  <span class="keyword">val</span> oldHighWatermark = leaderReplica.highWatermark</div><div class="line">  <span class="keyword">if</span> (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset || oldHighWatermark.onOlderSegment(newHighWatermark)) &#123;</div><div class="line">    leaderReplica.highWatermark = newHighWatermark</div><div class="line">    debug(<span class="string">"High watermark for partition [%s,%d] updated to %s"</span>.format(topic, partitionId, newHighWatermark))</div><div class="line">    <span class="literal">true</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    debug(<span class="string">"Skipping update high watermark since Old hw %s is larger than new hw %s for partition [%s,%d]. All leo's are %s"</span></div><div class="line">      .format(oldHighWatermark, newHighWatermark, topic, partitionId, allLogEndOffsets.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="literal">false</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateIsr</span></span>(newIsr: <span class="type">Set</span>[<span class="type">Replica</span>]) &#123;</div><div class="line">  <span class="keyword">val</span> newLeaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(localBrokerId, leaderEpoch, newIsr.map(r =&gt; r.brokerId).toList, zkVersion)</div><div class="line">  <span class="keyword">val</span> (updateSucceeded,newVersion) = <span class="type">ReplicationUtils</span>.updateLeaderAndIsr(zkUtils, topic, partitionId,</div><div class="line">    newLeaderAndIsr, controllerEpoch, zkVersion) <span class="comment">//note: 执行更新操作</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span>(updateSucceeded) &#123; <span class="comment">//note: 成功更新到 zk 上</span></div><div class="line">    replicaManager.recordIsrChange(topicPartition) <span class="comment">//note: 告诉 replicaManager 这个 partition 的 isr 需要更新</span></div><div class="line">    inSyncReplicas = newIsr</div><div class="line">    zkVersion = newVersion</div><div class="line">    trace(<span class="string">"ISR updated to [%s] and zkVersion updated to [%d]"</span>.format(newIsr.mkString(<span class="string">","</span>), zkVersion))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    info(<span class="string">"Cached zkVersion [%d] not equal to that in zookeeper, skip updating ISR"</span>.format(zkVersion))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="maybePropagateIsrChanges"><a href="#maybePropagateIsrChanges" class="headerlink" title="maybePropagateIsrChanges"></a>maybePropagateIsrChanges</h3><p>ReplicaManager <code>maybePropagateIsrChanges()</code> 方法的作用是将那些 isr 变动的 topic-partition 列表（<code>isrChangeSet</code>）通过 ReplicationUtils 的 <code>propagateIsrChanges()</code> 方法更新 zk 上，这时候 Controller 才能知道哪些 topic-partition 的 isr 发生了变动。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是周期性的运行,来判断 partition 的 isr 是否需要更新,</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybePropagateIsrChanges</span></span>() &#123;</div><div class="line">  <span class="keyword">val</span> now = <span class="type">System</span>.currentTimeMillis()</div><div class="line">  isrChangeSet synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (isrChangeSet.nonEmpty &amp;&amp; <span class="comment">//note:  有 topic-partition 的 isr 需要更新</span></div><div class="line">      (lastIsrChangeMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationBlackOut</span> &lt; now || <span class="comment">//note: 5s 内没有触发过</span></div><div class="line">        lastIsrPropagationMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationInterval</span> &lt; now)) &#123; <span class="comment">//note: 距离上次触发有60s</span></div><div class="line">      <span class="type">ReplicationUtils</span>.propagateIsrChanges(zkUtils, isrChangeSet) <span class="comment">//note: 在 zk 创建 isr 变动的提醒</span></div><div class="line">      isrChangeSet.clear() <span class="comment">//note: 清空 isrChangeSet,它记录着 isr 变动的 topic-partition 信息</span></div><div class="line">      lastIsrPropagationMs.set(now) <span class="comment">//note: 最近一次触发这个方法的时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Partition-ISR-变化"><a href="#Partition-ISR-变化" class="headerlink" title="Partition ISR 变化"></a>Partition ISR 变化</h2><p>前面讲述了 ReplicaManager 周期性调度的两个方法：<code>maybeShrinkIsr()</code> 和 <code>maybePropagateIsrChanges()</code> ，其中 <code>maybeShrinkIsr()</code> 是来检查 isr 中是否有 replica 需要从 isr 中移除，也就是说这个方法只会减少 isr 中的副本数，那么 isr 中副本数的增加是在哪里触发的呢？</p>
<p>观察上面流程图的第三部分，ReplicaManager 在处理来自 replica 的 Fetch 请求时，会将 Fetch 的相关信息到更新 Partition 中，Partition 调用 <code>maybeExpandIsr()</code> 方法来判断 isr 是否需要更新。</p>
<p>举一个例子，一个 topic 的 partition 1有三个副本，其中 replica 1 为 leader replica，那么这个副本之间关系图如下所示：</p>
<p><img src="/images/kafka/partition_replica.png" alt="Leader replica 与 follower replica"></p>
<p>简单分析一下上面的图：</p>
<ol>
<li>对于 replica 1 而言，它是 leader，首先 replica 1 有对应的 Log 实例对象，而且它会记录其他远程副本的 LEO，以便更新这个 Partition 的 HW；</li>
<li>对于 replica 2 而言，它是 follower，replica 2 有对应的 Log 实例对象，它只会有本地的 LEO 和 HW 记录，没有其他副本的 LEO 记录。</li>
<li>replica 2 和 replica 3 从 replica 1 上拉取数据，进行数据同步。</li>
</ol>
<p>再来看前面的流程图，ReplicaManager 在 <code>FetchMessages()</code> 方法对来自副本的 Fetch 请求进行处理的，实际上是会更新相应 replica 的 LEO 信息的，这时候 leader 可以根据副本 LEO 信息的变动来判断 这个副本是否满足加入 isr 的条件，下面详细来看下这个过程。</p>
<h3 id="updateFollowerLogReadResults"><a href="#updateFollowerLogReadResults" class="headerlink" title="updateFollowerLogReadResults"></a>updateFollowerLogReadResults</h3><p>在 ReplicaManager 的 <code>FetchMessages()</code> 方法中，如果 Fetch 请求是来自副本，那么会调用 <code>updateFollowerLogReadResults()</code> 更新远程副本的信息，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的作用就是找到本节点这个 Partition 对象，然后调用其 <code>updateReplicaLogReadResult()</code> 方法更新副本的 LEO 信息和拉取时间信息。</p>
<h3 id="updateReplicaLogReadResult"><a href="#updateReplicaLogReadResult" class="headerlink" title="updateReplicaLogReadResult"></a>updateReplicaLogReadResult</h3><p>这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新这个 partition replica 的 the end offset</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateReplicaLogReadResult</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  getReplica(replicaId) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(replica) =&gt;</div><div class="line">      <span class="comment">//note: 更新副本的信息</span></div><div class="line">      replica.updateLogReadResult(logReadResult)</div><div class="line">      <span class="comment">// check if we need to expand ISR to include this replica</span></div><div class="line">      <span class="comment">// if it is not in the ISR yet</span></div><div class="line">      <span class="comment">//note: 如果该副本不在 isr 中,检查是否需要进行更新</span></div><div class="line">      maybeExpandIsr(replicaId, logReadResult)</div><div class="line"></div><div class="line">      debug(<span class="string">"Recorded replica %d log end offset (LEO) position %d for partition %s."</span></div><div class="line">        .format(replicaId, logReadResult.info.fetchOffsetMetadata.messageOffset, topicPartition))</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotAssignedReplicaException</span>((<span class="string">"Leader %d failed to record follower %d's position %d since the replica"</span> +</div><div class="line">        <span class="string">" is not recognized to be one of the assigned replicas %s for partition %s."</span>)</div><div class="line">        .format(localBrokerId,</div><div class="line">                replicaId,</div><div class="line">                logReadResult.info.fetchOffsetMetadata.messageOffset,</div><div class="line">                assignedReplicas.map(_.brokerId).mkString(<span class="string">","</span>),</div><div class="line">                topicPartition))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法分为以下两步：</p>
<ol>
<li><code>updateLogReadResult()</code>：更新副本的相关信息，这里是更新该副本的 LEO、lastFetchLeaderLogEndOffset 和 lastFetchTimeMs；</li>
<li><code>maybeExpandIsr()</code>：判断 isr 是否需要扩充，即是否有不在 isr 内的副本满足进入 isr 的条件。</li>
</ol>
<h3 id="maybeExpandIsr"><a href="#maybeExpandIsr" class="headerlink" title="maybeExpandIsr"></a>maybeExpandIsr</h3><p><code>maybeExpandIsr()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查当前 Partition 是否需要扩充 ISR, 副本的 LEO 大于等于 hw 的副本将会被添加到 isr 中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeExpandIsr</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="comment">// check if this replica needs to be added to the ISR</span></div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> replica = getReplica(replicaId).get</div><div class="line">        <span class="keyword">val</span> leaderHW = leaderReplica.highWatermark</div><div class="line">        <span class="keyword">if</span>(!inSyncReplicas.contains(replica) &amp;&amp;</div><div class="line">           assignedReplicas.map(_.brokerId).contains(replicaId) &amp;&amp;</div><div class="line">           replica.logEndOffset.offsetDiff(leaderHW) &gt;= <span class="number">0</span>) &#123; <span class="comment">//note: replica LEO 大于 HW 的情况下,加入 isr 列表</span></div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas + replica</div><div class="line">          info(<span class="string">s"Expanding ISR for partition <span class="subst">$topicPartition</span> from <span class="subst">$&#123;inSyncReplicas.map(_.brokerId).mkString(",")&#125;</span> "</span> +</div><div class="line">            <span class="string">s"to <span class="subst">$&#123;newInSyncReplicas.map(_.brokerId).mkString(",")&#125;</span>"</span>)</div><div class="line">          <span class="comment">// update ISR in ZK and cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新到 zk</span></div><div class="line">          replicaManager.isrExpandRate.mark()</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// check if the HW of the partition can now be incremented</span></div><div class="line">        <span class="comment">// since the replica may already be in the ISR and its LEO has just incremented</span></div><div class="line">        <span class="comment">//note: 检查 HW 是否需要更新</span></div><div class="line">        maybeIncrementLeaderHW(leaderReplica, logReadResult.fetchTimeMs)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// nothing to do if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法会根据这个 replica 的 LEO 来判断它是否满足进入 ISR 的条件，如果满足的话，就添加到 ISR 中（前提是这个 replica 在 AR：assign replica 中，并且不在 ISR 中），之后再调用 <code>updateIsr()</code> 更新这个 topic-partition 的 isr 信息和更新 HW 信息。</p>
<h2 id="Updata-Metadata-请求的处理"><a href="#Updata-Metadata-请求的处理" class="headerlink" title="Updata-Metadata 请求的处理"></a>Updata-Metadata 请求的处理</h2><p>这里顺便讲述一下 Update-Metadata 请求的处理流程，先看下在 KafkaApis 中对 Update-Metadata 请求的处理流程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 update-metadata 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleUpdateMetadataRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> updateMetadataRequest = request.body.asInstanceOf[<span class="type">UpdateMetadataRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">val</span> updateMetadataResponse =</div><div class="line">    <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata, 并返回需要删除的 Partition</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = replicaManager.maybeUpdateMetadataCache(correlationId, updateMetadataRequest, metadataCache)</div><div class="line">      <span class="keyword">if</span> (deletedPartitions.nonEmpty)</div><div class="line">        coordinator.handleDeletedPartitions(deletedPartitions) <span class="comment">//note: GroupCoordinator 会清除相关 partition 的信息</span></div><div class="line"></div><div class="line">      <span class="keyword">if</span> (adminManager.hasDelayedTopicOperations) &#123;</div><div class="line">        updateMetadataRequest.partitionStates.keySet.asScala.map(_.topic).foreach &#123; topic =&gt;</div><div class="line">          adminManager.tryCompleteDelayedTopicOperations(topic)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, updateMetadataResponse))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个请求的处理还是调用 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法进行处理的，这个方法会先更新相关的 meta 信息，然后返回需要删除的 topic-partition 信息，GroupCoordinator 再从它的 meta 删除这个 topic-partition 的相关信息。</p>
<h3 id="maybeUpdateMetadataCache"><a href="#maybeUpdateMetadataCache" class="headerlink" title="maybeUpdateMetadataCache"></a>maybeUpdateMetadataCache</h3><p>先看下 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 向所有的 Broker 发送请求,让它们去更新各自的 meta 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeUpdateMetadataCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>, metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Seq</span>[<span class="type">TopicPartition</span>] =  &#123;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(updateMetadataRequest.controllerEpoch &lt; controllerEpoch) &#123; <span class="comment">//note: 来自过期的 controller</span></div><div class="line">      <span class="keyword">val</span> stateControllerEpochErrorMessage = (<span class="string">"Broker %d received update metadata request with correlation id %d from an "</span> +</div><div class="line">        <span class="string">"old controller %d with epoch %d. Latest known controller epoch is %d"</span>).format(localBrokerId,</div><div class="line">        correlationId, updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch,</div><div class="line">        controllerEpoch)</div><div class="line">      stateChangeLogger.warn(stateControllerEpochErrorMessage)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(stateControllerEpochErrorMessage)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata 信息,并返回需要删除的 Partition 信息</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = metadataCache.updateCache(correlationId, updateMetadataRequest)</div><div class="line">      controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      deletedPartitions</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法就是：调用 <code>metadataCache.updateCache()</code> 方法更新 meta 缓存，然后返回需要删除的 topic-partition 列表。</p>
<h3 id="updateCache"><a href="#updateCache" class="headerlink" title="updateCache"></a>updateCache</h3><p>MetadataCache 的 <code>updateCache()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新本地的 meta,并返回要删除的 topic-partition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = &#123;</div><div class="line">  inWriteLock(partitionMetadataLock) &#123;</div><div class="line">    controllerId = updateMetadataRequest.controllerId <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> id <span class="keyword">if</span> id &lt; <span class="number">0</span> =&gt; <span class="type">None</span></div><div class="line">        <span class="keyword">case</span> id =&gt; <span class="type">Some</span>(id)</div><div class="line">      &#125;</div><div class="line">    <span class="comment">//note: 清空 aliveNodes 和 aliveBrokers 记录,并更新成最新的记录</span></div><div class="line">    aliveNodes.clear()</div><div class="line">    aliveBrokers.clear()</div><div class="line">    updateMetadataRequest.liveBrokers.asScala.foreach &#123; broker =&gt;</div><div class="line">      <span class="comment">// `aliveNodes` is a hot path for metadata requests for large clusters, so we use java.util.HashMap which</span></div><div class="line">      <span class="comment">// is a bit faster than scala.collection.mutable.HashMap. When we drop support for Scala 2.10, we could</span></div><div class="line">      <span class="comment">// move to `AnyRefMap`, which has comparable performance.</span></div><div class="line">      <span class="keyword">val</span> nodes = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]</div><div class="line">      <span class="keyword">val</span> endPoints = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">EndPoint</span>]</div><div class="line">      broker.endPoints.asScala.foreach &#123; ep =&gt;</div><div class="line">        endPoints += <span class="type">EndPoint</span>(ep.host, ep.port, ep.listenerName, ep.securityProtocol)</div><div class="line">        nodes.put(ep.listenerName, <span class="keyword">new</span> <span class="type">Node</span>(broker.id, ep.host, ep.port))</div><div class="line">      &#125;</div><div class="line">      aliveBrokers(broker.id) = <span class="type">Broker</span>(broker.id, endPoints, <span class="type">Option</span>(broker.rack))</div><div class="line">      aliveNodes(broker.id) = nodes.asScala</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> deletedPartitions = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">TopicPartition</span>] <span class="comment">//note:</span></div><div class="line">    updateMetadataRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (tp, info) =&gt;</div><div class="line">      <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId</div><div class="line">      <span class="keyword">val</span> controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      <span class="keyword">if</span> (info.leader == <span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>) &#123; <span class="comment">//note: partition 被标记为了删除</span></div><div class="line">        removePartitionInfo(tp.topic, tp.partition) <span class="comment">//note: 从 cache 中删除</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> deleted partition <span class="subst">$tp</span> from metadata cache in response to UpdateMetadata "</span> +</div><div class="line">          <span class="string">s"request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">        deletedPartitions += tp</div><div class="line">      &#125; <span class="keyword">else</span> &#123;<span class="comment">//note: 更新</span></div><div class="line">        <span class="keyword">val</span> partitionInfo = partitionStateToPartitionStateInfo(info)</div><div class="line">        addOrUpdatePartitionInfo(tp.topic, tp.partition, partitionInfo) <span class="comment">//note: 更新 topic-partition meta</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> cached leader info <span class="subst">$partitionInfo</span> for partition <span class="subst">$tp</span> in response to "</span> +</div><div class="line">          <span class="string">s"UpdateMetadata request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    deletedPartitions</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它的处理流程如下：</p>
<ol>
<li>清空本节点的 aliveNodes 和 aliveBrokers 记录，并更新为最新的记录；</li>
<li>对于要删除的 topic-partition，从缓存中删除，并记录下来作为这个方法的返回；</li>
<li>对于其他的 topic-partition，执行 updateOrCreate 操作。</li>
</ol>
<p>到这里 ReplicaManager 算是讲述完了，Kafka 存储层的内容基本也介绍完了，后面会开始讲述 Kafka Controller 部分的内容，争取这部分能够在一个半月内总结完。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本同步机制实现（十四）</title>
    <link href="http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/"/>
    <id>http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/</id>
    <published>2018-04-29T10:36:52.000Z</published>
    <updated>2018-06-24T23:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。本篇文章我们就来看一下 Kafka 副本同步这块的内容，对于每个 broker 来说，它上面的 replica 对象，除了 leader 就是 follower，只要这台 broker 有 follower replica，broker 就会启动副本同步流程从 leader 同步数据，副本同步机制的实现是 Kafka Server 端非常重要的内容，在这篇文章中，主要会从以下几块来讲解：</p>
<ol>
<li>Kafka 在什么情况下会启动副本同步线程？</li>
<li>Kafka 副本同步线程启动流程及付副本同步流程的处理逻辑；</li>
<li>Kafka 副本同步需要解决的问题以及 Kafka 是如何解决这些问题的？</li>
<li>Kafka 在什么情况下会关闭一个副本同步线程。</li>
</ol>
<blockquote>
<p>小插曲：本来想先介绍一下与 LeaderAndIsr 请求相关的，因为副本同步线程的启动与这部分是息息相关的，但是发现涉及到了很多 controller 端的内容，而 controller 这部分还没开始涉及，所以本篇文章涉及到 LeaderAndIsr 请求的部分先简单讲述一下其处理逻辑，在 controller 这块再详细介绍。</p>
</blockquote>
<h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>Kafka Server 端的副本同步，是由 replica fetcher 线程来负责的，而它又是由 ReplicaManager 来控制的。关于 ReplicaManger，不知道大家还记不记得在 <a href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a> 有一个简单的表格，如下所示。ReplicaManager 通过对 Partition 对象的管理，来控制着 Partition 对应的 Replica 实例，而 Replica 实例又是通过 Log 对象实例来管理着其底层的存储内容。</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 的内容准备专门写一篇文章来介绍，刚好也作为对 Kafka 存储层内容的一个总结。</p>
<p>下面回到这篇文章的主题 —— 副本同步机制，在 ReplicaManager 中有一个实例变量 <code>replicaFetcherManager</code>，它负责管理所有副本同步线程，副本同步线程的启动和关闭都是由这个实例来操作的，关于副本同步相关处理逻辑，下面这张图可以作为一个整体流程，包括了 replica fetcher 线程的启动、工作流程、关闭三个部分，如下图所示：</p>
<p><img src="/images/kafka/fetcher_thread.png" alt="副本同步机制"></p>
<p>后面的讲述会围绕着这张图开始，这里看不懂或不理解也没有关系，后面会一一讲解。</p>
<h2 id="replica-fetcher-线程何时启动"><a href="#replica-fetcher-线程何时启动" class="headerlink" title="replica fetcher 线程何时启动"></a>replica fetcher 线程何时启动</h2><p>Broker 会在什么情况下启动副本同步线程呢？简单想一下这部分的逻辑：首先 broker 分配的任何一个 partition 都是以 Replica 对象实例的形式存在，而 Replica 在 Kafka 上是有两个角色： leader 和 follower，只要这个 Replica 是 follower，它便会向 leader 进行数据同步。</p>
<p>反应在 ReplicaManager 上就是如果 Broker 的本地副本被选举为 follower，那么它将会启动副本同步线程，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于给定的这些副本，将本地副本设置为 follower</span></div><div class="line"><span class="comment">//note: 1. 从 leader partition 集合移除这些 partition；</span></div><div class="line"><span class="comment">//note: 2. 将这些 partition 标记为 follower，之后这些 partition 就不会再接收 produce 的请求了；</span></div><div class="line"><span class="comment">//note: 3. 停止对这些 partition 的副本同步，这样这些副本就不会再有（来自副本请求线程）的数据进行追加了；</span></div><div class="line"><span class="comment">//note: 4. 对这些 partition 的 offset 进行 checkpoint，如果日志需要截断就进行截断操作；</span></div><div class="line"><span class="comment">//note: 5. 清空 purgatory 中的 produce 和 fetch 请求；</span></div><div class="line"><span class="comment">//note: 6. 如果 broker 没有掉线，向这些 partition 的新 leader 启动副本同步线程；</span></div><div class="line"><span class="comment">//note: 上面这些操作的顺序性，保证了这些副本在 offset checkpoint 之前将不会接收新的数据，这样的话，在 checkpoint 之前这些数据都可以保证刷到磁盘</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 删除对这些 partition 的副本同步线程</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 完成那些延迟请求的处理</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>makeFollowers()</code> 的处理过程如下：</p>
<ol>
<li>先从本地记录 leader partition 的集合中将这些 partition 移除，因为这些 partition 已经被选举为了 follower；</li>
<li>将这些 partition 的本地副本设置为 follower，后面就不会接收关于这个 partition 的 Produce 请求了，如果依然有 client 在向这台 broker 发送数据，那么它将会返回相应的错误；</li>
<li>先停止关于这些 partition 的副本同步线程（如果本地副本之前是 follower 现在还是 follower，先关闭的原因是：这个 partition 的 leader 发生了变化，如果 leader 没有发生变化，那么 <code>makeFollower</code> 方法返回的是 False，这个 Partition 就不会被添加到 partitionsToMakeFollower 集合中），这样的话可以保证这些 partition 的本地副本将不会再有新的数据追加；</li>
<li>对这些 partition 本地副本日志文件进行截断操作并进行 checkpoint 操作；</li>
<li>完成那些延迟处理的 Produce 和 Fetch 请求；</li>
<li>如果本地的 broker 没有掉线，那么向这些 partition 新选举出来的 leader 启动副本同步线程。</li>
</ol>
<p>关于第6步，并不一定会为每一个 partition 都启动一个 fetcher 线程，对于一个目的 broker，只会启动 <code>num.replica.fetchers</code> 个线程，具体这个 topic-partition 会分配到哪个 fetcher 线程上，是根据 topic 名和 partition id 进行计算得到，实现所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取分配到这个 topic-partition 的 fetcher 线程 id</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFetcherId</span></span>(topic: <span class="type">String</span>, partitionId: <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="type">Utils</span>.abs(<span class="number">31</span> * topic.hashCode() + partitionId) % numFetchers</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="replica-fetcher-线程参数设置"><a href="#replica-fetcher-线程参数设置" class="headerlink" title="replica fetcher 线程参数设置"></a>replica fetcher 线程参数设置</h3><p>关于副本同步线程有一些参数配置，具体如下表所示：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>num.replica.fetchers</td>
<td>从一个 broker 同步数据的 fetcher 线程数，增加这个值时也会增加该 broker 的 Io 并行度（也就是说：从一台 broker 同步数据，最多能开这么大的线程数）</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.wait.max.ms</td>
<td>对于 follower replica 而言，每个 Fetch 请求的最大等待时间，这个值应该比 <code>replica.lag.time.max.ms</code> 要小，否则对于那些吞吐量特别低的 topic 可能会导致 isr 频繁抖动</td>
<td>500</td>
</tr>
<tr>
<td>replica.high.watermark.checkpoint.interval.ms</td>
<td>hw 刷到磁盘频率</td>
<td>500</td>
</tr>
<tr>
<td>replica.lag.time.max.ms</td>
<td>如果一个 follower 在这个时间内没有发送任何 fetch 请求或者在这个时间内没有追上 leader 当前的 log end offset，那么将会从 isr 中移除</td>
<td>10000</td>
</tr>
<tr>
<td>replica.fetch.min.bytes</td>
<td>每次 fetch 请求最少拉取的数据量，如果不满足这个条件，那么要等待 replicaMaxWaitTimeMs</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.backoff.ms</td>
<td>拉取时，如果遇到错误，下次拉取等待的时间</td>
<td>1000</td>
</tr>
<tr>
<td>replica.fetch.max.bytes</td>
<td>在对每个 partition 拉取时，最大的拉取数量，这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>1048576</td>
</tr>
<tr>
<td>replica.fetch.response.max.bytes</td>
<td>对于一个 fetch 请求，返回的最大数据量（可能会涉及多个 partition），这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>10MB</td>
</tr>
</tbody>
</table>
<h2 id="replica-fetcher-线程启动"><a href="#replica-fetcher-线程启动" class="headerlink" title="replica fetcher 线程启动"></a>replica fetcher 线程启动</h2><p>如上面的图所示，在 ReplicaManager 调用 <code>makeFollowers()</code> 启动 replica fetcher 线程后，它实际上是通过 ReplicaFetcherManager 实例进行相关 topic-partition 同步线程的启动和关闭，其启动过程分为下面两步：</p>
<ol>
<li>ReplicaFetcherManager 调用 <code>addFetcherForPartitions()</code> 添加对这些 topic-partition 的数据同步流程；</li>
<li>ReplicaFetcherManager 调用 <code>createFetcherThread()</code> 初始化相应的 ReplicaFetcherThread 线程。</li>
</ol>
<h3 id="addFetcherForPartitions"><a href="#addFetcherForPartitions" class="headerlink" title="addFetcherForPartitions"></a>addFetcherForPartitions</h3><p><code>addFetcherForPartitions()</code> 的具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 为一个 topic-partition 添加 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addFetcherForPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">BrokerAndInitialOffset</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="comment">//note: 为这些 topic-partition 分配相应的 fetch 线程 id</span></div><div class="line">    <span class="keyword">val</span> partitionsPerFetcher = partitionAndOffsets.groupBy &#123; <span class="keyword">case</span>(topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">      <span class="type">BrokerAndFetcherId</span>(brokerAndInitialOffset.broker, getFetcherId(topicPartition.topic, topicPartition.partition))&#125;</div><div class="line">    <span class="keyword">for</span> ((brokerAndFetcherId, partitionAndOffsets) &lt;- partitionsPerFetcher) &#123;</div><div class="line">      <span class="comment">//note: 为 BrokerAndFetcherId 构造 fetcherThread 线程</span></div><div class="line">      <span class="keyword">var</span> fetcherThread: <span class="type">AbstractFetcherThread</span> = <span class="literal">null</span></div><div class="line">      fetcherThreadMap.get(brokerAndFetcherId) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(f) =&gt; fetcherThread = f</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">//note: 创建 fetcher 线程</span></div><div class="line">          fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)</div><div class="line">          fetcherThreadMap.put(brokerAndFetcherId, fetcherThread)</div><div class="line">          fetcherThread.start</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 添加 topic-partition 列表</span></div><div class="line">      fetcherThreadMap(brokerAndFetcherId).addPartitions(partitionAndOffsets.map &#123; <span class="keyword">case</span> (tp, brokerAndInitOffset) =&gt;</div><div class="line">        tp -&gt; brokerAndInitOffset.initOffset</div><div class="line">      &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  info(<span class="string">"Added fetcher for partitions %s"</span>.format(partitionAndOffsets.map &#123; <span class="keyword">case</span> (topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">    <span class="string">"["</span> + topicPartition + <span class="string">", initOffset "</span> + brokerAndInitialOffset.initOffset + <span class="string">" to broker "</span> + brokerAndInitialOffset.broker + <span class="string">"] "</span>&#125;))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法其实是做了下面这几件事：</p>
<ol>
<li>先计算这个 topic-partition 对应的 fetcher id；</li>
<li>根据 leader 和 fetcher id 获取对应的 replica fetcher 线程，如果没有找到，就调用 <code>createFetcherThread()</code> 创建一个新的 fetcher 线程；</li>
<li>如果是新启动的 replica fetcher 线程，那么就启动这个线程；</li>
<li>将 topic-partition 记录到 <code>fetcherThreadMap</code> 中，这个变量记录每个 replica fetcher 线程要同步的 topic-partition 列表。</li>
</ol>
<h3 id="createFetcherThread"><a href="#createFetcherThread" class="headerlink" title="createFetcherThread"></a>createFetcherThread</h3><p>ReplicaFetcherManager 创建 replica Fetcher 线程的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建 replica-fetch 线程</span></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createFetcherThread</span></span>(fetcherId: <span class="type">Int</span>, sourceBroker: <span class="type">BrokerEndPoint</span>): <span class="type">AbstractFetcherThread</span> = &#123;</div><div class="line">  <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="string">"ReplicaFetcherThread-%d-%d"</span>.format(fetcherId, sourceBroker.id)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt;</div><div class="line">      <span class="string">"%s:ReplicaFetcherThread-%d-%d"</span>.format(p, fetcherId, sourceBroker.id)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">ReplicaFetcherThread</span>(threadName, fetcherId, sourceBroker, brokerConfig,</div><div class="line">    replicaMgr, metrics, time, quotaManager) <span class="comment">//note: replica-fetch 线程</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="replica-fetcher-线程处理过程"><a href="#replica-fetcher-线程处理过程" class="headerlink" title="replica fetcher 线程处理过程"></a>replica fetcher 线程处理过程</h2><p>replica fetcher 线程在启动之后就开始进行正常数据同步流程了，在文章最开始流程图中的第二部分（线程处理过程）已经给出了大概的处理过程，这节会详细介绍一下，这个过程都是在 ReplicaFetcherThread 线程中实现的。</p>
<h3 id="doWoker"><a href="#doWoker" class="headerlink" title="doWoker"></a>doWoker</h3><p>ReplicaFetcherThread 的 <code>doWork()</code> 方法是一直在这个线程中的 <code>run()</code> 中调用的，实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  info(<span class="string">"Starting "</span>)</div><div class="line">  <span class="keyword">try</span>&#123;</div><div class="line">    <span class="keyword">while</span>(isRunning.get())&#123;</div><div class="line">      doWork()</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span>&#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span>(isRunning.get())</div><div class="line">        error(<span class="string">"Error due to "</span>, e)</div><div class="line">  &#125;</div><div class="line">  shutdownLatch.countDown()</div><div class="line">  info(<span class="string">"Stopped "</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">  <span class="comment">//note: 构造 fetch request</span></div><div class="line">  <span class="keyword">val</span> fetchRequest = inLock(partitionMapLock) &#123;</div><div class="line">    <span class="keyword">val</span> fetchRequest = buildFetchRequest(partitionStates.partitionStates.asScala.map &#123; state =&gt;</div><div class="line">      state.topicPartition -&gt; state.value</div><div class="line">    &#125;)</div><div class="line">    <span class="keyword">if</span> (fetchRequest.isEmpty) &#123; <span class="comment">//note: 如果没有活跃的 partition，在下次调用之前，sleep fetchBackOffMs 时间</span></div><div class="line">      trace(<span class="string">"There are no active partitions. Back off for %d ms before sending a fetch request"</span>.format(fetchBackOffMs))</div><div class="line">      partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    &#125;</div><div class="line">    fetchRequest</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!fetchRequest.isEmpty)</div><div class="line">    processFetchRequest(fetchRequest) <span class="comment">//note: 发送 fetch 请求，处理 fetch 的结果</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>doWork()</code> 方法中主要做了两件事：</p>
<ol>
<li>构造相应的 Fetch 请求（<code>buildFetchRequest()</code>）；</li>
<li>通过 <code>processFetchRequest()</code> 方法发送 Fetch 请求，并对其结果进行相应的处理。</li>
</ol>
<h3 id="buildFetchRequest"><a href="#buildFetchRequest" class="headerlink" title="buildFetchRequest"></a>buildFetchRequest</h3><p>通过 <code>buildFetchRequest()</code> 方法构造相应的 Fetcher 请求时，会设置 replicaId，该值会代表了这个 Fetch 请求是来自副本同步，而不是来自 consumer。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, 对于 consumer, 该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="processFetchRequest"><a href="#processFetchRequest" class="headerlink" title="processFetchRequest"></a>processFetchRequest</h3><p><code>processFetchRequest()</code> 这个方法的作用是发送 Fetch 请求，并对返回的结果进行处理，最终写入到本地副本的 Log 实例中，其具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processFetchRequest</span></span>(fetchRequest: <span class="type">REQ</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsWithError = mutable.<span class="type">Set</span>[<span class="type">TopicPartition</span>]()</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updatePartitionsWithError</span></span>(partition: <span class="type">TopicPartition</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    partitionsWithError += partition</div><div class="line">    partitionStates.moveToEnd(partition)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> responseData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PD</span>)] = <span class="type">Seq</span>.empty</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    trace(<span class="string">"Issuing to broker %d of fetch request %s"</span>.format(sourceBroker.id, fetchRequest))</div><div class="line">    responseData = fetch(fetchRequest) <span class="comment">//note: 发送 fetch 请求，获取 fetch 结果</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">        warn(<span class="string">s"Error in fetch <span class="subst">$fetchRequest</span>"</span>, t)</div><div class="line">        inLock(partitionMapLock) &#123; <span class="comment">//note: fetch 时发生错误，sleep 一会</span></div><div class="line">          partitionStates.partitionSet.asScala.foreach(updatePartitionsWithError)</div><div class="line">          <span class="comment">// there is an error occurred while fetching partitions, sleep a while</span></div><div class="line">          <span class="comment">// note that `ReplicaFetcherThread.handlePartitionsWithError` will also introduce the same delay for every</span></div><div class="line">          <span class="comment">// partition with error effectively doubling the delay. It would be good to improve this.</span></div><div class="line">          partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  fetcherStats.requestRate.mark()</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (responseData.nonEmpty) &#123; <span class="comment">//note: fetch 结果不为空</span></div><div class="line">    <span class="comment">// process fetched data</span></div><div class="line">    inLock(partitionMapLock) &#123;</div><div class="line"></div><div class="line">      responseData.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionData) =&gt;</div><div class="line">        <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">        <span class="keyword">val</span> partitionId = topicPartition.partition</div><div class="line">        <span class="type">Option</span>(partitionStates.stateValue(topicPartition)).foreach(currentPartitionFetchState =&gt;</div><div class="line">          <span class="comment">// we append to the log if the current offset is defined and it is the same as the offset requested during fetch</span></div><div class="line">          <span class="comment">//note: 如果 fetch 的 offset 与返回结果的 offset 相同，并且返回没有异常，那么就将拉取的数据追加到对应的 partition 上</span></div><div class="line">          <span class="keyword">if</span> (fetchRequest.offset(topicPartition) == currentPartitionFetchState.offset) &#123;</div><div class="line">            <span class="type">Errors</span>.forCode(partitionData.errorCode) <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NONE</span> =&gt;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line">                  <span class="keyword">val</span> newOffset = records.shallowEntries.asScala.lastOption.map(_.nextOffset).getOrElse(</div><div class="line">                    currentPartitionFetchState.offset)</div><div class="line"></div><div class="line">                  fetcherLagStats.getAndMaybePut(topic, partitionId).lag = <span class="type">Math</span>.max(<span class="number">0</span>L, partitionData.highWatermark - newOffset)</div><div class="line">                  <span class="comment">// Once we hand off the partition data to the subclass, we can't mess with it any more in this thread</span></div><div class="line">                  <span class="comment">//note: 将 fetch 的数据追加到日志文件中</span></div><div class="line">                  processPartitionData(topicPartition, currentPartitionFetchState.offset, partitionData)</div><div class="line"></div><div class="line">                  <span class="keyword">val</span> validBytes = records.validBytes</div><div class="line">                  <span class="keyword">if</span> (validBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">                    <span class="comment">// Update partitionStates only if there is no exception during processPartitionData</span></div><div class="line">                    <span class="comment">//note: 更新 fetch 的 offset 位置</span></div><div class="line">                    partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                    fetcherStats.byteRate.mark(validBytes) <span class="comment">//note: 更新 metrics</span></div><div class="line">                  &#125;</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> ime: <span class="type">CorruptRecordException</span> =&gt;</div><div class="line">                    <span class="comment">// we log the error and continue. This ensures two things</span></div><div class="line">                    <span class="comment">// 1. If there is a corrupt message in a topic partition, it does not bring the fetcher thread down and cause other topic partition to also lag</span></div><div class="line">                    <span class="comment">// 2. If the message is corrupt due to a transient state in the log (truncation, partial writes can cause this), we simply continue and</span></div><div class="line">                    <span class="comment">// should get fixed in the subsequent fetches</span></div><div class="line">                    <span class="comment">//note: CRC 验证失败时，打印日志，并继续进行（这个线程还会有其他的 tp 拉取，防止影响其他副本同步）</span></div><div class="line">                    logger.error(<span class="string">"Found invalid messages during fetch for partition ["</span> + topic + <span class="string">","</span> + partitionId + <span class="string">"] offset "</span> + currentPartitionFetchState.offset  + <span class="string">" error "</span> + ime.getMessage)</div><div class="line">                    updatePartitionsWithError(topicPartition);</div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    <span class="comment">//note: 这里还会抛出异常，是 RUNTimeException</span></div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"error processing data for partition [%s,%d] offset %d"</span></div><div class="line">                      .format(topic, partitionId, currentPartitionFetchState.offset), e)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">OFFSET_OUT_OF_RANGE</span> =&gt; <span class="comment">//note: Out-of-range 的情况处理</span></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> newOffset = handleOffsetOutOfRange(topicPartition)</div><div class="line">                  partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                  error(<span class="string">"Current offset %d for partition [%s,%d] out of range; reset offset to %d"</span></div><div class="line">                    .format(currentPartitionFetchState.offset, topic, partitionId, newOffset))</div><div class="line">                &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理 out-of-range 是抛出的异常</span></div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    error(<span class="string">"Error getting offset for partition [%s,%d] to broker %d"</span>.format(topic, partitionId, sourceBroker.id), e)</div><div class="line">                    updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> _ =&gt; <span class="comment">//note: 其他的异常情况</span></div><div class="line">                <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">                  error(<span class="string">"Error for partition [%s,%d] to broker %d:%s"</span>.format(topic, partitionId, sourceBroker.id,</div><div class="line">                    partitionData.exception.get))</div><div class="line">                  updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理拉取遇到的错误读的 tp</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithError.nonEmpty) &#123;</div><div class="line">    debug(<span class="string">"handling partitions with error for %s"</span>.format(partitionsWithError))</div><div class="line">    handlePartitionsWithErrors(partitionsWithError)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其处理过程简单总结一下：</p>
<ol>
<li>通过 <code>fetch()</code> 方法，发送 Fetch 请求，获取相应的 response（如果遇到异常，那么在下次发送 Fetch 请求之前，会 sleep 一段时间再发）；</li>
<li>如果返回的结果 不为空，并且 Fetch 请求的 offset 信息与返回结果的 offset 信息对得上，那么就会调用 <code>processPartitionData()</code> 方法将拉取到的数据追加本地副本的日志文件中，如果返回结果有错误信息，那么就对相应错误进行相应的处理；</li>
<li>对在 Fetch 过程中遇到异常或返回错误的 topic-partition，会进行 delay 操作，下次 Fetch 请求的发生至少要间隔 <code>replica.fetch.backoff.ms</code> 时间。</li>
</ol>
<h4 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h4><p><code>fetch()</code> 方法作用是发送 Fetch 请求，并返回相应的结果，其具体的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送 fetch 请求，获取拉取结果</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span></span>(fetchRequest: <span class="type">FetchRequest</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)] = &#123;</div><div class="line">  <span class="keyword">val</span> clientResponse = sendRequest(fetchRequest.underlying)</div><div class="line">  <span class="keyword">val</span> fetchResponse = clientResponse.responseBody.asInstanceOf[<span class="type">FetchResponse</span>]</div><div class="line">  fetchResponse.responseData.asScala.toSeq.map &#123; <span class="keyword">case</span> (key, value) =&gt;</div><div class="line">    key -&gt; <span class="keyword">new</span> <span class="type">PartitionData</span>(value)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 发送请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(requestBuilder: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>]): <span class="type">ClientResponse</span> = &#123;</div><div class="line">  <span class="keyword">import</span> kafka.utils.<span class="type">NetworkClientBlockingOps</span>._</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">if</span> (!networkClient.blockingReady(sourceNode, socketTimeout)(time))</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SocketTimeoutException</span>(<span class="string">s"Failed to connect within <span class="subst">$socketTimeout</span> ms"</span>)</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> clientRequest = networkClient.newClientRequest(sourceBroker.id.toString, requestBuilder,</div><div class="line">        time.milliseconds(), <span class="literal">true</span>)</div><div class="line">      networkClient.blockingSendAndReceive(clientRequest)(time) <span class="comment">//note: 阻塞直到获取返回结果</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      networkClient.close(sourceBroker.id.toString)</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processPartitionData"><a href="#processPartitionData" class="headerlink" title="processPartitionData"></a>processPartitionData</h4><p>这个方法的作用是，处理 Fetch 请求的具体数据内容，简单来说就是：检查一下数据大小是否超过限制、将数据追加到本地副本的日志文件中、更新本地副本的 hw 值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// process fetched data</span></div><div class="line"><span class="comment">//note: 处理 fetch 的数据，将 fetch 的数据追加的日志文件中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">processPartitionData</span></span>(topicPartition: <span class="type">TopicPartition</span>, fetchOffset: <span class="type">Long</span>, partitionData: <span class="type">PartitionData</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</div><div class="line">    <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line"></div><div class="line">    <span class="comment">//note: 检查 records</span></div><div class="line">    maybeWarnIfOversizedRecords(records, topicPartition)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (fetchOffset != replica.logEndOffset.messageOffset)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Offset mismatch for partition %s: fetched offset = %d, log end offset = %d."</span>.format(topicPartition, fetchOffset, replica.logEndOffset.messageOffset))</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d for partition %s. Received %d messages and leader hw %d"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))</div><div class="line">    replica.log.get.append(records, assignOffsets = <span class="literal">false</span>) <span class="comment">//note: 将 fetch 的数据追加到 log 中</span></div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d after appending %d bytes of messages for partition %s"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, records.sizeInBytes, topicPartition))</div><div class="line">    <span class="comment">//note: 更新 replica 的 hw（logEndOffset 在追加数据后也会立马进行修改)</span></div><div class="line">    <span class="keyword">val</span> followerHighWatermark = replica.logEndOffset.messageOffset.min(partitionData.highWatermark)</div><div class="line">    <span class="comment">// for the follower replica, we do not need to keep</span></div><div class="line">    <span class="comment">// its segment base offset the physical position,</span></div><div class="line">    <span class="comment">// these values will be computed upon making the leader</span></div><div class="line">    <span class="comment">//note: 这个值主要是用在 leader replica 上的</span></div><div class="line">    replica.highWatermark = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(followerHighWatermark)</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">s"Follower <span class="subst">$&#123;replica.brokerId&#125;</span> set replica high watermark for partition <span class="subst">$topicPartition</span> to <span class="subst">$followerHighWatermark</span>"</span>)</div><div class="line">    <span class="keyword">if</span> (quota.isThrottled(topicPartition))</div><div class="line">      quota.record(records.sizeInBytes)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">s"Disk error while replicating data for <span class="subst">$topicPartition</span>"</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="副本同步异常情况的处理"><a href="#副本同步异常情况的处理" class="headerlink" title="副本同步异常情况的处理"></a>副本同步异常情况的处理</h2><p>在副本同步的过程中，会遇到哪些异常情况呢？</p>
<p>大家一定会想到关于 offset 的问题，在 Kafka 中，关于 offset 的处理，无论是 producer 端、consumer 端还是其他地方，offset 似乎都是一个形影不离的问题。在副本同步时，关于 offset，会遇到什么问题呢？下面举两个异常的场景：</p>
<ol>
<li>假如当前本地（id：1）的副本现在是 leader，其 LEO 假设为1000，而另一个在 isr 中的副本（id：2）其 LEO 为800，此时出现网络抖动，id 为1 的机器掉线后又上线了，但是此时副本的 leader 实际上已经变成了 2，而2的 LEO 为800，这时候1启动副本同步线程去2上拉取数据，希望从 offset=1000 的地方开始拉取，但是2上最大的 offset 才是800，这种情况该如何处理呢？</li>
<li>假设一个 replica （id：1）其 LEO 是10，它已经掉线好几天，这个 partition leader 的 offset 范围是 [100, 800]，那么 1 重启启动时，它希望从 offset=10 的地方开始拉取数据时，这时候发生了 OutOfRange，不过跟上面不同的是这里是小于了 leader offset 的范围，这种情况又该怎么处理？</li>
</ol>
<p>以上两种情况都是 offset OutOfRange 的情况，只不过：一是 Fetch Offset 超过了 leader 的 LEO，二是 Fetch Offset 小于 leader 最小的 offset，在介绍 Kafka 解决方案之前，我们先来自己思考一下这两种情况应该怎么处理？</p>
<ol>
<li>如果 fetch offset 超过 leader 的 offset，这时候副本应该是回溯到 leader 的 LEO 位置（超过这个值的数据删除），然后再去进行副本同步，当然这种解决方案其实是无法保证 leader 与 follower 数据的完全一致，再次发生 leader 切换时，可能会导致数据的可见性不一致，但既然用户允许了脏选举的发生，其实我们是可以认为用户是可以接收这种情况发生的；</li>
<li>这种就比较容易处理，首先清空本地的数据，因为本地的数据都已经过期了，然后从 leader 的最小 offset 位置开始拉取数据。</li>
</ol>
<p>上面是我们比较容易想出的解决方案，而在 Kafka 中，其解决方案也很类似，不过遇到情况比上面我们列出的两种情况多了一些复杂，其解决方案如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">   * Unclean leader election: A follower goes down, in the meanwhile the leader keeps appending messages. The follower comes back up</div><div class="line">   * and before it has completely caught up with the leader's logs, all replicas in the ISR go down. The follower is now uncleanly</div><div class="line">   * elected as the new leader, and it starts appending messages from the client. The old leader comes back up, becomes a follower</div><div class="line">   * and it may discover that the current leader's end offset is behind its own end offset.</div><div class="line">   *</div><div class="line">   * In such a case, truncate the current follower's log to the current leader's end offset and continue fetching.</div><div class="line">   *</div><div class="line">   * There is a potential for a mismatch between the logs of the two replicas here. We don't fix this mismatch as of now.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 脏选举的发生</span></div><div class="line">  <span class="comment">//note: 获取最新的 offset</span></div><div class="line">  <span class="keyword">val</span> leaderEndOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">LATEST_TIMESTAMP</span>,</div><div class="line">    brokerConfig.brokerId)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (leaderEndOffset &lt; replica.logEndOffset.messageOffset) &#123; <span class="comment">//note: leaderEndOffset 小于 副本 LEO 的情况</span></div><div class="line">    <span class="comment">// Prior to truncating the follower's log, ensure that doing so is not disallowed by the configuration for unclean leader election.</span></div><div class="line">    <span class="comment">// This situation could only happen if the unclean election configuration for a topic changes while a replica is down. Otherwise,</span></div><div class="line">    <span class="comment">// we should never encounter this situation since a non-ISR leader cannot be elected if disallowed by the broker configuration.</span></div><div class="line">    <span class="comment">//note: 这种情况只是发生在 unclear election 的情况下</span></div><div class="line">    <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(brokerConfig.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(replicaMgr.zkUtils,</div><div class="line">      <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许 unclear elect 时,直接退出进程</span></div><div class="line">      <span class="comment">// Log a fatal error and shutdown the broker to ensure that data loss does not unexpectedly occur.</span></div><div class="line">      fatal(<span class="string">"Exiting because log truncation is not allowed for partition %s,"</span>.format(topicPartition) +</div><div class="line">        <span class="string">" Current leader %d's latest offset %d is less than replica %d's latest offset %d"</span></div><div class="line">        .format(sourceBroker.id, leaderEndOffset, brokerConfig.brokerId, replica.logEndOffset.messageOffset))</div><div class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: warn 日志信息</span></div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's latest offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderEndOffset))</div><div class="line">    <span class="comment">//note: 进行截断操作,将offset 大于等于targetOffset 的数据和索引删除</span></div><div class="line">    replicaMgr.logManager.truncateTo(<span class="type">Map</span>(topicPartition -&gt; leaderEndOffset))</div><div class="line">    leaderEndOffset</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: leader 的 LEO 大于 follower 的 LEO 的情况下,还发生了 OutOfRange</span></div><div class="line">    <span class="comment">//note: 1. follower 下线了很久,其 LEO 已经小于了 leader 的 StartOffset;</span></div><div class="line">    <span class="comment">//note: 2. 脏选举发生时, 如果 old leader 的 HW 大于 new leader 的 LEO,此时 old leader 回溯到 HW,并且这个位置开始拉取数据发生了 Out of range</span></div><div class="line">    <span class="comment">//note:    当这个方法调用时,随着 produce 持续产生数据,可能出现 leader LEO 大于 Follower LEO 的情况（不做任何处理,重试即可解决,但</span></div><div class="line">    <span class="comment">//note:    无法保证数据的一致性）。</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * If the leader's log end offset is greater than the follower's log end offset, there are two possibilities:</div><div class="line">     * 1. The follower could have been down for a long time and when it starts up, its end offset could be smaller than the leader's</div><div class="line">     * start offset because the leader has deleted old logs (log.logEndOffset &lt; leaderStartOffset).</div><div class="line">     * 2. When unclean leader election occurs, it is possible that the old leader's high watermark is greater than</div><div class="line">     * the new leader's log end offset. So when the old leader truncates its offset to its high watermark and starts</div><div class="line">     * to fetch from the new leader, an OffsetOutOfRangeException will be thrown. After that some more messages are</div><div class="line">     * produced to the new leader. While the old leader is trying to handle the OffsetOutOfRangeException and query</div><div class="line">     * the log end offset of the new leader, the new leader's log end offset becomes higher than the follower's log end offset.</div><div class="line">     *</div><div class="line">     * In the first case, the follower's current log end offset is smaller than the leader's log start offset. So the</div><div class="line">     * follower should truncate all its logs, roll out a new segment and start to fetch from the current leader's log</div><div class="line">     * start offset.</div><div class="line">     * In the second case, the follower should just keep the current log segments and retry the fetch. In the second</div><div class="line">     * case, there will be some inconsistency of data between old and new leader. We are not solving it here.</div><div class="line">     * If users want to have strong consistency guarantees, appropriate configurations needs to be set for both</div><div class="line">     * brokers and producers.</div><div class="line">     *</div><div class="line">     * Putting the two cases together, the follower should fetch from the higher one of its replica log end offset</div><div class="line">     * and the current leader's log start offset.</div><div class="line">     *</div><div class="line">     */</div><div class="line">    <span class="keyword">val</span> leaderStartOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">EARLIEST_TIMESTAMP</span>,</div><div class="line">      brokerConfig.brokerId)</div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's start offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderStartOffset))</div><div class="line">    <span class="keyword">val</span> offsetToFetch = <span class="type">Math</span>.max(leaderStartOffset, replica.logEndOffset.messageOffset)</div><div class="line">    <span class="comment">// Only truncate log when current leader's log start offset is greater than follower's log end offset.</span></div><div class="line">    <span class="keyword">if</span> (leaderStartOffset &gt; replica.logEndOffset.messageOffset) <span class="comment">//note: 如果 leader 的 startOffset 大于副本的最大 offset</span></div><div class="line">      <span class="comment">//note: 将这个 log 的数据全部清空,并且从 leaderStartOffset 开始拉取数据</span></div><div class="line">      replicaMgr.logManager.truncateFullyAndStartAt(topicPartition, leaderStartOffset)</div><div class="line">    offsetToFetch</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>针对第一种情况，在 Kafka 中，实际上还会发生这样一种情况，1 在收到 OutOfRange 错误时，这时去 leader 上获取的 LEO 值与最小的 offset 值，这时候却发现 leader 的 LEO 已经从 800 变成了 1100（这个 topic-partition 的数据量增长得比较快），再按照上面的解决方案就不太合理，Kafka 这边的解决方案是：遇到这种情况，进行重试就可以了，下次同步时就会正常了，但是依然会有上面说的那个问题。</p>
<h2 id="replica-fetcher-线程的关闭"><a href="#replica-fetcher-线程的关闭" class="headerlink" title="replica fetcher 线程的关闭"></a>replica fetcher 线程的关闭</h2><p>最后我们再来介绍一下 replica fetcher 线程在什么情况下会关闭，同样，看一下最开始那张图的第三部分，图中已经比较清晰地列出了 replica fetcher 线程关闭的条件，在三种情况下会关闭对这个 topic-partition 的拉取操作（<code>becomeLeaderOrFollower()</code> 这个方法会在对 LeaderAndIsr 请求处理的文章中讲解，这里先忽略）：</p>
<ol>
<li><code>stopReplica()</code>：broker 收到了 controller 发来的 StopReplica 请求，这时会开始关闭对指定 topic-partition 的同步线程；</li>
<li><code>makeLeaders</code>：这些 partition 的本地副本被选举成了 leader，这时候就会先停止对这些 topic-partition 副本同步线程；</li>
<li><code>makeFollowers()</code>：前面已经介绍过，这里实际上停止副本同步，然后再开启副本同步线程，因为这些 topic-partition 的 leader 可能发生了切换。</li>
</ol>
<blockquote>
<p>这里直接说线程关闭，其实不是很准确，因为每个 replica fetcher 线程操作的是多个 topic-partition，而在关闭的粒度是 partition 级别，只有这个线程分配的 partition 全部关闭后，这个线程才会真正被关闭。</p>
</blockquote>
<h3 id="关闭副本同步"><a href="#关闭副本同步" class="headerlink" title="关闭副本同步"></a>关闭副本同步</h3><p>看下 ReplicaManager 中触发 replica fetcher 线程关闭的三个方法。</p>
<h4 id="stopReplica"><a href="#stopReplica" class="headerlink" title="stopReplica"></a>stopReplica</h4><p>StopReplica 的请求实际上是 Controller 发送过来的，这个在 controller 部分会讲述，它触发的条件有多种，比如：broker 下线、partition replica 迁移等等，ReplicaManager 这里的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 tp 的 leader replica</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLeaderReplicaIfLocal</span></span>(topicPartition: <span class="type">TopicPartition</span>): <span class="type">Replica</span> =  &#123;</div><div class="line">  <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取对应的 Partiion 对象</span></div><div class="line">  partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">s"Partition <span class="subst">$topicPartition</span> doesn't exist on <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">      partition.leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt; leaderReplica <span class="comment">//note: 返回 leader 对应的副本</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">s"Leader not local for partition <span class="subst">$topicPartition</span> on broker <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h4><p><code>makeLeaders()</code> 方法的调用是在 broker 上这个 partition 的副本被设置为 leader 时触发的，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> * Make the current broker to become leader for a given set of partitions by:</div><div class="line"> *</div><div class="line"> * 1. Stop fetchers for these partitions</div><div class="line"> * 2. Update the partition metadata in cache</div><div class="line"> * 3. Add these partitions to the leader partitions set</div><div class="line"> *</div><div class="line"> * If an unexpected error is thrown in this function, it will be propagated to KafkaApis where</div><div class="line"> * the error message will be set on each partition since we do not know which partition caused it. Otherwise,</div><div class="line"> * return the set of partitions that are made leader due to this method</div><div class="line"> *</div><div class="line"> *  <span class="doctag">TODO:</span> the above may need to be fixed later</div><div class="line"> */</div><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，这个方法的过程逻辑如下：</p>
<ol>
<li>先停止对这些 partition 的副本同步流程，因为这些 partition 的本地副本已经被选举成为了 leader；</li>
<li>将这些 partition 的本地副本设置为 leader，并且开始更新相应 meta 信息（主要是记录其他 follower 副本的相关信息）；</li>
<li>将这些 partition 添加到本地记录的 leader partition 集合中。</li>
</ol>
<h4 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h4><p>这个在前面已经讲述过了，参考前面的讲述。</p>
<h3 id="removeFetcherForPartitions"><a href="#removeFetcherForPartitions" class="headerlink" title="removeFetcherForPartitions"></a>removeFetcherForPartitions</h3><p>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 删除对这些 topic-partition 的副本同步设置，这里在实现时，会遍历所有的 replica fetcher 线程，都执行 <code>removePartitions()</code> 方法来移除对应的 topic-partition 集合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除一个 partition 的 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeFetcherForPartitions</span></span>(partitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">for</span> (fetcher &lt;- fetcherThreadMap.values) <span class="comment">//note: 遍历所有的 fetchThread 去移除这个 topic-partition 集合</span></div><div class="line">      fetcher.removePartitions(partitions)</div><div class="line">  &#125;</div><div class="line">  info(<span class="string">"Removed fetcher for partitions %s"</span>.format(partitions.mkString(<span class="string">","</span>)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="removePartitions"><a href="#removePartitions" class="headerlink" title="removePartitions"></a>removePartitions</h3><p>这个方法的作用是：ReplicaFetcherThread 将这些 topic-partition 从自己要拉取的 partition 列表中移除。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removePartitions</span></span>(topicPartitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  partitionMapLock.lockInterruptibly()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    topicPartitions.foreach &#123; topicPartition =&gt;</div><div class="line">      partitionStates.remove(topicPartition)</div><div class="line">      fetcherLagStats.unregister(topicPartition.topic, topicPartition.partition)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> partitionMapLock.unlock()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ReplicaFetcherThread-的关闭"><a href="#ReplicaFetcherThread-的关闭" class="headerlink" title="ReplicaFetcherThread 的关闭"></a>ReplicaFetcherThread 的关闭</h3><p>前面介绍那么多，似乎还是没有真正去关闭，那么 ReplicaFetcherThread 真正关闭是哪里操作的呢？</p>
<p>实际上 ReplicaManager 每次处理完 LeaderAndIsr 请求后，都会调用 ReplicaFetcherManager 的 <code>shutdownIdleFetcherThreads()</code> 方法，如果 fetcher 线程要拉取的 topic-partition 集合为空，那么就会关闭掉对应的 fetcher 线程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 关闭没有拉取 topic-partition 任务的拉取线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownIdleFetcherThreads</span></span>() &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> keysToBeRemoved = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">BrokerAndFetcherId</span>]</div><div class="line">    <span class="keyword">for</span> ((key, fetcher) &lt;- fetcherThreadMap) &#123;</div><div class="line">      <span class="keyword">if</span> (fetcher.partitionCount &lt;= <span class="number">0</span>) &#123; <span class="comment">//note: 如果该线程拉取的 partition 数小于 0</span></div><div class="line">        fetcher.shutdown()</div><div class="line">        keysToBeRemoved += key</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    fetcherThreadMap --= keysToBeRemoved</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Replica Fetcher 线程这部分的内容终于讲解完了，希望能对大家有所帮助，有问题欢迎通过留言、微博或邮件进行交流。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
</feed>
