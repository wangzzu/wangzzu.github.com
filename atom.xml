<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Matt&#39;s Blog</title>
  <subtitle>王蒙</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://matt33.com/"/>
  <updated>2018-09-01T15:57:17.000Z</updated>
  <id>http://matt33.com/</id>
  
  <author>
    <name>Matt</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>YARN 架构学习总结</title>
    <link href="http://matt33.com/2018/09/01/yarn-architecture-learn/"/>
    <id>http://matt33.com/2018/09/01/yarn-architecture-learn/</id>
    <published>2018-09-01T14:39:47.000Z</published>
    <updated>2018-09-01T15:57:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>关于 Hadoop 的介绍，这里就不再多说，可以简答来说 Hadoop 的出现真正让更多的互联网公司开始有能力解决大数据场景下的问题，其中的 HDFS 和 YARN 已经成为大数据场景下存储和资源调度的统一解决方案（MR 现在正在被 Spark 所取代，Spark 在计算这块的地位也开始受到其他框架的冲击，流计算上有 Flink，AI 上有 Tensorflow，两面夹击，但是 Spark 的生态建设得很好，其他框架想要在生产环境立马取代还有很长的路要走）。本片文章就是关于 YARN 框架学习的简单总结，目的是希望自己能对分布式调度这块有更深入的了解，当然也希望也这篇文章能够对初学者有所帮助，文章的主要内容来自 <a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a> 和 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>。</p>
<h1 id="Yarn-背景"><a href="#Yarn-背景" class="headerlink" title="Yarn 背景"></a>Yarn 背景</h1><p>关于 YARN 出现的背景，还是得从 Hadoop1.0 说起，在 Hadoop1.0 中，MR 作业的调度还是有两个重要的组件：JobTracker 和 TaskTracker，其基础的架构如下图所示，从下图中可以大概看出原 MR 作业启动流程：</p>
<ol>
<li>首先用户程序 (Client) 提交了一个 job，job 的信息会发送到 JobTracker 中，JobTracker 是 Map-Reduce 框架的中心，它需要与集群中的机器定时通信 (心跳：heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理<strong>所有 job</strong> 失败、重启等操作；</li>
<li>TaskTracker 是 Map-Reduce 集群中每台机器都有的一个组件，它做的事情主要是监视自己所在机器的资源使用情况；</li>
<li>TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat 发送给 JobTracker，JobTracker 会搜集这些信息以便处理新提交的 job，来决定其应该分配运行在哪些机器上。</li>
</ol>
<p><img src="/images/hadoop/yarn10.png" alt="Hadoop 1.0 调度的架构图"></p>
<p>可以看出原来的调度框架实现非常简答明了，在 Hadoop 推出的最初几年，也获得业界的认可，但是随着集群规模的增大，很多的弊端开始显露出来，主要有以下几点：</p>
<ol>
<li>JobTracker 是 Map-Reduce 的集中处理点，存在<strong>单点故障</strong>；</li>
<li>JobTracker 赋予的功能太多，导致负载过重，1.0 时未将资源管理与作业控制（包括：作业监控、容错等）分开，导致负载重而且无法支撑更多的计算框架，当集群的作业非常多时，会有很大的内存开销，潜在来说，也增加了 JobTracker fail 的风险，这也是业界普遍总结出 Hadoop1.0 的 Map-Reduce 只能支持 4000 节点主机上限的原因；</li>
<li>在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/ 内存的占用情况，如果两个大内存消耗的 task 被调度到了一个节点上，很容易出现 OOM；</li>
<li>在 TaskTracker 端，把资源强制划分为 map task slot 和 reduce task slot, 如果当系统中只有 map task 或者只有 reduce task 的时候，会造成资源的浪费，也就是前面提过的集群资源利用的问题。</li>
</ol>
<p>Hadoop 2.0 中下一代 MR 框架的基本设计思想就是将 JobTracker 的两个主要功能，资源管理和作业控制（包括作业监控、容错等），分拆成两个独立的进程。资源管理与具体的应用程序无关，它负责整个集群的资源（内存、CPU、磁盘等）管理，而作业控制进程则是直接与应用程序相关的模块，且每个作业控制进程只负责管理一个作业，这样就是 YARN 诞生的背景，它是在 MapReduce 框架上衍生出的一个资源统一的管理平台。</p>
<h1 id="Yarn-架构"><a href="#Yarn-架构" class="headerlink" title="Yarn 架构"></a>Yarn 架构</h1><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：</p>
<p><img src="/images/hadoop/yarn20.gif" alt="YARN 基本架构"></p>
<h2 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h2><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：</p>
<ol>
<li>调度器：Scheduler；</li>
<li>应用程序管理器：Applications Manager，ASM。</li>
</ol>
<h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><p>调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 <strong>资源容器(Resource Container，也即 Container)</strong>，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。</p>
<h3 id="应用程序管理器"><a href="#应用程序管理器" class="headerlink" title="应用程序管理器"></a>应用程序管理器</h3><p>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p>
<h2 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h2><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p>
<h2 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h2><p>提交的每个作业都会包含一个 AM，主要功能包括：</p>
<ol>
<li>与 RM 协商以获取资源（用 container 表示）；</li>
<li>将得到的任务进一步分配给内部的任务；</li>
<li>与 NM 通信以启动/停止任务；</li>
<li>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。</li>
</ol>
<p>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。</p>
<h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><p>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>
<h1 id="YARN-作业提交流程"><a href="#YARN-作业提交流程" class="headerlink" title="YARN 作业提交流程"></a>YARN 作业提交流程</h1><p>当用户向 YARN 中提交一个应用程序后，YARN 将分两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成，如下图所示（此图来自<a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a>）：</p>
<p><img src="/images/hadoop/yarn-flow.png" alt="YARN 工作流程"></p>
<p>上图所示的 YARN 工作流程分为以下几个步骤：</p>
<ol>
<li>用户向 YARN 提交应用程序，其中包括 ApplicationMaster 程序，启动 ApplicationMaster 命令、用户程序等；</li>
<li>RM 为该应用程序分配第一个 Container，并与对应的 NM 通信，要求它在这个 Container 中启动应用程序的 ApplicationMaster；</li>
<li>ApplicationMaster 首先向 RM 注册，这样用户可以直接通过 NM 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，一直重复下面的 4-7 步；</li>
<li>ApplicationMaster 采用轮询的方式通过 RPC 协议向 RM 申请和领取资源；</li>
<li>一旦 ApplicationMaster 申请到资源后，便与对应的 NM 通信，要求它启动任务；</li>
<li>NM 为任务设置好运行环境（包括环境变量、jar 包等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</li>
<li>应用程序运行完成后，ApplicationMaster 向 RM 注销并关闭自己（当然像 Storm、Flink 这种常驻应用程序列外）。</li>
</ol>
<h1 id="调度器-1"><a href="#调度器-1" class="headerlink" title="调度器"></a>调度器</h1><p>YARN 的调度器是一个可插拔的组件，目前社区已经提供了 FIFO Scheduler（先进先出调度器）、Capacity Scheduler（能力调度器）、Fair Scheduler（公平调度器），用户也可以继承 ResourceScheduler 的接口实现自定义的调度器，就像 app on yarn 流程一样，不同的应用可以自己去实现，这里只是简单讲述上述三种调度器的基本原理。</p>
<h2 id="FIFO-Scheduler"><a href="#FIFO-Scheduler" class="headerlink" title="FIFO Scheduler"></a>FIFO Scheduler</h2><p>FIFO 是最简单的资源调度策略，提交的作业按照提交时间先后顺序或者根据优先级次序将其放入线性队列相应的位置，在资源调度时，<strong>按照队列的先后顺序、先进先出地进行调度和资源分配</strong>。</p>
<p>很明显这种调度器过于简单，在实际的生产中，应用不是很多，毕竟需要调度的作业是有不同的优先级的。</p>
<h2 id="公平调度器（Fair-Scheduler）"><a href="#公平调度器（Fair-Scheduler）" class="headerlink" title="公平调度器（Fair Scheduler）"></a>公平调度器（Fair Scheduler）</h2><p>公平调度器先将用户的任务分配到多个资源池（Pool）中，每个资源池设定资源分配最低保障和最高上限，管理员也可以指定资源池的优先级，优先级高的资源池将会被分配更多的资源，当一个资源池有剩余时，可以临时将剩余资源共享给其他资源池。公平调度器的调度过程如下：</p>
<ol>
<li>根据每个资源池的最小资源保障，将系统中的部分资源分配给各个资源池；</li>
<li>根据资源池的指定优先级讲剩余资源按照比例分配给各个资源池；</li>
<li>在各个资源池中，按照作业的优先级或者根据公平策略将资源分配给各个作业；</li>
</ol>
<p>公平调度器有以下几个特点：</p>
<ol>
<li><strong>支持抢占式调度</strong>，即如果某个资源池长时间未能被分配到公平共享量的资源，则调度器可以杀死过多分配资源的资源池的任务，以空出资源供这个资源池使用；</li>
<li><strong>强调作业之间的公平性</strong>：在每个资源池中，公平调度器默认使用公平策略来实现资源分配，这种公平策略是最大最小公平算法的一种具体实现，可以尽可能保证作业间的资源分配公平性；</li>
<li><strong>负载均衡</strong>：公平调度器提供了一个基于任务数目的负载均衡机制，该机制尽可能将系统中的任务均匀分配到给各个节点上；</li>
<li><strong>调度策略配置灵活</strong>：允许管理员为每个队列单独设置调度策略；</li>
<li><strong>提高小应用程序响应时间</strong>：由于采用了最大最小公平算法，小作业可以快速获得资源并运行完成。</li>
</ol>
<h2 id="能力调度器（Capacity-Scheduler）"><a href="#能力调度器（Capacity-Scheduler）" class="headerlink" title="能力调度器（Capacity Scheduler）"></a>能力调度器（Capacity Scheduler）</h2><p>能力调度器是 Yahool 为 Hadoop 开发的多用户调度器，应用于用户量众多的应用场景，与公平调度器相比，其更强调资源在用户之间而非作业之间的公平性。</p>
<p>它将用户和任务组织成多个队列，每个队列可以设定资源最低保障和使用上限，当一个队列的资源有剩余时，可以将剩余资源暂时分享给其他队列。调度器在调度时，优先将资源分配给资源使用率最低的队列（即队列已使用资源量占分配给队列的资源量比例最小的队列）；在队列内部，则按照作业优先级的先后顺序遵循 FIFO 策略进行调度。</p>
<p>能力调度器有以下几点特点：</p>
<ol>
<li><strong>容量保证</strong>：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源；</li>
<li><strong>灵活性</strong>：如果一个队列资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放的资源会归还给该队列；</li>
<li><strong>多重租赁</strong>：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增多多重约束；</li>
<li><strong>安全保证</strong>：每个队列有严格的 ACL 列表规定它访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序；</li>
<li><strong>动态更新配置文件</strong>：管理可以根据需要动态修改各种配置参数。</li>
</ol>
<h1 id="Yarn-容错"><a href="#Yarn-容错" class="headerlink" title="Yarn 容错"></a>Yarn 容错</h1><p>对于分布式系统，不论是调度系统还是其他系统，容错机制都是非常必要的，这里我们简单看下 YARN 的容错机制，YARN 需要做容错的地方，有以下四个地方：</p>
<ol>
<li>ApplicationMaster 容错：ResourceManager 会和 ApplicationMaster 保持通信，一旦发现 ApplicationMaster 失败或者超时，会为其重新分配资源并重启。重启后 ApplicationMaster 的运行状态需要自己恢复，比如 MRAppMaster 会把相关的状态记录到 HDFS 上，重启后从 HDFS 读取运行状态恢复；</li>
<li>NodeManager 容错：NodeManager 如果超时，则 ResourceManager 会认为它失败，将其上的所有 container 标记为失败并通知相应的 ApplicationMaster，由 AM 决定如何处理（可以重新分配任务，可以整个作业失败，重新拉起）；</li>
<li>container 容错：如果 ApplicationMaster 在一定时间内未启动分配的 container，RM 会将其收回，如果 Container 运行失败，RM 会告诉对应的 AM 由其处理；</li>
<li>RM 容错：RM 采用 HA 机制，这里详细讲述一下。</li>
</ol>
<h2 id="ResourceManager-HA"><a href="#ResourceManager-HA" class="headerlink" title="ResourceManager HA"></a>ResourceManager HA</h2><p>因为 RM 是 YARN 架构中的一个单点，所以他的容错很难做，一般是采用 HA 的方式，有一个 active master 和一个 standby master（可参考：<a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="external">ResourceManager High Availability</a>），HA 的架构方案如下图所示：</p>
<p><img src="/images/hadoop/rm-ha-overview.png" alt="YARN RM HA 机制"></p>
<p>关于 YARN 的 RM 的 HA 机制，其实现与 HDFS 的很像，可以参考前面关于 HDFS 文章的讲述 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-2-0-%E7%9A%84-HA-%E5%AE%9E%E7%8E%B0">HDFS NN HA 实现</a>。</p>
<h1 id="分布式调度器总结"><a href="#分布式调度器总结" class="headerlink" title="分布式调度器总结"></a>分布式调度器总结</h1><p>上面基本已经把 YARN 的相关内容总结完了，这个小节主要讲述一下分布式调度系统的一些内容（调度框架只是具体的一种实现方案），主要讲述分布式调度系统要解决的一些问题和分布式调度系统的调度模型。</p>
<h2 id="调度系统设计遇到的基本问题"><a href="#调度系统设计遇到的基本问题" class="headerlink" title="调度系统设计遇到的基本问题"></a>调度系统设计遇到的基本问题</h2><p>对于分布式调度系统，在实际的生产环境中，遇到的问题很相似，这个小节就是看下调度系统主要面对的问题。</p>
<h3 id="资源异构性与工作负载异构性"><a href="#资源异构性与工作负载异构性" class="headerlink" title="资源异构性与工作负载异构性"></a>资源异构性与工作负载异构性</h3><p>在资源管理与调度场景下，有两类异质性需要考虑：</p>
<ol>
<li>资源异质性：这个是从系统拥有资源的角度来看，对于数据中心来说非常常见，数据中心的机器很难保证完全一样的配置，有的配置会高一些，有的会低一些；</li>
<li>工作负载异质性：在大型互联网公司中很常见，因为各种服务和功能特性各异，对资源的需求千差万别。</li>
</ol>
<h3 id="数据局部性（Data-Locality）"><a href="#数据局部性（Data-Locality）" class="headerlink" title="数据局部性（Data Locality）"></a>数据局部性（Data Locality）</h3><p>在大数据场景下，还有一个基本的共识：将计算任务推送到数据所在地进行而不是反过来。因为数据的移动会产生大量低效的数据网络传输开销，而计算代码相比而言数据小得多，所以将计算任务推动到数据所在地是非常常见的，这就是<strong>数据局部性</strong>，在资源调度中，有三种类型的数据局部性，分别是：</p>
<ol>
<li>节点局部性（Node Locality）：计算任务分配到数据所在机器节点，无需任务网络传输；</li>
<li>机架局部性（Rack Locality）：虽然计算任务与数据分布在不同的节点，但这两个节点在同一个机架中，这也是效率较高的一种数据性；</li>
<li>全局局部性（Global Locality）：需要跨机架的传输，会产生较大的网络传输开销。</li>
</ol>
<h3 id="抢占式调度与非抢占式调度"><a href="#抢占式调度与非抢占式调度" class="headerlink" title="抢占式调度与非抢占式调度"></a>抢占式调度与非抢占式调度</h3><p>在多用户场景下，面对已经分配的资源，资源管理调度系统可以有两种不同类型的调度方式：</p>
<ol>
<li>抢占式调度：对于某个计算任务来说，如果空闲资源不足或者出现不同任务共同竞争同一资源，调度系统可以从比当前计算任务优先级低的其他任务中获取已经分配资源，而被抢占资源的计算任务则需要出让资源停止计算；</li>
<li>非抢占式调度：只允许从空闲资源中进行分配，如果当前空闲资源不足，则须等待其他任务释放资源后才能进行。</li>
</ol>
<h3 id="资源分配粒度（Allocation-Granularity）"><a href="#资源分配粒度（Allocation-Granularity）" class="headerlink" title="资源分配粒度（Allocation Granularity）"></a>资源分配粒度（Allocation Granularity）</h3><p>大数据场景下的计算任务往往由两层结构构成：作业级（Job）和任务级（Task），一个作业由多个并发任务构成，任务之间的依赖关系往往形成有向无环图（DAG），比如：MR 作业，关于作业资源分配的粒度，常见的有两种模式：</p>
<ol>
<li>群体分配（全分或不分）：需要将作业的所有所需资源一次性分配完成；</li>
<li>增量满足式分配策略：对于某个作业，只要分配部分资源就能启动一些任务开始运行，随着空闲资源的不断出现，可以逐步增量式分配给作业的其他任务以维护作业不断向后进行。</li>
</ol>
<p>还有一种策略是 <strong>资源储备策略</strong>，它指的是只有分配到一定量的资源资源才能启动，但是在未获得足够资源的时候，作业可以先持有目前已经分配的资源，并等待其他作业释放资源，这样从调度系统不断获取新资源并进行储备和累积，直到分配到的资源量达到最低标准后开始运行。</p>
<h3 id="饿死（Starvation）与死锁（Dead-Lock）问题"><a href="#饿死（Starvation）与死锁（Dead-Lock）问题" class="headerlink" title="饿死（Starvation）与死锁（Dead Lock）问题"></a>饿死（Starvation）与死锁（Dead Lock）问题</h3><p>饿死和死锁是一个合理的资源调度系统需要避免的两个问题：</p>
<ol>
<li>饿死：指的是这个计算任务持续上时间无法获得开始执行所需的最少资源量，导致一直处于等待执行的状态，比如在资源紧张的情形下，有些低优先级的任务始终无法获得资源分配机会，如果不断出现新提交的高优先级任务，则这些低优先级任务就会出现饿死现象；</li>
<li>死锁：指的是由于资源调度不当导致整个调度无法继续正常执行，比如前面提高的资源储备策略就有可能导致调度系统进入死锁状态，多个作业占有一定作业的情况下，都在等待新的资源释放。</li>
</ol>
<h3 id="资源隔离方法"><a href="#资源隔离方法" class="headerlink" title="资源隔离方法"></a>资源隔离方法</h3><p>目前对于资源隔离最常用的手段是 Linux 容器（Linux Container，LXC，可以参考<a href="https://www.redhat.com/zh/topics/containers/whats-a-linux-container" target="_blank" rel="external">什么是 Linux 容器？</a>），YARN 和 Mesos 都是采用了这种方式来实现资源隔离。LXC 是一种轻量级的内核虚拟化技术，可以用来进行资源和进程运行的隔离，通过 LXC 可以在一台物理机上隔离出多个互相隔离的容器。LXC 在资源管理方面依赖于 Linux 内核的 cgroups 子系统，cgroups 子系统是 Linux 内核提供的一个基于进程组的资源管理的框架，可以为特定的进程组限定可以使用的资源。</p>
<h2 id="调度器模型"><a href="#调度器模型" class="headerlink" title="调度器模型"></a>调度器模型</h2><p>关于资源管理与调度功能的实际功能，分布式调度器根据运行机制的不同进行分类，可以归纳为三种资源管理与调度系统泛型：</p>
<ol>
<li>集中式调度器；</li>
<li>两级调度器；</li>
<li>状态共享调度器。</li>
</ol>
<p>它们的区别与联系如下图所示：</p>
<p><img src="/images/hadoop/scheduler.png" alt="三种调度模型"></p>
<h3 id="集中式调度器"><a href="#集中式调度器" class="headerlink" title="集中式调度器"></a>集中式调度器</h3><p>集中式调度器在整个系统中只运行一个全局的中央调度器实例，所有之上的框架或者计算任务的资源请求全部经由中央调度器来满足（也就是说：资源的使用及任务的执行状态都由中央调度器管理），因此，整个调度系统缺乏并发性且所有调度逻辑全部由中央调度器来完成。集中式调度器有以下这些特点：</p>
<ol>
<li>适合批处理任务和吞吐量较大、运行时间较长的任务；</li>
<li>调度逻辑全部融入到了中央调度器，实现逻辑复杂，灵活性和策略的可扩展性不高；</li>
<li>并发性能较差，比较适合小规模的集群系统；</li>
<li>状态同步比较容易且稳定，这是因为资源使用和任务执行的状态被统一管理，降低了状态同步和并发控制的难度。</li>
</ol>
<h3 id="两级调度器"><a href="#两级调度器" class="headerlink" title="两级调度器"></a>两级调度器</h3><p>对于集中式调度器的不足之处，两级调度器是一个很好的解决方案，它可以看做一种策略下放的机制，它将整个系统的调度工作分为两个级别：</p>
<ol>
<li>中央调度器：中央调度器可以看到集群中所有机器的可用资源并管理其状态，它可以按照一定策略将集群中的所有资源更配各个计算框架，中央调度器级别的资源调度是一种粗粒度的资源调度方式；</li>
<li>框架调度器：各个计算框架在接收到所需资源后，可以根据自身计算任务的特性，使用自身的调度策略来进一步细粒度地分配从中央调度器获得的各种资源。</li>
</ol>
<p>在这种两级调度器架构中，只有中央调度器能够观察到所有集群资源的状态，而每个框架并无全局资源概念（不知道整个集群资源使用情况），只能看到由中央调度器分配给自己的资源，Mesos、YARN 和 Hadoop on Demand 系统是3个典型的两级调度器。两级调度的缺点也非常明显：</p>
<ol>
<li><strong>各个框架无法知道整个集群的实时资源使用情况</strong>：很多框架不需要知道整个集群的实时资源使用情况就可以运行得很顺畅，但是对于其他一些应用，为之提供实时资源使用情况可以挖掘潜在的优化空间；</li>
<li><strong>采用悲观锁，并发粒度小</strong>：悲观锁通常采用锁机制控制并发，这会大大降低性能。</li>
</ol>
<h3 id="状态共享调度器"><a href="#状态共享调度器" class="headerlink" title="状态共享调度器"></a>状态共享调度器</h3><p>通过前面两种模型的介绍，可以发现集群中需要管理的状态主要包括以下两种：</p>
<ol>
<li>系统中资源分配和使用的状态；</li>
<li>系统中任务调度和执行的状态</li>
</ol>
<p>在集中式调度器中，这两个状态都由中心调度器管理，并且一并集成了调度等功能；而在双层调度器中，这两个状态分别由中央调度器和框架调度器管理。集中式调度器可以容易地保证全局状态的一致性，但是可扩展性不够；双层调度器对共享状态的管理较难达到好的一致性保证，也不容易检测资源竞争和死锁。</p>
<p>这也就催生出了另一种调度器 —— 状态共享调度器（Shared-State Scheduler），它是 Google 的 Omega 调度系统提出的一种调度器模型。在这种调度器中，每个计算框架可以看到整个集群中的所有资源，并采用互相竞争的方式去获取自己所需的资源，根据自身特性采取不同的具体资源调整策略，同时系统采用了乐观并发控制手段解决不同框架在资源竞争过程中出现的需求冲突。这样，状态共享调度器在一下两个方面对两级调度器做了相应的优化：</p>
<ol>
<li><strong>乐观并发控制增加了系统的并发性能</strong>；</li>
<li>每个计算框架都可以获得全局的资源使用状况；</li>
</ol>
<p>与两级调度器对比，两者的根本区别在于 <strong>中央调度器功能的强弱不同</strong>，两级调度器依赖中央调度器来进行第一次资源分配，而 Omega 则严重弱化中央调度器的功能，只是维护一份可恢复的集群资源状态信息的主副本，这份数据被称为 <strong>单元状态（Cell State）</strong></p>
<ol>
<li>每个框架在自身内部会维护 单元状态 的一份私有并不断更新的副本信息，而框架对资源的需求则直接在这份副本信息上进行；</li>
<li>只要框架具有特定的优先级，就可以在这份副本信息上申请相应的闲置资源，也可以抢夺已经分配给其他比自身优先级低的计算任务的资源；</li>
<li>一旦框架做出资源决策，则可以改变私有 单元状态 信息并将其同步到全局的 单元状态 信息中区，这样就完成了资源申请并使得这种变化让其他框架可见；</li>
<li>上述资源竞争过程通过 <strong>事务操作</strong> 来进行，保证了操作的原子性。</li>
</ol>
<p>如果两个框架竞争同一份资源，因其决策过程都是在各自私有数据上做出的，并通过原子事务进行提交，系统保证此种情形下只有一个竞争胜出者，而失败者可以后续继续重新申请资源，这是一种乐观并发控制手段，可以增加系统的整体并发性能。</p>
<p>从上面的过程，可以看出，这种架构是一种 <strong>以效率优先，不太考虑资源分配公平性</strong> 的策略，很明显高优先级的任务总是能够在资源竞争过程中获胜，而低优先级的任务存在由于长时间无法竞争到所需资源而被【饿死】的风险。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://item.jd.com/15542271154.html" target="_blank" rel="external">《Hadoop 技术内幕：深入解析 YARN 架构设计与实现原理》</a>;</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="external">Hadoop Yarn</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/" target="_blank" rel="external">Hadoop 新 MapReduce 框架 Yarn 详解</a>；</li>
<li><a href="http://shiyanjun.cn/archives/1119.html" target="_blank" rel="external">Hadoop YARN架构设计要点</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/data/library/bd-yarn-intro/index.html" target="_blank" rel="external">YARN 简介</a>；</li>
<li><a href="https://blog.csdn.net/bingduanlbd/article/details/51880019" target="_blank" rel="external">理解Hadoop YARN架构</a>；</li>
<li><a href="https://dbaplus.cn/news-141-2004-1.html" target="_blank" rel="external">这里有7种主流案例，告诉你调度器架构设计通用法则</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于 Hadoop 的介绍，这里就不再多说，可以简答来说 Hadoop 的出现真正让更多的互联网公司开始有能力解决大数据场景下的问题，其中的 HDFS 和 YARN 已经成为大数据场景下存储和资源调度的统一解决方案（MR 现在正在被 Spark 所取代，Spark 在计算这
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hadoop" scheme="http://matt33.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>如何学习开源项目</title>
    <link href="http://matt33.com/2018/08/01/system-learn-summary/"/>
    <id>http://matt33.com/2018/08/01/system-learn-summary/</id>
    <published>2018-08-01T06:55:11.000Z</published>
    <updated>2018-08-02T11:52:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章的方法论内容基本来自订阅的极客时间-李运华老师的《从0开始学架构》中的一篇文章，会结合自己的学习经验、加上以 Flink 为例来做一个总结，也为了让自己再学习其他开源项目时能够按照这样的一个方法论高效的深入学习。先简单说一下开源项目，开源项目最早从上个世纪开始，我知道最早的是 linux 项目（其他的不是很了解），再到近几年大数据领域，发展非常迅速，开源给本公司带来的好处，首先是提高这家在技术界的影响力，然后如果这个项目比较受大家认可，那么这家公司的这个技术可能会成为业界的统一解决方案，就像 Hadoop、Kafka 等。对其他公司的好处是，节省成本、可以快速应用来解决业务中的问题。</p>
<p>但是对于公司技术员工不好的地方是，作为这些项目的维护者，以后真的可能变成运维工程师，社区这些项目发展一般都很快，小厂很难有足够的人力、能力在这上面做太多的研发，一般都是跟着社区升级，可能发展到最后的结果是: 项目的研发由社区（或者背后主导的公司）来负责，其他公司融入这一生态，做好运维工作或产品化的东西就可以了，稳定性、可靠性、新功能交给社区，随着项目逐渐庞大，到最后可能其他公司很少有人能对这些项目有较强的掌控力，研发完全依赖于社区。这些开源项目的接口变得越来越简单、内部越来越复杂时，虽然降低了开发者、维护者的门槛，但也降低对开发者、维护者的要求，这是一把双刃剑，对于在技术上对自己有一定要求的工程师，不应该仅限于使用、原理等。</p>
<p>上面是一些浅薄的想法，下面开始结合李运华老师的文章总结学习开源项目的方法论。首先在学习开源项目时，有几点需要明确的是：</p>
<ol>
<li>先树立正确观念，不管你是什么身份，都可以从开源项目中学到很多东西（比如：要学习 Redis 的网络模型，不需要我们成为 Redis 的开发者，也不需要一定要用到 Redis，只需要具备一定的网络编程基础，再通过阅读 Redis 源码，就可以学习 Redis 这种单进程的 Reactor 模型）；</li>
<li>不要只盯着数据结构和算法，这些在学习开源项目时并没有那么重要（Nginx 使用红黑树来管理定时器，对于大多数人只需要这一点就足够了，并不需要研究 Nginx 实现红黑树的源码是如何写的，除非需要修改这部分的逻辑代码）；</li>
<li>采取<strong>自顶向下</strong>的学习方法，源码不是第一步，而是最后一步（基本掌握了功能、原理、关键设计之后再去看源码，看源码的主要目的是为了学习其代码的写作方式以及关键技术的实现）。</li>
</ol>
<blockquote>
<p>例如，Redis 的 RDB 持久化模式「会将当前内存中的数据库快照保存到磁盘文件中」，那这里所谓的 “数据库快照” 到底是怎么做的呢？在 Linux 平台上其实就是 fork 一个子进程保存就可以了；那为何 fork 子进程就生成了数据库快照了呢？这又和 Linux 的父子进程机制以及 copy-on-write 技术相关了。通过这种学习方式，既能够快速掌握系统设计的关键点（Redis 和 RDB 模式），又能够掌握具体的变成技巧（内存快照）。</p>
</blockquote>
<p>下面来看下李运华老师的『自顶向下』的学习方法和步骤。</p>
<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h3><p>这里的安装并不是对着手册执行一下命令，而是要通过安装过程，获取到如下一些关键的信息：</p>
<ul>
<li>这个系统的依赖组件，而依赖的组件又是系统设计和实现的基础;</li>
<li>安装目录也能够提供一些使用和运行的基本信息；</li>
<li>系统提供了哪些工具方便我们使用（<strong>带着问题去学习效率是最高的</strong>）。</li>
</ul>
<p>以 Nginx 为例，源码安装时依赖的库有 pcre、pcre-devel、openssl、openssl-devel、zlib，光从名字上看能够了解一些信息，例如 openssl 可能和 https 有关，zlib 可能与压缩有关。再以 Memcache 为例，它最大的依赖库就是 libevent，而根据 libevent 是一个高性能的网络库，大概能够推测 Memcache 的网络实现应该是 Reactor 模型。</p>
<p>例如，flink1.5.0安装完成后，目录如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[XXX@matt@pro flink-1.5.0]$ ll</div><div class="line">total 52</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 bin</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:57 conf</div><div class="line">drwxr-xr-x 6 XXX XXX  4096 Jul  9 23:39 examples</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 lib</div><div class="line">-rw-r--r-- 1 XXX XXX 18197 Jul  9 23:39 LICENSE</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:57 <span class="built_in">log</span></div><div class="line">-rw-r--r-- 1 XXX XXX   779 Jul  9 23:39 NOTICE</div><div class="line">drwxr-xr-x 2 XXX XXX  4096 Jul  9 23:39 opt</div><div class="line">-rw-r--r-- 1 XXX XXX  1308 Jul  9 23:39 README.txt</div></pre></td></tr></table></figure>
<p>上面 bin 是运行程序，conf 是配置文件的目录，lib 和 opt 是依赖的相关 jar 包，但为什么分为两个目录去放，这个我还不是很明白。下面是目录的详细内容:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line">[XXX@matt@pro flink-1.5.0]$ ll bin/</div><div class="line">total 116</div><div class="line">-rwxr-xr-x 1 XXX XXX 23957 Jul  9 23:39 config.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2224 Jul  9 23:39 flink</div><div class="line">-rwxr-xr-x 1 XXX XXX  1271 Jul  9 23:39 flink.bat</div><div class="line">-rwxr-xr-x 1 XXX XXX  2823 Jul  9 23:39 flink-console.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  6407 Jul  9 23:39 flink-daemon.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1482 Jul  9 23:39 historyserver.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2652 Jul  9 23:39 jobmanager.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1802 Jul  9 23:39 mesos-appmaster-job.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1971 Jul  9 23:39 mesos-appmaster.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2013 Jul  9 23:39 mesos-taskmanager.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1164 Jul  9 23:39 pyflink.bat</div><div class="line">-rwxr-xr-x 1 XXX XXX  1107 Jul  9 23:39 pyflink.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1182 Jul  9 23:39 pyflink-stream.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  3434 Jul  9 23:39 sql-client.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  3364 Jul  9 23:39 start-cluster.bat</div><div class="line">-rwxr-xr-x 1 XXX XXX  1836 Jul  9 23:39 start-cluster.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2960 Jul  9 23:39 start-scala-shell.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1854 Jul  9 23:39 start-zookeeper-quorum.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1616 Jul  9 23:39 stop-cluster.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1845 Jul  9 23:39 stop-zookeeper-quorum.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  3543 Jul  9 23:39 taskmanager.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  1674 Jul  9 23:39 yarn-session.sh</div><div class="line">-rwxr-xr-x 1 XXX XXX  2281 Jul  9 23:39 zookeeper.sh</div><div class="line">[XXX@matt@pro flink-1.5.0]$ ll lib/</div><div class="line">total 88972</div><div class="line">-rw-r--r-- 1 XXX XXX 90458504 Jul  9 23:39 flink-dist_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   142041 Jul  9 23:39 flink-python_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   489884 Jul  9 23:39 <span class="built_in">log</span>4j-1.2.17.jar</div><div class="line">-rw-r--r-- 1 XXX XXX     8870 Jul  9 23:39 slf4j-log4j12-1.7.7.jar</div><div class="line">[XXX@matt@pro flink-1.5.0]$ ll opt/</div><div class="line">total 193956</div><div class="line">-rw-r--r-- 1 XXX XXX    48215 Jul  9 23:39 flink-avro-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   124115 Jul  9 23:39 flink-cep_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    49235 Jul  9 23:39 flink-cep-scala_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   630006 Jul  9 23:39 flink-gelly_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   759288 Jul  9 23:39 flink-gelly-scala_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    21140 Jul  9 23:39 flink-json-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    16835 Jul  9 23:39 flink-metrics-datadog-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   136599 Jul  9 23:39 flink-metrics-dropwizard-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   278137 Jul  9 23:39 flink-metrics-ganglia-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX   161637 Jul  9 23:39 flink-metrics-graphite-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    89072 Jul  9 23:39 flink-metrics-prometheus-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX     6029 Jul  9 23:39 flink-metrics-slf4j-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX     7712 Jul  9 23:39 flink-metrics-statsd-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 27197071 Jul  9 23:39 flink-ml_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX    17916 Jul  9 23:39 flink-queryable-state-runtime_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 30676687 Jul  9 23:39 flink<span class="_">-s</span>3-fs-hadoop-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 38244766 Jul  9 23:39 flink<span class="_">-s</span>3-fs-presto-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 18517471 Jul  9 23:39 flink-sql-client-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 37325999 Jul  9 23:39 flink-streaming-python_2.11-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 26088550 Jul  9 23:39 flink-swift-fs-hadoop-1.5.0.jar</div><div class="line">-rw-r--r-- 1 XXX XXX 18172108 Jul  9 23:39 flink-table_2.11-1.5.0.jar</div><div class="line">[XXX@matt@pro flink-1.5.0]$ ll conf/</div><div class="line">total 56</div><div class="line">-rw-r--r-- 1 XXX XXX 9866 Jul  9 23:57 flink-conf.yaml</div><div class="line">-rw-r--r-- 1 XXX XXX 2138 Jul  9 23:39 <span class="built_in">log</span>4j-cli.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 1884 Jul  9 23:39 <span class="built_in">log</span>4j-console.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 1939 Jul  9 23:39 <span class="built_in">log</span>4j.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 1709 Jul  9 23:39 <span class="built_in">log</span>4j-yarn-session.properties</div><div class="line">-rw-r--r-- 1 XXX XXX 2294 Jul  9 23:39 logback-console.xml</div><div class="line">-rw-r--r-- 1 XXX XXX 2331 Jul  9 23:39 logback.xml</div><div class="line">-rw-r--r-- 1 XXX XXX 1550 Jul  9 23:39 logback-yarn.xml</div><div class="line">-rw-r--r-- 1 XXX XXX   15 Jul  9 23:39 masters</div><div class="line">-rw-r--r-- 1 XXX XXX  120 Jul  9 23:39 slaves</div><div class="line">-rw-r--r-- 1 XXX XXX 2755 Jul  9 23:39 sql-client-defaults.yaml</div><div class="line">-rw-r--r-- 1 XXX XXX 1434 Jul  9 23:39 zoo.cfg</div></pre></td></tr></table></figure>
<p>比如这里我们想查一下 <code>sql-client.sh</code> 是做什么的？应该怎么使用？不同参数是什么意思，可以通过 help 信息查看。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">./bin/sql-client.sh</div><div class="line">./sql-client [MODE] [OPTIONS]</div><div class="line"></div><div class="line">The following options are available:</div><div class="line"></div><div class="line">Mode <span class="string">"embedded"</span> submits Flink <span class="built_in">jobs</span> from the <span class="built_in">local</span> machine.</div><div class="line"></div><div class="line">  Syntax: embedded [OPTIONS]</div><div class="line">  <span class="string">"embedded"</span> mode options:</div><div class="line">     <span class="_">-d</span>,--defaults &lt;environment file&gt;      The environment properties with <span class="built_in">which</span></div><div class="line">                                           every new session is initialized.</div><div class="line">                                           Properties might be overwritten by</div><div class="line">                                           session properties.</div><div class="line">     <span class="_">-e</span>,--environment &lt;environment file&gt;   The environment properties to be</div><div class="line">                                           imported into the session. It might</div><div class="line">                                           overwrite default environment</div><div class="line">                                           properties.</div><div class="line">     -h,--help                             Show the <span class="built_in">help</span> message with</div><div class="line">                                           descriptions of all options.</div><div class="line">     -j,--jar &lt;JAR file&gt;                   A JAR file to be imported into the</div><div class="line">                                           session. The file might contain</div><div class="line">                                           user-defined classes needed <span class="keyword">for</span> the</div><div class="line">                                           execution of statements such as</div><div class="line">                                           <span class="built_in">functions</span>, table sources, or sinks.</div><div class="line">                                           Can be used multiple times.</div><div class="line">     <span class="_">-l</span>,--library &lt;JAR directory&gt;          A JAR file directory with <span class="built_in">which</span> every</div><div class="line">                                           new session is initialized. The files</div><div class="line">                                           might contain user-defined classes</div><div class="line">                                           needed <span class="keyword">for</span> the execution of</div><div class="line">                                           statements such as <span class="built_in">functions</span>, table</div><div class="line">                                           sources, or sinks. Can be used</div><div class="line">                                           multiple times.</div><div class="line">     <span class="_">-s</span>,--session &lt;session identifier&gt;     The identifier <span class="keyword">for</span> a session.</div><div class="line">                                           <span class="string">'default'</span> is the default identifier.</div></pre></td></tr></table></figure>
<h3 id="2-运行"><a href="#2-运行" class="headerlink" title="2. 运行"></a>2. 运行</h3><p>安装完成后，我们需要真正将系统运行起来，运行系统的时候有两个地方要特别关注：<strong>命令行和配置文件</strong>，它们主要提供了两个非常关键的信息：</p>
<ol>
<li>系统具备哪些能力（提供哪些可配置化的参数，这些参数是做什么的以及不同的配置带来的影响是什么）；</li>
<li>系统将会如何运行。</li>
</ol>
<p>这些信息是我们窥视系统内部运行机制和原理的一扇窗口。</p>
<p>例如，下面 Flink 配置中一些配置参数（Flink 集群模式的安装和运行可以参考 <a href="http://wuchong.me/blog/2016/02/26/flink-docs-setup-cluster/" target="_blank" rel="external">Flink官方文档翻译：安装部署（集群模式）</a>），通过这几个启动时的配置参数，我们可以获取下面这些信息：</p>
<ul>
<li><code>jobmanager.rpc.address</code>：The external address of the JobManager, which is the master/coordinator of the distributed system (DEFAULT: localhost)；</li>
<li><code>jobmanager.rpc.port</code>：The port number of the JobManager (DEFAULT: 6123)；</li>
<li><code>jobmanager.heap.mb</code>：JVM heap size (in megabytes) for the JobManager. You may have to increase the heap size for the JobManager if you are running very large applications (with many operators), or if you are keeping a long history of them.</li>
<li><code>taskmanager.numberOfTaskSlots</code>： JVM heap size (in megabytes) for the TaskManagers, which are the parallel workers of the system；</li>
<li><code>taskmanager.numberOfTaskSlots</code>: The number of parallel operator or user function instances that a single TaskManager can run (DEFAULT: 1).</li>
<li><code>parallelism.default</code>：The default parallelism to use for programs that have no parallelism specified. (DEFAULT: 1).；</li>
</ul>
<p>通过上面这些配置参数，我们基本上可以看到 Flink 的 Master/Salve 模型，是分为 JobManager 和 TaskManager，而 TaskManager 中又有对应的 TaskSlot，系统也提供了相应配置参数进行设置，Flink 1.5.0 的配置信息可以参考 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.5/ops/config.html" target="_blank" rel="external">Flink 1.5.0 配置</a>，社区的文档对这些参数描述得非常清楚，如果之前有大数据系统的基础，比如了解 HDFS、YARN、Spark、Storm、Kafka 的架构，那么在看到这些参数时，其实并不会感觉到太陌生，分布式系统很多东西都是相通的。</p>
<p>在通常情况下，如果我们将每个命令行参数和配置项的作用和原理都全部掌握清楚了的话，基本上对系统已经很熟悉了。这里李运华老师介绍了一个他的经验，那么就是：<strong>不管三七二十一，先把所有的配置项全部研究一遍，包括配置项的原理、作用、影响，并且尝试去修改配置项然后看看系统会有什么变化</strong>。</p>
<h3 id="3-原理研究"><a href="#3-原理研究" class="headerlink" title="3. 原理研究"></a>3. 原理研究</h3><p>在完成前两个步骤后，我们对系统已经有了初步的感觉和理解，此时可以更进一步去研究其原理。其实在研究命令行和配置项的时候已经涉及一部分原理了，但是并不是很系统，因此我们要专门针对原理进行系统性的研究。这里的关键就是<strong>系统性</strong>三个字，怎么才算系统性呢？主要体现在如下几个方面：</p>
<h4 id="3-1-关键特性的基本实现原理"><a href="#3-1-关键特性的基本实现原理" class="headerlink" title="3.1. 关键特性的基本实现原理"></a>3.1. 关键特性的基本实现原理</h4><p>每个应用广泛的开源项目之所以能够受到大众的欢迎，肯定是有一些卖点的，有一些它们的应用场景，常见的有高性能、高可用、可扩展等特性，那到底这些项目是如何做到其所宣称的那么牛的呢？这些牛的地方就是我们需要深入学习的地方：</p>
<ol>
<li>Memcache 的高性能具体是怎么做到的呢？首先是基于 libevent 实现了高性能的网络模型，其次是内存管理 Slab Allocator 机制。为了彻底理解 Memcache 的高性能网络模型，我们需要掌握很多知识：多路复用、Linux epoll、Reactor 模型、多线程等，通过研究 Memcache 的高性能网络模型，我们能够学习一个具体的项目中如何将这些东西全部串起来实现了高性能。</li>
<li>再以 React 为例，Virtual DOM 的实现原理是什么、为何要实现 Virtual DOM、React 是如何构建 Virtual DOM 树、Virtual DOM 与 DOM 什么关系等，通过研究学习 Virtual DOM，即使不使用 React，我们也能够学习如何写出高性能的前端的代码。</li>
<li>这里再以 Kafka 为例，Kafka 作为在大数据领域应用非常广泛的消息队列，它是如何实现它宣称的高性能的、高可靠？以及在 0.11.0 之后的版本它是如何实现幂等性、事务性的？在 2.0 之后是如何实现可以支撑千台机器、百万 partition 规模的？通过深入学习一些，能够让我们学学习到大数据存储系统的可靠性、高性能实现方案，以及分布式一致性（事务性）的实现；</li>
<li>最后以 Flink 为例，Flink 最开始的卖点是 Exactly once 和低延迟，现在的话再加上流式系统 SQL 的支持，那么它与 Storm、Spark streaming 相比，Flink 的 Exactly once 是怎么实现的？为什么 Storm 在现有机制上（不含 Trident）无法实现 Exactly once？Spark Streaming 微批处理模型延迟消耗主要在什么地方？为什么 Flink 可以做到低延迟？Flink  怎么实现窗口计算以及 Flink SQL 是怎么实现的，以及 Flink SQL 现在面对的问题是什么？</li>
</ol>
<h4 id="3-2-优缺点对比分析"><a href="#3-2-优缺点对比分析" class="headerlink" title="3.2. 优缺点对比分析"></a>3.2. 优缺点对比分析</h4><p>这是我想特别强调的一点，<strong>只有清楚掌握技术方案的优缺点后才算真正的掌握这门技术，也只有掌握了技术方案的优缺点后才能在架构设计的时候做出合理的选择。优缺点主要通过对比来分析，即：我们将两个类似的系统进行对比，看看它们的实现差异，以及不同的实现优缺点都是什么</strong>。</p>
<ol>
<li>典型的对比有 Memcache 和 Redis，例如（仅举例说明，实际上对比的点很多），Memcache 用多线程，Redis 用单进程，各有什么优缺点？Memcache 和 Redis 的集群方式，各有什么优缺点？</li>
<li>即使是 Redis 自身，我们也可以对比 RDB 和 AOF 两种模式的优缺点。</li>
</ol>
<h4 id="3-3-如何系统性学习一个开源项目"><a href="#3-3-如何系统性学习一个开源项目" class="headerlink" title="3.3. 如何系统性学习一个开源项目"></a>3.3. 如何系统性学习一个开源项目</h4><p>在你了解了什么是【系统性】后，我来介绍一下原理研究的手段，主要有三种：</p>
<ol>
<li>通读项目的设计文档：例如 Kafka 的设计文档，基本涵盖了消息队列设计的关键决策部分；Disruptor 的设计白皮书，详细的阐述了 Java 单机高性能的设计技巧（官方文档是学习一个项目的必须资料）。</li>
<li>阅读网上已有的分析文档：通常情况下比较热门的开源项目，都已经有非常多的分析文档了，我们可以站在前人的基础上，避免大量的重复投入。但需要注意的是，由于经验、水平、关注点等差异，不同的人分析的结论可能有差异，甚至有的是错误的，因此不能完全参照。一个比较好的方式就是多方对照，也就是说看很多篇分析文档，比较它们的内容共同点和差异点（网上分析文档很多，但是要知道如何区分这些分析文档，多对比一些，同一个东西，每个人的理解并不一定相同）。</li>
<li>Demo 验证：如果有些技术点难以查到资料，自己又不确定，则可以真正去写 Demo 进行验证，通过打印一些日志或者调试，能清晰的理解具体的细节。例如，写一个简单的分配内存程序，然后通过日志和命令行（jmap、jstat、jstack 等）来查看 Java 虚拟机垃圾回收时的具体表现（开源项目一般都会有一些实例供我们学习参考，这也是我们学习一个项目的重要资料，先去看如何使用，再去看不同使用方式背后的原理）。</li>
</ol>
<h3 id="4-测试"><a href="#4-测试" class="headerlink" title="4. 测试"></a>4. 测试</h3><p>通常情况下，如果你真的准备在实际项目中使用某个开源项目的话，必须进行测试。有的同学可能会说，网上的分析和测试文档很多，直接找一篇看就可以了？如果只是自己学习和研究，这样做是可以的，因为构建完整的测试用例既需要耗费较多时间，又需要较多机器资源，如果每个项目都这么做的话，投入成本有点大；但如果是要在实践项目中使用，必须自己进行测试，因为网上搜的测试结果，不一定与自己的业务场景很契合，如果简单参考别人的测试结果，很可能会得出错误的结论。例如，开源系统的版本不同，测试结果可能差异较大。同样是 K-V 存储，别人测试的 value 是 128 字节，而你的场景 value 都达到了 128k 字节，两者的测试结果也差异很大，不能简单照搬（在实际真正应用前，需要足够的性能测试，而且要能分析出测试结论背后的理论原因，如果找不到理论做为支撑，这样的测试并不是可信的，因为网络中环境有很大的随机性）。</p>
<p>测试阶段需要特别强调的一点就是：测试一定要在原理研究之后做，不能安装完成立马就测试！原因在于如果对系统不熟悉，很可能出现命令行、配置参数没用对，或者运行模式选择不对，导致没有根据业务的特点搭建正确的环境、没有设计合理的测试用例，从而使得最终的测试结果得出了错误结论，误导了设计决策。曾经有团队安装完成 MySQL 5.1 后就进行性能测试，测试结果出来让人大跌眼镜，经过定位才发现 <code>innodb_buffer_pool_size</code> 使用的是默认值 8M。</p>
<h3 id="5-源码研究"><a href="#5-源码研究" class="headerlink" title="5. 源码研究"></a>5. 源码研究</h3><p>源码研究的主要目的是<strong>学习原理背后的具体编码如何实现，通过学习这些技巧来提升我们自己的技术能力</strong>。例如 Redis 的 RDB 快照、Nginx 的多 Reactor 模型、Disruptor 如何使用 volatile 以及 CAS 来做无锁设计、Netty 的 Zero-Copy 等，这些技巧都很精巧，掌握后能够大大提升自己的编码能力。</p>
<p>通常情况下，不建议通读所有源码，因为想掌握每行代码的含义和作用还是非常耗费时间的，尤其是 MySQL、Nginx 这种规模的项目，即使是他们的开发人员，都不一定每个人都掌握了所有代码。带着明确目的去研究源码，做到有的放矢，才能事半功倍，这也是源码研究要放在最后的原因。</p>
<h3 id="时间分配"><a href="#时间分配" class="headerlink" title="时间分配"></a>时间分配</h3><p>前面介绍的【自顶向下】五个步骤，完整执行下来需要花费较长时间，而时间又是大部分技术人员比较稀缺的资源。很多人在学习技术的时候都会反馈说时间不够，版本进度很紧，很难有大量的时间进行学习，但如果不学习感觉自己又很难提升？面对这种两难问题，具体该如何做呢？</p>
<p>通常情况下，以上 5 个步骤的前 3 个步骤，不管是已经成为架构师的技术人员，还是立志成为架构师的技术人员，在研究开源项目的时候都必不可少；第四步可以在准备采用开源项目的时候才实施，第五步可以根据你的时间来进行灵活安排。这里的“灵活安排”不是说省略不去做，而是在自己有一定时间和精力的时候做，因为只有这样才能真正理解和学到具体的技术。</p>
<p>如果感觉自己时间和精力不够，与其蜻蜓点水每个开源项目都去简单了解一下，还不如集中精力将一个开源项目研究通透，就算是每个季度只学习一个开源项目，积累几年后这个数量也是很客观的；而且一旦你将一个项目研究透以后，再去研究其他类似项目，你会发现自己学习的非常快，因为共性的部分你已经都掌握了，只需要掌握新项目差异的部分即可。</p>
<p>这里个人的感想是，对于初中级工程师，最好还是要有2-3项目或者2-3个方向需要走到第五步，毕竟我们是靠这个吃饭的，对于其他的项目（目前业界统一的解决方案，比如 hdfs、hbase、spark 等），至少需要走到第四步，这样与这方面的专业人士沟通交流时，至少让自己不至于处在懵逼状态。当然对于这些项目的核心代码，也是可以深入学习，比如 spark 的 shuffle 在代码上是如何实现的等。在一个项目上深入之后，再去看同一个领域的其他项目时，当看到其他的架构时，其实我们基本上就可以清楚这架构设计的原因、要解决的问题以及这种设计带来的其他问题，每种设计都有其应用场景，比如对 Kafka 有了深入了解后，再看 RocketMQ、phxqueue 时，看到它们的架构方案基本上就明白要解决的问题以及其特定的应用场景，当然对一些独特的特性，还是需要深入到代码层面去学习的，比如 RocketMQ 的延迟队列实现。这是一些个人的感想，并不一定对，大家可以共同交流，总之，是感觉李运华老师这篇文章是值得总结分享的，希望自己在后面学习开源项目时，能够静下心、认真坚持学下去。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章的方法论内容基本来自订阅的极客时间-李运华老师的《从0开始学架构》中的一篇文章，会结合自己的学习经验、加上以 Flink 为例来做一个总结，也为了让自己再学习其他开源项目时能够按照这样的一个方法论高效的深入学习。先简单说一下开源项目，开源项目最早从上个世纪开始，我知
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="思考" scheme="http://matt33.com/tags/%E6%80%9D%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>JVM 之 ParNew 和 CMS 日志分析</title>
    <link href="http://matt33.com/2018/07/28/jvm-cms/"/>
    <id>http://matt33.com/2018/07/28/jvm-cms/</id>
    <published>2018-07-28T11:53:04.000Z</published>
    <updated>2018-07-29T04:20:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>在两年前的文章 <a href="http://matt33.com/2016/09/18/jvm-basic2/">JVM 学习——垃圾收集器与内存分配策略</a> 中，已经对 GC 算法的原理以及常用的垃圾收集器做了相应的总结。今天这篇文章主要是对生产环境中（Java7）常用的两种垃圾收集器（ParNew：年轻代，CMS：老年代）从日志信息上进行分析，做一下总结，这样当我们在排查相应的问题时，看到 GC 的日志信息，不会再那么陌生，能清楚地知道这些日志是什么意思，GC 线程当前处在哪个阶段，正在做什么事情等。</p>
<h2 id="ParNew-收集器"><a href="#ParNew-收集器" class="headerlink" title="ParNew 收集器"></a>ParNew 收集器</h2><p><a href="http://matt33.com/2016/09/18/jvm-basic2/#ParNew-%E6%94%B6%E9%9B%86%E5%99%A8">ParNew 收集器</a>是年轻代常用的垃圾收集器，它采用的是复制算法，youngGC 时一个典型的日志信息如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.134+0800: 15578.050: [GC2018-04-12T13:48:26.135+0800: 15578.050: [ParNew: 3412467K-&gt;59681K(3774912K), 0.0971990 secs] 9702786K-&gt;6354533K(24746432K), 0.0974940 secs] [Times: user=0.95 sys=0.00, real=0.09 secs]</div></pre></td></tr></table></figure>
<p>依次分析一下上面日志信息的含义：</p>
<ul>
<li><code>2018-04-12T13:48:26.134+0800</code>：Mirror GC 发生的时间；</li>
<li><code>15578.050</code>：GC 开始时，相对 JVM 启动的相对时间，单位时秒，这里是4h+；</li>
<li><code>ParNew</code>：收集器名称，这里是 ParNew 收集器，它使用的是并行的 mark-copy 算法，GC 过程也会 Stop the World；</li>
<li><code>3412467K-&gt;59681K</code>：收集前后年轻代的使用情况，这里是 3.25G-&gt;58.28M；</li>
<li><code>3774912K</code>：整个年轻代的容量，这里是 3.6G；</li>
<li><code>0.0971990 secs</code>：Duration for the collection w/o final cleanup.</li>
<li><code>9702786K-&gt;6354533K</code>：收集前后整个堆的使用情况，这里是 9.25G-&gt;6.06G;</li>
<li><code>24746432K</code>：整个堆的容量，这里是 23.6G；</li>
<li><code>0.0974940 secs</code>：ParNew 收集器标记和复制年轻代活着的对象所花费的时间（包括和老年代通信的开销、对象晋升到老年代开销、垃圾收集周期结束一些最后的清理对象等的花销）；</li>
</ul>
<p>对于 <code>[Times: user=0.95 sys=0.00, real=0.09 secs]</code>，这里面涉及到三种时间类型，含义如下：</p>
<ul>
<li>user：GC 线程在垃圾收集期间所使用的 CPU 总时间；</li>
<li>sys：系统调用或者等待系统事件花费的时间；</li>
<li>real：应用被暂停的时钟时间，由于 GC 线程是多线程的，导致了 real 小于 (user+real)，如果是 gc 线程是单线程的话，real 是接近于 (user+real) 时间。</li>
</ul>
<h2 id="CMS-收集器"><a href="#CMS-收集器" class="headerlink" title="CMS 收集器"></a>CMS 收集器</h2><p><a href="http://matt33.com/2016/09/18/jvm-basic2/#CMS-%E6%94%B6%E9%9B%86%E5%99%A8">CMS 收集器</a>是老年代经常使用的收集器，它采用的是标记-清楚算法，应用程序在发生一次 Full GC 时，典型的 GC 日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.233+0800: 15578.148: [GC [1 CMS-initial-mark: 6294851K(20971520K)] 6354687K(24746432K), 0.0466580 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]</div><div class="line">2018-04-12T13:48:26.280+0800: 15578.195: [CMS-concurrent-mark-start]</div><div class="line">2018-04-12T13:48:26.418+0800: 15578.333: [CMS-concurrent-mark: 0.138/0.138 secs] [Times: user=1.01 sys=0.21, real=0.14 secs]</div><div class="line">2018-04-12T13:48:26.418+0800: 15578.334: [CMS-concurrent-preclean-start]</div><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-preclean: 0.056/0.057 secs] [Times: user=0.20 sys=0.12, real=0.06 secs]</div><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-abortable-preclean-start]</div><div class="line">2018-04-12T13:48:29.989+0800: 15581.905: [CMS-concurrent-abortable-preclean: 3.506/3.514 secs] [Times: user=11.93 sys=6.77, real=3.51 secs]</div><div class="line">2018-04-12T13:48:29.991+0800: 15581.906: [GC[YG occupancy: 1805641 K (3774912 K)]2018-04-12T13:48:29.991+0800: 15581.906: [GC2018-04-12T13:48:29.991+0800: 15581.906: [ParNew: 1805641K-&gt;48395K(3774912K), 0.0826620 secs] 8100493K-&gt;6348225K(24746432K), 0.0829480 secs] [Times: user=0.81 sys=0.00, real=0.09 secs]2018-04-12T13:48:30.074+0800: 15581.989: [Rescan (parallel) , 0.0429390 secs]2018-04-12T13:48:30.117+0800: 15582.032: [weak refs processing, 0.0027800 secs]2018-04-12T13:48:30.119+0800: 15582.035: [class unloading, 0.0033120 secs]2018-04-12T13:48:30.123+0800: 15582.038: [scrub symbol table, 0.0016780 secs]2018-04-12T13:48:30.124+0800: 15582.040: [scrub string table, 0.0004780 secs] [1 CMS-remark: 6299829K(20971520K)] 6348225K(24746432K), 0.1365130 secs] [Times: user=1.24 sys=0.00, real=0.14 secs]</div><div class="line">2018-04-12T13:48:30.128+0800: 15582.043: [CMS-concurrent-sweep-start]</div><div class="line">2018-04-12T13:48:36.638+0800: 15588.553: [GC2018-04-12T13:48:36.638+0800: 15588.554: [ParNew: 3403915K-&gt;52142K(3774912K), 0.0874610 secs] 4836483K-&gt;1489601K(24746432K), 0.0877490 secs] [Times: user=0.84 sys=0.00, real=0.09 secs]</div><div class="line">2018-04-12T13:48:38.412+0800: 15590.327: [CMS-concurrent-sweep: 8.193/8.284 secs] [Times: user=30.34 sys=16.44, real=8.28 secs]</div><div class="line">2018-04-12T13:48:38.419+0800: 15590.334: [CMS-concurrent-reset-start]</div><div class="line">2018-04-12T13:48:38.462+0800: 15590.377: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.15 sys=0.10, real=0.04 secs]</div></pre></td></tr></table></figure>
<p>CMS Full GC 拆分开来，涉及的阶段比较多，下面分别来介绍各个阶段的情况。</p>
<h3 id="阶段1：Initial-Mark"><a href="#阶段1：Initial-Mark" class="headerlink" title="阶段1：Initial Mark"></a>阶段1：Initial Mark</h3><p>这个是 CMS 两次 stop-the-wolrd 事件的其中一次，这个阶段的目标是：标记那些直接被 GC root 引用或者被年轻代存活对象所引用的所有对象，标记后示例如下所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>）：</p>
<p><img src="/images/java/cms-1.png" alt="CMS 初始标记阶段"></p>
<p>上述例子对应的日志信息为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.233+0800: 15578.148: [GC [1 CMS-initial-mark: 6294851K(20971520K)] 6354687K(24746432K), 0.0466580 secs] [Times: user=0.04 sys=0.00, real=0.04 secs]</div></pre></td></tr></table></figure>
<p>逐行介绍上面日志的含义：</p>
<ol>
<li><code>2018-04-12T13:48:26.233+0800: 15578.148</code>：GC 开始的时间，以及相对于 JVM 启动的相对时间（单位是秒，这里大概是4.33h），与前面 ParNew 类似，下面的分析中就直接跳过这个了；</li>
<li><code>CMS-initial-mark</code>：初始标记阶段，它会收集所有 GC Roots 以及其直接引用的对象；</li>
<li><code>6294851K</code>：当前老年代使用的容量，这里是 6G；</li>
<li><code>(20971520K)</code>：老年代可用的最大容量，这里是 20G；</li>
<li><code>6354687K</code>：整个堆目前使用的容量，这里是 6.06G；</li>
<li><code>(24746432K)</code>：堆可用的容量，这里是 23.6G；</li>
<li><code>0.0466580 secs</code>：这个阶段的持续时间；</li>
<li><code>[Times: user=0.04 sys=0.00, real=0.04 secs]</code>：与前面的类似，这里是相应 user、system and real 的时间统计。</li>
</ol>
<h3 id="阶段2：并发标记"><a href="#阶段2：并发标记" class="headerlink" title="阶段2：并发标记"></a>阶段2：并发标记</h3><p>在这个阶段 Garbage Collector 会遍历老年代，然后标记所有存活的对象，它会根据上个阶段找到的 GC Roots 遍历查找。并发标记阶段，它会与用户的应用程序并发运行。并不是老年代所有的存活对象都会被标记，因为在标记期间用户的程序可能会改变一些引用，如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>）：</p>
<p><img src="/images/java/cms-2.png" alt="CMS 并发标记阶段"></p>
<p>在上面的图中，与阶段1的图进行对比，就会发现有一个对象的引用已经发生了变化，这个阶段相应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.280+0800: 15578.195: [CMS-concurrent-mark-start]</div><div class="line">2018-04-12T13:48:26.418+0800: 15578.333: [CMS-concurrent-mark: 0.138/0.138 secs] [Times: user=1.01 sys=0.21, real=0.14 secs]</div></pre></td></tr></table></figure>
<p>这里详细对上面的日志解释，如下所示：</p>
<ol>
<li><code>CMS-concurrent-mark</code>：并发收集阶段，这个阶段会遍历老年代，并标记所有存活的对象；</li>
<li><code>0.138/0.138 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=1.01 sys=0.21, real=0.14 secs]</code>：如前面所示，但是这部的时间，其实意义不大，因为它是从并发标记的开始时间开始计算，这期间因为是并发进行，不仅仅包含 GC 线程的工作。</li>
</ol>
<h3 id="阶段3：Concurrent-Preclean"><a href="#阶段3：Concurrent-Preclean" class="headerlink" title="阶段3：Concurrent Preclean"></a>阶段3：Concurrent Preclean</h3><p>Concurrent Preclean：这也是一个并发阶段，与应用的线程并发运行，并不会 stop 应用的线程。在并发运行的过程中，一些对象的引用可能会发生变化，但是这种情况发生时，JVM 会将包含这个对象的区域（Card）标记为 Dirty，这也就是 Card Marking。如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：</p>
<p><img src="/images/java/cms-3.png" alt="Concurrent Preclean 1"></p>
<p>在pre-clean阶段，那些能够从 Dirty 对象到达的对象也会被标记，这个标记做完之后，dirty card 标记就会被清除了，如下（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：</p>
<p><img src="/images/java/cms-4.png" alt="Concurrent Preclean 2"></p>
<p>这个阶段相应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.418+0800: 15578.334: [CMS-concurrent-preclean-start]</div><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-preclean: 0.056/0.057 secs] [Times: user=0.20 sys=0.12, real=0.06 secs]</div></pre></td></tr></table></figure>
<p>其含义为：</p>
<ol>
<li><code>CMS-concurrent-preclean</code>：Concurrent Preclean 阶段，对在前面并发标记阶段中引用发生变化的对象进行标记；</li>
<li><code>0.056/0.057 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=0.20 sys=0.12, real=0.06 secs]</code>：同并发标记阶段中的含义。</li>
</ol>
<h3 id="阶段4：Concurrent-Abortable-Preclean"><a href="#阶段4：Concurrent-Abortable-Preclean" class="headerlink" title="阶段4：Concurrent Abortable Preclean"></a>阶段4：Concurrent Abortable Preclean</h3><p>这也是一个并发阶段，但是同样不会影响影响用户的应用线程，这个阶段是为了尽量承担 STW（stop-the-world）中最终标记阶段的工作。这个阶段持续时间依赖于很多的因素，由于这个阶段是在重复做很多相同的工作，直接满足一些条件（比如：重复迭代的次数、完成的工作量或者时钟时间等）。这个阶段的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:26.476+0800: 15578.391: [CMS-concurrent-abortable-preclean-start]</div><div class="line">2018-04-12T13:48:29.989+0800: 15581.905: [CMS-concurrent-abortable-preclean: 3.506/3.514 secs] [Times: user=11.93 sys=6.77, real=3.51 secs]</div></pre></td></tr></table></figure>
<ol>
<li><code>CMS-concurrent-abortable-preclean</code>：Concurrent Abortable Preclean 阶段；</li>
<li><code>3.506/3.514 secs</code>：这个阶段的持续时间与时钟时间，本质上，这里的 gc 线程会在 STW 之前做更多的工作，通常会持续 5s 左右；</li>
<li><code>[Times: user=11.93 sys=6.77, real=3.51 secs]</code>：同前面。</li>
</ol>
<h3 id="阶段5：Final-Remark"><a href="#阶段5：Final-Remark" class="headerlink" title="阶段5：Final Remark"></a>阶段5：Final Remark</h3><p>这是第二个 STW 阶段，也是 CMS 中的最后一个，这个阶段的目标是标记所有老年代所有的存活对象，由于之前的阶段是并发执行的，gc 线程可能跟不上应用程序的变化，为了完成标记老年代所有存活对象的目标，STW 就非常有必要了。</p>
<p>通常 CMS 的 Final Remark 阶段会在年轻代尽可能干净的时候运行，目的是为了减少连续 STW 发生的可能性（年轻代存活对象过多的话，也会导致老年代涉及的存活对象会很多）。这个阶段会比前面的几个阶段更复杂一些，相关日志如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:29.991+0800: 15581.906: [GC[YG occupancy: 1805641 K (3774912 K)]2018-04-12T13:48:29.991+0800: 15581.906: [GC2018-04-12T13:48:29.991+0800: 15581.906: [ParNew: 1805641K-&gt;48395K(3774912K), 0.0826620 secs] 8100493K-&gt;6348225K(24746432K), 0.0829480 secs] [Times: user=0.81 sys=0.00, real=0.09 secs]2018-04-12T13:48:30.074+0800: 15581.989: [Rescan (parallel) , 0.0429390 secs]2018-04-12T13:48:30.117+0800: 15582.032: [weak refs processing, 0.0027800 secs]2018-04-12T13:48:30.119+0800: 15582.035: [class unloading, 0.0033120 secs]2018-04-12T13:48:30.123+0800: 15582.038: [scrub symbol table, 0.0016780 secs]2018-04-12T13:48:30.124+0800: 15582.040: [scrub string table, 0.0004780 secs] [1 CMS-remark: 6299829K(20971520K)] 6348225K(24746432K), 0.1365130 secs] [Times: user=1.24 sys=0.00, real=0.14 secs]</div></pre></td></tr></table></figure>
<p>对上面的日志进行分析：</p>
<ol>
<li><code>YG occupancy: 1805641 K (3774912 K)</code>：年轻代当前占用量及容量，这里分别是 1.71G 和 3.6G；</li>
<li><code>ParNew:...</code>：触发了一次 young GC，这里触发的原因是为了减少年轻代的存活对象，尽量使年轻代更干净一些；</li>
<li><code>[Rescan (parallel) , 0.0429390 secs]</code>：这个 Rescan 是当应用暂停的情况下完成对所有存活对象的标记，这个阶段是并行处理的，这里花费了  0.0429390s；</li>
<li><code>[weak refs processing, 0.0027800 secs]</code>：第一个子阶段，它的工作是处理弱引用；</li>
<li><code>[class unloading, 0.0033120 secs]</code>：第二个子阶段，它的工作是：unloading the unused classes；</li>
<li><code>[scrub symbol table, 0.0016780 secs] ... [scrub string table, 0.0004780 secs]</code>：最后一个子阶段，它的目的是：cleaning up symbol and string tables which hold class-level metadata and internalized string respectively，时钟的暂停也包含在这里；</li>
<li><code>6299829K(20971520K)</code>：这个阶段之后，老年代的使用量与总量，这里分别是 6G 和 20G；</li>
<li><code>6348225K(24746432K)</code>：这个阶段之后，堆的使用量与总量（包括年轻代，年轻代在前面发生过 GC），这里分别是 6.05G 和 23.6G；</li>
<li><code>0.1365130 secs</code>：这个阶段的持续时间；</li>
<li><code>[Times: user=1.24 sys=0.00, real=0.14 secs]</code>：对应的时间信息。</li>
</ol>
<p>经历过这五个阶段之后，老年代所有存活的对象都被标记过了，现在可以通过清除算法去清理那些老年代不再使用的对象。</p>
<h3 id="阶段6：Concurrent-Sweep"><a href="#阶段6：Concurrent-Sweep" class="headerlink" title="阶段6：Concurrent Sweep"></a>阶段6：Concurrent Sweep</h3><p>这里不需要 STW，它是与用户的应用程序并发运行，这个阶段是：清除那些不再使用的对象，回收它们的占用空间为将来使用。如下图所示（插图来自：<a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>：<br>）：</p>
<p><img src="/images/java/cms-5.png" alt="CMS Concurrent Sweep 阶段"></p>
<p>这个阶段对应的日志信息如下（这中间又发生了一次 Young GC）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:30.128+0800: 15582.043: [CMS-concurrent-sweep-start]</div><div class="line">2018-04-12T13:48:36.638+0800: 15588.553: [GC2018-04-12T13:48:36.638+0800: 15588.554: [ParNew: 3403915K-&gt;52142K(3774912K), 0.0874610 secs] 4836483K-&gt;1489601K(24746432K), 0.0877490 secs] [Times: user=0.84 sys=0.00, real=0.09 secs]</div><div class="line">2018-04-12T13:48:38.412+0800: 15590.327: [CMS-concurrent-sweep: 8.193/8.284 secs] [Times: user=30.34 sys=16.44, real=8.28 secs]</div></pre></td></tr></table></figure>
<p>分别介绍一下：</p>
<ol>
<li><code>CMS-concurrent-sweep</code>：这个阶段主要是清除那些没有被标记的对象，回收它们的占用空间；</li>
<li><code>8.193/8.284 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=30.34 sys=16.44, real=8.28 secs]</code>：同前面；</li>
</ol>
<h3 id="阶段7：Concurrent-Reset"><a href="#阶段7：Concurrent-Reset" class="headerlink" title="阶段7：Concurrent Reset."></a>阶段7：Concurrent Reset.</h3><p>这个阶段也是并发执行的，它会重设 CMS 内部的数据结构，为下次的 GC 做准备，对应的日志信息如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">2018-04-12T13:48:38.419+0800: 15590.334: [CMS-concurrent-reset-start]</div><div class="line">2018-04-12T13:48:38.462+0800: 15590.377: [CMS-concurrent-reset: 0.044/0.044 secs] [Times: user=0.15 sys=0.10, real=0.04 secs]</div></pre></td></tr></table></figure>
<p>日志详情分别如下：</p>
<ol>
<li><code>CMS-concurrent-reset</code>：这个阶段的开始，目的如前面所述；</li>
<li><code>0.044/0.044 secs</code>：这个阶段的持续时间与时钟时间；</li>
<li><code>[Times: user=0.15 sys=0.10, real=0.04 secs]</code>：同前面。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>CMS 通过将大量工作分散到并发处理阶段来在减少 STW 时间，在这块做得非常优秀，但是 CMS 也有一些其他的问题：</p>
<ol>
<li>CMS 收集器无法处理浮动垃圾（ Floating Garbage），可能出现 “Concurrnet Mode Failure” 失败而导致另一次 Full GC 的产生，可能引发串行 Full GC；</li>
<li>空间碎片，导致无法分配大对象，CMS 收集器提供了一个 <code>-XX:+UseCMSCompactAtFullCollection</code> 开关参数（默认就是开启的），用于在 CMS 收集器顶不住要进行 Full GC 时开启内存碎片的合并整理过程，内存整理的过程是无法并发的，空间碎片问题没有了，但停顿时间不得不变长；</li>
<li>对于堆比较大的应用上，GC 的时间难以预估。</li>
</ol>
<p>CMS 的一些缺陷也是 G1 收集器兴起的原因。</p>
<p>参考：</p>
<ul>
<li><a href="https://t.hao0.me/jvm/2016/03/15/jvm-gc-log.html" target="_blank" rel="external">JVM各类GC日志剖析</a>；</li>
<li><a href="http://www.cnblogs.com/zhangxiaoguang/p/5792468.html" target="_blank" rel="external">GC之详解CMS收集过程和日志分析</a>；</li>
<li><a href="http://www.oracle.com/technetwork/tutorials/tutorials-1876574.html" target="_blank" rel="external">Getting Started with the G1 Garbage Collector</a>；</li>
<li><a href="http://ifeve.com/useful-jvm-flags-part-7-cms-collector/" target="_blank" rel="external">JVM实用参数（七）CMS收集器</a>；</li>
<li><a href="https://plumbr.io/handbook/garbage-collection-algorithms-implementations#concurrent-mark-and-sweep" target="_blank" rel="external">GC Algorithms：Implementations —— Concurrent Mark and Sweep —— Full GC</a>；</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在两年前的文章 &lt;a href=&quot;http://matt33.com/2016/09/18/jvm-basic2/&quot;&gt;JVM 学习——垃圾收集器与内存分配策略&lt;/a&gt; 中，已经对 GC 算法的原理以及常用的垃圾收集器做了相应的总结。今天这篇文章主要是对生产环境中（Java7
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="jvm" scheme="http://matt33.com/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>HDFS 架构学习总结</title>
    <link href="http://matt33.com/2018/07/15/hdfs-architecture-learn/"/>
    <id>http://matt33.com/2018/07/15/hdfs-architecture-learn/</id>
    <published>2018-07-15T15:46:45.000Z</published>
    <updated>2018-07-15T15:46:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS（Hadoop Distributed File System）是一个分布式文件存储系统，几乎是离线存储领域的标准解决方案（有能力自研的大厂列外），业内应用非常广泛。近段抽时间，看一下 HDFS 的架构设计，虽然研究生也学习过相关内容，但是现在基本忘得差不多了，今天抽空对这块做了一个简单的总结，也算是再温习了一下这块的内容，这样后续再看 HDFS 方面的文章时，不至于处于懵逼状态。</p>
<h2 id="HDFS-1-0-架构"><a href="#HDFS-1-0-架构" class="headerlink" title="HDFS 1.0 架构"></a>HDFS 1.0 架构</h2><p>HDFS 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点，如下图所示（这个图是 HDFS1.0的架构图，经典的架构图）：</p>
<p><img src="/images/hadoop/hdfs.jpg" alt="HDFS 1.0 架构图"></p>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>NameNode 负责管理整个分布式系统的元数据，主要包括：</p>
<ul>
<li>目录树结构；</li>
<li>文件到数据库 Block 的映射关系；</li>
<li>Block 副本及其存储位置等管理数据；</li>
<li>DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。</li>
</ul>
<p>这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。</p>
<ul>
<li>fsimage：是内存命名空间元数据在外存的镜像文件；</li>
<li>editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。</li>
</ul>
<p>这两个文件相结合可以构造完整的内存数据。</p>
<h3 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h3><p>Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。</p>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。</p>
<h3 id="文件写入过程"><a href="#文件写入过程" class="headerlink" title="文件写入过程"></a>文件写入过程</h3><p>Client 向 HDFS 文件写入的过程可以参考<a href="http://shiyanjun.cn/archives/942.html" target="_blank" rel="external">HDFS写文件过程分析</a>，整体过程如下图（这个图比较经典，最开始来自<a href="https://book.douban.com/subject/3220004/" target="_blank" rel="external">《Hadoop：The Definitive Guide》</a>）所示：</p>
<p><img src="/images/hadoop/hdfs-write-flow.png" alt="HDFS 文件写入过程"></p>
<p>具体过程如下：</p>
<ol>
<li>Client 调用 DistributedFileSystem 对象的 <code>create</code> 方法，创建一个文件输出流（FSDataOutputStream）对象；</li>
<li>通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；</li>
<li>通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；</li>
<li>DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；</li>
<li>DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；</li>
<li>完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 <code>close</code> 方法，完成文件写入；</li>
<li>调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。</li>
</ol>
<h3 id="文件读取过程"><a href="#文件读取过程" class="headerlink" title="文件读取过程"></a>文件读取过程</h3><p>相对于文件写入，文件的读取就简单一些，流程如下图所示：</p>
<p><img src="/images/hadoop/hdfs-read-flow.png" alt="HDFS 文件读取过程"></p>
<p>其具体过程总结如下（简单总结一下）：</p>
<ol>
<li>Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；</li>
<li>NameNode 返回存储的每个块的 DataNode 列表；</li>
<li>Client 将连接到列表中最近的 DataNode；</li>
<li>Client 开始从 DataNode 并行读取数据；</li>
<li>一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。</li>
</ol>
<p>在处理 Client 的读取请求时，HDFS 会利用机架感知选举最接近 Client 位置的副本，这将会减少读取延迟和带宽消耗。</p>
<h2 id="HDFS-1-0-的问题"><a href="#HDFS-1-0-的问题" class="headerlink" title="HDFS 1.0 的问题"></a>HDFS 1.0 的问题</h2><p>在前面的介绍中，关于 HDFS1.0 的架构，首先都会看到 NameNode 的单点问题，这个在生产环境中是非常要命的问题，早期的 HDFS 由于规模较小，有些问题就被隐藏了，但自从进入了移动互联网时代，很多公司都开始进入了 PB 级的大数据时代，HDFS 1.0的设计缺陷已经无法满足生产的需求，最致命的问题有以下两点：</p>
<ul>
<li>NameNode 的单点问题，如果 NameNode 挂掉了，数据读写都会受到影响，HDFS 整体将变得不可用，这在生产环境中是不可接受的；</li>
<li>水平扩展问题，随着集群规模的扩大，1.0 时集群规模达到3000时，会导致整个集群管理的文件数目达到上限（因为 NameNode 要管理整个集群 block 元信息、数据目录信息等）。</li>
</ul>
<p>为了解决上面的两个问题，Hadoop2.0 提供一套统一的解决方案：</p>
<ol>
<li>HA（High Availability 高可用方案）：这个是为了解决 NameNode 单点问题；</li>
<li>NameNode Federation：是用来解决 HDFS 集群的线性扩展能力。</li>
</ol>
<h2 id="HDFS-2-0-的-HA-实现"><a href="#HDFS-2-0-的-HA-实现" class="headerlink" title="HDFS 2.0 的 HA 实现"></a>HDFS 2.0 的 HA 实现</h2><p>关于 HDFS 高可用方案，非常推荐这篇文章：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a>，IBM 博客的质量确实很高，这部分我这里也是主要根据这篇文章做一个总结，这里会从问题的原因、如何解决的角度去总结，并不会深入源码的实现细节，想有更深入了解还是推荐上面文章。</p>
<p>这里先看下 HDFS 高可用解决方案的架构设计，如下图（下图来自上面的文章）所示：</p>
<p><img src="/images/hadoop/hdfs-ha.png" alt="HDFS HA 架构实现"></p>
<p>这里与前面 1.0 的架构已经有很大变化，简单介绍一下上面的组件：</p>
<ol>
<li>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li>
<li>ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；</li>
<li>Zookeeper 集群：为主备切换控制器提供主备选举支持；</li>
<li>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在<strong>确认元数据完全同步之后才能继续对外提供服务</strong>。</li>
<li>DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li>
</ol>
<h3 id="FailoverController"><a href="#FailoverController" class="headerlink" title="FailoverController"></a>FailoverController</h3><p>FC 最初的目的是为了实现 SNN 和 ANN 之间故障自动切换，FC 是独立与 NN 之外的故障切换控制器，ZKFC 作为 NameNode 机器上一个独立的进程启动 ，它启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，其中：</p>
<ol>
<li>HealthMonitor：主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举；</li>
<li>ActiveStandbyElector：主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。</li>
</ol>
<h3 id="自动触发主备选举"><a href="#自动触发主备选举" class="headerlink" title="自动触发主备选举"></a>自动触发主备选举</h3><p>NameNode 在选举成功后，会在 zk 上创建了一个 <code>/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock</code> 节点，而没有选举成功的备 NameNode 会监控这个节点，通过 Watcher 来监听这个节点的状态变化事件，ZKFC 的 ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件（这部分实现跟 Kafka 中 Controller 的选举一样）。</p>
<p>如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点 /hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建 /hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock  节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。</p>
<p>当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。</p>
<h3 id="HDFS-脑裂问题"><a href="#HDFS-脑裂问题" class="headerlink" title="HDFS 脑裂问题"></a>HDFS 脑裂问题</h3><p>在实际中，NameNode 可能会出现这种情况，NameNode 在垃圾回收（GC）时，可能会在长时间内整个系统无响应，因此，也就无法向 zk 写入心跳信息，这样的话可能会导致临时节点掉线，备 NameNode 会切换到 Active 状态，这种情况，可能会导致整个集群会有同时有两个 NameNode，这就是脑裂问题。</p>
<p>脑裂问题的解决方案是隔离（Fencing），主要是在以下三处采用隔离措施：</p>
<ol>
<li>第三方共享存储：任一时刻，只有一个 NN 可以写入；</li>
<li>DataNode：需要保证只有一个 NN 发出与管理数据副本有关的删除命令；</li>
<li>Client：需要保证同一时刻只有一个 NN 能够对 Client 的请求发出正确的响应。</li>
</ol>
<p>关于这个问题目前解决方案的实现如下：</p>
<ol>
<li>ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为 <strong>/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb</strong> 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息；</li>
<li>Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候，会一起删除这个持久节点；</li>
<li>但如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于 /hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来，后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing。</li>
</ol>
<p>在进行 fencing 的时候，会执行以下的操作：</p>
<ol>
<li>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 <code>transitionToStandby</code> 方法，看能不能把它转换为 Standby 状态；</li>
<li>如果 <code>transitionToStandby</code> 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施。</li>
</ol>
<p>Hadoop 目前主要提供两种隔离措施，通常会选择第一种：</p>
<ol>
<li>sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；</li>
<li>shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离。</li>
</ol>
<p>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 <code>becomeActive</code> 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。</p>
<p>NameNode 选举的实现机制与 Kafka 的 Controller 类似，那么 Kafka 是如何避免脑裂问题的呢？</p>
<ol>
<li>Controller 给 Broker 发送的请求中，都会携带 controller epoch 信息，如果 broker 发现当前请求的 epoch 小于缓存中的值，那么就证明这是来自旧 Controller 的请求，就会决绝这个请求，正常情况下是没什么问题的；</li>
<li>但是异常情况下呢？如果 Broker 先收到异常 Controller 的请求进行处理呢？现在看 Kafka 在这一部分并没有适合的方案；</li>
<li>正常情况下，Kafka 新的 Controller 选举出来之后，Controller 会向全局所有 broker 发送一个 metadata 请求，这样全局所有 Broker 都可以知道当前最新的 controller epoch，但是并不能保证可以完全避免上面这个问题，还是有出现这个问题的几率的，只不过非常小，而且即使出现了由于 Kafka 的高可靠架构，影响也非常有限，至少从目前看，这个问题并不是严重的问题。</li>
</ol>
<h3 id="第三方存储（共享存储）"><a href="#第三方存储（共享存储）" class="headerlink" title="第三方存储（共享存储）"></a>第三方存储（共享存储）</h3><p>上述 HA 方案还有一个明显缺点，那就是第三方存储节点有可能失效，之前有很多共享存储的实现方案，目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。</p>
<p>QJM（Quorum Journal Manager）本质上是利用 Paxos 协议来实现的，QJM 在 <code>2F+1</code>  个 JournalNode 上存储 NN 的 editlog，每次写入操作都通过 Paxos 保证写入的一致性，它最多可以允许有 F 个 JournalNode 节点同时故障，其实现如下（图片来自：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a> ）：</p>
<p><img src="/images/hadoop/hdfs-ha-qjm.png" alt="基于 QJM 的共享存储的数据同步机制"></p>
<p>Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog。</p>
<p>还有一点需要注意的是，在 2.0 中不再有 SNN 这个角色了，NameNode 在启动后，会先加载 FSImage 文件和共享目录上的 EditLog Segment 文件，之后 NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式，其中：</p>
<ol>
<li>EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog；</li>
<li>StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。</li>
</ol>
<h2 id="HDFS-2-0-Federation-实现"><a href="#HDFS-2-0-Federation-实现" class="headerlink" title="HDFS 2.0 Federation 实现"></a>HDFS 2.0 Federation 实现</h2><p>在 1.0 中，HDFS 的架构设计有以下缺点：</p>
<ol>
<li>namespace 扩展性差：在单一的 NN 情况下，因为所有 namespace 数据都需要加载到内存，所以物理机内存的大小限制了整个 HDFS 能够容纳文件的最大个数（namespace 指的是 HDFS 中树形目录和文件结构以及文件对应的 block 信息）；</li>
<li>性能可扩展性差：由于所有请求都需要经过 NN，单一 NN 导致所有请求都由一台机器进行处理，很容易达到单台机器的吞吐；</li>
<li>隔离性差：多租户的情况下，单一 NN 的架构无法在租户间进行隔离，会造成不可避免的相互影响。</li>
</ol>
<p>而 Federation 的设计就是为了解决这些问题，采用 Federation 的最主要原因是设计实现简单，而且还能解决问题。</p>
<h3 id="Federation-架构"><a href="#Federation-架构" class="headerlink" title="Federation 架构"></a>Federation 架构</h3><p>Federation 的架构设计如下图所示（图片来自 <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">HDFS Federation</a>）：</p>
<p><img src="/images/hadoop/hdfs-federation.gif" alt="HDFS Federation 架构实现"></p>
<h3 id="Federation-的核心设计思想"><a href="#Federation-的核心设计思想" class="headerlink" title="Federation 的核心设计思想"></a>Federation 的核心设计思想</h3><p>Federation 的核心思想是将一个大的 namespace 划分多个子 namespace，并且每个 namespace 分别由单独的 NameNode 负责，这些 NameNode 之间互相独立，不会影响，不需要做任何协调工作（其实跟拆集群有一些相似），集群的所有 DataNode 会被多个 NameNode 共享。</p>
<p>其中，每个子 namespace 和 DataNode 之间会由数据块管理层作为中介建立映射关系，数据块管理层由若干数据块池（Pool）构成，每个数据块只会唯一属于某个固定的数据块池，而一个子 namespace 可以对应多个数据块池。每个 DataNode 需要向集群中所有的 NameNode 注册，且周期性地向所有 NameNode 发送心跳和块报告，并执行来自所有 NameNode 的命令。</p>
<ul>
<li>一个 block pool 由属于同一个 namespace 的数据块组成，每个 DataNode 可能会存储集群中所有 block pool 的数据块；</li>
<li>每个 block pool 内部自治，也就是说各自管理各自的 block，不会与其他 block pool 交流，如果一个 NameNode 挂掉了，不会影响其他 NameNode;</li>
<li>某个 NameNode 上的 namespace 和它对应的 block pool 一起被称为 namespace volume，它是管理的基本单位。当一个 NameNode/namespace 被删除后，其所有 DataNode 上对应的 block pool 也会被删除，当集群升级时，每个 namespace volume 可以作为一个基本单元进行升级。</li>
</ul>
<p>到这里，基本对 HDFS 这部分总结完了，虽然文章的内容基本都来自下面的参考资料，但是自己在总结的过程中，也对 HDFS 的基本架构有一定的了解，后续结合公司 HDFS 团队的 CaseStudy 深入学习这部分的内容，工作中，也慢慢感觉到分布式系统，很多的设计实现与问题解决方案都很类似，只不过因为面对业务场景的不同而采用了不同的实现。</p>
<hr>
<p>参考：</p>
<ul>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html" target="_blank" rel="external">HDFS Architecture</a>;</li>
<li><a href="http://shiyanjun.cn/archives/942.html" target="_blank" rel="external">HDFS 写文件过程分析</a>;</li>
<li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSRouterFederation.html" target="_blank" rel="external">HDFS Router-based Federation</a>；</li>
<li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="external">HDFS High Availability Using the Quorum Journal Manager</a>；</li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="external">HDFS Commands Guide</a>；</li>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/" target="_blank" rel="external">Hadoop NameNode 高可用 (High Availability) 实现解析</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="external">HDFS Federation</a>；</li>
<li><a href="http://dongxicheng.org/mapreduce/hdfs-federation-introduction/" target="_blank" rel="external">HDFS Federation设计动机与基本原理</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://tech.meituan.com/namenode-restart-optimization.html" target="_blank" rel="external">HDFS NameNode重启优化</a>；</li>
<li><a href="https://tech.meituan.com/hdfs-federation.html" target="_blank" rel="external">HDFS Federation在美团点评的应用与改进</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS（Hadoop Distributed File System）是一个分布式文件存储系统，几乎是离线存储领域的标准解决方案（有能力自研的大厂列外），业内应用非常广泛。近段抽时间，看一下 HDFS 的架构设计，虽然研究生也学习过相关内容，但是现在基本忘得差不多了，今天
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hadoop" scheme="http://matt33.com/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Controller Redesign 方案</title>
    <link href="http://matt33.com/2018/07/14/kafka-controller-redesign/"/>
    <id>http://matt33.com/2018/07/14/kafka-controller-redesign/</id>
    <published>2018-07-14T15:13:56.000Z</published>
    <updated>2018-07-15T04:17:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是一次重构，因此，社区准备在新版的系统里对 Controller 做一些相应的优化（0.11.0及以后的版本），相应的设计方案见：<a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>，本文的内容就是结合这篇文章做一个简单的总结。</p>
<h2 id="Controller-功能"><a href="#Controller-功能" class="headerlink" title="Controller 功能"></a>Controller 功能</h2><p>在一个 Kafka 中，Controller 要处理的事情总结如下表所示：</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>详情</th>
</tr>
</thead>
<tbody>
<tr>
<td>cluster metadata updates</td>
<td>producer 或 consumer 可以通过 MetadataRequest 请求从集群任何一台 broker 上查询到某个 Partition 的 metadata 信息，如果一个 Partition 的 leader 或 isr 等信息变化，Controller 会广播到集群的所有 broker 上，这样每台 Broker 都会有该 Partition 的最新 Metadata 信息</td>
</tr>
<tr>
<td>topic creation</td>
<td>用户可以通过多种方式创建一个 topic，最终的结果都是在 zk 的 <code>/brokers/topics</code> 目录下新建一个 topic 节点信息，controller 通过监控这个目录来判断是否有新的 topic 需要创建</td>
</tr>
<tr>
<td>topic deletion</td>
<td>Controller 通过监控 zk 的 <code>/admin/delete_topics</code> 节点来触发 topic 删除操作</td>
</tr>
<tr>
<td>partition reassignment</td>
<td>Controller 通过监控 zk 的 <code>/admin/reassign_partitions</code> 节点来触发 Partition 的副本迁移操作</td>
</tr>
<tr>
<td>preferred replica leader election</td>
<td>Controller 通过监控 zk 的 <code>/admin/preferred_replica_election</code> 节点来触发最优 leader 选举操作，该操作的目的选举 Partition 的第一个 replica 作为 leader</td>
</tr>
<tr>
<td>topic partition expansion</td>
<td>Controller 通过监控 zk 的 <code>/brokers/topics/&lt;topic&gt;</code> 数据内容的变化，来触发 Topic 的 Partition 扩容操作</td>
</tr>
<tr>
<td>broker join</td>
<td>Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 是最新加入的，进而触发 broker 的上线操作</td>
</tr>
<tr>
<td>broker failure</td>
<td>同样，Controller 通过监控 zk 的 <code>/brokers/ids</code> 目录变化，就会知道哪些 broker 掉线了，进而触发 broker 的下线操作</td>
<td></td>
</tr>
<tr>
<td>controlled shutdown</td>
<td>Controller 通过处理 ControlledShudownRequest 请求来优雅地关闭一个 broker 节点，主动关闭与直接 kill 的区别，它可以减少 Partition 的不可用时间，因为一个 broker 的 zk 临时节点消失是需要一定时间的</td>
</tr>
<tr>
<td>controller leader election</td>
<td>集群中所有 broker 会监听 zk 的 <code>/controller</code> 节点，如果该节点消失，所有的 broker 都回去抢占 controller 节点，抢占成功的，就成了最新的 controller</td>
</tr>
</tbody>
</table>
<h2 id="Controller-目前存在的问题"><a href="#Controller-目前存在的问题" class="headerlink" title="Controller 目前存在的问题"></a>Controller 目前存在的问题</h2><p>之所以要重新设计 Controller，是因为现在的 Controller 积累了一些比较难解决的问题，这些问题解决起来，代码改动量都是巨大的，甚至需要改变 controller 部门的设计，基本就跟重构差不多了，下面我们先来了看一下 controller 之前（主要是 0.11.0 之前的版本）存在的一些问题。</p>
<p>目前遇到的比较大的问题有以下几个：</p>
<ol>
<li>Partition 级别同步 zk 写；</li>
<li>sequential per-partition controller-to-broker requests；</li>
<li>Controller 复杂的并发语义；</li>
<li>代码组织混乱；</li>
<li>控制类请求与数据类请求未分离；</li>
<li>Controller 给 broker 的请求中没有 broker 的 generation信息；</li>
<li>ZkClient 阻碍 Client 的状态管理。</li>
</ol>
<h3 id="Partition-级别同步-zk-写"><a href="#Partition-级别同步-zk-写" class="headerlink" title="Partition 级别同步 zk 写"></a>Partition 级别同步 zk 写</h3><p>zookeeper 的同步写意味着在下次写之前需要等待前面整个过程的结束，而且由于它们都是 partition 粒度的（一个 Partition 一个 Partition 的去执行写操作），对于 Partition 非常多的集群来说，需要等待的时间会更长，Controller 通常会在下面这两个地方做 Partition 级别 zookeeper 同步写操作：</p>
<ol>
<li>PartitionStateMachine 在进行触发 leader 选举（partition 目的状态是 OnlinePartition），将会触发上面的操作；</li>
<li>ReplicaStateMachine 更新 LeaderAndIsr 信息到 zk（replica 状态转变为 OfflineReplica），这种情况也触发这种情况，它既阻碍了 Controller 进程，也有可能会 zk 造成压力。</li>
</ol>
<h3 id="sequential-per-partition-controller-to-broker-requests"><a href="#sequential-per-partition-controller-to-broker-requests" class="headerlink" title="sequential per-partition controller-to-broker requests"></a>sequential per-partition controller-to-broker requests</h3><p>Controller 在向 Broker 发送请求，有些情况下也是 Partition 粒度去发送的，效率非常低，比如在 Controller 处理 broker shutdown 请求时，这里是按 Partition 级别处理，每处理一个 Partition 都会执行 Partition、Replica 状态变化以及 Metadata 更新，并且调用 <code>sendRequestsToBrokers()</code> 向 broker 发送请求，这样的话，效率将变得非常低。</p>
<h3 id="Controller-复杂的并发语义"><a href="#Controller-复杂的并发语义" class="headerlink" title="Controller 复杂的并发语义"></a>Controller 复杂的并发语义</h3><p>Controller 需要在多个线程之间共享状态信息，这些线程有：</p>
<ol>
<li>IO threads handling controlled shutdown requests</li>
<li>The ZkClient org.I0Itec.zkclient.ZkEventThread processing zookeeper callbacks sequentially；</li>
<li>The TopicDeletionManager kafka.controller.DeleteTopicsThread；</li>
<li>Per-broker RequestSendThread within ControllerChannelManager.</li>
</ol>
<p>所有这些线程都需要访问或修改状态信息（ControllerContext），现在它们是通过 ControllerContext 的 controllerLock（排它锁）实现的，Controller 的并发变得虚弱无力。</p>
<h3 id="代码组织混乱"><a href="#代码组织混乱" class="headerlink" title="代码组织混乱"></a>代码组织混乱</h3><p>KafkaController 部分的代码组织（KafkaController、PartitionStateMachine 和 ReplicaStateMachine）不是很清晰，比如，下面的问题就很难回答：</p>
<ol>
<li>where and when does zookeeper get updated?</li>
<li>where and when does a controller-to-broker request get formed?</li>
<li>what impact does a failing zookeeper update or controller-to-broker request have on the cluster state?</li>
</ol>
<p>这也导致了这部分很多开发者不敢轻易去改动。</p>
<h3 id="控制类请求与数据类请求未分离"><a href="#控制类请求与数据类请求未分离" class="headerlink" title="控制类请求与数据类请求未分离"></a>控制类请求与数据类请求未分离</h3><p>现在 broker 收到的请求，有来自 client、broker 和 controller 的请求，这些请求都会被放到同一个 requestQueue 中，它们有着同样的优先级，所以来自 client 的请求很可能会影响来自 controller 请求的处理（如果是 leader 变动的请求，ack 设置的不是 all，这种情况有可能会导致数据丢失）。</p>
<h3 id="Controller-给-broker-的请求中没有-broker-的-generation信息"><a href="#Controller-给-broker-的请求中没有-broker-的-generation信息" class="headerlink" title="Controller 给 broker 的请求中没有 broker 的 generation信息"></a>Controller 给 broker 的请求中没有 broker 的 generation信息</h3><p>这里的 Broker generation 代表着一个标识，每当它重新加入集群时，这个标识都会变化。如果 Controller 的请求没有这个信息的话，可能会导致一个重启的 Broker 收到之前的请求，让 Broker 进入到一个错误的状态。</p>
<p>比如，Broker 收到之前的 StopReplica 请求，可能会导致副本同步线程退出。</p>
<h3 id="ZkClient-阻碍-Client-的状态管理"><a href="#ZkClient-阻碍-Client-的状态管理" class="headerlink" title="ZkClient 阻碍 Client 的状态管理"></a>ZkClient 阻碍 Client 的状态管理</h3><p>这里的状态管理指的是当 Client 发生重连或会话过期时，Client 可以监控这种状态变化，并做出一些处理，因为开源版的 ZKClient 在处理 notification 时，是线性处理的，一些 notification 会被先放到 ZkEventThread’s queue 中，这样会导致一些最新的 notification 不能及时被处理，特别是与 zk 连接断开重连的情况。</p>
<h2 id="Controller-改进方案"><a href="#Controller-改进方案" class="headerlink" title="Controller 改进方案"></a>Controller 改进方案</h2><p>关于上述问题，Kafka 提出了一些改进方案，有些已经在最新版的系统中实现，有的还在规划中。</p>
<h3 id="使用异步的-zk-api"><a href="#使用异步的-zk-api" class="headerlink" title="使用异步的 zk api"></a>使用异步的 zk api</h3><p>Zookeeper 的 client 提供三种执行请求的方式：</p>
<ol>
<li>同步调用，意味着下次请求需要等待当前当前请求的完成；</li>
<li>异步调用，意味着不需要等待当前请求的完成就可以开始下次请求的执行，并且我们可以通过回调机制去处理请求返回的结果；</li>
<li>单请求的 batch 调用，意味着 batch 内的所有请求都会在一次事务处理中完成，这里需要关注的是 zookeeper 的 server 对单请求的大小是有限制的（jute.maxbuffer）。</li>
</ol>
<p>文章中给出了三种请求的测试结果，Kafka 最后选取的是异步处理机制，因为对于单请求处理，异步处理更加简洁，并且相比于同步处理还可以保持一个更好的写性能。</p>
<h3 id="improve-controller-to-broker-request-batching"><a href="#improve-controller-to-broker-request-batching" class="headerlink" title="improve controller-to-broker request batching"></a>improve controller-to-broker request batching</h3><p>这个在设计文档还是 TODO 状态，具体的方案还没确定，不过基本可以猜测一下，因为目的是提高 batch 发送能力，那么只能是在调用对每个 broker 的 RequestSenderThread 线程发送请求之前，做一下检测，而不是来一个请求立马就发送，这是一个性能与时间的权衡，如果不是立马发送请求，那么可能会带来 broker 短时 metadata 信息的不一致，这个不一致时间不同的应用场景要求是不一样的。</p>
<h3 id="单线程的事件处理模型"><a href="#单线程的事件处理模型" class="headerlink" title="单线程的事件处理模型"></a>单线程的事件处理模型</h3><p>采用单线程的时间处理模型将极大简化 Controller 的并发实现，只允许这个线程访问和修改 Controller 的本地状态信息，因此在 Controller 部分也就不需要到处加锁来保证线程安全了。</p>
<p>目前 1.1.0 的实现中，Controller 使用了一个 ControllerEventThread 线程来处理所有的 event，目前可以支持13种不同类型事件：</p>
<ol>
<li>Idle：代表当前 ControllerEventThread 处理空闲状态；</li>
<li>ControllerChange：Controller 切换处理；</li>
<li>BrokerChange：Broker 变动处理，broker 可能有上线或掉线；</li>
<li>TopicChange：Topic 新增处理；</li>
<li>TopicDeletion：Topic 删除处理；</li>
<li>PartitionReassignment：Partition 副本迁移处理；</li>
<li>AutoLeaderBalance：自动 rebalance 处理；</li>
<li>ManualLeaderBalance：最优 leader 选举处理，这里叫做手动 rebalance，手动去切流量；</li>
<li>ControlledShutdown：优雅关闭 broker；</li>
<li>IsrChange：Isr 变动处理；</li>
<li>LeaderAndIsrResponseReceived；</li>
<li>LogDirChange：Broker 某个目录失败后的处理（比如磁盘坏掉等）；</li>
<li>ControllerShutdown：ControllerEventThread 处理这个事件时，会关闭当前线程。</li>
</ol>
<h3 id="重构集群状态管理"><a href="#重构集群状态管理" class="headerlink" title="重构集群状态管理"></a>重构集群状态管理</h3><p>这部分的改动，目前社区也没有一个很好的解决思路，重构这部分的目的是希望 Partition、Replica 的状态管理变得更清晰一些，让我们从代码中可以清楚地明白状态是在什么时间、什么地方、什么条件下被触发的。这个优化其实是跟上面那个有很大关联，采用单线程的事件处理模型，可以让状态管理也变得更清晰。</p>
<h4 id="prioritize-controller-requests"><a href="#prioritize-controller-requests" class="headerlink" title="prioritize controller requests"></a>prioritize controller requests</h4><p>我们想要把控制类请求与数据类请求分开，提高 controller 请求的优先级，这样的话即使 Broker 中请求有堆积，Broker 也会优先处理控制类的请求。</p>
<p>这部分的优化可以在网络层的 RequestChannel 中做，RequestChannel 可以根据请求的 id 信息把请求分为正常的和优先的，如果请求是 UpdateMetadataRequest、LeaderAndIsrRequest 或者 StopReplicaRequest，那么这个请求的优先级应该提高。实现方案有以下两种：</p>
<ol>
<li>在请求队列中增加一个优先级队列，优先级高的请求放到 the prioritized request queue 中，优先级低的放到普通请求队列中，但是无论使用一个定时拉取（poll）还是2个定时拉取，都会带来其他的问题，要么是增大普通请求的处理延迟，要么是增大了优先级高请求的延迟；</li>
<li>直接使用优先级队列代替现在的普通队列，设计上更倾向与这一种。</li>
</ol>
<p>目前这部分在1.1.0中还未实现。</p>
<h3 id="Controller-发送请求中添加-broker-的-generation-信息"><a href="#Controller-发送请求中添加-broker-的-generation-信息" class="headerlink" title="Controller 发送请求中添加 broker 的 generation 信息"></a>Controller 发送请求中添加 broker 的 generation 信息</h3><p>generation 信息是用来标识当前 broker 加入集群 epoch 信息，每当 broker 重新加入集群中，该 broker.id 对应的 generation 都应该变化（要求递增），目前有两种实现方案：</p>
<ol>
<li>为 broker 分配的一个全局唯一的 id，由 controller 广播给其他 broker；</li>
<li>直接使用 zookeeper 的 zxid 信息（broker.id 注册时的 zxid）。</li>
</ol>
<h3 id="直接使用原生的-Zookeeper-client"><a href="#直接使用原生的-Zookeeper-client" class="headerlink" title="直接使用原生的 Zookeeper client"></a>直接使用原生的 Zookeeper client</h3><p>Client 端的状态管理意味着当 Client 端发生状态变化（像连接中断或回话超时）时，我们有能力做一些操作。其中，zookeeper client 有效的状态（目前的 client 比下面又多了几种状态，这里先不深入）是:</p>
<ul>
<li>NOT_CONNECTED： the initial state of the client；</li>
<li>CONNECTING： the client is establishing a connection to zookeeper；</li>
<li>CONNECTED： the client has established a connection and session to zookeeper；</li>
<li>CLOSED： the session has closed or expired。</li>
</ul>
<p>有效的状态转移是：</p>
<ul>
<li>NOT_CONNECTED &gt; CONNECTING</li>
<li>CONNECTING &gt; CONNECTED</li>
<li>CONNECTING &gt; CLOSED</li>
<li>CONNECTED &gt; CONNECTING</li>
<li>CONNECTED &gt; CLOSED</li>
</ul>
<p>最开始的设想是直接使用原生 Client 的异步调用方式，这样的话依然可以通过回调方法监控到状态的变化（像连接中断或回话超时），同样，在每次事件处理时，可以通过检查状态信息来监控到 Client 状态的变化，及时做一些处理。</p>
<p>当一个 Client 接收到连接中断的 notification（Client 状态变成了 CONNECTING 状态），它意味着 Client 不能再从 zookeeper 接收到任何 notification 了。如果断开连接，对于 Controller 而言，无论它现在正在做什么它都应该先暂停，因为可能集群的 Controller 已经切换到其他机器上了，只是它还没接收到通知，它如果还在工作，可能会导致集群状态不一致。当连接断开后，Client 可以重新建立连接（re-establish，状态变为 CONNECTED）或者会话过期（状态变为 CLOSED，会话过期是由 zookeeper Server 来决定的）。如果变成了 CONNECTED 状态，Controller 应该重新开始这些暂停的操作，而如果状态变成了 CLOSED 状态，旧的 Controller 就会知道它不再是 controller，应该丢弃掉这些任务。</p>
<p>参考：</p>
<ul>
<li><a href="https://docs.google.com/document/d/1rLDmzDOGQQeSiMANP0rC2RYp_L7nUGHzFD9MQISgXYM/edit#heading=h.pxfjarumuhko" target="_blank" rel="external">Kafka Controller Redesign</a>；</li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6980045.html" target="_blank" rel="external">Kafka controller重设计</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka Controller 是 Kafka 的核心组件，在前面的文章中，已经详细讲述过 Controller 部分的内容。在过去的几年根据大家在生产环境中应用的反馈，Controller 也积累了一些比较大的问题，而针对这些问题的修复，代码的改动量都是非常大的，无疑是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统的一致性协议之 2PC 和 3PC</title>
    <link href="http://matt33.com/2018/07/08/distribute-system-consistency-protocol/"/>
    <id>http://matt33.com/2018/07/08/distribute-system-consistency-protocol/</id>
    <published>2018-07-08T15:21:34.000Z</published>
    <updated>2018-07-08T16:02:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，最多只能满足其中两个。但在实际中，由于网络环境是不可信的，所以分区容忍性几乎是必不可选的，设计者基本就是在一致性和可用性之间做选择，当然大部分情况下，大家都会选择牺牲一部分的一致性来保证可用性（可用性较差的系统非常影响用户体验的，但是对另一些场景，比如支付场景，强一致性是必须要满足）。但是分布式系统又无法彻底放弃一致性（Consistency），如果真的放弃一致性，那么就说明这个系统中的数据根本不可信，数据也就没有意义，那么这个系统也就没有任何价值可言。</p>
<h2 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a>CAP 理论</h2><p>CAP 理论三个特性的详细含义如下：</p>
<ol>
<li>一致性（Consistency）：每次读取要么是最新的数据，要么是一个错误；</li>
<li>可用性（Availability）：client 在任何时刻的读写操作都能在限定的延迟内完成的，即每次请求都能获得一个响应（非错误），但不保证是最新的数据；</li>
<li>分区容忍性（Partition tolerance）：在大规模分布式系统中，网络分区现象，即分区间的机器无法进行网络通信的情况是必然会发生的，系统应该能保证在这种情况下可以正常工作。</li>
</ol>
<h3 id="分区容忍性"><a href="#分区容忍性" class="headerlink" title="分区容忍性"></a>分区容忍性</h3><p>很多人可能对分区容忍性不太理解，知乎有一个回答对这个解释的比较清楚（<a href="https://www.zhihu.com/question/54105974" target="_blank" rel="external">CAP理论中的P到底是个什么意思？</a>），这里引用一下：</p>
<ul>
<li>一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。</li>
<li>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</li>
<li>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里，容忍性就提高了。</li>
<li>然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。</li>
<li>要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</li>
<li>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</li>
</ul>
<h3 id="CAP-如何选择"><a href="#CAP-如何选择" class="headerlink" title="CAP 如何选择"></a>CAP 如何选择</h3><p>CAP 理论一个经典原理如下所示：</p>
<p><img src="/images/distribute/CAP.png" alt="CAP 理论原理"></p>
<p>CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。但是，对于大多数互联网应用来说，因为规模比较大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。但是对于一些金融相关行业，它有很多场景需要确保一致性，这种情况通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p>
<p>在一个分布式系统中，对于这三个特性，我们只能三选二，无法同时满足这三个特性，三选二的组合以及这样系统的特点总结如下（来自<a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>）：</p>
<ul>
<li>CA (Consistency + Availability)：关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li>
<li>CP (consistency + partition tolerance)：关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)：这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p>对于分布式系统分区容忍性是天然具备的要求，否则一旦出现网络分区，系统就拒绝所有写入只允许可读，这对大部分的场景是不可接收的，因此，在设计分布式系统时，更多的情况下是选举 CP 还是 AP，要么选择强一致性弱可用性，要么选择高可用性容忍弱一致性。</p>
<h3 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h3><p>关于分布式系统的一致性模型有以下几种：</p>
<h4 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h4><p>当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值，直到这个数据被其他数据更新为止。</p>
<p>但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。</p>
<h4 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a>弱一致性</h4><p>系统并不保证进程或者线程的访问都会返回最新更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。</p>
<h4 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h4><p>最终一致性也是弱一致性的一种，它无法保证数据更新后，所有后续的访问都能看到最新数值，而是需要一个时间，在这个时间之后可以保证这一点，而在这个时间内，数据也许是不一致的，这个系统无法保证强一致性的时间片段被称为「不一致窗口」。不一致窗口的时间长短取决于很多因素，比如备份数据的个数、网络传输延迟速度、系统负载等。</p>
<p>最终一致性在实际应用中又有多种变种：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>因果一致性</td>
<td>如果 A 进程在更新之后向 B 进程通知更新的完成，那么 B 的访问操作将会返回更新的值。而没有因果关系的 C 进程将会遵循最终一致性的规则（C 在不一致窗口内还是看到是旧值）。</td>
</tr>
<tr>
<td>读你所写一致性</td>
<td>因果一致性的特定形式。一个进程进行数据更新后，会给自己发送一条通知，该进程后续的操作都会以最新值作为基础，而其他的进程还是只能在不一致窗口之后才能看到最新值。</td>
</tr>
<tr>
<td>会话一致性</td>
<td>读你所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程可以读取到最新之，但如果会话终止，重新连接后，如果此时还在不一致窗口内，还是可嫩读取到旧值。</td>
</tr>
<tr>
<td>单调读一致性</td>
<td>如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。</td>
</tr>
<tr>
<td>单调写一致性</td>
<td>系统保证对同一个进程的写操作串行化。</td>
</tr>
</tbody>
</table>
<p>它们的关系又如下图所示（图来自 <a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>）：</p>
<p><img src="/images/distribute/consistency.png" alt="一致性模型之间关系"></p>
<h2 id="分布式一致性协议"><a href="#分布式一致性协议" class="headerlink" title="分布式一致性协议"></a>分布式一致性协议</h2><p>为了解决分布式系统的一致性问题，在长期的研究探索过程中，业内涌现出了一大批经典的一致性协议和算法，其中比较著名的有二阶段提交协议（2PC），三阶段提交协议（3PC）和 Paxos 算法（本文暂时先不介绍）。</p>
<p>Google 2009年 在<a href="https://snarfed.org/transactions_across_datacenters_io.html" target="_blank" rel="external">Transaction Across DataCenter</a> 的分享中，对一致性协议在业内的实践做了一简单的总结，如下图所示，这是 CAP 理论在工业界应用的实践经验。</p>
<p><img src="/images/distribute/cap-sumarry.png" alt="CAP 理论在工业界的实践"></p>
<p>其中，第一行表头代表了分布式系统中通用的一致性方案，包括冷备、Master/Slave、Master/Master、两阶段提交以及基于 Paxos 算法的解决方案，第一列表头代表了分布式系统大家所关心的各项指标，包括一致性、事务支持程度、数据延迟、系统吞吐量、数据丢失可能性、故障自动恢复方式。</p>
<h2 id="两阶段提交协议（2PC）"><a href="#两阶段提交协议（2PC）" class="headerlink" title="两阶段提交协议（2PC）"></a>两阶段提交协议（2PC）</h2><p>二阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保证在分布式事务中，要么所有参与进程都提交事务，要么都取消事务，即实现 ACID 的原子性（A）。在数据一致性中，它的含义是：要么所有副本（备份数据）同时修改某个数值，要么都不更改，以此来保证数据的强一致性。</p>
<p>2PC 要解决的问题可以简单总结为：在分布式系统中，每个节点虽然可以知道自己的操作是成功还是失败，却是无法知道其他节点的操作状态。当一个事务需要跨越多个节点时，为了保持事务的 ACID 特性，需要引入一个作为<strong>协调者</strong>的组件来统一掌控所有节点（参与者）的操作结果并最终指示这些节点是否要把操作结果进行真正的提交（比如将更新后的数据写入磁盘等等）。因此，二阶段提交的算法思路可以概括为： 参与者将操作结果通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</p>
<h3 id="2PC-过程"><a href="#2PC-过程" class="headerlink" title="2PC 过程"></a>2PC 过程</h3><p>关于两阶段提交的过程如下图所示：</p>
<p><img src="/images/distribute/2pc_process.png" alt="两阶段提交过程"></p>
<p>顾名思义，2PC 分为两个过程：</p>
<ol>
<li>表决阶段：此时 Coordinator （协调者）向所有的参与者发送一个 vote request，参与者在收到这请求后，如果准备好了就会向 Coordinator 发送一个 <code>VOTE_COMMIT</code> 消息作为回应，告知 Coordinator 自己已经做好了准备，否则会返回一个 <code>VOTE_ABORT</code> 消息；</li>
<li>提交阶段：Coordinator 收到所有参与者的表决信息，如果所有参与者一致认为可以提交事务，那么 Coordinator 就会发送 <code>GLOBAL_COMMIT</code> 消息，否则发送 <code>GLOBAL_ABORT</code> 消息；对于参与者而言，如果收到 <code>GLOBAL_COMMIT</code> 消息，就会提交本地事务，否则就会取消本地事务。</li>
</ol>
<h3 id="2PC-一致性问题"><a href="#2PC-一致性问题" class="headerlink" title="2PC 一致性问题"></a>2PC 一致性问题</h3><p>这里先讨论一下，2PC 是否可以在任何情况下都可以解决一致性问题，在实际的网络生产中，各种情况都有可能发生，这里，我们先从理论上分析各种意外情况。</p>
<p>2PC 在执行过程中可能发生 Coordinator 或者参与者突然宕机的情况，在不同时期宕机可能有不同的现象。</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 挂了，参与者没挂</td>
<td>这种情况其实比较好解决，只要找一个 Coordinator 的替代者。当他成为新的 Coordinator 的时候，询问所有参与者的最后那条事务的执行情况，他就可以知道是应该做什么样的操作了。所以，这种情况不会导致数据不一致。</td>
</tr>
<tr>
<td>参与者挂了（无法恢复），Coordinator 没挂</td>
<td>如果挂了之后没有恢复，那么是不会导致数据一致性问题。</td>
</tr>
<tr>
<td>参与者挂了（后来恢复），Coordinator 没挂</td>
<td>恢复后参与者如果发现有未执行完的事务操作，直接取消，然后再询问 Coordinator 目前我应该怎么做，协调者就会比对自己的事务执行记录和该参与者的事务执行记录，告诉他应该怎么做来保持数据的一致性。</td>
</tr>
</tbody>
</table>
<p>还有一种情况是：参与者挂了，Coordinator 也挂了，需要再细分为几种类型来讨论：</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>分析及解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coordinator 和参与者在第一阶段挂了</td>
<td>由于这时还没有执行 commit 操作，新选出来的 Coordinator 可以询问各个参与者的情况，再决定是进行 commit 还是 roolback。因为还没有 commit，所以不会导致数据一致性问题。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，但是挂的这个参与者在挂之前还没有做相关操作</td>
<td>这种情况下，当新的 Coordinator 被选出来之后，他同样是询问所有参与者的情况。只要有机器执行了 abort（roolback）操作或者第一阶段返回的信息是 No 的话，那就直接执行 roolback 操作。如果没有人执行 abort 操作，但是有机器执行了 commit 操作，那么就直接执行 commit 操作。这样，当挂掉的参与者恢复之后，只要按照 Coordinator 的指示进行事务的 commit 还是 roolback 操作就可以了。因为挂掉的机器并没有做 commit 或者 roolback 操作，而没有挂掉的机器们和新的 Coordinator 又执行了同样的操作，那么这种情况不会导致数据不一致现象。</td>
</tr>
<tr>
<td>Coordinator 和参与者在第二阶段挂了，挂的这个参与者在挂之前已经执行了操作。但是由于他挂了，没有人知道他执行了什么操作。</td>
<td>这种情况下，新的 Coordinator 被选出来之后，如果他想负起 Coordinator 的责任的话他就只能按照之前那种情况来执行 commit 或者 roolback 操作。这样新的 Coordinator 和所有没挂掉的参与者就保持了数据的一致性，我们假定他们执行了 commit。但是，这个时候，那个挂掉的参与者恢复了怎么办，因为他已经执行完了之前的事务，如果他执行的是 commit 那还好，和其他的机器保持一致了，万一他执行的是 roolback 操作呢？这不就导致数据的不一致性了么？虽然这个时候可以再通过手段让他和 Coordinator 通信，再想办法把数据搞成一致的，但是，这段时间内他的数据状态已经是不一致的了！</td>
</tr>
</tbody>
</table>
<p>所以，2PC协议中，如果出现协调者和参与者都挂了的情况，有可能导致数据不一致。为了解决这个问题，衍生除了3PC。</p>
<h3 id="2PC-优缺点"><a href="#2PC-优缺点" class="headerlink" title="2PC 优缺点"></a>2PC 优缺点</h3><p>简单总结一下 2PC 的优缺点：</p>
<ul>
<li>优点：原理简洁清晰、实现方便；</li>
<li>缺点：同步阻塞、单点问题、某些情况可能导致数据不一致。</li>
</ul>
<p>关于这几个缺点，在实际应用中，都是对2PC 做了相应的改造：</p>
<ol>
<li>同步阻塞：2PC 有几个过程（比如 Coordinator 等待所有参与者表决的过程中）都是同步阻塞的，在实际的应用中，这可能会导致长阻塞问题，这个问题是通过超时判断机制来解决的，但并不能完全解决同步阻塞问题；</li>
<li>Coordinator 单点问题：实际生产应用中，Coordinator 都会有相应的备选节点；</li>
<li>数据不一致：这个在前面已经讲述过了，如果在第二阶段，Coordinator 和参与者都出现挂掉的情况下，是有可能导致数据不一致的。</li>
</ol>
<h2 id="三阶段提交协议（3PC）"><a href="#三阶段提交协议（3PC）" class="headerlink" title="三阶段提交协议（3PC）"></a>三阶段提交协议（3PC）</h2><p>三阶段提交协议（Three-Phase Commit， 3PC）最关键要解决的就是 Coordinator 和参与者同时挂掉导致数据不一致的问题，所以 3PC 把在 2PC 中又添加一个阶段，这样三阶段提交就有：CanCommit、PreCommit 和 DoCommit 三个阶段。</p>
<h3 id="3PC-过程"><a href="#3PC-过程" class="headerlink" title="3PC 过程"></a>3PC 过程</h3><p>三阶段提交协议的过程如下图（图来自 <a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>）所示：</p>
<p><img src="/images/distribute/Three-phase_commit_diagram.png" alt="三节点提交过程"></p>
<p>3PC 的详细过程如下（这个过程步骤内容来自 <a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>）：</p>
<h4 id="阶段一-CanCommit"><a href="#阶段一-CanCommit" class="headerlink" title="阶段一 CanCommit"></a>阶段一 CanCommit</h4><ol>
<li>事务询问：Coordinator 向各参与者发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应；</li>
<li>参与者向 Coordinator 反馈询问的响应：参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No。</li>
</ol>
<h4 id="阶段二-PreCommit"><a href="#阶段二-PreCommit" class="headerlink" title="阶段二 PreCommit"></a>阶段二 PreCommit</h4><p><strong>执行事务预提交</strong>：如果 Coordinator 接收到各参与者反馈都是Yes，那么执行事务预提交：</p>
<ol>
<li>发送预提交请求：Coordinator 向各参与者发送 preCommit 请求，并进入 prepared 阶段；</li>
<li>事务预提交：参与者接收到 preCommit 请求后，会执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中；</li>
<li>各参与者向 Coordinator 反馈事务执行的响应：如果各参与者都成功执行了事务操作，那么反馈给协调者 ACK 响应，同时等待最终指令，提交 commit 或者终止 abort，结束流程；</li>
</ol>
<p><strong>中断事务</strong>：如果任何一个参与者向 Coordinator 反馈了 No 响应，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务。</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者发送 abort 请求；</li>
<li>中断事务：无论是收到来自 Coordinator 的 abort 请求，还是等待超时，参与者都中断事务。</li>
</ol>
<h4 id="阶段三-doCommit"><a href="#阶段三-doCommit" class="headerlink" title="阶段三 doCommit"></a>阶段三 doCommit</h4><p><strong>执行提交</strong></p>
<ol>
<li>发送提交请求：假设 Coordinator 正常工作，接收到了所有参与者的 ack 响应，那么它将从预提交阶段进入提交状态，并向所有参与者发送 doCommit 请求；</li>
<li>事务提交：参与者收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源；</li>
<li>反馈事务提交结果：参与者完成事务提交后，向 Coordinator 发送 ACK 信息；</li>
<li>完成事务：Coordinator 接收到所有参与者 ack 信息，完成事务。</li>
</ol>
<p><strong>中断事务</strong>：假设 Coordinator 正常工作，并且有任一参与者反馈 No，或者在等待超时后无法接收所有参与者的反馈，都会中断事务</p>
<ol>
<li>发送中断请求：Coordinator 向所有参与者节点发送 abort 请求；</li>
<li>事务回滚：参与者接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资源；</li>
<li>反馈事务回滚结果：参与者在完成事务回滚之后，向 Coordinator 发送 ack 信息；</li>
<li>中断事务：Coordinator 接收到所有参与者反馈的 ack 信息后，中断事务。</li>
</ol>
<h3 id="3PC-分析"><a href="#3PC-分析" class="headerlink" title="3PC 分析"></a>3PC 分析</h3><p>3PC 虽然解决了 Coordinator 与参与者都异常情况下导致数据不一致的问题，3PC 依然带来其他问题：比如，网络分区问题，在 preCommit 消息发送后突然两个机房断开，这时候 Coordinator 所在机房会 abort, 另外剩余参与者的机房则会 commit。</p>
<p>而且由于3PC 的设计过于复杂，在解决2PC 问题的同时也引入了新的问题，所以在实际上应用不是很广泛。</p>
<p>参考：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external">维基百科：二阶段提交</a>；</li>
<li><a href="https://en.wikipedia.org/wiki/Three-phase_commit_protocol" target="_blank" rel="external">维基百科：三阶段提交</a>；</li>
<li><a href="http://www.infoq.com/cn/news/2018/05/distributed-system-architecture" target="_blank" rel="external">左耳朵耗子推荐：分布式系统架构经典资料</a>；</li>
<li><a href="http://www.hollischuang.com/archives/663" target="_blank" rel="external">关于分布式一致性的探究</a>；</li>
<li><a href="http://www.hollischuang.com/archives/681" target="_blank" rel="external">关于分布式事务、两阶段提交协议、三阶提交协议</a>；</li>
<li><a href="http://www.hollischuang.com/archives/1580" target="_blank" rel="external">深入理解分布式系统的2PC和3PC</a>；</li>
<li><a href="https://segmentfault.com/a/1190000004474543" target="_blank" rel="external">2PC到3PC到Paxos到Raft到ISR</a>；</li>
<li><a href="https://item.jd.com/11540991.html" target="_blank" rel="external">《大数据日知录：架构与算法》</a>；</li>
<li><a href="https://coolshell.cn/articles/10910.html" target="_blank" rel="external">分布式系统的事务处理</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在分布式系统领域，有一个理论，对于分布式系统的设计影响非常大，那就是 CAP 理论，即对于一个分布式系统而言，它是无法同时满足 Consistency(强一致性)、Availability(可用性) 和  Partition tolerance(分区容忍性) 这三个条件的，
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="分布式系统" scheme="http://matt33.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Java 守护线程</title>
    <link href="http://matt33.com/2018/07/07/java-daemon-thread/"/>
    <id>http://matt33.com/2018/07/07/java-daemon-thread/</id>
    <published>2018-07-07T13:43:21.000Z</published>
    <updated>2018-07-07T14:40:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 <code>setDaemon()</code> 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了<strong>守护线程(daemon thread)</strong>，那么什么是守护线程呢？或者说守护线程与非守护线程（普通线程）的区别在什么地方呢？这个就是本文主要讲述的内容。</p>
<h2 id="守护线程"><a href="#守护线程" class="headerlink" title="守护线程"></a>守护线程</h2><p>一般来说，Java 中的线程可以分为两种：守护线程和普通线程。在 JVM 刚启动时，它创建的所有线程，除了主线程（main thread）外，其他的线程都是守护线程（比如：垃圾收集器、以及其他执行辅助操作的线程）。</p>
<p>当创建一个新线程时，新线程将会继承它线程的守护状态，默认情况下，主线程创建的所有线程都是普通线程。</p>
<p>什么情况下会需要守护线程呢？一般情况下是，当我们希望创建一个线程来执行一些辅助的工作，但是又不希望这个线程阻碍 JVM 的关闭，在这种情况下，我们就需要使用守护线程了。</p>
<h2 id="守护线程的作用"><a href="#守护线程的作用" class="headerlink" title="守护线程的作用"></a>守护线程的作用</h2><p>守护线程与普通线程唯一的区别是：当线程退出时，JVM 会检查其他正在运行的线程，如果这些线程都是守护线程，那么 JVM 会正常退出操作，但是如果有普通线程还在运行，JVM 是不会执行退出操作的。当 JVM 退出时，所有仍然存在的守护线程都将被抛弃，既不会执行 finally 部分的代码，也不会执行 stack unwound 操作，JVM 会直接退出。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">When the JVM halts any remaining daemon threads are abandoned:</div><div class="line"></div><div class="line"> 1. finally blocks are not executed,</div><div class="line"> 2. stacks are not unwound - the JVM just exits.</div></pre></td></tr></table></figure>
<p>下面有个小示例，来自 <a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DaemonTest</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="keyword">new</span> WorkerThread().start();</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            Thread.sleep(<span class="number">7500</span>);</div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">            <span class="comment">// handle here exception</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(<span class="string">"Main Thread ending"</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WorkerThread</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="comment">// When false, (i.e. when it's a user thread), the Worker thread continues to run.</span></div><div class="line">        <span class="comment">// When true, (i.e. when it's a daemon thread), the Worker thread terminates when the main thread terminates.</span></div><div class="line">        setDaemon(<span class="keyword">false</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            System.out.println(<span class="string">"Hello from Worker "</span> + count++);</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">                sleep(<span class="number">5000</span>);</div><div class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                <span class="comment">// handle exception here</span></div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当为普通线程时，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Hello from Worker 0</div><div class="line">Hello from Worker 1</div><div class="line">Main Thread ending</div><div class="line">Hello from Worker 2</div><div class="line">Hello from Worker 3</div><div class="line">Hello from Worker 4</div><div class="line">Hello from Worker 5</div><div class="line">....</div></pre></td></tr></table></figure>
<p>也就是说，此时即使主线程执行完了，JVM 也会等待 WorkerThread 执行完毕才会退出，而如果将该线程设置守护线程的话，输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Hello from Worker 0</div><div class="line">Hello from Worker 1</div><div class="line">Main Thread ending</div></pre></td></tr></table></figure>
<p>在 main 线程执行完毕后，JVM 进程就退出了，不会 care WorkerThread 线程是否执行完毕。</p>
<p>参考：</p>
<ul>
<li><a href="https://stackoverflow.com/questions/2213340/what-is-a-daemon-thread-in-java" target="_blank" rel="external">What is a daemon thread in Java?</a>;</li>
<li><a href="http://www.javaconcurrencyinpractice.com/" target="_blank" rel="external">《Java 并发编程实战》</a>。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Java 并发编程实践或看涉及到 Java 并发相关的代码时，经常会遇到一些线程（比如做 metrics 统计的线程等）会通过 &lt;code&gt;setDaemon()&lt;/code&gt; 方法设置将该线程的 daemon 变量设置为 True，也就是将这个线程设置为了&lt;stron
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="java" scheme="http://matt33.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 1+N+M 网络处理模型（二十三）</title>
    <link href="http://matt33.com/2018/06/27/kafka-server-process-model/"/>
    <id>http://matt33.com/2018/06/27/kafka-server-process-model/</id>
    <published>2018-06-27T15:18:01.000Z</published>
    <updated>2018-06-27T15:43:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实现，如果还有印象，就会知道它们都是通过 KafkaApis 对象处理的，但是前面并没有详细讲述 Server 端是如何监听到相应的请求、请求是如何交给 KafkaApis 对象进行处理，以及处理后是如何返回给请求者（请求者可以是 client 也可以是 server），这些都属于 Server 的网络处理模型，也是本文讲述的主要内容。</p>
<h2 id="Server-网络模型整体流程"><a href="#Server-网络模型整体流程" class="headerlink" title="Server 网络模型整体流程"></a>Server 网络模型整体流程</h2><p>Kafka Server 启动后，会通过 KafkaServer 的 <code>startup()</code> 方法初始化涉及到网络模型的相关对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>()&#123;</div><div class="line">  <span class="comment">//note: socketServer</span></div><div class="line">  socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</div><div class="line">  socketServer.startup()</div><div class="line">  <span class="comment">//<span class="doctag">NOTE:</span> 初始化 KafkaApis 实例,每个 Server 只会启动一个线程</span></div><div class="line">  apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator,</div><div class="line">    kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</div><div class="line">    clusterId, time)</div><div class="line"></div><div class="line">  requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</div><div class="line">    config.numIoThreads)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Kafka Server 在启动时会初始化 SocketServer、KafkaApis 和 KafkaRequestHandlerPool 对象，这也是 Server 网络处理模型的主要组成部分。Kafka Server 的网络处理模型也是基于 Java NIO 机制实现的，实现模式与 Reactor 模式类似，其完整的处理流程图如下所示：</p>
<p><img src="/images/kafka/server-nio.png" alt="Kafka Server 1+N+M 网络处理模型"></p>
<p>上图如果现在不理解，并不要紧，这里先简单介绍一些，讲述一下整体的流程，本节下面会结合 Kafka 的代码详细来讲述图中的过程。上图的网络模型可以简要总结为以下三个重要组成部分：</p>
<ol>
<li>1 个 Acceptor 线程，负责监听 Socket 新的连接请求，注册了 <code>OP_ACCEPT</code> 事件，将新的连接按照 round robin 方式交给对应的 Processor 线程处理；</li>
<li>N 个 Processor 线程，其中每个 Processor 都有自己的 selector，它会向 Acceptor 分配的 SocketChannel 注册相应的 <code>OP_READ</code> 事件，N 的大小由 <code>num.networker.threads</code> 决定；</li>
<li>M 个 KafkaRequestHandler  线程处理请求，并将处理的结果返回给 Processor 线程对应的 response queue 中，由 Processor 将处理的结果返回给相应的请求发送者，M 的大小由 <code>num.io.threads</code> 来决定。</li>
</ol>
<p>上图展示的整体的处理流程如下所示：</p>
<ol>
<li>Acceptor 监听到来自请求者（请求者可以是来自 client，也可以来自 server）的新的连接，Acceptor 将这个请求者按照 round robin 的方式交给对对应的 Processor 进行处理；</li>
<li>Processor 注册这个 SocketChannel 的 <code>OP_READ</code> 的事件，如果有请求发送过来就可以被 Processor 的 Selector 选中；</li>
<li>Processor 将请求者发送的请求放入到一个 Request Queue 中，这是所有 Processor 共有的一个队列；</li>
<li>KafkaRequestHandler 从 Request Queue 中取出请求；</li>
<li>调用 KafkaApis 进行相应的处理；</li>
<li>处理的结果放入到该 Processor 对应的 Response Queue 中（每个 request 都标识它们来自哪个 Processor），Request Queue 的数量与 Processor 的数量保持一致；</li>
<li>Processor 从对应的 Response Queue 中取出 response；</li>
<li>Processor 将处理的结果返回给对应的请求者。</li>
</ol>
<p>上面是 Server 端网络处理的整体流程，下面我们开始详细讲述上面内容在 Kafka 中实现。</p>
<h2 id="SocketServer"><a href="#SocketServer" class="headerlink" title="SocketServer"></a>SocketServer</h2><p>SocketServer 是接收 Socket 连接、处理请求并返回处理结果的地方，Acceptor 及 Processor 的初始化、处理逻辑都是在这里实现的。在SocketServer 内有几个比较重要的变量，这里先来看下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SocketServer</span>(<span class="params">val config: <span class="type">KafkaConfig</span>, val metrics: <span class="type">Metrics</span>, val time: <span class="type">Time</span>, val credentialProvider: <span class="type">CredentialProvider</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> endpoints = config.listeners.map(l =&gt; l.listenerName -&gt; l).toMap <span class="comment">//note: broker 开放的端口数</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> numProcessorThreads = config.numNetworkThreads <span class="comment">//note: num.network.threads 默认为 3个，即 processor</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxQueuedRequests = config.queuedMaxRequests <span class="comment">//note:  queued.max.requests，request 队列中允许的最多请求数，默认是500</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> totalProcessorThreads = numProcessorThreads * endpoints.size <span class="comment">//note: 每个端口会对应 N 个 processor</span></div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIp = config.maxConnectionsPerIp <span class="comment">//note: 默认 2147483647</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxConnectionsPerIpOverrides = config.maxConnectionsPerIpOverrides</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Socket Server on Broker "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">  <span class="comment">//note: 请求队列</span></div><div class="line">  <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(totalProcessorThreads, maxQueuedRequests)</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> processors = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Processor</span>](totalProcessorThreads)</div><div class="line"></div><div class="line">  <span class="keyword">private</span>[network] <span class="keyword">val</span> acceptors = mutable.<span class="type">Map</span>[<span class="type">EndPoint</span>, <span class="type">Acceptor</span>]()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val numProcessors: <span class="type">Int</span>, val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">var</span> responseListeners: <span class="type">List</span>[(<span class="type">Int</span>) =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></div><div class="line">  <span class="comment">//note: 一个 requestQueue 队列,N 个 responseQueues 队列</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Request</span>](queueSize)</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> responseQueues = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]](numProcessors)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其中</p>
<ol>
<li><code>numProcessorThreads</code>：决定了 Processor 的个数，默认是3个，也就是 1+N+M 的 N 的数值；</li>
<li><code>maxQueuedRequests</code>：决定了 request queue 中最多允许放入多少个请求（等待处理的请求），默认是 500；</li>
<li>在 <code>RequestChannel</code> 中初始化了一个 requestQueue 和 N 个 responseQueue。</li>
</ol>
<h3 id="SocketServer-初始化"><a href="#SocketServer-初始化" class="headerlink" title="SocketServer 初始化"></a>SocketServer 初始化</h3><p>在 SocketServer 初始化方法 <code>startup()</code> 中，会初始化 1 个 Acceptor 和 N 个 Processor 线程（每个 EndPoint 都会初始化这么多，一般来说一个 Server 只会设置一个端口），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">this</span>.synchronized &#123;</div><div class="line">    <span class="comment">//note: 一台 broker 一般只设置一个端口，当然这里也可以设置两个</span></div><div class="line">    config.listeners.foreach &#123; endpoint =&gt;</div><div class="line">      <span class="keyword">val</span> listenerName = endpoint.listenerName</div><div class="line">      <span class="keyword">val</span> securityProtocol = endpoint.securityProtocol</div><div class="line">      <span class="keyword">val</span> processorEndIndex = processorBeginIndex + numProcessorThreads</div><div class="line"></div><div class="line">      <span class="comment">//note: N 个 processor</span></div><div class="line">      <span class="keyword">for</span> (i &lt;- processorBeginIndex until processorEndIndex)</div><div class="line">        processors(i) = newProcessor(i, connectionQuotas, listenerName, securityProtocol)</div><div class="line"></div><div class="line">      <span class="comment">//note: 1个 Acceptor</span></div><div class="line">      <span class="keyword">val</span> acceptor = <span class="keyword">new</span> <span class="type">Acceptor</span>(endpoint, sendBufferSize, recvBufferSize, brokerId,</div><div class="line">        processors.slice(processorBeginIndex, processorEndIndex), connectionQuotas)</div><div class="line">      acceptors.put(endpoint, acceptor)</div><div class="line">      <span class="type">Utils</span>.newThread(<span class="string">s"kafka-socket-acceptor-<span class="subst">$listenerName</span>-<span class="subst">$securityProtocol</span>-<span class="subst">$&#123;endpoint.port&#125;</span>"</span>, acceptor, <span class="literal">false</span>).start()</div><div class="line">      acceptor.awaitStartup()</div><div class="line"></div><div class="line">      processorBeginIndex = processorEndIndex</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Acceptor-处理"><a href="#Acceptor-处理" class="headerlink" title="Acceptor 处理"></a>Acceptor 处理</h3><p>SocketServer 在初始化后 Acceptor 线程后，Acceptor 启动，会首先注册 <code>OP_ACCEPT</code> 事件，监听是否有新的连接，如果来了新的连接就将该 SocketChannel 交给对应的 Processor 进行处理，Processor 是通过 round robin 方法选择的，这样可以保证 Processor 的负载相差无几（至少可以保证监听的 SocketChannel 差不多），实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  serverChannel.register(nioSelector, <span class="type">SelectionKey</span>.<span class="type">OP_ACCEPT</span>)<span class="comment">//note: 注册 accept 事件</span></div><div class="line">  startupComplete()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">var</span> currentProcessor = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> (isRunning) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">val</span> ready = nioSelector.select(<span class="number">500</span>)</div><div class="line">        <span class="keyword">if</span> (ready &gt; <span class="number">0</span>) &#123;</div><div class="line">          <span class="keyword">val</span> keys = nioSelector.selectedKeys()</div><div class="line">          <span class="keyword">val</span> iter = keys.iterator()</div><div class="line">          <span class="keyword">while</span> (iter.hasNext &amp;&amp; isRunning) &#123;</div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line">              <span class="keyword">val</span> key = iter.next</div><div class="line">              iter.remove()</div><div class="line">              <span class="keyword">if</span> (key.isAcceptable)</div><div class="line">                accept(key, processors(currentProcessor))<span class="comment">//note: 拿到一个socket 连接，轮询选择一个processor进行处理</span></div><div class="line">              <span class="keyword">else</span></div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Unrecognized key state for acceptor thread."</span>)</div><div class="line"></div><div class="line">              <span class="comment">//note: 轮询算法,使用 round robin</span></div><div class="line">              <span class="comment">// round robin to the next processor thread</span></div><div class="line">              currentProcessor = (currentProcessor + <span class="number">1</span>) % processors.length</div><div class="line">            &#125; <span class="keyword">catch</span> &#123;</div><div class="line">              <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while accepting connection"</span>, e)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="comment">// We catch all the throwables to prevent the acceptor thread from exiting on exceptions due</span></div><div class="line">        <span class="comment">// to a select operation on a specific channel or a bad request. We don't want</span></div><div class="line">        <span class="comment">// the broker to stop responding to requests from other clients in these scenarios.</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error occurred"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> &#123;</div><div class="line">    debug(<span class="string">"Closing server socket and selector."</span>)</div><div class="line">    swallowError(serverChannel.close())</div><div class="line">    swallowError(nioSelector.close())</div><div class="line">    shutdownComplete()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Acceptor 通过 <code>accept()</code> 将该新连接交给对应的 Processor，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理一个新的连接</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</div><div class="line">  <span class="comment">//note: accept 事件发生时，获取注册到 selector 上的 ServerSocketChannel</span></div><div class="line">  <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]</div><div class="line">  <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    connectionQuotas.inc(socketChannel.socket().getInetAddress)</div><div class="line">    socketChannel.configureBlocking(<span class="literal">false</span>)</div><div class="line">    socketChannel.socket().setTcpNoDelay(<span class="literal">true</span>)</div><div class="line">    socketChannel.socket().setKeepAlive(<span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (sendBufferSize != <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>)</div><div class="line">      socketChannel.socket().setSendBufferSize(sendBufferSize)</div><div class="line"></div><div class="line">    debug(<span class="string">"Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]"</span></div><div class="line">          .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id,</div><div class="line">                socketChannel.socket.getSendBufferSize, sendBufferSize,</div><div class="line">                socketChannel.socket.getReceiveBufferSize, recvBufferSize))</div><div class="line"></div><div class="line">    <span class="comment">//note: 轮询选择不同的 processor 进行处理</span></div><div class="line">    processor.accept(socketChannel)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">TooManyConnectionsException</span> =&gt;</div><div class="line">      info(<span class="string">"Rejected connection from %s, address already has the configured maximum of %d connections."</span>.format(e.ip, e.count))</div><div class="line">      close(socketChannel)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Processor-处理"><a href="#Processor-处理" class="headerlink" title="Processor 处理"></a>Processor 处理</h3><p>在前面，Acceptor 通过 <code>accept()</code> 将新的连接交给 Processor，Processor 实际上是将该 SocketChannel 添加到该 Processor 的 <code>newConnections</code> 队列中，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</div><div class="line">  newConnections.add(socketChannel)<span class="comment">//note: 添加到队列中</span></div><div class="line">  wakeup()<span class="comment">//note: 唤醒 Processor 的 selector（如果此时在阻塞的话）</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里详细看下 Processor 线程做了什么事情，其 <code>run()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  startupComplete()</div><div class="line">  <span class="keyword">while</span> (isRunning) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">// setup any new connections that have been queued up</span></div><div class="line">      configureNewConnections()<span class="comment">//note: 对新的 socket 连接,并注册 READ 事件</span></div><div class="line">      <span class="comment">// register any new responses for writing</span></div><div class="line">      processNewResponses()<span class="comment">//note: 处理 response 队列中 response</span></div><div class="line">      poll() <span class="comment">//note: 监听所有的 socket channel，是否有新的请求发送过来</span></div><div class="line">      processCompletedReceives() <span class="comment">//note: 处理接收到请求，将其放入到 request queue 中</span></div><div class="line">      processCompletedSends() <span class="comment">//note: 处理已经完成的发送</span></div><div class="line">      processDisconnected() <span class="comment">//note: 处理断开的连接</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// We catch all the throwables here to prevent the processor thread from exiting. We do this because</span></div><div class="line">      <span class="comment">// letting a processor exit might cause a bigger impact on the broker. Usually the exceptions thrown would</span></div><div class="line">      <span class="comment">// be either associated with a specific socket channel or a bad request. We just ignore the bad socket channel</span></div><div class="line">      <span class="comment">// or request. This behavior might need to be reviewed if we see an exception that need the entire broker to stop.</span></div><div class="line">      <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Processor got uncaught exception."</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  debug(<span class="string">"Closing selector - processor "</span> + id)</div><div class="line">  swallowError(closeAll())</div><div class="line">  shutdownComplete()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Processor 在一次循环中，主要做的事情如下：</p>
<ol>
<li><code>configureNewConnections()</code>：对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，这里主要是 Processor 的 selector 注册该连接的 <code>OP_READ</code> 事件；</li>
<li><code>processNewResponses()</code>：从该 Processor 对应的 response queue 中取出一个 response，进行发送；</li>
<li><code>poll()</code>：调用 selector 的 <code>poll()</code> 方法，遍历注册的 SocketChannel，查看是否有事件准备就绪；</li>
<li><code>processCompletedReceives()</code>：将接收到请求添加到的 request queue 中；</li>
<li><code>processCompletedSends()</code>：处理已经完成的响应发送；</li>
<li><code>processDisconnected()</code>：处理断开的 SocketChannel。</li>
</ol>
<p>上面就是 Processor 线程处理的主要逻辑，先是向新的 SocketChannel 注册相应的事件，监控是否有请求发送过来，接着从 response queue 中取出处理完成的请求发送给对应的请求者，然后调用一下 selector 的 <code>poll()</code>，遍历一下注册的所有 SocketChannel，判断是否有事件就绪，然后做相应的处理。这里需要注意的是，request queue 是所有 Processor 公用的一个队列，而 response queue 则是与 Processor 一一对应的，因为每个 Processor 监听的 SocketChannel 并不是同一批的，如果公有一个 response queue，那么这个 N 个 Processor 的 selector 要去监听所有的 SocketChannel，而不是现在这种，只需要去关注分配给自己的 SocketChannel。</p>
<p>下面分别看下上面的这些方法的具体实现。</p>
<h4 id="configureNewConnections"><a href="#configureNewConnections" class="headerlink" title="configureNewConnections"></a>configureNewConnections</h4><p><code>configureNewConnections()</code> 对新添加到 <code>newConnections</code> 队列中的 SocketChannel 进行处理，主要是 selector 注册相应的 <code>OP_READ</code> 事件，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果有新的连接过来，将该 Channel 的 OP_READ 事件注册到 selector 上</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">configureNewConnections</span></span>() &#123;</div><div class="line">  <span class="keyword">while</span> (!newConnections.isEmpty) &#123;</div><div class="line">    <span class="keyword">val</span> channel = newConnections.poll()</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      debug(<span class="string">s"Processor <span class="subst">$id</span> listening to new connection from <span class="subst">$&#123;channel.socket.getRemoteSocketAddress&#125;</span>"</span>)</div><div class="line">      <span class="keyword">val</span> localHost = channel.socket().getLocalAddress.getHostAddress</div><div class="line">      <span class="keyword">val</span> localPort = channel.socket().getLocalPort</div><div class="line">      <span class="keyword">val</span> remoteHost = channel.socket().getInetAddress.getHostAddress</div><div class="line">      <span class="keyword">val</span> remotePort = channel.socket().getPort</div><div class="line">      <span class="keyword">val</span> connectionId = <span class="type">ConnectionId</span>(localHost, localPort, remoteHost, remotePort).toString</div><div class="line">      selector.register(connectionId, channel)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// We explicitly catch all non fatal exceptions and close the socket to avoid a socket leak. The other</span></div><div class="line">      <span class="comment">// throwables will be caught in processor and logged as uncaught exceptions.</span></div><div class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</div><div class="line">        <span class="keyword">val</span> remoteAddress = channel.getRemoteAddress</div><div class="line">        <span class="comment">// need to close the channel here to avoid a socket leak.</span></div><div class="line">        close(channel)</div><div class="line">        error(<span class="string">s"Processor <span class="subst">$id</span> closed connection from <span class="subst">$remoteAddress</span>"</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processNewResponses"><a href="#processNewResponses" class="headerlink" title="processNewResponses"></a>processNewResponses</h4><p><code>processNewResponses()</code> 方法是从该 Processor 对应的 response queue 中取出一个 response，Processor 是通过 RequestChannel 的 <code>receiveResponse()</code> 从该 Processor 对应的 response queue 中取出 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 response</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">receiveResponse</span></span>(processor: <span class="type">Int</span>): <span class="type">RequestChannel</span>.<span class="type">Response</span> = &#123;</div><div class="line">  <span class="keyword">val</span> response = responseQueues(processor).poll()</div><div class="line">  <span class="keyword">if</span> (response != <span class="literal">null</span>)</div><div class="line">    response.request.responseDequeueTimeMs = <span class="type">Time</span>.<span class="type">SYSTEM</span>.milliseconds</div><div class="line">  response</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>取到相应的 response 之后，会判断该 response 的类型，进行相应的操作，如果需要返回，那么会调用 <code>sendResponse()</code> 发送该 response，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理一个新的 response 响应</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processNewResponses</span></span>() &#123;</div><div class="line">  <span class="keyword">var</span> curr = requestChannel.receiveResponse(id)</div><div class="line">  <span class="keyword">while</span> (curr != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      curr.responseAction <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">NoOpAction</span> =&gt; <span class="comment">//note: 如果这个请求不需要返回 response，再次注册该监听事件</span></div><div class="line">          <span class="comment">// There is no response to send to the client, we need to read more pipelined requests</span></div><div class="line">          <span class="comment">// that are sitting in the server's socket buffer</span></div><div class="line">          curr.request.updateRequestMetrics</div><div class="line">          trace(<span class="string">"Socket server received empty response to send, registering for read: "</span> + curr)</div><div class="line">          <span class="keyword">val</span> channelId = curr.request.connectionId</div><div class="line">          <span class="keyword">if</span> (selector.channel(channelId) != <span class="literal">null</span> || selector.closingChannel(channelId) != <span class="literal">null</span>)</div><div class="line">              selector.unmute(channelId)</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">SendAction</span> =&gt; <span class="comment">//note: 需要发送的 response，那么进行发送</span></div><div class="line">          sendResponse(curr)</div><div class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">CloseConnectionAction</span> =&gt; <span class="comment">//note: 要关闭的 response</span></div><div class="line">          curr.request.updateRequestMetrics</div><div class="line">          trace(<span class="string">"Closing socket connection actively according to the response code."</span>)</div><div class="line">          close(selector, curr.request.connectionId)</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      curr = requestChannel.receiveResponse(id)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/* `protected` for test usage */</span></div><div class="line"><span class="comment">//note: 发送的对应的 response</span></div><div class="line"><span class="keyword">protected</span>[network] <span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</div><div class="line">  trace(<span class="string">s"Socket server received response to send, registering for write and sending data: <span class="subst">$response</span>"</span>)</div><div class="line">  <span class="keyword">val</span> channel = selector.channel(response.responseSend.destination)</div><div class="line">  <span class="comment">// `channel` can be null if the selector closed the connection because it was idle for too long</span></div><div class="line">  <span class="keyword">if</span> (channel == <span class="literal">null</span>) &#123;</div><div class="line">    warn(<span class="string">s"Attempting to send response via channel for which there is no open connection, connection id <span class="subst">$id</span>"</span>)</div><div class="line">    response.request.updateRequestMetrics()</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    selector.send(response.responseSend) <span class="comment">//note: 发送该 response</span></div><div class="line">    inflightResponses += (response.request.connectionId -&gt; response) <span class="comment">//note: 添加到 inflinght 中</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processCompletedReceives"><a href="#processCompletedReceives" class="headerlink" title="processCompletedReceives"></a>processCompletedReceives</h4><p><code>processCompletedReceives()</code> 方法的主要作用是处理接收到请求，并将其放入到 request queue 中，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理接收到的所有请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedReceives</span></span>() &#123;</div><div class="line">  selector.completedReceives.asScala.foreach &#123; receive =&gt;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">val</span> openChannel = selector.channel(receive.source)</div><div class="line">      <span class="keyword">val</span> session = &#123;</div><div class="line">        <span class="comment">// Only methods that are safe to call on a disconnected channel should be invoked on 'channel'.</span></div><div class="line">        <span class="keyword">val</span> channel = <span class="keyword">if</span> (openChannel != <span class="literal">null</span>) openChannel <span class="keyword">else</span> selector.closingChannel(receive.source)</div><div class="line">        <span class="type">RequestChannel</span>.<span class="type">Session</span>(<span class="keyword">new</span> <span class="type">KafkaPrincipal</span>(<span class="type">KafkaPrincipal</span>.<span class="type">USER_TYPE</span>, channel.principal.getName), channel.socketAddress)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">val</span> req = <span class="type">RequestChannel</span>.<span class="type">Request</span>(processor = id, connectionId = receive.source, session = session,</div><div class="line">        buffer = receive.payload, startTimeMs = time.milliseconds, listenerName = listenerName,</div><div class="line">        securityProtocol = securityProtocol)</div><div class="line">      requestChannel.sendRequest(req) <span class="comment">//note: 添加到请求队列，如果队列满了，将会阻塞</span></div><div class="line">      selector.mute(receive.source) <span class="comment">//note: 移除该连接的 OP_READ 监听</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e @ (_: <span class="type">InvalidRequestException</span> | _: <span class="type">SchemaException</span>) =&gt;</div><div class="line">        <span class="comment">// note that even though we got an exception, we can assume that receive.source is valid. Issues with constructing a valid receive object were handled earlier</span></div><div class="line">        error(<span class="string">s"Closing socket for <span class="subst">$&#123;receive.source&#125;</span> because of error"</span>, e)</div><div class="line">        close(selector, receive.source)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processCompletedSends"><a href="#processCompletedSends" class="headerlink" title="processCompletedSends"></a>processCompletedSends</h4><p><code>processCompletedSends()</code> 方法是处理已经完成的发送，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedSends</span></span>() &#123;</div><div class="line">  selector.completedSends.asScala.foreach &#123; send =&gt;</div><div class="line">    <span class="comment">//note: response 发送完成，从正在发送的集合中移除</span></div><div class="line">    <span class="keyword">val</span> resp = inflightResponses.remove(send.destination).getOrElse &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Send for <span class="subst">$&#123;send.destination&#125;</span> completed, but not in `inflightResponses`"</span>)</div><div class="line">    &#125;</div><div class="line">    resp.request.updateRequestMetrics()</div><div class="line">    selector.unmute(send.destination) <span class="comment">//note: 完成这个请求之后再次监听 OP_READ 事件</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="KafkaRequestHandlerPool"><a href="#KafkaRequestHandlerPool" class="headerlink" title="KafkaRequestHandlerPool"></a>KafkaRequestHandlerPool</h2><p>上面主要是讲述 SocketServer 中 Acceptor 与 Processor 的处理内容，也就是 1+N+M 模型中 1+N 部分，下面开始讲述 M 部分，也就是 KafkaRequestHandler 的内容，其初始化实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></div><div class="line">                              val requestChannel: <span class="type">RequestChannel</span>,</div><div class="line">                              val apis: <span class="type">KafkaApis</span>,</div><div class="line">                              time: <span class="type">Time</span>,</div><div class="line">                              numThreads: <span class="type">Int</span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> &#123;</div><div class="line"></div><div class="line">  <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">"RequestHandlerAvgIdlePercent"</span>, <span class="string">"percent"</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Request Handler on Broker "</span> + brokerId + <span class="string">"], "</span></div><div class="line">  <span class="keyword">val</span> threads = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Thread</span>](numThreads)</div><div class="line">  <span class="keyword">val</span> runnables = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</div><div class="line">  <span class="comment">//note: 建立 M 个（numThreads）KafkaRequestHandler</span></div><div class="line">  <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numThreads) &#123;</div><div class="line">    <span class="comment">//note: requestChannel 是 Processor 存放 request 请求的地方,也是 Handler 处理完请求存放 response 的地方</span></div><div class="line">    runnables(i) = <span class="keyword">new</span> <span class="type">KafkaRequestHandler</span>(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis, time)</div><div class="line">    threads(i) = <span class="type">Utils</span>.daemonThread(<span class="string">"kafka-request-handler-"</span> + i, runnables(i))</div><div class="line">    threads(i).start()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shutdown</span></span>() &#123;</div><div class="line">    info(<span class="string">"shutting down"</span>)</div><div class="line">    <span class="keyword">for</span>(handler &lt;- runnables)</div><div class="line">      handler.shutdown</div><div class="line">    <span class="keyword">for</span>(thread &lt;- threads)</div><div class="line">      thread.join</div><div class="line">    info(<span class="string">"shut down completely"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如上面实现所示：</p>
<ol>
<li>KafkaRequestHandlerPool 会初始化 M 个 KafkaRequestHandler 线程，并启动该线程；</li>
<li>在初始化 KafkaRequestHandler 时，传入一个 requestChannel 变量，这个是 Processor 存放 request 的地方，KafkaRequestHandler 在处理请求时，会从这个 queue 中取出相应的 request。</li>
</ol>
<h3 id="KafkaRequestHandler"><a href="#KafkaRequestHandler" class="headerlink" title="KafkaRequestHandler"></a>KafkaRequestHandler</h3><p>KafkaRequestHandler 线程的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</div><div class="line">  <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">var</span> req : <span class="type">RequestChannel</span>.<span class="type">Request</span> = <span class="literal">null</span></div><div class="line">      <span class="keyword">while</span> (req == <span class="literal">null</span>) &#123;</div><div class="line">        <span class="comment">// We use a single meter for aggregate idle percentage for the thread pool.</span></div><div class="line">        <span class="comment">// Since meter is calculated as total_recorded_value / time_window and</span></div><div class="line">        <span class="comment">// time_window is independent of the number of threads, each recorded idle</span></div><div class="line">        <span class="comment">// time should be discounted by # threads.</span></div><div class="line">        <span class="keyword">val</span> startSelectTime = time.nanoseconds</div><div class="line">        req = requestChannel.receiveRequest(<span class="number">300</span>) <span class="comment">//note: 从 request queue 中拿去 request</span></div><div class="line">        <span class="keyword">val</span> idleTime = time.nanoseconds - startSelectTime</div><div class="line">        aggregateIdleMeter.mark(idleTime / totalHandlerThreads)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(req eq <span class="type">RequestChannel</span>.<span class="type">AllDone</span>) &#123;</div><div class="line">        debug(<span class="string">"Kafka request handler %d on broker %d received shut down command"</span>.format(</div><div class="line">          id, brokerId))</div><div class="line">        <span class="keyword">return</span></div><div class="line">      &#125;</div><div class="line">      req.requestDequeueTimeMs = time.milliseconds</div><div class="line">      trace(<span class="string">"Kafka request handler %d on broker %d handling request %s"</span>.format(id, brokerId, req))</div><div class="line">      apis.handle(req) <span class="comment">//note: 处理请求,并将处理的结果通过 sendResponse 放入 response queue 中</span></div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Exception when handling request"</span>, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法的实现逻辑：</p>
<ol>
<li>从 RequestChannel 取出相应的 request；</li>
<li>KafkaApis 处理这个 request，并通过 <code>requestChannel.sendResponse()</code> 将处理的结果放入 requestChannel 的 response queue 中，如下所示：</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 response 添加到对应的队列中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(response: <span class="type">RequestChannel</span>.<span class="type">Response</span>) &#123;</div><div class="line">  responseQueues(response.processor).put(response)</div><div class="line">  <span class="keyword">for</span>(onResponse &lt;- responseListeners)</div><div class="line">    onResponse(response.processor) <span class="comment">//note: 调用对应 processor 的 wakeup 方法</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里为止，一个请求从 Processor 接收，到 KafkaRequestHandler 通过 KafkaApis 处理并放回该 Processor 对应的 response queue 这整个过程就完成了（建议阅读本文的时候结合最前面的流程图一起看）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面7篇对 Kafka Controller 的内容做了相应的总结，Controller 这部分的总结算是暂时告一段落，本节会讲述 Kafka 源码分析系列中最后一节的内容，是关于 Server 端对不同类型请求处理的网络模型。在前面的文章中也讲述过几种不同类型的请求处理实
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 LeaderAndIsr 请求的处理（二十二）</title>
    <link href="http://matt33.com/2018/06/25/leaderAndIsr-process/"/>
    <id>http://matt33.com/2018/06/25/leaderAndIsr-process/</id>
    <published>2018-06-25T01:01:12.000Z</published>
    <updated>2018-06-25T04:11:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka Server 是如何处理 LeaderAndIsr 请求的。</p>
<h2 id="LeaderAndIsr-请求"><a href="#LeaderAndIsr-请求" class="headerlink" title="LeaderAndIsr 请求"></a>LeaderAndIsr 请求</h2><p>LeaderAndIsr 请求是在一个 Topic Partition 的 leader、isr、assignment replicas 变动时，Controller 向 Broker 发送的一种请求，有时候是向这个 Topic Partition 的所有副本发送，有时候是其中的某个副本，跟具体的触发情况有关系。在一个 LeaderAndIsr 请求中，会封装多个 Topic Partition 的信息，每个 Topic Partition 会对应一个 PartitionState 对象，这个对象主要成员变量如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionState</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> controllerEpoch;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leader;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> leaderEpoch;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> List&lt;Integer&gt; isr;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> zkVersion;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">final</span> Set&lt;Integer&gt; replicas;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>由此可见，在 LeaderAndIsr 请求中，会包含一个 Partition 的以下信息：</p>
<ol>
<li>当前 Controller 的 epoch（Broker 收到这个请求后，如果发现是过期的 Controller 请求，就会拒绝这个请求）；</li>
<li>leader，Partition 的 leader 信息；</li>
<li>leader epoch，Partition leader epoch 信息（leader、isr、AR 变动时，这个 epoch 都会加1）；</li>
<li>isr 列表；</li>
<li>zkVersion，；</li>
<li>AR，所有的 replica 列表。</li>
</ol>
<h3 id="LeaderAndIsr-请求处理"><a href="#LeaderAndIsr-请求处理" class="headerlink" title="LeaderAndIsr 请求处理"></a>LeaderAndIsr 请求处理</h3><h3 id="处理整体流程"><a href="#处理整体流程" class="headerlink" title="处理整体流程"></a>处理整体流程</h3><p>LeaderAndIsr 请求可谓是包含了一个 Partition 的所有 metadata 信息，Server 在接收到 Controller 发送的这个请求后，其处理的逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//KafkaApis</span></div><div class="line"><span class="comment">//note: LeaderAndIsr 请求的处理</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleLeaderAndIsrRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></div><div class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></div><div class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> leaderAndIsrRequest = request.body.asInstanceOf[<span class="type">LeaderAndIsrRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</div><div class="line">      <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></div><div class="line">      <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></div><div class="line">      <span class="comment">// leadership changes</span></div><div class="line">      <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></div><div class="line">      updatedLeaders.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">          coordinator.handleGroupImmigration(partition.partitionId)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></div><div class="line">      updatedFollowers.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">          coordinator.handleGroupEmigration(partition.partitionId)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> leaderAndIsrResponse =</div><div class="line">      <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;<span class="comment">//note: 有权限的情况下</span></div><div class="line">        <span class="comment">//note: replicaManager 进行相应的处理</span></div><div class="line">        <span class="keyword">val</span> result = replicaManager.becomeLeaderOrFollower(correlationId, leaderAndIsrRequest, metadataCache, onLeadershipChange)</div><div class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(result.errorCode, result.responseMap.mapValues(<span class="keyword">new</span> <span class="type">JShort</span>(_)).asJava)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> result = leaderAndIsrRequest.partitionStates.asScala.keys.map((_, <span class="keyword">new</span> <span class="type">JShort</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code))).toMap</div><div class="line">        <span class="keyword">new</span> <span class="type">LeaderAndIsrResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code, result.asJava)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, leaderAndIsrResponse))</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">"Disk error during leadership change."</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述处理逻辑分为以下两步：</p>
<ol>
<li>ReplicaManager 调用 <code>becomeLeaderOrFollower()</code> 方法对这个请求进行相应的处理；</li>
<li>如果请求中包含 <code>__consumer_offset</code> 的 Partition（对应两种情况：之前是 fllower 现在变成了 leader、之前是 leader 现在变成了 follower），那么还需要调用这个方法中定义的 <code>onLeadershipChange()</code> 方法进行相应的处理。</li>
</ol>
<p><code>becomeLeaderOrFollower()</code>  的整体处理流程如下：</p>
<p><img src="/images/kafka/leader-and-isr.png" alt="LeaderAndIsr 请求的处理"></p>
<h3 id="becomeLeaderOrFollower"><a href="#becomeLeaderOrFollower" class="headerlink" title="becomeLeaderOrFollower"></a>becomeLeaderOrFollower</h3><p>这里先看下 ReplicaManager 的 <code>becomeLeaderOrFollower()</code> 方法，它是 LeaderAndIsr 请求处理的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 LeaderAndIsr 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">becomeLeaderOrFollower</span></span>(correlationId: <span class="type">Int</span>,leaderAndISRRequest: <span class="type">LeaderAndIsrRequest</span>,</div><div class="line">                           metadataCache: <span class="type">MetadataCache</span>,</div><div class="line">                           onLeadershipChange: (<span class="type">Iterable</span>[<span class="type">Partition</span>], <span class="type">Iterable</span>[<span class="type">Partition</span>]) =&gt; <span class="type">Unit</span>): <span class="type">BecomeLeaderOrFollowerResult</span> = &#123;</div><div class="line">  leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</div><div class="line">    stateChangeLogger.trace(<span class="string">"Broker %d received LeaderAndIsr request %s correlation id %d from controller %d epoch %d for partition [%s,%d]"</span></div><div class="line">                              .format(localBrokerId, stateInfo, correlationId,</div><div class="line">                                      leaderAndISRRequest.controllerId, leaderAndISRRequest.controllerEpoch, topicPartition.topic, topicPartition.partition))</div><div class="line">  &#125;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> responseMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]</div><div class="line">    <span class="comment">//note: 1. 验证 controller 的 epoch，如果是来自旧的 controller，就拒绝这个请求</span></div><div class="line">    <span class="keyword">if</span> (leaderAndISRRequest.controllerEpoch &lt; controllerEpoch) &#123;</div><div class="line">      stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d since "</span> +</div><div class="line">        <span class="string">"its controller epoch %d is old. Latest known controller epoch is %d"</span>).format(localBrokerId, leaderAndISRRequest.controllerId,</div><div class="line">        correlationId, leaderAndISRRequest.controllerEpoch, controllerEpoch))</div><div class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 controller 的请求</span></div><div class="line">      <span class="keyword">val</span> controllerId = leaderAndISRRequest.controllerId</div><div class="line">      controllerEpoch = leaderAndISRRequest.controllerEpoch</div><div class="line"></div><div class="line">      <span class="comment">// First check partition's leader epoch</span></div><div class="line">      <span class="comment">//note: 2. 检查 leader epoch，得到一个 partitionState map，epoch 满足条件并且有副本在本地的集合</span></div><div class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>]()</div><div class="line">      leaderAndISRRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</div><div class="line">        <span class="keyword">val</span> partition = getOrCreatePartition(topicPartition) <span class="comment">//note: 对应的 tp 如果没有 Partition 实例的话,就新建一个</span></div><div class="line">        <span class="keyword">val</span> partitionLeaderEpoch = partition.getLeaderEpoch <span class="comment">//note: 更新 leader epoch</span></div><div class="line">        <span class="comment">// If the leader epoch is valid record the epoch of the controller that made the leadership decision.</span></div><div class="line">        <span class="comment">// This is useful while updating the isr to maintain the decision maker controller's epoch in the zookeeper path</span></div><div class="line">        <span class="keyword">if</span> (partitionLeaderEpoch &lt; stateInfo.leaderEpoch) &#123;</div><div class="line">          <span class="keyword">if</span>(stateInfo.replicas.contains(localBrokerId))</div><div class="line">            partitionState.put(partition, stateInfo)  <span class="comment">//note: 更新 replica 的 stateInfo</span></div><div class="line">          <span class="keyword">else</span> &#123;</div><div class="line">            stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</div><div class="line">              <span class="string">"epoch %d for partition [%s,%d] as itself is not in assigned replica list %s"</span>)</div><div class="line">              .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</div><div class="line">                topicPartition.topic, topicPartition.partition, stateInfo.replicas.asScala.mkString(<span class="string">","</span>)))</div><div class="line">            responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code)</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;  <span class="comment">//note: 忽略这个请求，因为请求的 leader epoch 小于缓存的 epoch</span></div><div class="line">          <span class="comment">// Otherwise record the error code in response</span></div><div class="line">          stateChangeLogger.warn((<span class="string">"Broker %d ignoring LeaderAndIsr request from controller %d with correlation id %d "</span> +</div><div class="line">            <span class="string">"epoch %d for partition [%s,%d] since its associated leader epoch %d is not higher than the current leader epoch %d"</span>)</div><div class="line">            .format(localBrokerId, controllerId, correlationId, leaderAndISRRequest.controllerEpoch,</div><div class="line">              topicPartition.topic, topicPartition.partition, stateInfo.leaderEpoch, partitionLeaderEpoch))</div><div class="line">          responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 3. 过滤出本地副本设置为 leader 的 Partition 列表</span></div><div class="line">      <span class="keyword">val</span> partitionsTobeLeader = partitionState.filter &#123; <span class="keyword">case</span> (_, stateInfo) =&gt;</div><div class="line">        stateInfo.leader == localBrokerId</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 4. 过滤出本地副本设置为 follower 的 Partition 列表</span></div><div class="line">      <span class="keyword">val</span> partitionsToBeFollower = partitionState -- partitionsTobeLeader.keys <span class="comment">//note: 这些 tp 设置为了 follower</span></div><div class="line"></div><div class="line">      <span class="comment">//note: 5. 将为 leader 的副本设置为 leader</span></div><div class="line">      <span class="keyword">val</span> partitionsBecomeLeader = <span class="keyword">if</span> (partitionsTobeLeader.nonEmpty)</div><div class="line">        makeLeaders(controllerId, controllerEpoch, partitionsTobeLeader, correlationId, responseMap)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</div><div class="line"></div><div class="line">      <span class="comment">//note: 6. 将为 follower 的副本设置为 follower</span></div><div class="line">      <span class="keyword">val</span> partitionsBecomeFollower = <span class="keyword">if</span> (partitionsToBeFollower.nonEmpty)</div><div class="line">        makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap, metadataCache)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">Set</span>.empty[<span class="type">Partition</span>]</div><div class="line"></div><div class="line">      <span class="comment">//note: 7. 如果 hw checkpoint 的线程没有初始化，这里需要进行一次初始化</span></div><div class="line">      <span class="comment">// we initialize highwatermark thread after the first leaderisrrequest. This ensures that all the partitions</span></div><div class="line">      <span class="comment">// have been completely populated before starting the checkpointing there by avoiding weird race conditions</span></div><div class="line">      <span class="keyword">if</span> (!hwThreadInitialized) &#123;</div><div class="line">        startHighWaterMarksCheckPointThread()</div><div class="line">        hwThreadInitialized = <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 8. 检查 replica fetcher 是否需要关闭（有些副本需要关闭因为可能从 follower 变为 leader）</span></div><div class="line">      replicaFetcherManager.shutdownIdleFetcherThreads()</div><div class="line"></div><div class="line">      <span class="comment">//note: 9. 检查是否 __consumer_offset 的 Partition 的 leaderAndIsr 信息，有的话进行相应的操作</span></div><div class="line">      onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)</div><div class="line">      <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述实现，其处理逻辑总结如下：</p>
<ol>
<li>检查 Controller 的 epoch，如果是来自旧的 Controller，那么就拒绝这个请求；</li>
<li>获取请求的 Partition 列表的 PartitionState 信息，在遍历的过程中，会进行一个检查，如果 leader epoch 小于缓存中的 epoch 值，那么就过滤掉这个 Partition 信息，如果这个 Partition 在本地不存在，那么会初始化这个 Partition 的对象（这时候并不会初始化本地副本）；</li>
<li>获取出本地副本为 leader 的 Partition 列表（partitionsTobeLeader）；</li>
<li>获取出本地副本为 follower 的 Partition 列表（partitionsToBeFollower）；</li>
<li>调用 <code>makeLeaders()</code> 方法将 leader 的副本设置为 leader；</li>
<li>调用 <code>makeFollowers()</code> 方法将 leader 的副本设置为 follower；</li>
<li>检查 HW checkpoint 的线程是否初始化，如果没有，这里需要进行一次初始化；</li>
<li>检查 ReplicaFetcherManager 是否有线程需要关闭（如果这个线程上没有分配要拉取的 Topic Partition，那么在这里这个线程就会被关闭，下次需要时会再次启动）；</li>
<li>检查是否有 <code>__consumer_offset</code> Partition 的 leaderAndIsr 信息，有的话进行相应的操作。</li>
</ol>
<p>这其中，比较复杂的部分是第 5、6、9步，也前面图中标出的 1、2、4步，文章下面接着分析这三部分。</p>
<h3 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h3><p>ReplicaManager 的 <code>makeLeaders()</code> 的作用是将指定的这批 Partition 列表设置为 Leader，并返回是新 leader 对应的 Partition 列表（之前不是 leader，现在选举为了 leader），其实实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 1. 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 2. 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>实现逻辑如下：</p>
<ol>
<li>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 方法移除这些 Partition 的副本同步线程；</li>
<li>遍历这些 Partition，通过 Partition 的 <code>makeLeader()</code> 方法将这个 Partition 设置为 Leader，如果设置成功（如果 leader 没有变化，证明这个 Partition 之前就是 leader，这个方法返回的是 false，这种情况下不会更新到缓存中），那么将 leader 信息更新到缓存中。</li>
</ol>
<p>下面来看下在 Partition 中是如何真正初始化一个 Partition 的 leader？其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将本地副本设置为 leader, 如果 leader 不变,向 ReplicaManager 返回 false</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeLeader</span></span>(controllerId: <span class="type">Int</span>, partitionStateInfo: <span class="type">PartitionState</span>, correlationId: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="keyword">val</span> (leaderHWIncremented, isNewLeader) = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="keyword">val</span> allReplicas = partitionStateInfo.replicas.asScala.map(_.toInt)</div><div class="line">    <span class="comment">// record the epoch of the controller that made the leadership decision. This is useful while updating the isr</span></div><div class="line">    <span class="comment">// to maintain the decision maker controller's epoch in the zookeeper path</span></div><div class="line">    controllerEpoch = partitionStateInfo.controllerEpoch</div><div class="line">    <span class="comment">// add replicas that are new</span></div><div class="line">    <span class="comment">//note: 为了新的 replica 创建副本实例</span></div><div class="line">    allReplicas.foreach(replica =&gt; getOrCreateReplica(replica))</div><div class="line">    <span class="comment">//note: 获取新的 isr 列表</span></div><div class="line">    <span class="keyword">val</span> newInSyncReplicas = partitionStateInfo.isr.asScala.map(r =&gt; getOrCreateReplica(r)).toSet</div><div class="line">    <span class="comment">// remove assigned replicas that have been removed by the controller</span></div><div class="line">    <span class="comment">//note: 将已经在不在 AR 中的副本移除</span></div><div class="line">    (assignedReplicas.map(_.brokerId) -- allReplicas).foreach(removeReplica)</div><div class="line">    inSyncReplicas = newInSyncReplicas</div><div class="line">    leaderEpoch = partitionStateInfo.leaderEpoch</div><div class="line">    zkVersion = partitionStateInfo.zkVersion</div><div class="line">    <span class="comment">//note: 判断是否是新的 leader</span></div><div class="line">    <span class="keyword">val</span> isNewLeader =</div><div class="line">      <span class="keyword">if</span> (leaderReplicaIdOpt.isDefined &amp;&amp; leaderReplicaIdOpt.get == localBrokerId) &#123;<span class="comment">//note: leader 没有更新</span></div><div class="line">        <span class="literal">false</span></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        leaderReplicaIdOpt = <span class="type">Some</span>(localBrokerId)</div><div class="line">        <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">    <span class="keyword">val</span> leaderReplica = getReplica().get <span class="comment">//note: 获取在当前上的副本,也就是 leader replica</span></div><div class="line">    <span class="keyword">val</span> curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset <span class="comment">//note: 获取 leader replica 的 the end offset</span></div><div class="line">    <span class="keyword">val</span> curTimeMs = time.milliseconds</div><div class="line">    <span class="comment">// initialize lastCaughtUpTime of replicas as well as their lastFetchTimeMs and lastFetchLeaderLogEndOffset.</span></div><div class="line">    (assignedReplicas - leaderReplica).foreach &#123; replica =&gt; <span class="comment">//note: 对于 isr 中的 replica,更新 LastCaughtUpTime</span></div><div class="line">      <span class="keyword">val</span> lastCaughtUpTimeMs = <span class="keyword">if</span> (inSyncReplicas.contains(replica)) curTimeMs <span class="keyword">else</span> <span class="number">0</span>L</div><div class="line">      replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line">    <span class="keyword">if</span> (isNewLeader) &#123;  <span class="comment">//note: 如果是新的 leader,那么需要</span></div><div class="line">      <span class="comment">// construct the high watermark metadata for the new leader replica</span></div><div class="line">      <span class="comment">//note: 为新的 leader 构造 replica 的 HW metadata</span></div><div class="line">      leaderReplica.convertHWToLocalOffsetMetadata()</div><div class="line">      <span class="comment">// reset log end offset for remote replicas</span></div><div class="line">      <span class="comment">//note: 更新远程副本的副本同步信息（设置为 unKnown）</span></div><div class="line">      assignedReplicas.filter(_.brokerId != localBrokerId).foreach(_.updateLogReadResult(<span class="type">LogReadResult</span>.<span class="type">UnknownLogReadResult</span>))</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 如果满足更新 isr 的条件,就更新 HW 信息</span></div><div class="line">    (maybeIncrementLeaderHW(leaderReplica), isNewLeader)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented) <span class="comment">//note: HW 更新的情况下</span></div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">  isNewLeader</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单总结一下上述的实现：</p>
<ol>
<li>首先更新这个 Partition 的相应信息，包括：isr、AR、leader epoch、zkVersion 等，并为每个副本创建一个 Replica 对象（如果不存在该对象的情况下才会创建，只有本地副本才会初始化相应的日志对象）；</li>
<li>如果这个 Partition 的 leader 本来就是本地副本，那么返回的结果设置为 false，证明这个 leader 并不是新的 leader；</li>
<li>对于 isr 中的所有 Replica，更新 LastCaughtUpTime 值，即最近一次赶得上 leader 的时间；</li>
<li>如果是新的 leader，那么为 leader 初始化相应的 HighWatermarkMetadata 对象，并将所有副本的副本同步信息更新为 UnknownLogReadResult；</li>
<li>检查一下是否需要更新 HW 值。</li>
</ol>
<p>如果这个本地副本是新选举的 leader，那么它所做的事情就是初始化 Leader 应该记录的相关信息。</p>
<h3 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h3><p>ReplicaManager 的 <code>makeFollowers()</code> 方法，是将哪些 Partition 设置为 Follower，返回的结果是那些新的 follower 对应的 Partition 列表（之前是 leader，现在变成了 follower），其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 1. 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的 Partition</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: 2. 将 Partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 3. 移除这些 Partition 的副本同步线程,这样在 MakeFollower 期间,这些 Partition 就不会进行副本同步了</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 4. Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 5. 完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 6. 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单总结一下上述的逻辑过程：</p>
<ol>
<li>首先遍历所有的 Partition，获到那些 leader 可用、并且 Partition 可以成功设置为 Follower 的 Partition 列表（partitionsToMakeFollower）；</li>
<li>在上面遍历的过程中，会调用 Partition 的 <code>makeFollower()</code> 方法将 Partition 设置为 Follower（在这里，如果该 Partition 的本地副本不存在，会初始化相应的日志对象，如果该 Partition 的 leader 已经存在，并且没有变化，那么就返回 false，只有 leader 变化的 Partition，才会返回 true，才会加入到 partitionsToMakeFollower 集合中，这是因为 leader 没有变化的 Partition 是不需要变更副本同步线程的）；</li>
<li>移除这些 Partition 的副本同步线程，这样在 MakeFollower 期间，这些 Partition 就不会进行副本同步了；</li>
<li>Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset，因为前面已经移除了这个 Partition 的副本同步线程，所以这里在 checkpoint 后可以保证所有缓存的数据都可以刷新到磁盘；</li>
<li>完成那些延迟请求的处理（Produce 和 FetchConsumer 请求）；</li>
<li>启动相应的副本同步线程。</li>
</ol>
<p>到这里 LeaderAndIsr 请求的大部分处理已经完成，但是有一个比较特殊的 topic（<code>__consumer_offset</code>），如果这 Partition 的 leader 发生变化，是需要一些额外的处理。</p>
<h2 id="consumer-offset-leader-切换处理"><a href="#consumer-offset-leader-切换处理" class="headerlink" title="__consumer_offset leader 切换处理"></a><code>__consumer_offset</code> leader 切换处理</h2><p><code>__consumer_offset</code> 这个 Topic 如果发生了 leader 切换，GroupCoordinator 需要进行相应的处理，其处理过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onLeadershipChange</span></span>(updatedLeaders: <span class="type">Iterable</span>[<span class="type">Partition</span>], updatedFollowers: <span class="type">Iterable</span>[<span class="type">Partition</span>]) &#123;</div><div class="line">  <span class="comment">// for each new leader or follower, call coordinator to handle consumer group migration.</span></div><div class="line">  <span class="comment">// this callback is invoked under the replica state change lock to ensure proper order of</span></div><div class="line">  <span class="comment">// leadership changes</span></div><div class="line">  <span class="comment">//note: __consumer_offset 是 leader 的情况，读取相应 group 的 offset 信息</span></div><div class="line">  updatedLeaders.foreach &#123; partition =&gt;</div><div class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">      coordinator.handleGroupImmigration(partition.partitionId)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: __consumer_offset 是 follower 的情况，如果之前是 leader，那么移除这个 partition 对应的信息</span></div><div class="line">  updatedFollowers.foreach &#123; partition =&gt;</div><div class="line">    <span class="keyword">if</span> (partition.topic == <span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>)</div><div class="line">      coordinator.handleGroupEmigration(partition.partitionId)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="成为-leader"><a href="#成为-leader" class="headerlink" title="成为 leader"></a>成为 leader</h3><p>如果当前节点这个 <code>__consumer_offset</code> 有 Partition 成为 leader，GroupCoordinator 通过 <code>handleGroupImmigration()</code> 方法进行相应的处理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 加载这个 Partition 对应的 group offset 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupImmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</div><div class="line">  groupManager.loadGroupsForPartition(offsetTopicPartitionId, onGroupLoaded)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 异步地加载这个 offset Partition 的信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doLoadGroupsAndOffsets</span></span>() &#123;</div><div class="line">    info(<span class="string">s"Loading offsets and group metadata from <span class="subst">$topicPartition</span>"</span>)</div><div class="line"></div><div class="line">    <span class="comment">//note: 添加到  loadingPartitions 集合中</span></div><div class="line">    inLock(partitionLock) &#123;</div><div class="line">      <span class="keyword">if</span> (loadingPartitions.contains(offsetsPartition)) &#123;</div><div class="line">        info(<span class="string">s"Offset load from <span class="subst">$topicPartition</span> already in progress."</span>)</div><div class="line">        <span class="keyword">return</span></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        loadingPartitions.add(offsetsPartition)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 开始加载，加载成功的话，将该 Partition 从 loadingPartitions 集合中移除，添加到 ownedPartition 集合中</span></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      loadGroupsAndOffsets(topicPartition, onGroupLoaded)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt; error(<span class="string">s"Error loading offsets from <span class="subst">$topicPartition</span>"</span>, t)</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      inLock(partitionLock) &#123;</div><div class="line">        ownedPartitions.add(offsetsPartition)</div><div class="line">        loadingPartitions.remove(offsetsPartition)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  scheduler.schedule(topicPartition.toString, doLoadGroupsAndOffsets)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的做的事情是：</p>
<ol>
<li>将正在处理的 Partition 添加到 loadingPartitions 集合中，这个集合内都是当前正在加载的 Partition（特指 <code>__consumer_offset</code> Topic）；</li>
<li>通过 <code>loadGroupsAndOffsets()</code> 加载这个 Partition 的数据，处理完成后，该 Partition 从 loadingPartitions 中清除，并添加到 ownedPartitions 集合中。</li>
</ol>
<p><code>loadGroupsAndOffsets()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 读取该 group offset Partition 数据</span></div><div class="line"><span class="keyword">private</span>[coordinator] <span class="function"><span class="keyword">def</span> <span class="title">loadGroupsAndOffsets</span></span>(topicPartition: <span class="type">TopicPartition</span>, onGroupLoaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="comment">//note: 这个必然有本地副本，现获取 hw（如果本地是 leader 的情况，否则返回-1）</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">highWaterMark</span> </span>= replicaManager.getHighWatermark(topicPartition).getOrElse(<span class="number">-1</span>L)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds()</div><div class="line">  replicaManager.getLog(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      warn(<span class="string">s"Attempted to load offsets and group metadata from <span class="subst">$topicPartition</span>, but found no log"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</div><div class="line">      <span class="keyword">var</span> currOffset = log.logStartOffset <span class="comment">//note: 这副本最起始的 offset</span></div><div class="line">      <span class="keyword">val</span> buffer = <span class="type">ByteBuffer</span>.allocate(config.loadBufferSize) <span class="comment">//note: 默认5MB</span></div><div class="line">      <span class="comment">// loop breaks if leader changes at any time during the load, since getHighWatermark is -1</span></div><div class="line">      <span class="comment">//note: group 与 offset 的对应关系</span></div><div class="line">      <span class="keyword">val</span> loadedOffsets = mutable.<span class="type">Map</span>[<span class="type">GroupTopicPartition</span>, <span class="type">OffsetAndMetadata</span>]()</div><div class="line">      <span class="keyword">val</span> removedOffsets = mutable.<span class="type">Set</span>[<span class="type">GroupTopicPartition</span>]()</div><div class="line">      <span class="comment">//note: Group 对应的 meta 信息</span></div><div class="line">      <span class="keyword">val</span> loadedGroups = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">GroupMetadata</span>]()</div><div class="line">      <span class="keyword">val</span> removedGroups = mutable.<span class="type">Set</span>[<span class="type">String</span>]()</div><div class="line"></div><div class="line">      <span class="keyword">while</span> (currOffset &lt; highWaterMark &amp;&amp; !shuttingDown.get()) &#123; <span class="comment">//note: 直到读取到 hw 位置，或服务关闭</span></div><div class="line">        buffer.clear()</div><div class="line">        <span class="keyword">val</span> fileRecords = log.read(currOffset, config.loadBufferSize, maxOffset = <span class="type">None</span>, minOneMessage = <span class="literal">true</span>)</div><div class="line">          .records.asInstanceOf[<span class="type">FileRecords</span>]</div><div class="line">        <span class="keyword">val</span> bufferRead = fileRecords.readInto(buffer, <span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="type">MemoryRecords</span>.readableRecords(bufferRead).deepEntries.asScala.foreach &#123; entry =&gt;</div><div class="line">          <span class="keyword">val</span> record = entry.record</div><div class="line">          require(record.hasKey, <span class="string">"Group metadata/offset entry key should not be null"</span>)</div><div class="line"></div><div class="line">          <span class="type">GroupMetadataManager</span>.readMessageKey(record.key) <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> offsetKey: <span class="type">OffsetKey</span> =&gt; <span class="comment">//note: GroupTopicPartition，有 group 和 topic-partition</span></div><div class="line">              <span class="comment">// load offset</span></div><div class="line">              <span class="comment">//note: 加载 offset 信息</span></div><div class="line">              <span class="keyword">val</span> key = offsetKey.key</div><div class="line">              <span class="keyword">if</span> (record.hasNullValue) &#123; <span class="comment">//note: value 为空</span></div><div class="line">                loadedOffsets.remove(key)</div><div class="line">                removedOffsets.add(key)</div><div class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 有 commit offset 信息</span></div><div class="line">                <span class="keyword">val</span> value = <span class="type">GroupMetadataManager</span>.readOffsetMessageValue(record.value)</div><div class="line">                loadedOffsets.put(key, value)</div><div class="line">                removedOffsets.remove(key)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> groupMetadataKey: <span class="type">GroupMetadataKey</span> =&gt;</div><div class="line">              <span class="comment">// load group metadata</span></div><div class="line">              <span class="comment">//note: 加载 group metadata 信息</span></div><div class="line">              <span class="keyword">val</span> groupId = groupMetadataKey.key</div><div class="line">              <span class="keyword">val</span> groupMetadata = <span class="type">GroupMetadataManager</span>.readGroupMessageValue(groupId, record.value)</div><div class="line">              <span class="keyword">if</span> (groupMetadata != <span class="literal">null</span>) &#123;</div><div class="line">                trace(<span class="string">s"Loaded group metadata for group <span class="subst">$groupId</span> with generation <span class="subst">$&#123;groupMetadata.generationId&#125;</span>"</span>)</div><div class="line">                removedGroups.remove(groupId)</div><div class="line">                loadedGroups.put(groupId, groupMetadata)</div><div class="line">              &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 更新最新的信息</span></div><div class="line">                loadedGroups.remove(groupId)</div><div class="line">                removedGroups.add(groupId)</div><div class="line">              &#125;</div><div class="line"></div><div class="line">            <span class="keyword">case</span> unknownKey =&gt;</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected message key <span class="subst">$unknownKey</span> while loading offsets and group metadata"</span>)</div><div class="line">          &#125;</div><div class="line"></div><div class="line">          currOffset = entry.nextOffset</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">val</span> (groupOffsets, emptyGroupOffsets) = loadedOffsets</div><div class="line">        .groupBy(_._1.group)</div><div class="line">        .mapValues(_.map &#123; <span class="keyword">case</span> (groupTopicPartition, offset) =&gt; (groupTopicPartition.topicPartition, offset)&#125; )</div><div class="line">        .partition &#123; <span class="keyword">case</span> (group, _) =&gt; loadedGroups.contains(group) &#125; <span class="comment">//note: 把集合根据条件分两个部分</span></div><div class="line"></div><div class="line">      loadedGroups.values.foreach &#123; group =&gt;</div><div class="line">        <span class="keyword">val</span> offsets = groupOffsets.getOrElse(group.groupId, <span class="type">Map</span>.empty[<span class="type">TopicPartition</span>, <span class="type">OffsetAndMetadata</span>])</div><div class="line">        loadGroup(group, offsets) <span class="comment">//note: 在缓存中添加 group 和初始化 offset 信息</span></div><div class="line">        onGroupLoaded(group) <span class="comment">//note: 设置 group 下一次心跳超时时间</span></div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// load groups which store offsets in kafka, but which have no active members and thus no group</span></div><div class="line">      <span class="comment">// metadata stored in the log</span></div><div class="line">      <span class="comment">//note: 加载哪些有 offset 信息但是当前没有活跃的 member 信息的 group</span></div><div class="line">      emptyGroupOffsets.foreach &#123; <span class="keyword">case</span> (groupId, offsets) =&gt;</div><div class="line">        <span class="keyword">val</span> group = <span class="keyword">new</span> <span class="type">GroupMetadata</span>(groupId)</div><div class="line">        loadGroup(group, offsets)</div><div class="line">        onGroupLoaded(group)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      removedGroups.foreach &#123; groupId =&gt;</div><div class="line">        <span class="comment">// if the cache already contains a group which should be removed, raise an error. Note that it</span></div><div class="line">        <span class="comment">// is possible (however unlikely) for a consumer group to be removed, and then to be used only for</span></div><div class="line">        <span class="comment">// offset storage (i.e. by "simple" consumers)</span></div><div class="line">        <span class="keyword">if</span> (groupMetadataCache.contains(groupId) &amp;&amp; !emptyGroupOffsets.contains(groupId))</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"Unexpected unload of active group <span class="subst">$groupId</span> while "</span> +</div><div class="line">            <span class="string">s"loading partition <span class="subst">$topicPartition</span>"</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (!shuttingDown.get())</div><div class="line">        info(<span class="string">"Finished loading offsets from %s in %d milliseconds."</span></div><div class="line">          .format(topicPartition, time.milliseconds() - startMs))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面方法的实现虽然比较长，但是处理逻辑还是比较简单的，实现结果如下：</p>
<ol>
<li>获取这个 Partition 的 HW 值（如果 leader 不在本地，那么返回-1）；</li>
<li>初始化 loadedOffsets 和 removedOffsets、loadedGroups 和 removedGroups 集合，它们就是 group offset 信息以及 consumer member 信息；</li>
<li>从这个 Partition 第一条数据开始读取，直到读取到 HW 位置，加载相应的 commit offset、consumer member 信息，因为是顺序读取的，所以会新的值会覆盖前面的值；</li>
<li>通过 <code>loadGroup()</code> 加载到 GroupCoordinator 的缓存中。</li>
</ol>
<p>经过上面这些步骤，这个 Partition 的数据就被完整加载缓存中了。</p>
<h3 id="变成-follower"><a href="#变成-follower" class="headerlink" title="变成 follower"></a>变成 follower</h3><p>如果 <code>__consumer_offset</code> 有 Partition 变成了 follower（之前是 leader，如果之前不是 leader，不会走到这一步的），GroupCoordinator 通过 <code>handleGroupEmigration()</code> 移除这个 Partition 相应的缓存信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 移除这个 Partition 对应的 group offset 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleGroupEmigration</span></span>(offsetTopicPartitionId: <span class="type">Int</span>) &#123;</div><div class="line">  groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>removeGroupsForPartition()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当一个 broker 变成一个 follower 时，清空这个 partition 的相关缓存信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeGroupsForPartition</span></span>(offsetsPartition: <span class="type">Int</span>,</div><div class="line">                             onGroupUnloaded: <span class="type">GroupMetadata</span> =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(<span class="type">Topic</span>.<span class="type">GroupMetadataTopicName</span>, offsetsPartition)</div><div class="line">  scheduler.schedule(topicPartition.toString, removeGroupsAndOffsets)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">removeGroupsAndOffsets</span></span>() &#123;</div><div class="line">    <span class="keyword">var</span> numOffsetsRemoved = <span class="number">0</span></div><div class="line">    <span class="keyword">var</span> numGroupsRemoved = <span class="number">0</span></div><div class="line"></div><div class="line">    inLock(partitionLock) &#123;</div><div class="line">      <span class="comment">// we need to guard the group removal in cache in the loading partition lock</span></div><div class="line">      <span class="comment">// to prevent coordinator's check-and-get-group race condition</span></div><div class="line">      ownedPartitions.remove(offsetsPartition)</div><div class="line"></div><div class="line">      <span class="keyword">for</span> (group &lt;- groupMetadataCache.values) &#123;</div><div class="line">        <span class="keyword">if</span> (partitionFor(group.groupId) == offsetsPartition) &#123;</div><div class="line">          onGroupUnloaded(group) <span class="comment">//note: 将 group 状态转移成 dead</span></div><div class="line">          groupMetadataCache.remove(group.groupId, group) <span class="comment">//note: 清空 group 的信息</span></div><div class="line">          numGroupsRemoved += <span class="number">1</span></div><div class="line">          numOffsetsRemoved += group.numOffsets</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (numOffsetsRemoved &gt; <span class="number">0</span>)</div><div class="line">      info(<span class="string">s"Removed <span class="subst">$numOffsetsRemoved</span> cached offsets for <span class="subst">$topicPartition</span> on follower transition."</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (numGroupsRemoved &gt; <span class="number">0</span>)</div><div class="line">      info(<span class="string">s"Removed <span class="subst">$numGroupsRemoved</span> cached groups for <span class="subst">$topicPartition</span> on follower transition."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onGroupUnloaded</span></span>(group: <span class="type">GroupMetadata</span>) &#123;</div><div class="line">  group synchronized &#123;</div><div class="line">    info(<span class="string">s"Unloading group metadata for <span class="subst">$&#123;group.groupId&#125;</span> with generation <span class="subst">$&#123;group.generationId&#125;</span>"</span>)</div><div class="line">    <span class="keyword">val</span> previousState = group.currentState</div><div class="line">    group.transitionTo(<span class="type">Dead</span>) <span class="comment">//note: 状态转移成 dead</span></div><div class="line"></div><div class="line">    previousState <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Empty</span> | <span class="type">Dead</span> =&gt;</div><div class="line">      <span class="keyword">case</span> <span class="type">PreparingRebalance</span> =&gt;</div><div class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息返回异常</span></div><div class="line">          <span class="keyword">if</span> (member.awaitingJoinCallback != <span class="literal">null</span>) &#123;</div><div class="line">            member.awaitingJoinCallback(joinError(member.memberId, <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code))</div><div class="line">            member.awaitingJoinCallback = <span class="literal">null</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        joinPurgatory.checkAndComplete(<span class="type">GroupKey</span>(group.groupId))</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">Stable</span> | <span class="type">AwaitingSync</span> =&gt;</div><div class="line">        <span class="keyword">for</span> (member &lt;- group.allMemberMetadata) &#123; <span class="comment">//note: 如果有 member 信息，返回异常</span></div><div class="line">          <span class="keyword">if</span> (member.awaitingSyncCallback != <span class="literal">null</span>) &#123;</div><div class="line">            member.awaitingSyncCallback(<span class="type">Array</span>.empty[<span class="type">Byte</span>], <span class="type">Errors</span>.<span class="type">NOT_COORDINATOR_FOR_GROUP</span>.code)</div><div class="line">            member.awaitingSyncCallback = <span class="literal">null</span></div><div class="line">          &#125;</div><div class="line">          heartbeatPurgatory.checkAndComplete(<span class="type">MemberKey</span>(member.groupId, member.memberId))</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于在这个 Partition 上的所有 Group，会按下面的步骤执行：</p>
<ol>
<li>通过 <code>onGroupUnloaded()</code> 方法先将这个 Group 的状态转换为 dead，如果 Group 处在 PreparingRebalance/Stable/AwaitingSync 状态，并且设置了相应的回调函数，那么就在回调函数中返回带有 NOT_COORDINATOR_FOR_GROUP 异常信息的响应，consumer 在收到这个异常信息会重新加入 group；</li>
<li>从缓存中移除这个 Group 的信息。</li>
</ol>
<p>这个遍历执行完成之后，这个 Topic Partition 就从 Leader 变成了 follower 状态。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇算是 Controller 部分的最后一篇，在前面讲述 ReplicaManager 时，留一个地方没有讲解，是关于 Broker 对 Controller 发送的 LeaderAndIsr 请求的处理，这个请求的处理实现会稍微复杂一些，本篇文章主要就是讲述 Kafka
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 发送模型（二十一）</title>
    <link href="http://matt33.com/2018/06/23/controller-request-model/"/>
    <id>http://matt33.com/2018/06/23/controller-request-model/</id>
    <published>2018-06-23T05:26:38.000Z</published>
    <updated>2018-06-23T05:42:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 部分的一个补充，在这篇文章中，将会看到 Controller 在处理 leader 切换、ShutDown 请求时如何向 Broker 发送相应的请求。</p>
<p>Kafka Controller 向 Broker 发送的请求类型主要分为三种：LeaderAndIsr、UpdateMetadata、StopReplica 请求，正如  <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager</a> 这里介绍的，Controller 会为每台 Broker 初始化为一个 ControllerBrokerStateInfo 对象，该对象主要包含以下四个内容：</p>
<ol>
<li>NetworkClient：与 Broker 的网络连接对象；</li>
<li>Node：Broker 的节点信息；</li>
<li>MessageQueue：每个 Broker 对应的请求队列，Controller 向 Broker 发送的请求会想放在这个队列里；</li>
<li>RequestSendThread：每台 Broker 对应的请求发送线程。</li>
</ol>
<h2 id="Controller-的请求发送模型"><a href="#Controller-的请求发送模型" class="headerlink" title="Controller 的请求发送模型"></a>Controller 的请求发送模型</h2><p>在讲述 Controller 发送模型之前，先看下 Controller 是如何向 Broker 发送请求的，这里以发送 metadata 更新请求为例，简略的代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建新的批量请求</span></div><div class="line">brokerRequestBatch.newBatch()</div><div class="line"><span class="comment">//note: 为目标 Broker 添加相应的请求</span></div><div class="line">brokerRequestBatch.addUpdateMetadataRequestForBrokers(brokers, partitions)</div><div class="line"><span class="comment">//note: 发送请求，实际上只是把请求添加发送线程的 request queue 中</span></div><div class="line">brokerRequestBatch.sendRequestsToBrokers(epoch)</div></pre></td></tr></table></figure>
<p>这里有一个比较重要的对象，就是 ControllerBrokerRequestBatch 对象，可以认为它是一个专门用于批量请求发送的对象，在这个对象中有几个重要成员变量：</p>
<ol>
<li>leaderAndIsrRequestMap：记录每个 broker 与要发送的 LeaderAndIsr 请求集合的 map；</li>
<li>stopReplicaRequestMap：记录每个 broker 与要发送的 StopReplica 集合的 map；</li>
<li>updateMetadataRequestBrokerSet：记录要发送的 update-metadata 请求的 broker 集合；</li>
<li>updateMetadataRequestPartitionInfoMap：记录 update-metadata 请求要更新的 Topic Partition 集合。</li>
</ol>
<p>Controller 可以通过下面这三方法向这些集合添加相应的请求：</p>
<ol>
<li><code>addLeaderAndIsrRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 LeaderAndIsr 请求；</li>
<li><code>addStopReplicaRequestForBrokers()</code>：向给定的 Broker 发送某个 Topic Partition 的 StopReplica 请求；</li>
<li><code>addUpdateMetadataRequestForBrokers()</code>：向给定的 Broker 发送某一批 Partitions 的 UpdateMetadata 请求。</li>
</ol>
<p>Controller 整体的请求模型概况如下图所示：</p>
<p><img src="/images/kafka/controller-request-model.png" alt="Controller 的请求发送模型"></p>
<p>上述三个方法将相应的请求添加到对应的集合中后，然后通过 <code>sendRequestsToBrokers()</code> 方法将该请求添加到该 Broker 对应的请求队列中，接着再由该 Broker 对应的 RequestSendThread 去发送相应的请求。</p>
<h2 id="ControllerBrokerRequestBatch"><a href="#ControllerBrokerRequestBatch" class="headerlink" title="ControllerBrokerRequestBatch"></a>ControllerBrokerRequestBatch</h2><p>这节详细讲述一下关于 ControllerBrokerRequestBatch 的一些方法实现。</p>
<h3 id="newBatch-方法"><a href="#newBatch-方法" class="headerlink" title="newBatch 方法"></a>newBatch 方法</h3><p>Controller 在添加请求前，都会先调用 <code>newBatch()</code> 方法，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建新的请求前,确保前一批请求全部发送完毕</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">newBatch</span></span>() &#123;</div><div class="line">  <span class="comment">// raise error if the previous batch is not empty</span></div><div class="line">  <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating "</span> +</div><div class="line">      <span class="string">"a new one. Some LeaderAndIsr state changes %s might be lost "</span>.format(leaderAndIsrRequestMap.toString()))</div><div class="line">  <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</div><div class="line">      <span class="string">"new one. Some StopReplica state changes %s might be lost "</span>.format(stopReplicaRequestMap.toString()))</div><div class="line">  <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Controller to broker state change requests batch is not empty while creating a "</span> +</div><div class="line">      <span class="string">"new one. Some UpdateMetadata state changes to brokers %s with partition info %s might be lost "</span>.format(</div><div class="line">        updateMetadataRequestBrokerSet.toString(), updateMetadataRequestPartitionInfoMap.toString()))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的主要作用是检查上一波的 LeaderAndIsr、UpdateMetadata、StopReplica 请求是否已经发送，正常情况下，Controller 在调用 <code>sendRequestsToBrokers()</code> 方法之后，这些集合中的请求都会被发送，发送之后，会将相应的请求集合清空，当然在异常情况可能会导致部分集合没有被清空，导致无法 <code>newBatch()</code>，这种情况下，通常策略是重启 controller，因为现在 Controller 的设计还是有些复杂，在某些情况下还是可能会导致异常发生，并且有些异常还是无法恢复的。</p>
<h3 id="添加-LeaderAndIsr-请求"><a href="#添加-LeaderAndIsr-请求" class="headerlink" title="添加 LeaderAndIsr 请求"></a>添加 LeaderAndIsr 请求</h3><p>Controller 可以通过 <code>addLeaderAndIsrRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 LeaderAndIsr 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 LeaderAndIsr 添加到对应的 broker 中,还未开始发送数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addLeaderAndIsrRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</div><div class="line">                                     leaderIsrAndControllerEpoch: <span class="type">LeaderIsrAndControllerEpoch</span>,</div><div class="line">                                     replicas: <span class="type">Seq</span>[<span class="type">Int</span>], callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition)</div><div class="line"></div><div class="line">  <span class="comment">//note: 将请求添加到对应的 broker 上</span></div><div class="line">  brokerIds.filter(_ &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</div><div class="line">    <span class="keyword">val</span> result = leaderAndIsrRequestMap.getOrElseUpdate(brokerId, mutable.<span class="type">Map</span>.empty)</div><div class="line">    result.put(topicPartition, <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas.toSet))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 在更新 LeaderAndIsr 信息时,主题的 metadata 相当于也进行了更新,需要发送这个 topic 的 metadata 给所有存活的 broker</span></div><div class="line">  addUpdateMetadataRequestForBrokers(controllerContext.liveOrShuttingDownBrokerIds.toSeq,</div><div class="line">                                     <span class="type">Set</span>(<span class="type">TopicAndPartition</span>(topic, partition)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的处理流程如下：</p>
<ol>
<li>向对应的 Broker 添加 LeaderAndIsr 请求，请求会被添加到 leaderAndIsrRequestMap 集合中；</li>
<li>并通过 <code>addUpdateMetadataRequestForBrokers()</code> 方法向所有的 Broker 添加这个 Topic-Partition 的 UpdateMatedata 请求，leader 或 isr 变动时，会向所有 broker 同步这个 Partition 的 metadata 信息，这样可以保证每台 Broker 上都有最新的 metadata 信息。</li>
</ol>
<h3 id="添加-UpdateMetadata-请求"><a href="#添加-UpdateMetadata-请求" class="headerlink" title="添加 UpdateMetadata 请求"></a>添加 UpdateMetadata 请求</h3><p>Controller 可以通过 <code>addUpdateMetadataRequestForBrokers()</code> 向指定 Broker 列表添加某批 Partitions 的 UpdateMetadata 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向给行的 Broker 发送 UpdateMetadataRequest 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addUpdateMetadataRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>],</div><div class="line">                                       partitions: collection.<span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = <span class="type">Set</span>.empty[<span class="type">TopicAndPartition</span>],</div><div class="line">                                       callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  <span class="comment">//note: 将 Topic-Partition 添加到对应的 map 中</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateMetadataRequestPartitionInfo</span></span>(partition: <span class="type">TopicAndPartition</span>, beingDeleted: <span class="type">Boolean</span>) &#123;</div><div class="line">    <span class="keyword">val</span> leaderIsrAndControllerEpochOpt = controllerContext.partitionLeadershipInfo.get(partition)</div><div class="line">    leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">        <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(partition).toSet</div><div class="line">        <span class="keyword">val</span> partitionStateInfo = <span class="keyword">if</span> (beingDeleted) &#123; <span class="comment">//note: 正在删除的 Partition,设置 leader 为-2</span></div><div class="line">          <span class="keyword">val</span> leaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(<span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>, leaderIsrAndControllerEpoch.leaderAndIsr.isr)</div><div class="line">          <span class="type">PartitionStateInfo</span>(<span class="type">LeaderIsrAndControllerEpoch</span>(leaderAndIsr, leaderIsrAndControllerEpoch.controllerEpoch), replicas)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="type">PartitionStateInfo</span>(leaderIsrAndControllerEpoch, replicas)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: 添加到对应的 request map 中</span></div><div class="line">        updateMetadataRequestPartitionInfoMap.put(<span class="keyword">new</span> <span class="type">TopicPartition</span>(partition.topic, partition.partition), partitionStateInfo)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        info(<span class="string">"Leader not yet assigned for partition %s. Skip sending UpdateMetadataRequest."</span>.format(partition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note:过滤出要发送的 partition</span></div><div class="line">  <span class="keyword">val</span> filteredPartitions = &#123;</div><div class="line">    <span class="keyword">val</span> givenPartitions = <span class="keyword">if</span> (partitions.isEmpty)</div><div class="line">      controllerContext.partitionLeadershipInfo.keySet <span class="comment">//note: Partitions 为空时，就过滤出所有的 topic</span></div><div class="line">    <span class="keyword">else</span></div><div class="line">      partitions</div><div class="line">    <span class="keyword">if</span> (controller.deleteTopicManager.partitionsToBeDeleted.isEmpty)</div><div class="line">      givenPartitions</div><div class="line">    <span class="keyword">else</span></div><div class="line">      givenPartitions -- controller.deleteTopicManager.partitionsToBeDeleted <span class="comment">//note: 将要删除的 topic 过滤掉</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  updateMetadataRequestBrokerSet ++= brokerIds.filter(_ &gt;= <span class="number">0</span>) <span class="comment">//note: 将 broker 列表更新到要发送的集合中</span></div><div class="line">  <span class="comment">//note: 对于要更新 metadata 的 Partition,设置 beingDeleted 为 False</span></div><div class="line">  filteredPartitions.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">false</span>))</div><div class="line">  <span class="comment">//note: 要删除的 Partition 设置 BeingDeleted 为 True</span></div><div class="line">  controller.deleteTopicManager.partitionsToBeDeleted.foreach(partition =&gt; updateMetadataRequestPartitionInfo(partition, beingDeleted = <span class="literal">true</span>))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的实现逻辑如下：</p>
<ol>
<li>首先过滤出要发送的 Partition 列表，如果没有指定要发送 partitions 列表，那么默认就是发送全局的 metadata 信息；</li>
<li>接着将已经标记为删除的 Partition 从上面的列表中移除；</li>
<li>将要发送的 Broker 列表添加到 updateMetadataRequestBrokerSet 集合中；</li>
<li>将前面过滤的 Partition 列表对应的 metadata 信息添加到对应的 updateMetadataRequestPartitionInfoMap 集合中;</li>
<li>将当前设置为删除的所有 Partition 的 metadata 信息也添加到 updateMetadataRequestPartitionInfoMap 集合中，添加前会把其 leader 设置为-2，这样 Broker 收到这个 Partition 的 metadata 信息之后就会知道这个 Partition 是设置删除标志。</li>
</ol>
<h3 id="添加-StopReplica-请求"><a href="#添加-StopReplica-请求" class="headerlink" title="添加 StopReplica 请求"></a>添加 StopReplica 请求</h3><p>Controller 可以通过 <code>addStopReplicaRequestForBrokers()</code> 向指定 Broker 列表添加某个 Topic Partition 的 StopReplica 请求，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 将 StopReplica 添加到对应的 Broker 中,还未开始发送数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addStopReplicaRequestForBrokers</span></span>(brokerIds: <span class="type">Seq</span>[<span class="type">Int</span>], topic: <span class="type">String</span>, partition: <span class="type">Int</span>, deletePartition: <span class="type">Boolean</span>,</div><div class="line">                                    callback: (<span class="type">AbstractResponse</span>, <span class="type">Int</span>) =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  brokerIds.filter(b =&gt; b &gt;= <span class="number">0</span>).foreach &#123; brokerId =&gt;</div><div class="line">    stopReplicaRequestMap.getOrElseUpdate(brokerId, <span class="type">Seq</span>.empty[<span class="type">StopReplicaRequestInfo</span>])</div><div class="line">    <span class="keyword">val</span> v = stopReplicaRequestMap(brokerId)</div><div class="line">    <span class="keyword">if</span>(callback != <span class="literal">null</span>)</div><div class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</div><div class="line">        deletePartition, (r: <span class="type">AbstractResponse</span>) =&gt; callback(r, brokerId))</div><div class="line">    <span class="keyword">else</span></div><div class="line">      stopReplicaRequestMap(brokerId) = v :+ <span class="type">StopReplicaRequestInfo</span>(<span class="type">PartitionAndReplica</span>(topic, partition, brokerId),</div><div class="line">        deletePartition)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的实现逻辑比较简单，直接将 StopReplica 添加到 stopReplicaRequestMap 中。</p>
<h3 id="向-Broker-发送请求"><a href="#向-Broker-发送请求" class="headerlink" title="向 Broker 发送请求"></a>向 Broker 发送请求</h3><p>Controller 在添加完相应的请求后，最后一步都会去调用 <code>sendRequestsToBrokers()</code> 方法构造相应的请求，并把请求添加到 Broker 对应的 RequestQueue 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送请求给 broker（只是将对应处理后放入到对应的 queue 中）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequestsToBrokers</span></span>(controllerEpoch: <span class="type">Int</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">//note: LeaderAndIsr 请求</span></div><div class="line">    leaderAndIsrRequestMap.foreach &#123; <span class="keyword">case</span> (broker, partitionStateInfos) =&gt;</div><div class="line">      partitionStateInfos.foreach &#123; <span class="keyword">case</span> (topicPartition, state) =&gt;</div><div class="line">        <span class="keyword">val</span> typeOfRequest = <span class="keyword">if</span> (broker == state.leaderIsrAndControllerEpoch.leaderAndIsr.leader) <span class="string">"become-leader"</span> <span class="keyword">else</span> <span class="string">"become-follower"</span></div><div class="line">        stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending %s LeaderAndIsr request %s to broker %d "</span> +</div><div class="line">                                 <span class="string">"for partition [%s,%d]"</span>).format(controllerId, controllerEpoch, typeOfRequest,</div><div class="line">                                                                 state.leaderIsrAndControllerEpoch, broker,</div><div class="line">                                                                 topicPartition.topic, topicPartition.partition))</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: leader id 集合</span></div><div class="line">      <span class="keyword">val</span> leaderIds = partitionStateInfos.map(_._2.leaderIsrAndControllerEpoch.leaderAndIsr.leader).toSet</div><div class="line">      <span class="keyword">val</span> leaders = controllerContext.liveOrShuttingDownBrokers.filter(b =&gt; leaderIds.contains(b.id)).map &#123;</div><div class="line">        _.getNode(controller.config.interBrokerListenerName)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: requests.PartitionState</span></div><div class="line">      <span class="keyword">val</span> partitionStates = partitionStateInfos.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</div><div class="line">        <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</div><div class="line">        <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</div><div class="line">          leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</div><div class="line">          partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</div><div class="line">        topicPartition -&gt; partitionState</div><div class="line">      &#125;</div><div class="line">      <span class="comment">//note: 构造 LeaderAndIsr 请求,并添加到对应的 queue 中</span></div><div class="line">      <span class="keyword">val</span> leaderAndIsrRequest = <span class="keyword">new</span> <span class="type">LeaderAndIsrRequest</span>.</div><div class="line">          <span class="type">Builder</span>(controllerId, controllerEpoch, partitionStates.asJava, leaders.asJava)</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">LEADER_AND_ISR</span>, leaderAndIsrRequest, <span class="literal">null</span>)</div><div class="line">    &#125;</div><div class="line">    leaderAndIsrRequestMap.clear() <span class="comment">//note: 清空 leaderAndIsr 集合</span></div><div class="line"></div><div class="line">    <span class="comment">//note: update-metadata 请求</span></div><div class="line">    updateMetadataRequestPartitionInfoMap.foreach(p =&gt; stateChangeLogger.trace((<span class="string">"Controller %d epoch %d sending UpdateMetadata request %s "</span> +</div><div class="line">      <span class="string">"to brokers %s for partition %s"</span>).format(controllerId, controllerEpoch, p._2.leaderIsrAndControllerEpoch,</div><div class="line">      updateMetadataRequestBrokerSet.toString(), p._1)))</div><div class="line">    <span class="keyword">val</span> partitionStates = updateMetadataRequestPartitionInfoMap.map &#123; <span class="keyword">case</span> (topicPartition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> <span class="type">LeaderIsrAndControllerEpoch</span>(leaderIsr, controllerEpoch) = partitionStateInfo.leaderIsrAndControllerEpoch</div><div class="line">      <span class="keyword">val</span> partitionState = <span class="keyword">new</span> requests.<span class="type">PartitionState</span>(controllerEpoch, leaderIsr.leader,</div><div class="line">        leaderIsr.leaderEpoch, leaderIsr.isr.map(<span class="type">Integer</span>.valueOf).asJava, leaderIsr.zkVersion,</div><div class="line">        partitionStateInfo.allReplicas.map(<span class="type">Integer</span>.valueOf).asJava)</div><div class="line">      topicPartition -&gt; partitionState</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> version: <span class="type">Short</span> =</div><div class="line">      <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_2_IV0</span>) <span class="number">3</span></div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_10_0_IV1</span>) <span class="number">2</span></div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (controller.config.interBrokerProtocolVersion &gt;= <span class="type">KAFKA_0_9_0</span>) <span class="number">1</span></div><div class="line">      <span class="keyword">else</span> <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment">//note: 构造 update-metadata 请求</span></div><div class="line">    <span class="keyword">val</span> updateMetadataRequest = &#123;</div><div class="line">      <span class="keyword">val</span> liveBrokers = <span class="keyword">if</span> (version == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">// Version 0 of UpdateMetadataRequest only supports PLAINTEXT.</span></div><div class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</div><div class="line">          <span class="keyword">val</span> securityProtocol = <span class="type">SecurityProtocol</span>.<span class="type">PLAINTEXT</span></div><div class="line">          <span class="keyword">val</span> listenerName = <span class="type">ListenerName</span>.forSecurityProtocol(securityProtocol)</div><div class="line">          <span class="keyword">val</span> node = broker.getNode(listenerName)</div><div class="line">          <span class="keyword">val</span> endPoints = <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">EndPoint</span>(node.host, node.port, securityProtocol, listenerName))</div><div class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        controllerContext.liveOrShuttingDownBrokers.map &#123; broker =&gt;</div><div class="line">          <span class="keyword">val</span> endPoints = broker.endPoints.map &#123; endPoint =&gt;</div><div class="line">            <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">EndPoint</span>(endPoint.host, endPoint.port, endPoint.securityProtocol, endPoint.listenerName)</div><div class="line">          &#125;</div><div class="line">          <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Broker</span>(broker.id, endPoints.asJava, broker.rack.orNull)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataRequest</span>.<span class="type">Builder</span>(</div><div class="line">        controllerId, controllerEpoch, partitionStates.asJava, liveBrokers.asJava).</div><div class="line">        setVersion(version)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 将请求添加到对应的 queue</span></div><div class="line">    updateMetadataRequestBrokerSet.foreach &#123; broker =&gt;</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">UPDATE_METADATA_KEY</span>, updateMetadataRequest, <span class="literal">null</span>)</div><div class="line">    &#125;</div><div class="line">    updateMetadataRequestBrokerSet.clear() <span class="comment">//note: 清空对应的请求记录</span></div><div class="line">    updateMetadataRequestPartitionInfoMap.clear()</div><div class="line"></div><div class="line">    <span class="comment">//note: StopReplica 请求的处理</span></div><div class="line">    stopReplicaRequestMap.foreach &#123; <span class="keyword">case</span> (broker, replicaInfoList) =&gt;</div><div class="line">      <span class="keyword">val</span> stopReplicaWithDelete = replicaInfoList.filter(_.deletePartition).map(_.replica).toSet</div><div class="line">      <span class="keyword">val</span> stopReplicaWithoutDelete = replicaInfoList.filterNot(_.deletePartition).map(_.replica).toSet</div><div class="line">      debug(<span class="string">"The stop replica request (delete = true) sent to broker %d is %s"</span></div><div class="line">        .format(broker, stopReplicaWithDelete.mkString(<span class="string">","</span>)))</div><div class="line">      debug(<span class="string">"The stop replica request (delete = false) sent to broker %d is %s"</span></div><div class="line">        .format(broker, stopReplicaWithoutDelete.mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">      <span class="keyword">val</span> (replicasToGroup, replicasToNotGroup) = replicaInfoList.partition(r =&gt; !r.deletePartition &amp;&amp; r.callback == <span class="literal">null</span>)</div><div class="line"></div><div class="line">      <span class="comment">// Send one StopReplicaRequest for all partitions that require neither delete nor callback. This potentially</span></div><div class="line">      <span class="comment">// changes the order in which the requests are sent for the same partitions, but that's OK.</span></div><div class="line">      <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(controllerId, controllerEpoch, <span class="literal">false</span>,</div><div class="line">        replicasToGroup.map(r =&gt; <span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).toSet.asJava)</div><div class="line">      controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest)</div><div class="line"></div><div class="line">      replicasToNotGroup.foreach &#123; r =&gt;</div><div class="line">        <span class="keyword">val</span> stopReplicaRequest = <span class="keyword">new</span> <span class="type">StopReplicaRequest</span>.<span class="type">Builder</span>(</div><div class="line">            controllerId, controllerEpoch, r.deletePartition,</div><div class="line">            <span class="type">Set</span>(<span class="keyword">new</span> <span class="type">TopicPartition</span>(r.replica.topic, r.replica.partition)).asJava)</div><div class="line">        controller.sendRequest(broker, <span class="type">ApiKeys</span>.<span class="type">STOP_REPLICA</span>, stopReplicaRequest, r.callback)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    stopReplicaRequestMap.clear()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (leaderAndIsrRequestMap.nonEmpty) &#123;</div><div class="line">        error(<span class="string">"Haven't been able to send leader and isr requests, current state of "</span> +</div><div class="line">            <span class="string">s"the map is <span class="subst">$leaderAndIsrRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (updateMetadataRequestBrokerSet.nonEmpty) &#123;</div><div class="line">        error(<span class="string">s"Haven't been able to send metadata update requests to brokers <span class="subst">$updateMetadataRequestBrokerSet</span>, "</span> +</div><div class="line">              <span class="string">s"current state of the partition info is <span class="subst">$updateMetadataRequestPartitionInfoMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (stopReplicaRequestMap.nonEmpty) &#123;</div><div class="line">        error(<span class="string">"Haven't been able to send stop replica requests, current state of "</span> +</div><div class="line">            <span class="string">s"the map is <span class="subst">$stopReplicaRequestMap</span>. Exception message: <span class="subst">$e</span>"</span>)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面这个方法看着很复杂，其实做的事情很明确，就是将三个集合中的请求发送对应 Broker 的请求队列中，这里简单作一个总结：</p>
<ol>
<li>从 leaderAndIsrRequestMap 集合中构造相应的 LeaderAndIsr 请求，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 leaderAndIsrRequestMap 集合；</li>
<li>从 updateMetadataRequestPartitionInfoMap 集合中构造相应的 UpdateMetadata 请求，，通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 updateMetadataRequestBrokerSet 和 updateMetadataRequestPartitionInfoMap 集合；</li>
<li>从 stopReplicaRequestMap 集合中构造相应的 StopReplica 请求，在构造时会根据是否设置删除标志将要涉及的 Partition 分成两类，构造对应的请求，对于要删除数据的 StopReplica 会设置相应的回调函数，然后通过 Controller 的 <code>sendRequest()</code> 方法将请求添加到 Broker 对应的 MessageQueue 中，最后清空 stopReplicaRequestMap 集合。</li>
</ol>
<p>走到这一步，Controller 要发送的请求算是都添加到对应 Broker 的 MessageQueue 中，后台的 RequestSendThread 线程会从这个请求队列中遍历相应的请求，发送给对应的 Broker。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要讲述 Controller 向各个 Broker 发送请求的模型，算是对 &lt;a href=&quot;http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager&quot;&gt;Contro
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Topic 的新建/扩容/删除（二十）</title>
    <link href="http://matt33.com/2018/06/18/topic-create-alter-delete/"/>
    <id>http://matt33.com/2018/06/18/topic-create-alter-delete/</id>
    <published>2018-06-18T09:24:21.000Z</published>
    <updated>2018-06-18T09:30:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 <a href="http://matt33.com/2017/07/21/kafka-topic-create/">Kafka 源码解析之 topic 创建过程（三）</a> 中已经讲述过了，本篇与前面不同的是，本篇主要是从 Controller 角度来讲述，而且是把新建、扩容、删除这三个 Topic 级别的操作放在一起做一个总结。</p>
<h2 id="Topic-新建与扩容"><a href="#Topic-新建与扩容" class="headerlink" title="Topic 新建与扩容"></a>Topic 新建与扩容</h2><p>这里把 Topic 新建与扩容放在一起讲解，主要是因为无论 Topic 是新建还是扩容，在 Kafka 内部其实都是 Partition 的新建，底层的实现机制是一样的，Topic 的新建与扩容的整体流程如下图所示：</p>
<p><img src="/images/kafka/topic-create-alter.png" alt="Topic 新建与扩容流程"></p>
<p>Topic 新建与扩容触发条件的不同如下所示：</p>
<ol>
<li>对于 Topic 扩容，监控的节点是 <code>/brokers/topics/TOPIC_NAME</code>，监控的是具体的 Topic 节点，通过 PartitionStateMachine 的 <code>registerPartitionChangeListener(topic)</code> 方法注册的相应 listener；</li>
<li>对于 Topic 新建，监控的节点是 <code>/brokers/topics</code>，监控的是 Topic 列表，通过 PartitionStateMachine 的 <code>registerTopicChangeListener()</code> 方法注册的相应 listener。</li>
</ol>
<p>下面开始详细讲述这两种情况。</p>
<h3 id="Topic-扩容"><a href="#Topic-扩容" class="headerlink" title="Topic 扩容"></a>Topic 扩容</h3><p>Kafka 提供了 Topic 扩容工具，假设一个 Topic（topic_test）只有一个 partition，这时候我们想把它扩容到两个 Partition，可以通过下面两个命令来实现：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --partitions 2</div><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --alter --replica-assignment 1:2,2:1 --partitions 2</div></pre></td></tr></table></figure>
<p>这两种方法的区别是：第二种方法直接指定了要扩容的 Partition 2 的副本需要分配到哪台机器上，这样的话我们可以精确控制到哪些 Topic 放下哪些机器上。</p>
<p>无论是使用哪种方案，上面两条命令产生的结果只有一个，将 Topic 各个 Partition 的副本写入到 ZK 对应的节点上，这样的话 <code>/brokers/topics/topic_test</code> 节点的内容就会发生变化，PartitionModificationsListener 监听器就会被触发，该监听器的处理流程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Partition change 监听器,主要是用于 Partition 扩容的监听</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionModificationsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"AddPartitionsListener"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        info(<span class="string">s"Partition modification triggered <span class="subst">$data</span> for path <span class="subst">$dataPath</span>"</span>)</div><div class="line">        <span class="keyword">val</span> partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(<span class="type">List</span>(topic))</div><div class="line">        <span class="comment">//note: 获取新增的 partition 列表及其对应的分配副本列表</span></div><div class="line">        <span class="keyword">val</span> partitionsToBeAdded = partitionReplicaAssignment.filter(p =&gt;</div><div class="line">          !controllerContext.partitionReplicaAssignment.contains(p._1))</div><div class="line">        <span class="comment">//note: 如果该 topic 被标记为删除,那么直接跳过,不再处理,否则创建该 Partition</span></div><div class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(topic))</div><div class="line">          error(<span class="string">"Skipping adding partitions %s for topic %s since it is currently being deleted"</span></div><div class="line">                .format(partitionsToBeAdded.map(_._1.partition).mkString(<span class="string">","</span>), topic))</div><div class="line">        <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">if</span> (partitionsToBeAdded.nonEmpty) &#123;</div><div class="line">            info(<span class="string">"New partitions to be added %s"</span>.format(partitionsToBeAdded))</div><div class="line">            controllerContext.partitionReplicaAssignment.++=(partitionsToBeAdded)</div><div class="line">            controller.onNewPartitionCreation(partitionsToBeAdded.keySet)<span class="comment">//note: 创建新的 partition</span></div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling add partitions for data path "</span> + dataPath, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// this is not implemented for partition change</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(parentPath: <span class="type">String</span>): <span class="type">Unit</span> = &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其 <code>doHandleDataChange()</code> 方法的处理流程如下：</p>
<ol>
<li>首先获取该 Topic 在 ZK 的 Partition 副本列表，跟本地的缓存做对比，获取新增的 Partition 列表；</li>
<li>检查这个 Topic 是否被标记为删除，如果被标记了，那么直接跳过，不再处理这个 Partition 扩容的请求；</li>
<li>调用 KafkaController 的 <code>onNewPartitionCreation()</code> 新建该 Partition。</li>
</ol>
<p>下面我们看下 <code>onNewPartitionCreation()</code> 方法，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 用于 Topic Partition 的新建</span></div><div class="line"><span class="comment">//note: 1. 将新创建的 partition 状态置为 NewPartition 状态;</span></div><div class="line"><span class="comment">//note: 2. 将新创建的 Replica 状态置为 NewReplica 状态;</span></div><div class="line"><span class="comment">//note: 3. 将该 Partition 从 NewPartition 改为 OnlinePartition 状态,这期间会 为该 Partition 选举 leader 和 isr，更新到 zk 和 controller的缓存中</span></div><div class="line"><span class="comment">//note: 4. 将副本状态从 NewReplica 改为 OnlineReplica 状态。</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewPartitionCreation</span></span>(newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"New partition creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">NewPartition</span>)</div><div class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">NewReplica</span>)</div><div class="line">  partitionStateMachine.handleStateChanges(newPartitions, <span class="type">OnlinePartition</span>, offlinePartitionSelector)</div><div class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions), <span class="type">OnlineReplica</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Partition 的新建，总共分了以下四步：</p>
<ol>
<li>将新创建的 Partition 状态置为 NewPartition 状态，此时 Partition 刚刚创建，只是分配了相应的 Replica 但是还没有 leader 和 isr，不能正常工作;</li>
<li>将该 Partition 对应的 Replica 列表状态设置为 NewReplica 状态，这部分只是将 Replica 的状态设置为了 NewReplica，并没有做其他的处理;</li>
<li>将该 Partition 的状态从 NewPartition 改为 OnlinePartition 状态，这期间会为该 Partition 选举 leader 和 isr，并将结果更新到 ZK 和 Controller 的缓存中，并向该 Partition 的所有副本发送对应的 LeaderAndIsr 信息（发送 LeaderAndIsr 请求的同时也会向所有 Broker 发送该 Topic 的 leader、isr metadata 信息）；</li>
<li>将副本状态从 NewReplica 转移为 OnlineReplica 状态。</li>
</ol>
<p>经过上面几个阶段，一个 Partition 算是真正创建出来，可以正常进行读写工作了，当然上面只是讲述了 Controller 端做的内容，Partition 副本所在节点对 LeaderAndIsr 请求会做更多的工作，这部分会在后面关于 LeaderAndIsr 请求的处理中只能够详细讲述。</p>
<h3 id="Topic-新建"><a href="#Topic-新建" class="headerlink" title="Topic 新建"></a>Topic 新建</h3><p>Kafka 也提供了 Topic 创建的工具，假设我们要创建一个名叫 topic_test，Partition 数为2的 Topic，创建的命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --partitions 2 --replication-factor 2</div><div class="line">./bin/kafka-topics.sh --zookeeper zk01:2181/kafka --topic topic_test --create --replica-assignment 1:2,2:1 --partitions 2</div></pre></td></tr></table></figure>
<p>跟前面的类似，方法二是可以精确控制新建 Topic 每个 Partition 副本所在位置，Topic 创建的本质上是在 <code>/brokers/topics</code> 下新建一个节点信息，并将 Topic 的分区详情写入进去，当 <code>/brokers/topics</code> 有了新增的 Topic 节点后，会触发 TopicChangeListener 监听器，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 监控 zk 上 Topic 子节点的变化 ,KafkaController 会进行相应的处理</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"TopicChangeListener"</span></div><div class="line"></div><div class="line">  <span class="comment">//note: 当 zk 上 topic 节点上有变更时,这个方法就会调用</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">          <span class="keyword">val</span> currentChildren = &#123;</div><div class="line">            debug(<span class="string">"Topic change listener fired for path %s with children %s"</span>.format(parentPath, children.mkString(<span class="string">","</span>)))</div><div class="line">            children.toSet</div><div class="line">          &#125;</div><div class="line">          <span class="comment">//note: 新创建的 topic 列表</span></div><div class="line">          <span class="keyword">val</span> newTopics = currentChildren -- controllerContext.allTopics</div><div class="line">          <span class="comment">//note: 已经删除的 topic 列表</span></div><div class="line">          <span class="keyword">val</span> deletedTopics = controllerContext.allTopics -- currentChildren</div><div class="line">          controllerContext.allTopics = currentChildren</div><div class="line"></div><div class="line">          <span class="comment">//note: 新创建 topic 对应的 partition 列表及副本列表添加到 Controller 的缓存中</span></div><div class="line">          <span class="keyword">val</span> addedPartitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(newTopics.toSeq)</div><div class="line">          <span class="comment">//note: Controller 从缓存中把已经删除 partition 过滤掉</span></div><div class="line">          controllerContext.partitionReplicaAssignment = controllerContext.partitionReplicaAssignment.filter(p =&gt;</div><div class="line">            !deletedTopics.contains(p._1.topic))</div><div class="line">          controllerContext.partitionReplicaAssignment.++=(addedPartitionReplicaAssignment)<span class="comment">//note: 将新增的 tp-replicas 更新到缓存中</span></div><div class="line">          info(<span class="string">"New topics: [%s], deleted topics: [%s], new partition replica assignment [%s]"</span>.format(newTopics,</div><div class="line">            deletedTopics, addedPartitionReplicaAssignment))</div><div class="line">          <span class="keyword">if</span> (newTopics.nonEmpty)<span class="comment">//note: 处理新建的 topic</span></div><div class="line">            controller.onNewTopicCreation(newTopics, addedPartitionReplicaAssignment.keySet)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling new topic"</span>, e)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>只要 <code>/brokers/topics</code> 下子节点信息有变化（topic 新增或者删除），TopicChangeListener 都会被触发，其 <code>doHandleChildChange()</code> 方法的处理流程如下：</p>
<ol>
<li>获取 ZK 当前的所有 Topic 列表，根据本地缓存的 Topic 列表记录，可以得到新增的 Topic 记录与已经删除的 Topic 列表；</li>
<li>将新增 Topic 的相信信息更新到 Controller 的缓存中，将已经删除的 Topic 从 Controller 的副本缓存中移除；</li>
<li>调用 KafkaController 的 <code>onNewTopicCreation()</code> 方法创建该 topic。</li>
</ol>
<p>接着看下 <code>onNewTopicCreation()</code> 方法实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 当 partition state machine 监控到有新 topic 或 partition 时,这个方法将会被调用</span></div><div class="line"><span class="comment">//note: 1. 注册 partition change listener, 监听 Parition 变化;</span></div><div class="line"><span class="comment">//note: 2. 触发 the new partition, 也即是 onNewPartitionCreation()</span></div><div class="line"><span class="comment">//note: 3. 发送 metadata 请求给所有的 Broker</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onNewTopicCreation</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>], newPartitions: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"New topic creation callback for %s"</span>.format(newPartitions.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">// subscribe to partition changes</span></div><div class="line">  topics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">  onNewPartitionCreation(newPartitions)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法主要做了两件事：</p>
<ol>
<li>注册这个 topic 的 PartitionModificationsListener 监听器；</li>
<li>通过 <code>onNewPartitionCreation()</code> 创建该 Topic 的所有 Partition。</li>
</ol>
<p><code>onNewPartitionCreation()</code> 的实现在前面 Topic 扩容部分已经讲述过，这里不再重复，最好参考前面流程图来梳理 Topic 扩容和新建的整个过程。</p>
<h2 id="Topic-删除"><a href="#Topic-删除" class="headerlink" title="Topic 删除"></a>Topic 删除</h2><p>Kafka Topic 删除这部分的逻辑是一个单独线程去做的，这个线程是在 Controller 启动时初始化和启动的。</p>
<h3 id="TopicDeletionManager-初始化"><a href="#TopicDeletionManager-初始化" class="headerlink" title="TopicDeletionManager 初始化"></a>TopicDeletionManager 初始化</h3><p>TopicDeletionManager 启动实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Invoked at the end of new controller initiation</div><div class="line"> */</div><div class="line"><span class="comment">//note: Controller 初始化完成,触发这个操作,删除 topic 线程启动</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span> (isDeleteTopicEnabled) &#123;</div><div class="line">    deleteTopicsThread = <span class="keyword">new</span> <span class="type">DeleteTopicsThread</span>()</div><div class="line">    <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty)</div><div class="line">      deleteTopicStateChanged.set(<span class="literal">true</span>)</div><div class="line">    deleteTopicsThread.start() <span class="comment">//note: 启动 DeleteTopicsThread</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>TopicDeletionManager 启动时只是初始化了一个 DeleteTopicsThread 线程，并启动该线程。TopicDeletionManager 这个类从名字上去看，它是 Topic 删除的管理器，它是如何实现 Topic 删除管理呢，这里先看下该类的几个重要的成员变量：</p>
<ol>
<li>topicsToBeDeleted：需要删除的 Topic 列表，每当有新的 topic 需要删除时，Controller 就通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到这个列表中，而 DeleteTopicsThread 线程则会从列表拿到需要进行删除的 Topic 信息；</li>
<li>partitionsToBeDeleted：需要删除的 Partition 列表，跟上面的 Topic 列表保持一致，只不过纬度不同；</li>
<li>topicsIneligibleForDeletion：非法删除的 Topic 列表，当一个 Topic 正在进行副本迁移、leader 选举或者有副本 dead 的情况下，该 Topic 都会设置被非法删除状态，只有恢复正常后，这个状态才会解除，处在这个状态的 Topic 是无法删除的。</li>
</ol>
<h3 id="Topic-删除整体流程"><a href="#Topic-删除整体流程" class="headerlink" title="Topic 删除整体流程"></a>Topic 删除整体流程</h3><p>前面一小节，简单介绍了 TopicDeletionManager、DeleteTopicsThread 的启动以及它们之间的关系，这里我们看下一个 Topic 被设置删除后，其处理的整理流程，简单做了一个小图，如下所示：</p>
<p><img src="/images/kafka/topic-delete.png" alt="Topic 删除整理流程"></p>
<p>这里先简单讲述上面的流程，当一个 Topic 设置为删除后：</p>
<ol>
<li>首先 DeleteTopicsListener 会被触发，然后通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表中；</li>
<li>DeleteTopicsThread 这个线程会不断调用 <code>doWork()</code> 方法，这个方法被调用时，它会遍历 <code>topicsToBeDeleted</code> 中的所有 Topic 列表；</li>
<li>对于之前没有处理过的 Topic（之前还没有开始删除），会通过 TopicDeletionManager 的 <code>onTopicDeletion()</code> 方法执行删除操作；</li>
<li>如果 Topic 删除完成（所有 Replica 的状态都变为 ReplicaDeletionSuccessful 状态），那么就执行 TopicDeletionManager 的 <code>completeDeleteTopic()</code> 完成删除流程，即更新状态信息，并将 Topic 的 meta 信息从缓存和 ZK 中清除。</li>
</ol>
<h3 id="Topic-删除详细实现"><a href="#Topic-删除详细实现" class="headerlink" title="Topic 删除详细实现"></a>Topic 删除详细实现</h3><p>先看下 DeleteTopicsListener 的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除 Topic 包括以下操作:</span></div><div class="line"><span class="comment">//note: 1. 如果要删除的 topic 存在,将 Topic 添加到 Topic 将要删除的缓存中;</span></div><div class="line"><span class="comment">//note: 2. 如果有 Topic 将要被删除,那么将触发 Topic 删除线程</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"DeleteTopicsListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when a topic is being deleted</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当 topic 需要被删除时,才会触发</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, children: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">var</span> topicsToBeDeleted = children.toSet</div><div class="line">      debug(<span class="string">"Delete topics listener fired for topics %s to be deleted"</span>.format(topicsToBeDeleted.mkString(<span class="string">","</span>)))</div><div class="line">      <span class="comment">//note: 不存在的、需要删除的 topic, 直接清除 zk 上的记录</span></div><div class="line">      <span class="keyword">val</span> nonExistentTopics = topicsToBeDeleted -- controllerContext.allTopics</div><div class="line">      <span class="keyword">if</span> (nonExistentTopics.nonEmpty) &#123;</div><div class="line">        warn(<span class="string">"Ignoring request to delete non-existing topics "</span> + nonExistentTopics.mkString(<span class="string">","</span>))</div><div class="line">        nonExistentTopics.foreach(topic =&gt; zkUtils.deletePathRecursive(getDeleteTopicPath(topic)))</div><div class="line">      &#125;</div><div class="line">      topicsToBeDeleted --= nonExistentTopics</div><div class="line">      <span class="keyword">if</span> (controller.config.deleteTopicEnable) &#123; <span class="comment">//note: 如果允许 topic 删除</span></div><div class="line">        <span class="keyword">if</span> (topicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 有 Topic 需要删除</span></div><div class="line">          info(<span class="string">"Starting topic deletion for topics "</span> + topicsToBeDeleted.mkString(<span class="string">","</span>))</div><div class="line">          <span class="comment">// mark topic ineligible for deletion if other state changes are in progress</span></div><div class="line">          topicsToBeDeleted.foreach &#123; topic =&gt; <span class="comment">//note: 如果 topic 正在最优 leader 选举或正在迁移,那么将 topic 标记为非法删除状态</span></div><div class="line">            <span class="keyword">val</span> preferredReplicaElectionInProgress =</div><div class="line">              controllerContext.partitionsUndergoingPreferredReplicaElection.map(_.topic).contains(topic)</div><div class="line">            <span class="keyword">val</span> partitionReassignmentInProgress =</div><div class="line">              controllerContext.partitionsBeingReassigned.keySet.map(_.topic).contains(topic)</div><div class="line">            <span class="keyword">if</span> (preferredReplicaElectionInProgress || partitionReassignmentInProgress)</div><div class="line">              controller.deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">          &#125;</div><div class="line">          <span class="comment">// add topic to deletion list</span></div><div class="line">          <span class="comment">//note: 将要删除的 topic 添加到待删除的 topic</span></div><div class="line">          controller.deleteTopicManager.enqueueTopicsForDeletion(topicsToBeDeleted)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// If delete topic is disabled remove entries under zookeeper path : /admin/delete_topics</span></div><div class="line">        <span class="keyword">for</span> (topic &lt;- topicsToBeDeleted) &#123;</div><div class="line">          info(<span class="string">"Removing "</span> + getDeleteTopicPath(topic) + <span class="string">" since delete topic is disabled"</span>)</div><div class="line">          zkUtils.zkClient.delete(getDeleteTopicPath(topic))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其 <code>doHandleChildChange()</code> 的实现逻辑如下：</p>
<ol>
<li>根据要删除的 Topic 列表，过滤出那些不存在的 Topic 列表，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）；</li>
<li>如果集群不允许 Topic 删除，直接从 ZK 中清除（只是从 <code>/admin/delete_topics</code> 中移除）这些 Topic 列表，结束流程；</li>
<li>如果这个列表中有正在进行副本迁移或 leader 选举的 Topic，那么先将这些 Topic 加入到 <code>topicsIneligibleForDeletion</code> 中，即标记为非法删除；</li>
<li>通过 <code>enqueueTopicsForDeletion()</code> 方法将 Topic 添加到要删除的 Topic 列表（<code>topicsToBeDeleted</code>）、将 Partition 添加到要删除的 Partition 列表中（<code>partitionsToBeDeleted</code>）。</li>
</ol>
<p>接下来，看下 Topic 删除线程 DeleteTopicsThread 的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">/note: topic 删除线程</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeleteTopicsThread</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">ShutdownableThread</span>(<span class="params">name = "delete-topics-thread-" + controller.config.brokerId, isInterruptible = false</span>) </span>&#123;</div><div class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">    awaitTopicDeletionNotification()</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (!isRunning.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="comment">//note: 要删除的 topic 列表</span></div><div class="line">      <span class="keyword">val</span> topicsQueuedForDeletion = <span class="type">Set</span>.empty[<span class="type">String</span>] ++ topicsToBeDeleted</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(topicsQueuedForDeletion.nonEmpty)</div><div class="line">        info(<span class="string">"Handling deletion for topics "</span> + topicsQueuedForDeletion.mkString(<span class="string">","</span>))</div><div class="line"></div><div class="line">      topicsQueuedForDeletion.foreach &#123; topic =&gt;</div><div class="line">      <span class="comment">// if all replicas are marked as deleted successfully, then topic deletion is done</span></div><div class="line">        <span class="keyword">if</span>(controller.replicaStateMachine.areAllReplicasForTopicDeleted(topic)) &#123;<span class="comment">//note: 如果 Topic 所有副本都删除成功的情况下</span></div><div class="line">          <span class="comment">// clear up all state for this topic from controller cache and zookeeper</span></div><div class="line">          <span class="comment">//note: 从 controller 的缓存和 zk 中清除这个 topic 的所有记录,这个 topic 彻底删除成功了</span></div><div class="line">          completeDeleteTopic(topic)</div><div class="line">          info(<span class="string">"Deletion of topic %s successfully completed"</span>.format(topic))</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="keyword">if</span>(controller.replicaStateMachine.isAtLeastOneReplicaInDeletionStartedState(topic)) &#123;</div><div class="line">            <span class="comment">//note: Topic 的副本至少有一个状态为 ReplicaDeletionStarted 时</span></div><div class="line">            <span class="comment">// ignore since topic deletion is in progress</span></div><div class="line">            <span class="comment">//note: 过滤出 Topic 中副本状态为 ReplicaDeletionStarted 的 Partition 列表</span></div><div class="line">            <span class="keyword">val</span> replicasInDeletionStartedState = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionStarted</span>)</div><div class="line">            <span class="comment">//note: 表明了上面这些副本正在删除中</span></div><div class="line">            <span class="keyword">val</span> replicaIds = replicasInDeletionStartedState.map(_.replica)</div><div class="line">            <span class="keyword">val</span> partitions = replicasInDeletionStartedState.map(r =&gt; <span class="type">TopicAndPartition</span>(r.topic, r.partition))</div><div class="line">            info(<span class="string">"Deletion for replicas %s for partition %s of topic %s in progress"</span>.format(replicaIds.mkString(<span class="string">","</span>),</div><div class="line">              partitions.mkString(<span class="string">","</span>), topic))</div><div class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note:副本既没有全部删除完成、也没有一个副本是在删除过程中，证明这个 topic 还没有开始删除或者删除完成但是至少一个副本删除失败</span></div><div class="line">            <span class="comment">// if you come here, then no replica is in TopicDeletionStarted and all replicas are not in</span></div><div class="line">            <span class="comment">// TopicDeletionSuccessful. That means, that either given topic haven't initiated deletion</span></div><div class="line">            <span class="comment">// or there is at least one failed replica (which means topic deletion should be retried).</span></div><div class="line">            <span class="keyword">if</span>(controller.replicaStateMachine.isAnyReplicaInState(topic, <span class="type">ReplicaDeletionIneligible</span>)) &#123;</div><div class="line">              <span class="comment">//note: 如果有副本删除失败,那么进行重试操作</span></div><div class="line">              <span class="comment">// mark topic for deletion retry</span></div><div class="line">              markTopicForDeletionRetry(topic)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="comment">// Try delete topic if it is eligible for deletion.</span></div><div class="line">        <span class="keyword">if</span>(isTopicEligibleForDeletion(topic)) &#123; <span class="comment">//note: 如果 topic 可以被删除</span></div><div class="line">          info(<span class="string">"Deletion of topic %s (re)started"</span>.format(topic))</div><div class="line">          <span class="comment">// topic deletion will be kicked off</span></div><div class="line">          <span class="comment">//note: 开始删除 topic</span></div><div class="line">          onTopicDeletion(<span class="type">Set</span>(topic))</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(isTopicIneligibleForDeletion(topic)) &#123;</div><div class="line">          info(<span class="string">"Not retrying deletion of topic %s at this time since it is marked ineligible for deletion"</span>.format(topic))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>doWork()</code> 方法处理逻辑如下：</p>
<ol>
<li>遍历所有要删除的 Topic，进行如下处理；</li>
<li>如果该 Topic 的所有副本都下线成功（状态为 ReplicaDeletionSuccessful）时，那么执行 <code>completeDeleteTopic()</code> 方法完成 Topic 的删除；</li>
<li>否则，如果 Topic 在删除过程有失败的副本（状态为 ReplicaDeletionIneligible），那么执行 <code>markTopicForDeletionRetry()</code> 将失败的 Replica 状态设置为 OfflineReplica；</li>
<li>判断 Topic 是否允许删除（不在非法删除的集合中就代表运允许），调用 <code>onTopicDeletion()</code> 执行 Topic 删除。</li>
</ol>
<p>先看下 <code>onTopicDeletion()</code> 方法，这是 Topic 最开始删除时的实现，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Topic 删除</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onTopicDeletion</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>]) &#123;</div><div class="line">  info(<span class="string">"Topic deletion callback for %s"</span>.format(topics.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">// send update metadata so that brokers stop serving data for topics to be deleted</span></div><div class="line">  <span class="keyword">val</span> partitions = topics.flatMap(controllerContext.partitionsForTopic) <span class="comment">//note: topic 的所有 Partition</span></div><div class="line">  controller.sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, partitions) <span class="comment">//note: 更新meta</span></div><div class="line">  <span class="keyword">val</span> partitionReplicaAssignmentByTopic = controllerContext.partitionReplicaAssignment.groupBy(p =&gt; p._1.topic)</div><div class="line">  topics.foreach &#123; topic =&gt; <span class="comment">//note:  删除 topic 的每一个 Partition</span></div><div class="line">    onPartitionDeletion(partitionReplicaAssignmentByTopic(topic).keySet)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 这个方法是用于 delete-topic, 用于删除 topic 的所有 partition</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onPartitionDeletion</span></span>(partitionsToBeDeleted: <span class="type">Set</span>[<span class="type">TopicAndPartition</span>]) &#123;</div><div class="line">  info(<span class="string">"Partition deletion callback for %s"</span>.format(partitionsToBeDeleted.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="keyword">val</span> replicasPerPartition = controllerContext.replicasForPartition(partitionsToBeDeleted)</div><div class="line">  startReplicaDeletion(replicasPerPartition)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Topic 的删除的真正实现方法还是在 <code>startReplicaDeletion()</code> 方法中，Topic 删除时，会先调用 <code>onPartitionDeletion()</code> 方法删除所有的 Partition，然后在 Partition 删除时，执行 <code>startReplicaDeletion()</code> 方法删除该 Partition 的副本，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 被 onPartitionDeletion 方法触发,删除副本具体的实现的地方</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startReplicaDeletion</span></span>(replicasForTopicsToBeDeleted: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>]) &#123;</div><div class="line">  replicasForTopicsToBeDeleted.groupBy(_.topic).keys.foreach &#123; topic =&gt;</div><div class="line">    <span class="comment">//note: topic 所有存活的 replica</span></div><div class="line">    <span class="keyword">val</span> aliveReplicasForTopic = controllerContext.allLiveReplicas().filter(p =&gt; p.topic == topic)</div><div class="line">    <span class="comment">//note: topic 的 dead replica</span></div><div class="line">    <span class="keyword">val</span> deadReplicasForTopic = replicasForTopicsToBeDeleted -- aliveReplicasForTopic</div><div class="line">    <span class="comment">//note: topic 中已经处于 ReplicaDeletionSuccessful 状态的副本</span></div><div class="line">    <span class="keyword">val</span> successfullyDeletedReplicas = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">    <span class="comment">//note: 还没有成功删除的、存活的副本</span></div><div class="line">    <span class="keyword">val</span> replicasForDeletionRetry = aliveReplicasForTopic -- successfullyDeletedReplicas</div><div class="line">    <span class="comment">// move dead replicas directly to failed state</span></div><div class="line">    <span class="comment">//note: 将 dead replica 设置为 ReplicaDeletionIneligible（删除无效的状态）</span></div><div class="line">    replicaStateMachine.handleStateChanges(deadReplicasForTopic, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">    <span class="comment">// send stop replica to all followers that are not in the OfflineReplica state so they stop sending fetch requests to the leader</span></div><div class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 OfflineReplica（发送 StopReplica 请求）</span></div><div class="line">    replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">OfflineReplica</span>)</div><div class="line">    debug(<span class="string">"Deletion started for replicas %s"</span>.format(replicasForDeletionRetry.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="comment">//note: 将 replicasForDeletionRetry 设置为 ReplicaDeletionStarted 状态</span></div><div class="line">    controller.replicaStateMachine.handleStateChanges(replicasForDeletionRetry, <span class="type">ReplicaDeletionStarted</span>,</div><div class="line">      <span class="keyword">new</span> <span class="type">Callbacks</span>.<span class="type">CallbackBuilder</span>().stopReplicaCallback(deleteTopicStopReplicaCallback).build)</div><div class="line">    <span class="keyword">if</span>(deadReplicasForTopic.nonEmpty) &#123; <span class="comment">//note: 将 topic 标记为不能删除</span></div><div class="line">      debug(<span class="string">"Dead Replicas (%s) found for topic %s"</span>.format(deadReplicasForTopic.mkString(<span class="string">","</span>), topic))</div><div class="line">      markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>该方法的执行逻辑如下：</p>
<ol>
<li>首先获取当前集群所有存活的 broker 信息，根据这个信息可以知道 Topic 哪些副本所在节点是处于 dead 状态；</li>
<li>找到那些已经成功删除的 Replica 列表（状态为 ReplicaDeletionSuccessful），进而可以得到那些还没有成功删除、并且存活的 Replica 列表（<code>replicasForDeletionRetry</code>）；</li>
<li>将处于 dead 节点上的 Replica 的状态设置为 ReplicaDeletionIneligible 状态；</li>
<li>然后重新删除 replicasForDeletionRetry 列表中的副本，先将其状态转移为 OfflineReplica，再转移为 ReplicaDeletionStarted 状态（真正从发送 StopReplica +从物理上删除数据）；</li>
<li>如果有 Replica 所在的机器处于 dead 状态，那么将 Topic 设置为非法删除状态。</li>
</ol>
<p>在将副本状态从 OfflineReplica 转移成 ReplicaDeletionStarted 时，会设置一个回调方法 <code>deleteTopicStopReplicaCallback()</code>，该方法会将删除成功的 Replica 设置为 ReplicaDeletionSuccessful 状态，删除失败的 Replica 设置为 ReplicaDeletionIneligible 状态（需要根据 StopReplica 请求处理的过程，看下哪些情况下 Replica 会删除失败，这个会在后面讲解）。</p>
<p>下面看下这个方法 <code>completeDeleteTopic()</code>，当一个 Topic 的所有 Replica 都删除成功时，即其状态都在 ReplicaDeletionSuccessful 时，会调用这个方法，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: topic 删除后,从 controller 缓存、状态机以及 zk 移除这个 topic 相关记录</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">completeDeleteTopic</span></span>(topic: <span class="type">String</span>) &#123;</div><div class="line">  <span class="comment">// deregister partition change listener on the deleted topic. This is to prevent the partition change listener</span></div><div class="line">  <span class="comment">// firing before the new topic listener when a deleted topic gets auto created</span></div><div class="line">  <span class="comment">//note: 1. 取消 zk 对这个 topic 的 partition-modify-listener</span></div><div class="line">  partitionStateMachine.deregisterPartitionChangeListener(topic)</div><div class="line">  <span class="comment">//note: 2. 过滤出副本状态为 ReplicaDeletionSuccessful 的副本列表</span></div><div class="line">  <span class="keyword">val</span> replicasForDeletedTopic = controller.replicaStateMachine.replicasInState(topic, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">  <span class="comment">// controller will remove this replica from the state machine as well as its partition assignment cache</span></div><div class="line">  <span class="comment">//note: controller 将会从副本状态机移除这些副本</span></div><div class="line">  replicaStateMachine.handleStateChanges(replicasForDeletedTopic, <span class="type">NonExistentReplica</span>)</div><div class="line">  <span class="keyword">val</span> partitionsForDeletedTopic = controllerContext.partitionsForTopic(topic)</div><div class="line">  <span class="comment">// move respective partition to OfflinePartition and NonExistentPartition state</span></div><div class="line">  <span class="comment">//note: 3. 从分区状态机中下线并移除这个 topic 的分区</span></div><div class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">OfflinePartition</span>)</div><div class="line">  partitionStateMachine.handleStateChanges(partitionsForDeletedTopic, <span class="type">NonExistentPartition</span>)</div><div class="line">  topicsToBeDeleted -= topic <span class="comment">//note: 删除成功,从删除 topic 列表中移除</span></div><div class="line">  partitionsToBeDeleted.retain(_.topic != topic) <span class="comment">//note: 从 partitionsToBeDeleted 移除这个 topic</span></div><div class="line">  <span class="keyword">val</span> zkUtils = controllerContext.zkUtils</div><div class="line">  <span class="comment">//note: 4. 删除 zk 上关于这个 topic 的相关记录</span></div><div class="line">  zkUtils.zkClient.deleteRecursive(getTopicPath(topic))</div><div class="line">  zkUtils.zkClient.deleteRecursive(getEntityConfigPath(<span class="type">ConfigType</span>.<span class="type">Topic</span>, topic))</div><div class="line">  zkUtils.zkClient.delete(getDeleteTopicPath(topic))</div><div class="line">  <span class="comment">//note: 5. 从 controller 的所有缓存中再次移除关于这个 topic 的信息</span></div><div class="line">  controllerContext.removeTopic(topic)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>当一个 Topic 所有副本都删除后，会进行如下处理：</p>
<ol>
<li>取消对该 Topic 的 partition-modify-listener 监听器；</li>
<li>将状态为 ReplicaDeletionSuccessful 的副本状态都转移成 NonExistentReplica；</li>
<li>将该 Topic Partition 状态先后转移成 OfflinePartition、NonExistentPartition 状态，正式下线了该 Partition；</li>
<li>从分区状态机和副本状态机中移除这个 Topic 记录；</li>
<li>从 Controller 缓存和 ZK 中清除这个 Topic 的相关记录。</li>
</ol>
<p>至此，一个 Topic 算是真正删除完成。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 的功能方面的内容，在 Kafka 中，一个 Topic 的新建、扩容或者删除都是由 Controller 来操作的，本篇文章也是主要聚焦在 Topic 的操作处理上（新建、扩容、删除），实际上 Topic 的创建在 &lt;a href=&quot;ht
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Broker 上线下线（十九）</title>
    <link href="http://matt33.com/2018/06/17/broker-online-offline/"/>
    <id>http://matt33.com/2018/06/17/broker-online-offline/</id>
    <published>2018-06-17T07:04:21.000Z</published>
    <updated>2018-06-17T07:56:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 <code>/brokers/ids</code> 下注册一个节点，节点名字就是 broker id，这个节点是临时节点，该节点内部会有这个 Broker 的详细节点信息。Controller 会监听 <code>/brokers/ids</code> 这个路径下的所有子节点，如果有新的节点出现，那么就代表有新的 Broker 上线，如果有节点消失，就代表有 broker 下线，Controller 会进行相应的处理，Kafka 就是利用 ZK 的这种 watch 机制及临时节点的特性来完成集群 Broker 的上下线，本文将会深入讲解这一过程。</p>
<h2 id="BrokerChangeListener"><a href="#BrokerChangeListener" class="headerlink" title="BrokerChangeListener"></a>BrokerChangeListener</h2><p>KafkaController 在启动时，会通过副本状态机注册一个监控 broker 上下线的监听器，通过 ReplicaStateMachine 的 <code>registerListeners()</code> 方法实现的，该方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// register ZK listeners of the replica state machine</span></div><div class="line"> <span class="function"><span class="keyword">def</span> <span class="title">registerListeners</span></span>() &#123;</div><div class="line">   <span class="comment">// register broker change listener</span></div><div class="line">   registerBrokerChangeListener() <span class="comment">//note: 监听【/brokers/ids】，broker 的上线下线</span></div><div class="line"> &#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerBrokerChangeListener</span></span>() = &#123;</div><div class="line">  zkUtils.zkClient.subscribeChildChanges(<span class="type">ZkUtils</span>.<span class="type">BrokerIdsPath</span>, brokerChangeListener)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>BrokerChangeListener 是监听 <code>/brokers/ids</code> 节点的监听器，当该节点有变化时会触发 <code>doHandleChildChange()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果 【/brokers/ids】 目录下子节点有变化将会触发这个操作</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrokerChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkChildListener</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"BrokerChangeListener"</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleChildChange</span></span>(parentPath: <span class="type">String</span>, currentBrokerList: <span class="type">Seq</span>[<span class="type">String</span>]) &#123;</div><div class="line">    info(<span class="string">"Broker change listener fired for path %s with children %s"</span>.format(parentPath, currentBrokerList.sorted.mkString(<span class="string">","</span>)))</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">if</span> (hasStarted.get) &#123;</div><div class="line">        <span class="type">ControllerStats</span>.leaderElectionTimer.time &#123;</div><div class="line">          <span class="keyword">try</span> &#123;</div><div class="line">            <span class="comment">//note: 当前 zk 的 broker 列表</span></div><div class="line">            <span class="keyword">val</span> curBrokers = currentBrokerList.map(_.toInt).toSet.flatMap(zkUtils.getBrokerInfo)</div><div class="line">            <span class="comment">//note: ZK 中的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> curBrokerIds = curBrokers.map(_.id)</div><div class="line">            <span class="comment">//note: Controller 缓存中的 broker 列表</span></div><div class="line">            <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</div><div class="line">            <span class="comment">//note: 新上线的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokerIds = curBrokerIds -- liveOrShuttingDownBrokerIds</div><div class="line">            <span class="comment">//note: 掉线的 broker id 列表</span></div><div class="line">            <span class="keyword">val</span> deadBrokerIds = liveOrShuttingDownBrokerIds -- curBrokerIds</div><div class="line">            <span class="comment">//note: 新上线的 Broker 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokers = curBrokers.filter(broker =&gt; newBrokerIds(broker.id))</div><div class="line">            controllerContext.liveBrokers = curBrokers <span class="comment">//note: 更新缓存中当前 broker 列表</span></div><div class="line">            <span class="keyword">val</span> newBrokerIdsSorted = newBrokerIds.toSeq.sorted</div><div class="line">            <span class="keyword">val</span> deadBrokerIdsSorted = deadBrokerIds.toSeq.sorted</div><div class="line">            <span class="keyword">val</span> liveBrokerIdsSorted = curBrokerIds.toSeq.sorted</div><div class="line">            info(<span class="string">"Newly added brokers: %s, deleted brokers: %s, all live brokers: %s"</span></div><div class="line">              .format(newBrokerIdsSorted.mkString(<span class="string">","</span>), deadBrokerIdsSorted.mkString(<span class="string">","</span>), liveBrokerIdsSorted.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="comment">//note: Broker 上线, 在 Controller Channel Manager 中添加该 broker</span></div><div class="line">            newBrokers.foreach(controllerContext.controllerChannelManager.addBroker)</div><div class="line">            <span class="comment">//note: Broker 下线处理, 在 Controller Channel Manager 移除该 broker</span></div><div class="line">            deadBrokerIds.foreach(controllerContext.controllerChannelManager.removeBroker)</div><div class="line">            <span class="keyword">if</span>(newBrokerIds.nonEmpty) <span class="comment">//note: 启动该 Broker</span></div><div class="line">              controller.onBrokerStartup(newBrokerIdsSorted)</div><div class="line">            <span class="keyword">if</span>(deadBrokerIds.nonEmpty) <span class="comment">//note: broker 掉线后开始 leader 选举</span></div><div class="line">              controller.onBrokerFailure(deadBrokerIdsSorted)</div><div class="line">          &#125; <span class="keyword">catch</span> &#123;</div><div class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling broker changes"</span>, e)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里需要重点关注 <code>doHandleChildChange()</code> 方法的实现，该方法处理逻辑如下：</p>
<ol>
<li>从 ZK 获取当前的 Broker 列表（<code>curBrokers</code>）及 broker id 的列表（<code>curBrokerIds</code>）；</li>
<li>获取当前 Controller 中缓存的 broker id 列表（<code>liveOrShuttingDownBrokerIds</code>）；</li>
<li>获取新上线 broker id 列表：<code>newBrokerIds</code> = <code>curBrokerIds</code> – <code>liveOrShuttingDownBrokerIds</code>；</li>
<li>获取掉线的 broker id 列表：<code>deadBrokerIds</code> = <code>liveOrShuttingDownBrokerIds</code> – <code>curBrokerIds</code>；</li>
<li>对于新上线的 broker，先在 ControllerChannelManager 中添加该 broker（即建立与该 Broker 的连接、初始化相应的发送线程和请求队列），最后 Controller 调用 <code>onBrokerStartup()</code> 上线该 Broker；</li>
<li>对于掉线的 broker，先在 ControllerChannelManager 中移除该 broker（即关闭与 Broker 的连接、关闭相应的发送线程和清空请求队列），最后 Controller 调用 <code>onBrokerFailure()</code> 下线该 Broker。</li>
</ol>
<p>整体的处理流程如下图所示：</p>
<p><img src="/images/kafka/broker_online_offline.png" alt="Broker 上线下线处理过程"></p>
<h2 id="Broker-上线"><a href="#Broker-上线" class="headerlink" title="Broker 上线"></a>Broker 上线</h2><p>本节主要讲述一台 Broker 上线的过程，如前面图中所示，一台 Broker 上线主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">controllerContext.controllerChannelManager.addBroker</div><div class="line">controller.onBrokerStartup(newBrokerIdsSorted)</div></pre></td></tr></table></figure>
<ol>
<li>在 Controller Channel Manager 中添加该 Broker 节点，主要的内容是：Controller 建立与该 Broker 的连接、初始化相应的请求发送线程与请求队列；</li>
<li>调用 Controller 的 <code>onBrokerStartup()</code> 方法上线该节点。</li>
</ol>
<p>Controller Channel Manager 添加 Broker 的实现如下，这里就不重复讲述了，前面讲述 Controller 服务初始化的文章（ <a href="http://matt33.com/2018/06/15/kafka-controller-start/#Controller-Channel-Manager">Controller Channel Manager </a>）已经讲述过这部分的内容。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</div><div class="line">  <span class="comment">// be careful here. Maybe the startup() API has already started the request send thread</span></div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(!brokerStateInfo.contains(broker.id)) &#123;</div><div class="line">      addNewBroker(broker)</div><div class="line">      startRequestSendThread(broker.id)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面再看下 Controller 如何在 <code>onBrokerStartup()</code> 方法中实现 Broker 上线操作的，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个是被 副本状态机触发的</span></div><div class="line"><span class="comment">//note: 1. 发送 update-metadata 请求给所有存活的 broker;</span></div><div class="line"><span class="comment">//note: 2. 对于所有 new/offline partition 触发选主操作, 选举成功的, Partition 状态设置为 Online</span></div><div class="line"><span class="comment">//note: 3. 检查是否有分区的重新副本分配分配到了这个台机器上, 如果有, 就进行相应的操作</span></div><div class="line"><span class="comment">//note: 4. 检查这台机器上是否有 Topic 被设置为了删除标志, 如果是, 那么机器启动完成后, 重新尝试删除操作</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerStartup</span></span>(newBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</div><div class="line">  info(<span class="string">"New broker startup callback for %s"</span>.format(newBrokers.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="keyword">val</span> newBrokersSet = newBrokers.toSet <span class="comment">//note: 新启动的 broker</span></div><div class="line">  <span class="comment">// send update metadata request to all live and shutting down brokers. Old brokers will get to know of the new</span></div><div class="line">  <span class="comment">// broker via this update.</span></div><div class="line">  <span class="comment">// In cases of controlled shutdown leaders will not be elected when a new broker comes up. So at least in the</span></div><div class="line">  <span class="comment">// common controlled shutdown case, the metadata will reach the new brokers faster</span></div><div class="line">  <span class="comment">//note: 发送 metadata 更新给所有的 broker, 这样的话旧的 broker 将会知道有机器新上线了</span></div><div class="line">  sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line">  <span class="comment">// the very first thing to do when a new broker comes up is send it the entire list of partitions that it is</span></div><div class="line">  <span class="comment">// supposed to host. Based on that the broker starts the high watermark threads for the input list of partitions</span></div><div class="line">  <span class="comment">//note:  获取这个机器上的所有 replica 请求</span></div><div class="line">  <span class="keyword">val</span> allReplicasOnNewBrokers = controllerContext.replicasOnBrokers(newBrokersSet)</div><div class="line">  <span class="comment">//note: 将这些副本的状态设置为 OnlineReplica</span></div><div class="line">  replicaStateMachine.handleStateChanges(allReplicasOnNewBrokers, <span class="type">OnlineReplica</span>)</div><div class="line">  <span class="comment">// when a new broker comes up, the controller needs to trigger leader election for all new and offline partitions</span></div><div class="line">  <span class="comment">// to see if these brokers can become leaders for some/all of those</span></div><div class="line">  <span class="comment">//note: 新的 broker 上线也会触发所有处于 new/offline 的 partition 进行 leader 选举</span></div><div class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</div><div class="line">  <span class="comment">// check if reassignment of some partitions need to be restarted</span></div><div class="line">  <span class="comment">//note: 检查是否副本的重新分配分配到了这台机器上</span></div><div class="line">  <span class="keyword">val</span> partitionsWithReplicasOnNewBrokers = controllerContext.partitionsBeingReassigned.filter &#123;</div><div class="line">    <span class="keyword">case</span> (_, reassignmentContext) =&gt; reassignmentContext.newReplicas.exists(newBrokersSet.contains(_))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 如果需要副本进行迁移的话,就执行副本迁移操作</span></div><div class="line">  partitionsWithReplicasOnNewBrokers.foreach(p =&gt; onPartitionReassignment(p._1, p._2))</div><div class="line">  <span class="comment">// check if topic deletion needs to be resumed. If at least one replica that belongs to the topic being deleted exists</span></div><div class="line">  <span class="comment">// on the newly restarted brokers, there is a chance that topic deletion can resume</span></div><div class="line">  <span class="comment">//note: 检查 topic 删除操作是否需要重新启动</span></div><div class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnNewBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123;</div><div class="line">    info((<span class="string">"Some replicas %s for topics scheduled for deletion %s are on the newly restarted brokers %s. "</span> +</div><div class="line">      <span class="string">"Signaling restart of topic deletion for these topics"</span>).format(replicasForTopicsToBeDeleted.mkString(<span class="string">","</span>),</div><div class="line">      deleteTopicManager.topicsToBeDeleted.mkString(<span class="string">","</span>), newBrokers.mkString(<span class="string">","</span>)))</div><div class="line">    deleteTopicManager.resumeDeletionForTopics(replicasForTopicsToBeDeleted.map(_.topic))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>onBrokerStartup()</code> 方法在实现的逻辑上分为以下几步：</p>
<ol>
<li>调用 <code>sendUpdateMetadataRequest()</code> 方法向当前集群所有存活的 Broker 发送 Update Metadata 请求，这样的话其他的节点就会知道当前的 Broker 已经上线了；</li>
<li>获取当前节点分配的所有的 Replica 列表，并将其状态转移为 OnlineReplica 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 leader 选举，如果 leader 选举成功，那么该 Partition 的状态就会转移到 OnlinePartition 状态，否则状态转移失败；</li>
<li>如果副本迁移中有新的 Replica 落在这台新上线的节点上，那么开始执行副本迁移操作（见<a href="http://matt33.com/2018/06/16/partition-reassignment/">Kafka 源码解析之 Partition 副本迁移实现</a>）;</li>
<li>如果之前由于这个 Topic 设置为删除标志，但是由于其中有 Replica 掉线而导致无法删除，这里在节点启动后，尝试重新执行删除操作。</li>
</ol>
<p>到此为止，一台 Broker 算是真正加入到了 Kafka 的集群中，在上述过程中，涉及到 leader 选举的操作，都会触发 LeaderAndIsr 请求及 Metadata 请求的发送。</p>
<h2 id="Broker-掉线"><a href="#Broker-掉线" class="headerlink" title="Broker 掉线"></a>Broker 掉线</h2><p>本节主要讲述一台 Broker 掉线后的处理过程，正如前面图中所示，一台 Broker 掉线后主要有以下两步：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">controllerContext.controllerChannelManager.removeBroker</div><div class="line">controller.onBrokerFailure(deadBrokerIdsSorted)</div></pre></td></tr></table></figure>
<ol>
<li>首先在 Controller Channel Manager 中移除该 Broker 节点，主要的内容是：关闭 Controller  与 Broker 的连接和相应的请求发送线程，并清空请求队列；</li>
<li>调用 Controller 的 <code>onBrokerFailure()</code> 方法下线该节点。</li>
</ol>
<p>Controller Channel Manager 下线 Broker 的处理如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeBroker</span></span>(brokerId: <span class="type">Int</span>) &#123;</div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    removeExistingBroker(brokerStateInfo(brokerId))</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 移除旧的 broker（关闭网络连接、关闭请求发送线程）</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">removeExistingBroker</span></span>(brokerState: <span class="type">ControllerBrokerStateInfo</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    brokerState.networkClient.close()</div><div class="line">    brokerState.messageQueue.clear()</div><div class="line">    brokerState.requestSendThread.shutdown()</div><div class="line">    brokerStateInfo.remove(brokerState.brokerNode.id)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while removing broker by the controller"</span>, e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 Controller Channel Manager 处理完掉线的 Broker 节点后，下面 KafkaController 将会调用 <code>onBrokerFailure()</code> 进行相应的处理，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法会被副本状态机调用（进行 broker 节点下线操作）</span></div><div class="line"><span class="comment">//note: 1. 将 leader 在这台机器上的分区设置为 Offline</span></div><div class="line"><span class="comment">//note: 2. 通过 OfflinePartitionLeaderSelector 为 new/offline partition 选举新的 leader</span></div><div class="line"><span class="comment">//note: 3. leader 选举后,发送 LeaderAndIsr 请求给该分区所有存活的副本;</span></div><div class="line"><span class="comment">//note: 4. 分区选举 leader 后,状态更新为 Online</span></div><div class="line"><span class="comment">//note: 5. 要下线的 broker 上的所有 replica 改为 Offline 状态</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onBrokerFailure</span></span>(deadBrokers: <span class="type">Seq</span>[<span class="type">Int</span>]) &#123;</div><div class="line">  info(<span class="string">"Broker failure callback for %s"</span>.format(deadBrokers.mkString(<span class="string">","</span>)))</div><div class="line">  <span class="comment">//note: 从正在下线的 broker 集合中移除已经下线的机器</span></div><div class="line">  <span class="keyword">val</span> deadBrokersThatWereShuttingDown =</div><div class="line">    deadBrokers.filter(id =&gt; controllerContext.shuttingDownBrokerIds.remove(id))</div><div class="line">  info(<span class="string">"Removed %s from list of shutting down brokers."</span>.format(deadBrokersThatWereShuttingDown))</div><div class="line">  <span class="keyword">val</span> deadBrokersSet = deadBrokers.toSet</div><div class="line">  <span class="comment">// trigger OfflinePartition state for all partitions whose current leader is one amongst the dead brokers</span></div><div class="line">  <span class="comment">//note: 1. 将 leader 在这台机器上的、并且未设置删除的分区状态设置为 Offline</span></div><div class="line">  <span class="keyword">val</span> partitionsWithoutLeader = controllerContext.partitionLeadershipInfo.filter(partitionAndLeader =&gt;</div><div class="line">    deadBrokersSet.contains(partitionAndLeader._2.leaderAndIsr.leader) &amp;&amp;</div><div class="line">      !deleteTopicManager.isTopicQueuedUpForDeletion(partitionAndLeader._1.topic)).keySet</div><div class="line">  partitionStateMachine.handleStateChanges(partitionsWithoutLeader, <span class="type">OfflinePartition</span>)</div><div class="line">  <span class="comment">// trigger OnlinePartition state changes for offline or new partitions</span></div><div class="line">  <span class="comment">//note: 2. 选举 leader, 选举成功后设置为 Online 状态</span></div><div class="line">  partitionStateMachine.triggerOnlinePartitionStateChange()</div><div class="line">  <span class="comment">// filter out the replicas that belong to topics that are being deleted</span></div><div class="line">  <span class="comment">//note: 过滤出 replica 在这个机器上、并且没有被设置为删除的 topic 列表</span></div><div class="line">  <span class="keyword">var</span> allReplicasOnDeadBrokers = controllerContext.replicasOnBrokers(deadBrokersSet)</div><div class="line">  <span class="keyword">val</span> activeReplicasOnDeadBrokers = allReplicasOnDeadBrokers.filterNot(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="comment">// handle dead replicas</span></div><div class="line">  <span class="comment">//note: 将这些 replica 状态转为 Offline</span></div><div class="line">  replicaStateMachine.handleStateChanges(activeReplicasOnDeadBrokers, <span class="type">OfflineReplica</span>)</div><div class="line">  <span class="comment">// check if topic deletion state for the dead replicas needs to be updated</span></div><div class="line">  <span class="comment">//note: 过滤设置为删除的 replica</span></div><div class="line">  <span class="keyword">val</span> replicasForTopicsToBeDeleted = allReplicasOnDeadBrokers.filter(p =&gt; deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))</div><div class="line">  <span class="keyword">if</span>(replicasForTopicsToBeDeleted.nonEmpty) &#123; <span class="comment">//note: 将上面这个 topic 列表的 topic 标记为删除失败</span></div><div class="line">    <span class="comment">// it is required to mark the respective replicas in TopicDeletionFailed state since the replica cannot be</span></div><div class="line">    <span class="comment">// deleted when the broker is down. This will prevent the replica from being in TopicDeletionStarted state indefinitely</span></div><div class="line">    <span class="comment">// since topic deletion cannot be retried until at least one replica is in TopicDeletionStarted state</span></div><div class="line">    deleteTopicManager.failReplicaDeletion(replicasForTopicsToBeDeleted)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// If broker failure did not require leader re-election, inform brokers of failed broker</span></div><div class="line">  <span class="comment">// Note that during leader re-election, brokers update their metadata</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithoutLeader.isEmpty) &#123;</div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 对于掉线 Broker 的处理过程主要有以下几步：</p>
<ol>
<li>首先找到 Leader 在该 Broker 上所有 Partition 列表，然后将这些 Partition 的状态全部转移为 OfflinePartition 状态；</li>
<li>触发 PartitionStateMachine 的 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有处于 NewPartition/OfflinePartition 状态的 Partition 进行 Leader 选举，如果 Leader 选举成功，那么该 Partition 的状态就会迁移到 OnlinePartition 状态，否则状态转移失败（Broker 上线/掉线、Controller 初始化时都会触发这个方法）；</li>
<li>获取在该 Broker 上的所有 Replica 列表，将其状态转移成 OfflineReplica 状态；</li>
<li>过滤出设置为删除、并且有副本在该节点上的 Topic 列表，先将该 Replica 的转移成 ReplicaDeletionIneligible 状态，然后再将该 Topic 标记为非法删除，即因为有 Replica 掉线导致该 Topic 无法删除；</li>
<li>如果 leader 在该 Broker 上所有 Partition 列表不为空，证明有 Partition 的 leader 需要选举，在最后一步会触发全局 metadata 信息的更新。</li>
</ol>
<p>到这里，一台掉线的 Broker 算是真正下线完成了。</p>
<h2 id="Broker-优雅下线"><a href="#Broker-优雅下线" class="headerlink" title="Broker 优雅下线"></a>Broker 优雅下线</h2><p>前面部分是关于通过监听节点变化来实现对 Broker 的上下线，这也是 Kafka 上下线 Broker 的主要流程，但是还有一种情况是：主动关闭 Kafka 服务，这种情况又被称为 Broker 的优雅关闭。</p>
<p>优雅关闭的节点会向 Controller 发送 ControlledShutdownRequest 请求，Controller 在收到这个情况会进行相应的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleControlledShutdownRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="comment">// ensureTopicExists is only for client facing requests</span></div><div class="line">  <span class="comment">// We can't have the ensureTopicExists check here since the controller sends it as an advisory to all brokers so they</span></div><div class="line">  <span class="comment">// stop serving data to clients for the topic being deleted</span></div><div class="line">  <span class="keyword">val</span> controlledShutdownRequest = request.requestObj.asInstanceOf[<span class="type">ControlledShutdownRequest</span>]</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断该连接是否经过认证</span></div><div class="line">  authorizeClusterAction(request)</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理该请求</span></div><div class="line">  <span class="keyword">val</span> partitionsRemaining = controller.shutdownBroker(controlledShutdownRequest.brokerId)</div><div class="line">  <span class="comment">//note: 返回的 response</span></div><div class="line">  <span class="keyword">val</span> controlledShutdownResponse = <span class="keyword">new</span> <span class="type">ControlledShutdownResponse</span>(controlledShutdownRequest.correlationId,</div><div class="line">    <span class="type">Errors</span>.<span class="type">NONE</span>.code, partitionsRemaining)</div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, <span class="keyword">new</span> <span class="type">RequestOrResponseSend</span>(request.connectionId, controlledShutdownResponse)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 在接收这个关闭服务的请求，通过 <code>shutdownBroker()</code> 方法进行处理，实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 优雅地关闭 Broker</span></div><div class="line"><span class="comment">//note: controller 首先决定将这个 broker 上的 leader 迁移到其他可用的机器上</span></div><div class="line"><span class="comment">//note: 返回还没有 leader 的迁移的 TopicPartition 集合</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownBroker</span></span>(id: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">TopicAndPartition</span>] = &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (!isActive) &#123;</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(<span class="string">"Controller moved to another broker. Aborting controlled shutdown"</span>)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  controllerContext.brokerShutdownLock synchronized &#123; <span class="comment">//note: 拿到 broker shutdown 的唯一锁</span></div><div class="line">    info(<span class="string">"Shutting down broker "</span> + id)</div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 拿到 controllerLock 的排它锁</span></div><div class="line">      <span class="keyword">if</span> (!controllerContext.liveOrShuttingDownBrokerIds.contains(id))</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BrokerNotAvailableException</span>(<span class="string">"Broker id %d does not exist."</span>.format(id))</div><div class="line"></div><div class="line">      controllerContext.shuttingDownBrokerIds.add(id) <span class="comment">//note: 将 broker id 添加到正在关闭的 broker 列表中</span></div><div class="line">      debug(<span class="string">"All shutting down brokers: "</span> + controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>))</div><div class="line">      debug(<span class="string">"Live brokers: "</span> + controllerContext.liveBrokerIds.mkString(<span class="string">","</span>))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 获取这个 broker 上所有 Partition 与副本数的 map</span></div><div class="line">    <span class="keyword">val</span> allPartitionsAndReplicationFactorOnBroker: <span class="type">Set</span>[(<span class="type">TopicAndPartition</span>, <span class="type">Int</span>)] =</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        controllerContext.partitionsOnBroker(id)</div><div class="line">          .map(topicAndPartition =&gt; (topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition).size))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 处理这些 TopicPartition，更新 Partition 或 Replica 的状态，必要时进行 leader 选举</span></div><div class="line">    allPartitionsAndReplicationFactorOnBroker.foreach &#123;</div><div class="line">      <span class="keyword">case</span>(topicAndPartition, replicationFactor) =&gt;</div><div class="line">        <span class="comment">// Move leadership serially to relinquish lock.</span></div><div class="line">        inLock(controllerContext.controllerLock) &#123;</div><div class="line">          controllerContext.partitionLeadershipInfo.get(topicAndPartition).foreach &#123; currLeaderIsrAndControllerEpoch =&gt;</div><div class="line">            <span class="keyword">if</span> (replicationFactor &gt; <span class="number">1</span>) &#123; <span class="comment">//note: 副本数大于1</span></div><div class="line">              <span class="keyword">if</span> (currLeaderIsrAndControllerEpoch.leaderAndIsr.leader == id) &#123; <span class="comment">//note: leader 正好是下线的节点</span></div><div class="line">                <span class="comment">// If the broker leads the topic partition, transition the leader and update isr. Updates zk and</span></div><div class="line">                <span class="comment">// notifies all affected brokers</span></div><div class="line">                <span class="comment">//todo: 这种情况下 Replica 的状态不需要修改么？（Replica 的处理还是通过监听器还实现的,这里只是在服务关闭前进行 leader 切换和停止副本同步）</span></div><div class="line">                <span class="comment">//note: 状态变化（变为 OnlinePartition，并且进行 leader 选举，使用 controlledShutdownPartitionLeaderSelector 算法）</span></div><div class="line">                partitionStateMachine.handleStateChanges(<span class="type">Set</span>(topicAndPartition), <span class="type">OnlinePartition</span>,</div><div class="line">                  controlledShutdownPartitionLeaderSelector)</div><div class="line">              &#125; <span class="keyword">else</span> &#123;</div><div class="line">                <span class="comment">// Stop the replica first. The state change below initiates ZK changes which should take some time</span></div><div class="line">                <span class="comment">// before which the stop replica request should be completed (in most cases)</span></div><div class="line">                <span class="keyword">try</span> &#123; <span class="comment">//note: 要下线的机器停止副本迁移，发送 StopReplica 请求</span></div><div class="line">                  brokerRequestBatch.newBatch()</div><div class="line">                  brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">Seq</span>(id), topicAndPartition.topic,</div><div class="line">                    topicAndPartition.partition, deletePartition = <span class="literal">false</span>)</div><div class="line">                  brokerRequestBatch.sendRequestsToBrokers(epoch)</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> e : <span class="type">IllegalStateException</span> =&gt; &#123;</div><div class="line">                    <span class="comment">// Resign if the controller is in an illegal state</span></div><div class="line">                    error(<span class="string">"Forcing the controller to resign"</span>)</div><div class="line">                    brokerRequestBatch.clear()</div><div class="line">                    controllerElector.resign()</div><div class="line"></div><div class="line">                    <span class="keyword">throw</span> e</div><div class="line">                  &#125;</div><div class="line">                &#125;</div><div class="line">                <span class="comment">// If the broker is a follower, updates the isr in ZK and notifies the current leader</span></div><div class="line">                <span class="comment">//note: 更新这个副本的状态，变为 OfflineReplica</span></div><div class="line">                replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="type">PartitionAndReplica</span>(topicAndPartition.topic,</div><div class="line">                  topicAndPartition.partition, id)), <span class="type">OfflineReplica</span>)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 返回 leader 在这个要下线节点上并且副本数大于 1 的 TopicPartition 集合</span></div><div class="line">    <span class="comment">//note: 在已经进行前面 leader 迁移后</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replicatedPartitionsBrokerLeads</span></span>() = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      trace(<span class="string">"All leaders = "</span> + controllerContext.partitionLeadershipInfo.mkString(<span class="string">","</span>))</div><div class="line">      controllerContext.partitionLeadershipInfo.filter &#123;</div><div class="line">        <span class="keyword">case</span> (topicAndPartition, leaderIsrAndControllerEpoch) =&gt;</div><div class="line">          leaderIsrAndControllerEpoch.leaderAndIsr.leader == id &amp;&amp; controllerContext.partitionReplicaAssignment(topicAndPartition).size &gt; <span class="number">1</span></div><div class="line">      &#125;.keys</div><div class="line">    &#125;</div><div class="line">    replicatedPartitionsBrokerLeads().toSet</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上述方法的处理逻辑如下：</p>
<ol>
<li>先将要下线的 Broker 添加到 shuttingDownBrokerIds 集合中，该集合记录了当前正在进行关闭的 broker 列表；</li>
<li>获取副本在该节点上的所有 Partition 的列表集合；</li>
<li>遍历上述 Partition 列表进行处理：如果该 Partition 的 leader 是要下线的节点，那么通过 PartitionStateMachine 进行状态转移（OnlinePartition –&gt; OnlinePartition）触发 leader 选举，使用的 leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#ControlledShutdownLeaderSelector">ControlledShutdownLeaderSelector</a>，它会选举 isr 中第一个没有正在关闭的 Replica 作为 leader，否则抛出 StateChangeFailedException 异常；</li>
<li>否则的话，即要下线的节点不是 leader，那么就向要下线的节点发送 StopReplica 请求停止副本同步，并将该副本设置为 OfflineReplica 状态，这里对 Replica 进行处理的原因是为了让要下线的机器关闭副本同步流程，这样 Kafka 服务才能正常关闭。</li>
</ol>
<p>我在看这部分的代码是有一个疑问的，那就是如果要下线的节点是 Partition leader 的情况下，并没有对 Replica 进行相应的处理，这里的原因是，这部分 Replica 的处理可以放在 <code>onBrokerFailure()</code> 方法中处理，即使通过优雅下线的方法下线了 Broker，但是监听 ZK 的 BrokerChangeListener 监听器还是会被触发的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇接着讲述 Controller 对于监听器的处理内容 —— Broker 节点上下线的处理流程。每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 &lt;code&gt;/brokers/ids&lt;/code&gt; 下注册一个节点，节点名字就是 brok
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Partition 副本迁移实现（十八）</title>
    <link href="http://matt33.com/2018/06/16/partition-reassignment/"/>
    <id>http://matt33.com/2018/06/16/partition-reassignment/</id>
    <published>2018-06-16T15:40:00.000Z</published>
    <updated>2018-06-18T05:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Controller 在初始化时，会利用 ZK 的 watch 机制注册很多不同类型的监听器，当监听的事件被触发时，Controller 就会触发相应的操作。</p>
<p>Controller 在初始化时，会注册多种类型的监听器，主要有以下6种：</p>
<ol>
<li>监听 <code>/admin/reassign_partitions</code> 节点，用于分区副本迁移的监听；</li>
<li>监听 <code>/isr_change_notification</code> 节点，用于 Partition Isr 变动的监听，；</li>
<li>监听 <code>/admin/preferred_replica_election</code> 节点，用于需要进行 Partition 最优 leader 选举的监听；</li>
<li>监听 <code>/brokers/topics</code> 节点，用于 Topic 新建的监听；</li>
<li>监听 <code>/brokers/topics/TOPIC_NAME</code> 节点，用于 Topic Partition 扩容的监听；</li>
<li>监听 <code>/admin/delete_topics</code> 节点，用于 Topic 删除的监听；</li>
<li>监听 <code>/brokers/ids</code> 节点，用于 Broker 上下线的监听。</li>
</ol>
<p>本文主要讲解第一部分，也就是 Controller 对 Partition 副本迁移的处理，后续会单独一篇文章讲述 Topic 的新建、扩容和删除，再单独一篇文章讲述 Broker 的上下线，另外两部分将会在对 LeaderAndIsr 请求处理的文章中讲述。</p>
<h2 id="Partition-副本迁移整体流程"><a href="#Partition-副本迁移整体流程" class="headerlink" title="Partition 副本迁移整体流程"></a>Partition 副本迁移整体流程</h2><p>Partition 的副本迁移实际上就是将分区的副本重新分配到不同的代理节点上，如果 zk 中新副本的集合与 Partition 原来的副本集合相同，那么这个副本就不需要重新分配了。</p>
<p>Partition 的副本迁移是通过监听 zk 的 <code>/admin/reassign_partitions</code> 节点触发的，Kafka 也向用户提供相应的脚本工具进行副本迁移，副本迁移的脚本使用方法如下所示：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/kafka-reassign-partitions.sh --zookeeper XXX --reassignment-json-file XXX.json --execute</div></pre></td></tr></table></figure>
<p>其中 XXX.json 为要进行 Partition 副本迁移的 json 文件，json 文件的格式如下所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"version"</span>:<span class="number">1</span>,</div><div class="line">    <span class="attr">"partitions"</span>:[</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">19</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">9</span>,</div><div class="line">                <span class="number">2</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">26</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">2</span>,</div><div class="line">                <span class="number">6</span>,</div><div class="line">                <span class="number">4</span></div><div class="line">            ]</div><div class="line">        &#125;,</div><div class="line">        &#123;</div><div class="line">            <span class="attr">"topic"</span>:<span class="string">"__consumer_offsets"</span>,</div><div class="line">            <span class="attr">"partition"</span>:<span class="number">27</span>,</div><div class="line">            <span class="attr">"replicas"</span>:[</div><div class="line">                <span class="number">5</span>,</div><div class="line">                <span class="number">3</span>,</div><div class="line">                <span class="number">8</span></div><div class="line">            ]</div><div class="line">        &#125;</div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个 json 文件的意思是将 Topic <code>__consumer_offsets</code> Partition 19 的副本迁移到 {3, 2, 9} 上，Partition 26 的副本迁移到 {6, 2, 4} 上，Partition 27 的副本迁移到 {5, 3, 8} 上。</p>
<p>在调用脚本向 zk 提交 Partition 的迁移计划时，迁移计划更新到 zk 前需要进行一步判断，如果该节点（写入迁移计划的节点）已经存在，即副本迁移还在进行，那么本次副本迁移计划是无法提交的，实现的逻辑如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">executeAssignment</span></span>(zkUtils: <span class="type">ZkUtils</span>, reassignmentJsonString: <span class="type">String</span>, throttle: <span class="type">Long</span> = <span class="number">-1</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsToBeReassigned = parseAndValidate(zkUtils, reassignmentJsonString)</div><div class="line">  <span class="keyword">val</span> reassignPartitionsCommand = <span class="keyword">new</span> <span class="type">ReassignPartitionsCommand</span>(zkUtils, partitionsToBeReassigned.toMap)</div><div class="line"></div><div class="line">  <span class="comment">// If there is an existing rebalance running, attempt to change its throttle</span></div><div class="line">  <span class="comment">//note: 如果副本迁移正在进行,那么这次的副本迁移计划是无法提交的</span></div><div class="line">  <span class="keyword">if</span> (zkUtils.pathExists(<span class="type">ZkUtils</span>.<span class="type">ReassignPartitionsPath</span>)) &#123;</div><div class="line">    println(<span class="string">"There is an existing assignment running."</span>)</div><div class="line">    reassignPartitionsCommand.maybeLimit(throttle)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    printCurrentAssignment(zkUtils, partitionsToBeReassigned)</div><div class="line">    <span class="keyword">if</span> (throttle &gt;= <span class="number">0</span>)</div><div class="line">      println(<span class="type">String</span>.format(<span class="string">"Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value."</span>))</div><div class="line">    <span class="comment">//note: 将迁移计划更新到 zk 上</span></div><div class="line">    <span class="keyword">if</span> (reassignPartitionsCommand.reassignPartitions(throttle)) &#123;</div><div class="line">      println(<span class="string">"Successfully started reassignment of partitions."</span>)</div><div class="line">    &#125; <span class="keyword">else</span></div><div class="line">      println(<span class="string">"Failed to reassign partitions %s"</span>.format(partitionsToBeReassigned))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在迁移计划提交到 zk 之后，Controller 的 PartitionsReassignedListener 就会被触发，Controller 开始 Partition 的副本迁移，触发之后 Controller 的处理流程大体如下图所示：</p>
<p><img src="/images/kafka/partition_reassignment.png" alt="Partition 迁移过程"></p>
<h2 id="PartitionsReassignedListener-副本迁移处理"><a href="#PartitionsReassignedListener-副本迁移处理" class="headerlink" title="PartitionsReassignedListener 副本迁移处理"></a>PartitionsReassignedListener 副本迁移处理</h2><p>在 zk 的 <code>/admin/reassign_partitions</code> 节点数据有变化时，就会触发 PartitionsReassignedListener 的 <code>doHandleDataChange()</code> 方法，实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 开始进行 partition reassignment 除非这三种情况发生:</span></div><div class="line"><span class="comment">//note: 1. 这个 partition 的 reassignment 之前已经存在, 即正在迁移中;</span></div><div class="line"><span class="comment">//note: 2. new replica 与已经存在的 replicas 相同;</span></div><div class="line"><span class="comment">//note: 3. Partition 所有新分配 replica 都已经 dead;</span></div><div class="line"><span class="comment">//note: 这种情况发生时,会输出一条日志,并从 zk 移除该 Partition 的迁移计划。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionsReassignedListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span></span>) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"PartitionsReassignedListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when some partitions are reassigned by the admin command</div><div class="line">   *</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当一些分区需要进行迁移时</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    debug(<span class="string">"Partitions reassigned listener fired for path %s. Record partitions to be reassigned %s"</span></div><div class="line">      .format(dataPath, data))</div><div class="line">    <span class="keyword">val</span> partitionsReassignmentData = <span class="type">ZkUtils</span>.parsePartitionReassignmentData(data.toString)</div><div class="line">    <span class="keyword">val</span> partitionsToBeReassigned = inLock(controllerContext.controllerLock) &#123; <span class="comment">//note: 需要迁移的新副本</span></div><div class="line">      <span class="comment">//note: 过滤掉正在迁移的副本,如果 Partition 正在迁移,这一波迁移完之前不允许再次迁移</span></div><div class="line">      partitionsReassignmentData.filterNot(p =&gt; controllerContext.partitionsBeingReassigned.contains(p._1))</div><div class="line">    &#125;</div><div class="line">    partitionsToBeReassigned.foreach &#123; partitionToBeReassigned =&gt;</div><div class="line">      inLock(controllerContext.controllerLock) &#123;</div><div class="line">        <span class="keyword">if</span>(controller.deleteTopicManager.isTopicQueuedUpForDeletion(partitionToBeReassigned._1.topic)) &#123;</div><div class="line">          <span class="comment">//note: 如果这个 topic 已经设置了删除，那么就不会进行迁移了（从需要副本迁移的集合中移除）</span></div><div class="line">          error(<span class="string">"Skipping reassignment of partition %s for topic %s since it is currently being deleted"</span></div><div class="line">            .format(partitionToBeReassigned._1, partitionToBeReassigned._1.topic))</div><div class="line">          controller.removePartitionFromReassignedPartitions(partitionToBeReassigned._1)</div><div class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 添加到需要迁移的副本集合中</span></div><div class="line">          <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">ReassignedPartitionsContext</span>(partitionToBeReassigned._2)</div><div class="line">          controller.initiateReassignReplicasForTopicPartition(partitionToBeReassigned._1, context)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果 Partition 出现下面的情况，将不会进行副本迁移，直接将 Partition 的迁移计划从 ZK 移除：</p>
<ol>
<li>这个 Partition 的 reassignment 之前已经存在, 即正在迁移中;</li>
<li>这个 Partition 新分配的 replica 与之前的 replicas 相同;</li>
<li>这个 Partition 所有新分配 replica 都已经 dead;</li>
<li>这个 Partition 已经被设置了删除标志。</li>
</ol>
<p>对于可以进行副本迁移的 Partition 集合，这里将会调用 Kafka Controller 的 <code>initiateReassignReplicasForTopicPartition()</code> 方法对每个 Partition 进行处理。</p>
<h2 id="副本迁移初始化"><a href="#副本迁移初始化" class="headerlink" title="副本迁移初始化"></a>副本迁移初始化</h2><p>进行了前面的判断后，这个 Partition 满足了可以迁移的条件，Controller 会首先初始化副本迁移的流程，实现如下所示：</p>
<blockquote>
<p>如果 Partition 新分配的 replica 与之前的 replicas 相同，那么不会进行副本迁移，这部分的判断实际上是在这里实现的，前面只是为了更好地讲述。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化 Topic-Partition 的副本迁移</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initiateReassignReplicasForTopicPartition</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>,</div><div class="line">                                      reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</div><div class="line">  <span class="comment">//note: 要迁移的 topic-partition，及新的副本</span></div><div class="line">  <span class="keyword">val</span> newReplicas = reassignedPartitionContext.newReplicas</div><div class="line">  <span class="keyword">val</span> topic = topicAndPartition.topic</div><div class="line">  <span class="keyword">val</span> partition = topicAndPartition.partition</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> assignedReplicasOpt = controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="comment">//note: partition 的 AR</span></div><div class="line">    assignedReplicasOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</div><div class="line">        <span class="keyword">if</span> (assignedReplicas == newReplicas) &#123; <span class="comment">//note: 不需要迁移</span></div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Partition %s to be reassigned is already assigned to replicas"</span>.format(topicAndPartition) +</div><div class="line">            <span class="string">" %s. Ignoring request for partition reassignment"</span>.format(newReplicas.mkString(<span class="string">","</span>)))</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          info(<span class="string">"Handling reassignment of partition %s to new replicas %s"</span>.format(topicAndPartition, newReplicas.mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// first register ISR change listener</span></div><div class="line">          <span class="comment">//note: 首先注册 ISR 监听的变化</span></div><div class="line">          watchIsrChangesForReassignedPartition(topic, partition, reassignedPartitionContext)</div><div class="line">          <span class="comment">//note: 正在迁移 Partition 添加到缓存中</span></div><div class="line">          controllerContext.partitionsBeingReassigned.put(topicAndPartition, reassignedPartitionContext)</div><div class="line">          <span class="comment">// mark topic ineligible for deletion for the partitions being reassigned</span></div><div class="line">          <span class="comment">//note: 设置正在迁移的副本为不能删除</span></div><div class="line">          deleteTopicManager.markTopicIneligibleForDeletion(<span class="type">Set</span>(topic))</div><div class="line">          <span class="comment">//note: 进行副本迁移</span></div><div class="line">          onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</div><div class="line">        &#125;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Attempt to reassign partition %s that doesn't exist"</span></div><div class="line">        .format(topicAndPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error completing reassignment of partition %s"</span>.format(topicAndPartition), e)</div><div class="line">    <span class="comment">// remove the partition from the admin path to unblock the admin client</span></div><div class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于副本迁移流程初始化如下：</p>
<ol>
<li>通过 <code>watchIsrChangesForReassignedPartition()</code> 方法监控这个 Partition 的 LeaderAndIsr 变化，如果有新的副本数据同步完成，那么 leader 会将其加到 isr 中更新到 zk 中，这时候 Controller 是可以接收到相关的信息通知的；</li>
<li>将正在迁移的 Partition 添加到 partitionsBeingReassigned 中，它会记录当前正在迁移的 Partition 列表；</li>
<li>将要迁移的 Topic 设置为非法删除删除状态，在这个状态的 Topic 是无法进行删除的；</li>
<li>调用 <code>onPartitionReassignment()</code>，进行副本迁移。</li>
</ol>
<p>在第一步中，会向这个 Partition 注册一个额外的监听器，监听其 LeaderAndIsr 信息变化，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: ISR 变动的监听器（这个不是由 leader 主动触发的，而是 controller 自己触发的，主要用于 partition 迁移时，isr 变动的监听处理）</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionsIsrChangeListener</span>(<span class="params">protected val controller: <span class="type">KafkaController</span>, topic: <span class="type">String</span>, partition: <span class="type">Int</span>,</span></span></div><div class="line">                                            reassignedReplicas: <span class="type">Set</span>[<span class="type">Int</span>]) <span class="keyword">extends</span> <span class="title">ControllerZkDataListener</span> &#123;</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = controller.controllerContext.zkUtils</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> controllerContext = controller.controllerContext</div><div class="line"></div><div class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logName</span> </span>= <span class="string">"ReassignedPartitionsIsrChangeListener"</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Invoked when some partitions need to move leader to preferred replica</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">AnyRef</span>) &#123;</div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      debug(<span class="string">"Reassigned partitions isr change listener fired for path %s with children %s"</span>.format(dataPath, data))</div><div class="line">      <span class="keyword">val</span> topicAndPartition = <span class="type">TopicAndPartition</span>(topic, partition)</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">// check if this partition is still being reassigned or not</span></div><div class="line">        <span class="comment">//note: 检查这个副本是不是还在迁移中（这个方法只用于副本迁移中）</span></div><div class="line">        controllerContext.partitionsBeingReassigned.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(reassignedPartitionContext) =&gt;</div><div class="line">            <span class="comment">// need to re-read leader and isr from zookeeper since the zkclient callback doesn't return the Stat object</span></div><div class="line">            <span class="comment">//note: 从 zk 获取最新的 leader 和 isr 信息</span></div><div class="line">            <span class="keyword">val</span> newLeaderAndIsrOpt = zkUtils.getLeaderAndIsrForPartition(topic, partition)</div><div class="line">            newLeaderAndIsrOpt <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Some</span>(leaderAndIsr) =&gt; <span class="comment">// check if new replicas have joined ISR</span></div><div class="line">                <span class="keyword">val</span> caughtUpReplicas = reassignedReplicas &amp; leaderAndIsr.isr.toSet</div><div class="line">                <span class="keyword">if</span>(caughtUpReplicas == reassignedReplicas) &#123; <span class="comment">//note: 新分配的副本已经全部在 isr 中了</span></div><div class="line">                  <span class="comment">// resume the partition reassignment process</span></div><div class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></div><div class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</div><div class="line">                    <span class="string">"Resuming partition reassignment"</span>)</div><div class="line">                  <span class="comment">//note: 再次触发 onPartitionReassignment 方法,副本已经迁移完成</span></div><div class="line">                  controller.onPartitionReassignment(topicAndPartition, reassignedPartitionContext)</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span> &#123;  <span class="comment">//note: 否则不进行任何处理</span></div><div class="line">                  info(<span class="string">"%d/%d replicas have caught up with the leader for partition %s being reassigned."</span></div><div class="line">                    .format(caughtUpReplicas.size, reassignedReplicas.size, topicAndPartition) +</div><div class="line">                    <span class="string">"Replica(s) %s still need to catch up"</span>.format((reassignedReplicas -- leaderAndIsr.isr.toSet).mkString(<span class="string">","</span>)))</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">None</span> =&gt; error(<span class="string">"Error handling reassignment of partition %s to replicas %s as it was never created"</span></div><div class="line">                .format(topicAndPartition, reassignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">            &#125;</div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while handling partition reassignment"</span>, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doHandleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>如果该 Partition 的 LeaderAndIsr 信息有变动，那么就会触发这个 listener 的 <code>doHandleDataChange()</code> 方法：</p>
<ol>
<li>首先检查这个 Partition 是否在还在迁移中，不在的话直接结束流程，因为这个监听器本来就是为了 Partition 副本迁移而服务的；</li>
<li>从 zk 获取最新的 leader 和 isr 信息，如果新分配的副本全部都在 isr 中，那么就再次触发 controller 的 <code>onPartitionReassignment()</code> 方法，再次调用时实际上已经证明了这个 Partition 的副本迁移已经完成，否则的话就会不进行任何处理，等待新分配的所有副本迁移完成。</li>
</ol>
<h2 id="副本迁移"><a href="#副本迁移" class="headerlink" title="副本迁移"></a>副本迁移</h2><p>Partition 副本迁移真正实际处理是在 Controller 的 <code>onPartitionReassignment()</code> 方法完成的，在看这个方法之前，先介绍几个基本的概念（假设一个 Partition 原来的 replica 是 {1、2、3}，新分配的副本列表是：{2、3、4}）：</p>
<ul>
<li>RAR = Reassigned replicas，即新分配的副本列表，也就是 {2、3、4}；</li>
<li>OAR = Original list of replicas for partition，即这个 Partition 原来的副本列表，也就是 {1、2、3}；</li>
<li>AR = current assigned replicas，该 Partition 当前的副本列表，这个会随着阶段的不同而变化；</li>
<li>RAR-OAR：需要创建、数据同步的新副本，也就是 {4}；</li>
<li>OAR-RAR：不需要创建、数据同步的副本，也就是{2、3}</li>
</ul>
<p>这个方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个回调方法被 reassigned partitions listener 触发,当需要进行分区副本迁移时,会在【/admin/reassign_partitions】下创建一个节点来触发操作</span></div><div class="line"><span class="comment">//note: RAR: 重新分配的副本, OAR: 这个分区原来的副本列表, AR: 当前的分配的副本</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPartitionReassignment</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, reassignedPartitionContext: <span class="type">ReassignedPartitionsContext</span>) &#123;</div><div class="line">  <span class="keyword">val</span> reassignedReplicas = reassignedPartitionContext.newReplicas</div><div class="line">  <span class="keyword">if</span> (!areReplicasInIsr(topicAndPartition.topic, topicAndPartition.partition, reassignedReplicas)) &#123;</div><div class="line">    <span class="comment">//note: 新分配的并没有权限在 isr 中</span></div><div class="line">    info(<span class="string">"New replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</div><div class="line">      <span class="string">"reassigned not yet caught up with the leader"</span>)</div><div class="line">    <span class="comment">//note: RAR-OAR</span></div><div class="line">    <span class="keyword">val</span> newReplicasNotInOldReplicaList = reassignedReplicas.toSet -- controllerContext.partitionReplicaAssignment(topicAndPartition).toSet</div><div class="line">    <span class="comment">//note: RAR+OAR</span></div><div class="line">    <span class="keyword">val</span> newAndOldReplicas = (reassignedPartitionContext.newReplicas ++ controllerContext.partitionReplicaAssignment(topicAndPartition)).toSet</div><div class="line">    <span class="comment">//1. Update AR in ZK with OAR + RAR.</span></div><div class="line">    updateAssignedReplicasForPartition(topicAndPartition, newAndOldReplicas.toSeq)</div><div class="line">    <span class="comment">//2. Send LeaderAndIsr request to every replica in OAR + RAR (with AR as OAR + RAR).</span></div><div class="line">    updateLeaderEpochAndSendRequest(topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition),</div><div class="line">      newAndOldReplicas.toSeq)</div><div class="line">    <span class="comment">//3. replicas in RAR - OAR -&gt; NewReplica</span></div><div class="line">    <span class="comment">//note: 新分配的副本状态更新为 NewReplica（在第二步中发送 LeaderAndIsr 请求时,新的副本会开始创建并且同步数据）</span></div><div class="line">    startNewReplicasForReassignedPartition(topicAndPartition, reassignedPartitionContext, newReplicasNotInOldReplicaList)</div><div class="line">    info(<span class="string">"Waiting for new replicas %s for partition %s being "</span>.format(reassignedReplicas.mkString(<span class="string">","</span>), topicAndPartition) +</div><div class="line">      <span class="string">"reassigned to catch up with the leader"</span>)</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 新副本全在 isr 中了</span></div><div class="line">    <span class="comment">//4. Wait until all replicas in RAR are in sync with the leader.</span></div><div class="line">   <span class="comment">//note: 【OAR-RAR】</span></div><div class="line">    <span class="keyword">val</span> oldReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).toSet -- reassignedReplicas.toSet</div><div class="line">    <span class="comment">//5. replicas in RAR -&gt; OnlineReplica</span></div><div class="line">    <span class="comment">//note: RAR 中的副本都在 isr 中了,将副本状态设置为 OnlineReplica</span></div><div class="line">    reassignedReplicas.foreach &#123; replica =&gt;</div><div class="line">      replicaStateMachine.handleStateChanges(<span class="type">Set</span>(<span class="keyword">new</span> <span class="type">PartitionAndReplica</span>(topicAndPartition.topic, topicAndPartition.partition,</div><div class="line">        replica)), <span class="type">OnlineReplica</span>)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//6. Set AR to RAR in memory.</span></div><div class="line">    <span class="comment">//7. Send LeaderAndIsr request with a potential new leader (if current leader not in RAR) and</span></div><div class="line">    <span class="comment">//   a new AR (using RAR) and same isr to every broker in RAR</span></div><div class="line">    <span class="comment">//note: 到这一步,新加入的 replica 已经同步完成,leader和isr都更新到最新的结果</span></div><div class="line">    moveReassignedPartitionLeaderIfRequired(topicAndPartition, reassignedPartitionContext)</div><div class="line">    <span class="comment">//8. replicas in OAR - RAR -&gt; Offline (force those replicas out of isr)</span></div><div class="line">    <span class="comment">//9. replicas in OAR - RAR -&gt; NonExistentReplica (force those replicas to be deleted)</span></div><div class="line">    <span class="comment">//note: 下线旧的副本</span></div><div class="line">    stopOldReplicasOfReassignedPartition(topicAndPartition, reassignedPartitionContext, oldReplicas)</div><div class="line">    <span class="comment">//10. Update AR in ZK with RAR.</span></div><div class="line">    updateAssignedReplicasForPartition(topicAndPartition, reassignedReplicas)</div><div class="line">    <span class="comment">//11. Update the /admin/reassign_partitions path in ZK to remove this partition.</span></div><div class="line">    <span class="comment">//note: partition 迁移完成,从待迁移的集合中移除该 Partition</span></div><div class="line">    removePartitionFromReassignedPartitions(topicAndPartition)</div><div class="line">    info(<span class="string">"Removed partition %s from the list of reassigned partitions in zookeeper"</span>.format(topicAndPartition))</div><div class="line">    controllerContext.partitionsBeingReassigned.remove(topicAndPartition)</div><div class="line">    <span class="comment">//12. After electing leader, the replicas and isr information changes, so resend the update metadata request to every broker</span></div><div class="line">    <span class="comment">//note: 发送 metadata 更新请求给所有存活的 broker</span></div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq, <span class="type">Set</span>(topicAndPartition))</div><div class="line">    <span class="comment">// signal delete topic thread if reassignment for some partitions belonging to topics being deleted just completed</span></div><div class="line">    <span class="comment">//note: topic 删除恢复（如果当前 topic 设置了删除,之前由于无法删除）</span></div><div class="line">    deleteTopicManager.resumeDeletionForTopics(<span class="type">Set</span>(topicAndPartition.topic))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法整体分为以下12个步骤：</p>
<ol>
<li>把 AR = OAR+RAR （{1、2、3、4}）更新到 zk 及本地 Controller 缓存中;</li>
<li>发送 LeaderAndIsr 给 AR 中每一个副本,并且会强制更新 zk 中 leader 的 epoch;</li>
<li>创建需要新建的副本（【RAR-OAR】，即 {4}）,将其状态设置为 NewReplica；</li>
<li>等待直到 RAR（{2、3、4}） 中的所有副本都在 ISR 中;</li>
<li>把 RAR（{2、3、4}） 中的所有副本设置为 OnReplica 状态;</li>
<li>将缓存中 AR 更新为 RAR（重新分配的副本列表，即 {2、3、4}）;</li>
<li>如果 leader 不在 RAR 中, 就从 RAR 选择对应的 leader, 然后发送 LeaderAndIsr 请求；如果不需要，那么只会更新 leader epoch，然后发送 LeaderAndIsr 请求; 在发送 LeaderAndIsr 请求前设置了 AR=RAR, 这将确保了 leader 在 isr 中不会添加任何 【RAR-OAR】中的副本（old replica，即 {1}）；</li>
<li>将【OAR-RAR】（{1}）中的副本设置为 OfflineReplica 状态，OfflineReplica 状态的变化，将会从 ISR 中删除【OAR-RAR】的副本，更新到 zk 中并发送 LeaderAndIsr 请求给 leader，通知 leader isr 变动。之后再发送 StopReplica 请求（delete=false）给【OAR-RAR】中的副本；</li>
<li>将【OAR-RAR】中的副本设置为 NonExistentReplica 状态。这将发送 StopReplica 请求（delete=true）给【OAR-RAR】中的副本，这些副本将会从本地上删除数据；</li>
<li>在 zk 中更新 AR 为 RAR；</li>
<li>更新 zk 中路径 【/admin/reassign_partitions】信息，移除已经成功迁移的 Partition；</li>
<li>leader 选举之后，这个 replica 和 isr 信息将会变动，发送 metadata 更新给所有的 broker。</li>
</ol>
<p>上面的流程简单来说，就是先创建新的 replica，开始同步数据，等待所有新的分配都加入到了 isr 中后，开始进行 leader 选举（需要的情况下），下线不需要的副本（OAR-RAR），下线完成后将 Partition 的最新 AR （即 RAR）信息更新到 zk 中，最后发送相应的请求给 broker，到这里一个 Partition 的副本迁移算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面两篇关于 Controller 的内容分别讲述了 Controller 选举和启动，以及副本状态机和分区状态机的内容，从本文开始会详细讲述 Controller 的一些其他功能，主要是 Controller 的对不同类型监听器的处理，这部分预计分三篇左右的文章讲述。Co
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本状态机与分区状态机（十七）</title>
    <link href="http://matt33.com/2018/06/16/controller-state-machine/"/>
    <id>http://matt33.com/2018/06/16/controller-state-machine/</id>
    <published>2018-06-16T03:04:14.000Z</published>
    <updated>2018-08-18T05:49:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副本状态机则是记录着当前集群所有 Replica 的状态信息以及如何对 Replica 状态转变进行相应的处理。</p>
<h2 id="ReplicaStateMachine"><a href="#ReplicaStateMachine" class="headerlink" title="ReplicaStateMachine"></a>ReplicaStateMachine</h2><p>ReplicaStateMachine 记录着集群所有 Replica 的状态信息，它决定着一个 replica 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中副本的状态总共有以下七种类型：</p>
<ol>
<li>NewReplica：这种状态下 Controller 可以创建这个 Replica，这种状态下该 Replica 只能作为 follower，它可以是 Replica 删除后的一个临时状态，它有效的前置状态是 NonExistentReplica；</li>
<li>OnlineReplica：一旦这个 Replica 被分配到指定的 Partition 上，并且 Replica 创建完成，那么它将会被置为这个状态，在这个状态下，这个 Replica 既可以作为 leader 也可以作为 follower，它有效的前置状态是  NewReplica、OnlineReplica 或 OfflineReplica；</li>
<li>OfflineReplica：如果一个 Replica 挂掉（所在的节点宕机或者其他情况），该 Replica 将会被转换到这个状态，它有的效前置状态是 NewReplica、OfflineReplica 或者 OnlineReplica；</li>
<li>ReplicaDeletionStarted：Replica 开始删除时被置为的状态，它有效的前置状态是 OfflineReplica；</li>
<li>ReplicaDeletionSuccessful：如果 Replica 在删除时没有遇到任何错误信息，它将被置为这个状态，这个状态代表该 Replica 的数据已经从节点上清除了，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>ReplicaDeletionIneligible：如果 Replica 删除失败，它将会转移到这个状态，这个状态意思是非法删除，也就是删除是无法成功的，它有效的前置状态是 ReplicaDeletionStarted；</li>
<li>NonExistentReplica：如果 Replica 删除成功，它将被转移到这个状态，它有效的前置状态是：ReplicaDeletionSuccessful。</li>
</ol>
<p>上面的状态中其中后面4是专门为 Replica 删除而服务的，副本状态机转移图如下所示：</p>
<p><img src="/images/kafka/replica_state.png" alt="副本状态机"></p>
<p>这张图是副本状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 ReplicaStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="ReplicaStateMachine-初始化"><a href="#ReplicaStateMachine-初始化" class="headerlink" title="ReplicaStateMachine 初始化"></a>ReplicaStateMachine 初始化</h3><p>副本状态机初始化的过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 重新选举后触发的操作</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// initialize replica state</span></div><div class="line">  <span class="comment">//note: 初始化 zk 上所有的 Replica 状态信息（replica 存活的话设置为 Online,不存活的设置为 ReplicaDeletionIneligible）</span></div><div class="line">  initializeReplicaState()</div><div class="line">  <span class="comment">// set started flag</span></div><div class="line">  hasStarted.set(<span class="literal">true</span>)</div><div class="line">  <span class="comment">// move all Online replicas to Online</span></div><div class="line">  <span class="comment">//note: 将存活的副本状态转变为 OnlineReplica</span></div><div class="line">  handleStateChanges(controllerContext.allLiveReplicas(), <span class="type">OnlineReplica</span>)</div><div class="line"></div><div class="line">  info(<span class="string">"Started replica state machine with initial state -&gt; "</span> + replicaState.toString())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，ReplicaStateMachine 先调用 <code>initializeReplicaState()</code> 方法初始化集群中所有 Replica 的状态信息，如果 Replica 所在机器是 alive 的，那么将其状态设置为 OnlineReplica，否则设置为 ReplicaDeletionIneligible 状态，这里只是将 Replica 的状态信息更新副本状态机的缓存 <code>replicaState</code> 中，并没有真正进行状态转移的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化所有副本的状态信息</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeReplicaState</span></span>() &#123;</div><div class="line">  <span class="keyword">for</span>((topicPartition, assignedReplicas) &lt;- controllerContext.partitionReplicaAssignment) &#123;</div><div class="line">    <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">    <span class="keyword">val</span> partition = topicPartition.partition</div><div class="line">    assignedReplicas.foreach &#123; replicaId =&gt;</div><div class="line">      <span class="keyword">val</span> partitionAndReplica = <span class="type">PartitionAndReplica</span>(topic, partition, replicaId)</div><div class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(replicaId)) <span class="comment">//note: 如果副本是存活,那么将状态都设置为 OnlineReplica</span></div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">// mark replicas on dead brokers as failed for topic deletion, if they belong to a topic to be deleted.</span></div><div class="line">        <span class="comment">// This is required during controller failover since during controller failover a broker can go down,</span></div><div class="line">        <span class="comment">// so the replicas on that broker should be moved to ReplicaDeletionIneligible to be on the safer side.</span></div><div class="line">        <span class="comment">//note: 将不存活的副本状态设置为 ReplicaDeletionIneligible</span></div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>接着第二步调用 <code>handleStateChanges()</code> 将所有存活的副本状态转移为 OnlineReplica 状态，这里才是真正进行状态转移的地方，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 用于处理 Replica 状态的变化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Set</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</div><div class="line">                       callbacks: <span class="type">Callbacks</span> = (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build) &#123;</div><div class="line">  <span class="keyword">if</span>(replicas.nonEmpty) &#123;</div><div class="line">    info(<span class="string">"Invoking state change to %s for replicas %s"</span>.format(targetState, replicas.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      brokerRequestBatch.newBatch()</div><div class="line">      <span class="comment">//note: 状态转变</span></div><div class="line">      replicas.foreach(r =&gt; handleStateChange(r, targetState, callbacks))</div><div class="line">      <span class="comment">//note: 向 broker 发送相应请求</span></div><div class="line">      brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">    &#125;<span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some replicas to %s state"</span>.format(targetState), e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里是副本状态机 <code>startup()</code> 方法的最后一步，它的目的是将所有 alive 的 Replica 状态转移到 OnlineReplica 状态，由于前面已经这些 alive replica 的状态设置成了 OnlineReplica，所以这里 Replica 的状态转移情况是：<strong>OnlineReplica –&gt; OnlineReplica</strong>，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="副本的状态转移"><a href="#副本的状态转移" class="headerlink" title="副本的状态转移"></a>副本的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewReplica、ReplicaDeletionStarted、ReplicaDeletionIneligible、ReplicaDeletionSuccessful、NonExistentReplica、OnlineReplica 或者 OfflineReplica 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewReplica"><a href="#TargetState-NewReplica" class="headerlink" title="TargetState: NewReplica"></a>TargetState: NewReplica</h4><p>NewReplica 这个状态是 Replica 准备开始创建是的一个状态，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> currState = replicaState.getOrElseUpdate(partitionAndReplica, <span class="type">NonExistentReplica</span>)<span class="comment">//note: Replica 不存在的话,状态初始化为 NonExistentReplica</span></div><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">NonExistentReplica</span>), targetState)<span class="comment">//note: 验证</span></div><div class="line"><span class="comment">// start replica as a follower to the current leader for its partition</span></div><div class="line"><span class="comment">//note: 从 zk 获取 Partition 的 leaderAndIsr 信息</span></div><div class="line"><span class="keyword">val</span> leaderIsrAndControllerEpochOpt = <span class="type">ReplicationUtils</span>.getLeaderIsrAndEpochForPartition(zkUtils, topic, partition)</div><div class="line">leaderIsrAndControllerEpochOpt <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">    <span class="keyword">if</span>(leaderIsrAndControllerEpoch.leaderAndIsr.leader == replicaId)<span class="comment">//note: 这个状态的 Replica 不能作为 leader</span></div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Replica %d for partition %s cannot be moved to NewReplica"</span></div><div class="line">        .format(replicaId, topicAndPartition) + <span class="string">"state as it is being requested to become leader"</span>)</div><div class="line">    <span class="comment">//note: 向该 replicaId 发送 LeaderAndIsr 请求,这个方法同时也会向所有的 broker 发送 updateMeta 请求</span></div><div class="line">    brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId),</div><div class="line">                                                        topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">                                                        replicaAssignment)</div><div class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// new leader request will be sent to this replica when one gets elected</span></div><div class="line">&#125;</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">NewReplica</span>)<span class="comment">//note: 缓存这个 replica 对象的状态</span></div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                  targetState))</div></pre></td></tr></table></figure>
<p>当想要把 Replica 的状态转移为 NewReplica 时，副本状态机的处理逻辑如下：</p>
<ol>
<li>校验 Replica 的前置状态，只有处于 NonExistentReplica 状态的副本才能转移到 NewReplica 状态；</li>
<li>从 zk 中获取该 Topic-Partition 的 LeaderIsrAndControllerEpoch 信息；</li>
<li>如果获取不到上述信息，直接将该 Replica 的状态转移成 NewReplica，然后结束流程（对与新建的 Partition，处于这个状态时，该 Partition 是没有相应的 LeaderAndIsr 信息的）；</li>
<li>获取到 Partition 的 LeaderIsrAndControllerEpoch 信息，如果发现该 Partition 的 leader 是当前副本，那么就抛出 StateChangeFailedException 异常，因为处在这个状态的 Replica 是不能被选举为 leader 的；</li>
<li>获取到了 Partition 的 LeaderIsrAndControllerEpoch 信息，并且该 Partition 的 leader 不是当前 replica，那么向该 Partition 的所有 Replica 添加一个 LeaderAndIsr 请求（添加 LeaderAndIsr 请求时，实际上也会向所有的 Broker 都添加一个 Update-Metadata 请求）；</li>
<li>最后将该 Replica 的状态转移成 NewReplica，然后结束流程。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionStarted"><a href="#TargetState-ReplicaDeletionStarted" class="headerlink" title="TargetState: ReplicaDeletionStarted"></a>TargetState: ReplicaDeletionStarted</h4><p>这是 Replica 开始删除时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">OfflineReplica</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionStarted</span>)</div><div class="line"><span class="comment">// send stop replica command</span></div><div class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,并设置 deletePartition=true</span></div><div class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">true</span>,</div><div class="line">  callbacks.stopReplicaResponseCallback)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>这部分的实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 OfflineReplica 的情况下才能转移到这种状态；</li>
<li>更新向该 Replica 的状态为 ReplicaDeletionStarted；</li>
<li>向该 replica 发送 StopReplica 请求（deletePartition = true），收到这请求后，broker 会从物理存储上删除这个 Replica 的数据内容；</li>
<li>如果请求返回的话会触发其回调函数（这部分会在 topic 删除部分讲解）。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionIneligible"><a href="#TargetState-ReplicaDeletionIneligible" class="headerlink" title="TargetState: ReplicaDeletionIneligible"></a>TargetState: ReplicaDeletionIneligible</h4><p>ReplicaDeletionIneligible 是副本删除失败时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionIneligible。</li>
</ol>
<h4 id="TargetState-ReplicaDeletionSuccessful"><a href="#TargetState-ReplicaDeletionSuccessful" class="headerlink" title="TargetState: ReplicaDeletionSuccessful"></a>TargetState: ReplicaDeletionSuccessful</h4><p>ReplicaDeletionSuccessful 是副本删除成功时的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionStarted</span>), targetState)</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionSuccessful</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionStarted 下才能转移这种状态；</li>
<li>更新该 Replica 的状态为 ReplicaDeletionSuccessful。</li>
</ol>
<h4 id="TargetState-NonExistentReplica"><a href="#TargetState-NonExistentReplica" class="headerlink" title="TargetState: NonExistentReplica"></a>TargetState: NonExistentReplica</h4><p>NonExistentReplica 是副本完全删除、不存在这个副本的状态，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica, <span class="type">List</span>(<span class="type">ReplicaDeletionSuccessful</span>), targetState)</div><div class="line"><span class="comment">// remove this replica from the assigned replicas list for its partition</span></div><div class="line"><span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line"><span class="comment">//note: 从 controller 和副本状态机的缓存中清除这个 Replica 的记录西溪</span></div><div class="line">controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas.filterNot(_ == replicaId))</div><div class="line">replicaState.remove(partitionAndReplica)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">  .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>检验其前置状态，Replica 只能是在 ReplicaDeletionSuccessful 下才能转移这种状态；</li>
<li>在 controller 的 partitionReplicaAssignment 删除这个 Partition 对应的 replica 信息；</li>
<li>从 Controller 和副本状态机中将这个 Topic 从缓存中删除。</li>
</ol>
<h4 id="TargetState-OnlineReplica"><a href="#TargetState-OnlineReplica" class="headerlink" title="TargetState: OnlineReplica"></a>TargetState: OnlineReplica</h4><p>OnlineReplica 是副本正常工作时的状态，此时的 Replica 既可以作为 leader 也可以作为 follower，Replica 转移到这种状态的处理实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica,</div><div class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line">replicaState(partitionAndReplica) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NewReplica</span> =&gt; <span class="comment">//note: NewReplica --&gt; OnlineReplica</span></div><div class="line">    <span class="comment">// add this replica to the assigned replicas list for its partition</span></div><div class="line">    <span class="comment">//note: 向 the assigned replicas list 添加这个 replica（正常情况下这些 replicas 已经更新到 list 中了）</span></div><div class="line">    <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">if</span>(!currentAssignedReplicas.contains(replicaId))</div><div class="line">      controllerContext.partitionReplicaAssignment.put(topicAndPartition, currentAssignedReplicas :+ replicaId)</div><div class="line">    stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">                              .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState,</div><div class="line">                                      targetState))</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">//note: OnlineReplica/OfflineReplica/ReplicaDeletionIneligible --&gt; OnlineReplica</span></div><div class="line">    <span class="comment">// check if the leader for this partition ever existed</span></div><div class="line">    <span class="comment">//note: 如果该 Partition 的 LeaderIsrAndControllerEpoch 信息存在,那么就更新副本的状态,并发送相应的请求</span></div><div class="line">    controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</div><div class="line">        brokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, leaderIsrAndControllerEpoch,</div><div class="line">          replicaAssignment)</div><div class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div><div class="line">        stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">          .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">// that means the partition was never in OnlinePartition state, this means the broker never</span></div><div class="line">        <span class="comment">// started a log for that partition and does not have a high watermark value for this partition</span></div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</div></pre></td></tr></table></figure>
<p>从前面的状态转移图中可以看出，当 Replica 处在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，Replica 是可以转移到 OnlineReplica 状态的，下面分两种情况讲述：</p>
<p><strong>NewReplica –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionReplicaAssignment 中获取这个 Partition 的 AR；</li>
<li>如果 Replica 不在 AR 中的话，那么就将其添加到 Partition 的 AR 中；</li>
<li>最后将 Replica 的状态设置为 OnlineReplica 状态。</li>
</ol>
<p><strong>OnlineReplica/OfflineReplica/ReplicaDeletionIneligible –&gt; OnlineReplica</strong> 的处理逻辑如下：</p>
<ol>
<li>从 Controller 的 partitionLeadershipInfo 中获取 Partition 的 LeaderAndIsr 信息；</li>
<li>如果该信息存在，那么就向这个 Replica 所在 broker 添加这个 Partition 的 LeaderAndIsr 请求，并将 Replica 的状态设置为 OnlineReplica 状态；</li>
<li>否则不做任务处理；</li>
<li>最后更新R Replica 的状态为 OnlineReplica。</li>
</ol>
<h4 id="TargetState-OfflineReplica"><a href="#TargetState-OfflineReplica" class="headerlink" title="TargetState: OfflineReplica"></a>TargetState: OfflineReplica</h4><p>OfflineReplica 是 Replica 所在 Broker 掉线时 Replica 的状态，转移到这种状态的处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">assertValidPreviousStates(partitionAndReplica,</div><div class="line">  <span class="type">List</span>(<span class="type">NewReplica</span>, <span class="type">OnlineReplica</span>, <span class="type">OfflineReplica</span>, <span class="type">ReplicaDeletionIneligible</span>), targetState)</div><div class="line"><span class="comment">// send stop replica command to the replica so that it stops fetching from the leader</span></div><div class="line"><span class="comment">//note: 发送 StopReplica 请求给该副本,先停止副本同步</span></div><div class="line">brokerRequestBatch.addStopReplicaRequestForBrokers(<span class="type">List</span>(replicaId), topic, partition, deletePartition = <span class="literal">false</span>)</div><div class="line"><span class="comment">// As an optimization, the controller removes dead replicas from the ISR</span></div><div class="line"><span class="keyword">val</span> leaderAndIsrIsEmpty: <span class="type">Boolean</span> =</div><div class="line">  controllerContext.partitionLeadershipInfo.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</div><div class="line">      controller.removeReplicaFromIsr(topic, partition, replicaId) <span class="keyword">match</span> &#123; <span class="comment">//note: 从 isr 中移除这个副本（前提是 ISR 有其他有效副本）</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(updatedLeaderIsrAndControllerEpoch) =&gt;</div><div class="line">          <span class="comment">// send the shrunk ISR state change request to all the remaining alive replicas of the partition.</span></div><div class="line">          <span class="comment">//note: 发送 LeaderAndIsr 请求给剩余的其他副本,因为 ISR 变动了</span></div><div class="line">          <span class="keyword">val</span> currentAssignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">          <span class="keyword">if</span> (!controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition)) &#123;</div><div class="line">            brokerRequestBatch.addLeaderAndIsrRequestForBrokers(currentAssignedReplicas.filterNot(_ == replicaId),</div><div class="line">              topic, partition, updatedLeaderIsrAndControllerEpoch, replicaAssignment)</div><div class="line">          &#125;</div><div class="line">          replicaState.put(partitionAndReplica, <span class="type">OfflineReplica</span>)</div><div class="line">          stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed state of replica %d for partition %s from %s to %s"</span></div><div class="line">            .format(controllerId, controller.epoch, replicaId, topicAndPartition, currState, targetState))</div><div class="line">          <span class="literal">false</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="literal">true</span></div><div class="line">      &#125;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="literal">true</span></div><div class="line">  &#125;</div><div class="line"><span class="keyword">if</span> (leaderAndIsrIsEmpty &amp;&amp; !controller.deleteTopicManager.isPartitionToBeDeleted(topicAndPartition))</div><div class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(</div><div class="line">    <span class="string">"Failed to change state of replica %d for partition %s since the leader and isr path in zookeeper is empty"</span></div><div class="line">    .format(replicaId, topicAndPartition))</div></pre></td></tr></table></figure>
<p>处理逻辑如下：</p>
<ol>
<li>校验其前置状态，只有 Replica 在 NewReplica、OnlineReplica、OfflineReplica 或者 ReplicaDeletionIneligible 状态时，才能转移到这种状态；</li>
<li>向该 Replica 所在节点发送 StopReplica 请求（deletePartition = false）；</li>
<li>调用 Controller 的 <code>removeReplicaFromIsr()</code> 方法将该 replica 从 Partition 的 isr 移除这个 replica（前提 isr 中还有其他有效副本），然后向该 Partition 的其他副本发送 LeaderAndIsr 请求；</li>
<li>更新这个 Replica 的状态为 OfflineReplica。</li>
</ol>
<h3 id="状态转移触发的条件"><a href="#状态转移触发的条件" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Replica 各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onBrokerStartup()</td>
<td>Broker 启动时，目的是将在该节点的 Replica 状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>新建 Partition 时，Replica 初始化及 Partition 状态变成 OnlinePartition 后，新创建的 Replica 状态也变为 OnlineReplica；</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>KafkaController 的 onPartitionReassignment()</td>
<td>副本迁移完成后，RAR 中的副本设置为 OnlineReplica 状态</td>
</tr>
<tr>
<td>OnlineReplica</td>
<td>ReplicaStateMachine 的 startup()</td>
<td>副本状态机刚初始化启动时，将存活的副本状态设置为 OnlineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的  markTopicForDeletionRetry()</td>
<td>将删除失败的 Replica 设置为 OfflineReplica，重新进行删除</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>TopicDeletionManager 的 startReplicaDeletion()</td>
<td>开始副本删除时，先将副本设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 shutdownBroker() 方法</td>
<td>优雅关闭 broker 时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>OfflineReplica</td>
<td>KafkaController 的 onBrokerFailure()</td>
<td>broker 掉线时，目的是把下线节点上的副本状态设置为 OfflineReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 onNewPartitionCreation()</td>
<td>Partition 新建时，当 Partition 状态变为 NewPartition 后，副本的状态变为 NewReplica</td>
</tr>
<tr>
<td>NewReplica</td>
<td>KafkaController 的 startNewReplicasForReassignedPartition()</td>
<td>Partition 副本迁移时，将新分配的副本状态设置为 NewReplica；</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>下线副本时，将成功设置为 OfflineReplica 的 Replica 设置为 ReplicaDeletionStarted 状态，开始物理上删除副本数据（也是发送 StopReplica）</td>
</tr>
<tr>
<td>ReplicaDeletionStarted</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，目的是下线那些 old replica，新的 replica 已经迁移到新分配的副本上了</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>物理将数据成功删除的 Replica 状态会变为这个</td>
</tr>
<tr>
<td>ReplicaDeletionSuccessful</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本迁移时，在下线那些旧 Replica 时的一个状态，删除成功</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>TopicDeletionManager 的  startReplicaDeletion()</td>
<td>开始副本删除时，删除失败的副本会设置成这个状态</td>
</tr>
<tr>
<td>ReplicaDeletionIneligible</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 副本迁移时，在下线那些旧的 Replica 时的一个状态，删除失败</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>TopicDeletionManager 的  completeReplicaDeletion()</td>
<td>副本删除成功后（状态为 ReplicaDeletionSuccessful），从状态机和 Controller 的缓存中清除该副本的记录；</td>
</tr>
<tr>
<td>NonExistentReplica</td>
<td>KafkaController 的 stopOldReplicasOfReassignedPartition()</td>
<td>Partition 的副本成功迁移、旧副本成功删除后，从状态机和 Controller 的缓存中清除旧副本的记录</td>
</tr>
</tbody>
</table>
<h2 id="PartitionStateMachine"><a href="#PartitionStateMachine" class="headerlink" title="PartitionStateMachine"></a>PartitionStateMachine</h2><p>PartitionStateMachine 记录着集群所有 Partition 的状态信息，它决定着一个 Partition 处在什么状态以及它在什么状态下可以转变为什么状态，Kafka 中 Partition 的状态总共有以下四种类型：</p>
<ol>
<li>NonExistentPartition：这个代表着这个 Partition 之前没有被创建过或者之前创建了现在又被删除了，它有效的前置状态是 OfflinePartition；</li>
<li>NewPartition：Partition 创建后，它将处于这个状态，这个状态的 Partition 还没有 leader 和 isr，它有效的前置状态是 NonExistentPartition；</li>
<li>OnlinePartition：一旦这个 Partition 的 leader 被选举出来了，它将处于这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition；</li>
<li>OfflinePartition：如果这个 Partition 的 leader 掉线，这个 Partition 将被转移到这个状态，它有效的前置状态是 NewPartition、OnlinePartition、OfflinePartition。</li>
</ol>
<p>分区状态机转移图如下所示：</p>
<p><img src="/images/kafka/partition_state.png" alt="分区状态机"></p>
<p>这张图是分区状态机的核心，在下面会详细讲述，接下来先看下 KafkaController 在启动时，调用 PartitionStateMachine 的 <code>startup()</code> 方法初始化的处理过程。</p>
<h3 id="PartitionStateMachine-初始化"><a href="#PartitionStateMachine-初始化" class="headerlink" title="PartitionStateMachine 初始化"></a>PartitionStateMachine 初始化</h3><p>PartitionStateMachine 的初始化方法如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 启动时触发</span></div><div class="line"><span class="comment">//note: 初始化所有 Partition 的状态（从 zk 获取）, 然后对于 new/offline Partition 触发选主（选主成功的话,变为 OnlinePartition）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// initialize partition state</span></div><div class="line">  <span class="comment">//note: 初始化 partition 的状态,如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></div><div class="line">  initializePartitionState()</div><div class="line">  <span class="comment">// set started flag</span></div><div class="line">  hasStarted.set(<span class="literal">true</span>)</div><div class="line">  <span class="comment">// try to move partitions to online state</span></div><div class="line">  <span class="comment">//note: 为所有处理 NewPartition 或 OnlinePartition 状态 Partition 选举 leader</span></div><div class="line">  triggerOnlinePartitionStateChange()</div><div class="line"></div><div class="line">  info(<span class="string">"Started partition state machine with initial state -&gt; "</span> + partitionState.toString())</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法中，PartitionStateMachine 先调用 <code>initializePartitionState()</code> 方法初始化集群中所有 Partition 的状态信息：</p>
<ol>
<li>如果该 Partition 有 LeaderAndIsr 信息，那么如果 Partition leader 所在的机器是 alive 的，那么将其状态设置为 OnlinePartition，否则设置为 OfflinePartition 状态；</li>
<li>如果该 Partition 没有 LeaderAndIsr 信息，那么将其状态设置为 NewPartition。</li>
</ol>
<p>这里只是将 Partition 的状态信息更新分区状态机的缓存 <code>partitionState</code> 中，并没有真正进行状态的转移。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 根据从 zk 获取的所有 Partition,进行状态初始化</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializePartitionState</span></span>() &#123;</div><div class="line">  <span class="keyword">for</span> (topicPartition &lt;- controllerContext.partitionReplicaAssignment.keys) &#123;</div><div class="line">    <span class="comment">// check if leader and isr path exists for partition. If not, then it is in NEW state</span></div><div class="line">    controllerContext.partitionLeadershipInfo.get(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(currentLeaderIsrAndEpoch) =&gt;</div><div class="line">        <span class="comment">// else, check if the leader for partition is alive. If yes, it is in Online state, else it is in Offline state</span></div><div class="line">        <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(currentLeaderIsrAndEpoch.leaderAndIsr.leader))</div><div class="line">          <span class="comment">// leader is alive</span></div><div class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,并且 leader 存活,设置为 OnlinePartition 状态</span></div><div class="line">          partitionState.put(topicPartition, <span class="type">OnlinePartition</span>)</div><div class="line">        <span class="keyword">else</span></div><div class="line">          <span class="comment">//note: 有 LeaderAndIsr 信息,但是 leader 不存活,设置为 OfflinePartition 状态</span></div><div class="line">          partitionState.put(topicPartition, <span class="type">OfflinePartition</span>)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">//note: 没有 LeaderAndIsr 信息,设置为 NewPartition 状态（这个 Partition 还没有）</span></div><div class="line">        partitionState.put(topicPartition, <span class="type">NewPartition</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在初始化的第二步，将会调用 <code>triggerOnlinePartitionStateChange()</code> 方法，为所有的状态为 NewPartition/OnlinePartition 的 Partition 进行 leader 选举，选举成功后的话，其状态将会设置为 OnlinePartition，调用的 Leader 选举方法是 <a href="http://matt33.com/2018/06/15/kafka-controller-start/#OfflinePartitionLeaderSelector">OfflinePartitionLeaderSelector</a>（具体实现参考链接）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是在 controller 选举后或 broker 上线或下线时时触发的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">triggerOnlinePartitionStateChange</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    brokerRequestBatch.newBatch()</div><div class="line">    <span class="comment">// try to move all partitions in NewPartition or OfflinePartition state to OnlinePartition state except partitions</span></div><div class="line">    <span class="comment">// that belong to topics to be deleted</span></div><div class="line">    <span class="comment">//note: 开始为所有状态在 NewPartition or OfflinePartition 状态的 partition 更新状态（除去将要被删除的 topic）</span></div><div class="line">    <span class="keyword">for</span>((topicAndPartition, partitionState) &lt;- partitionState</div><div class="line">        <span class="keyword">if</span> !controller.deleteTopicManager.isTopicQueuedUpForDeletion(topicAndPartition.topic)) &#123;</div><div class="line">      <span class="keyword">if</span>(partitionState.equals(<span class="type">OfflinePartition</span>) || partitionState.equals(<span class="type">NewPartition</span>))</div><div class="line">        <span class="comment">//note: 尝试为处在 OfflinePartition 或 NewPartition 状态的 Partition 选主,成功后转换为 OnlinePartition</span></div><div class="line">        handleStateChange(topicAndPartition.topic, topicAndPartition.partition, <span class="type">OnlinePartition</span>, controller.offlinePartitionSelector,</div><div class="line">                          (<span class="keyword">new</span> <span class="type">CallbackBuilder</span>).build)</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 发送请求给所有的 broker,包括 LeaderAndIsr 请求和 UpdateMetadata 请求（这里只是添加到 Broker 对应的 RequestQueue 中,后台有线程去发送）</span></div><div class="line">    brokerRequestBatch.sendRequestsToBrokers(controller.epoch)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">"Error while moving some partitions to the online state"</span>, e)</div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> It is not enough to bail out and log an error, it is important to trigger leader election for those partitions</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面方法的目的是为尝试将所有的状态为 NewPartition/OnlinePartition 的 Partition 状态转移到 OnlinePartition，这个方法主要是做了两件事：</p>
<ol>
<li>状态转移（这个在下面详细讲述）；</li>
<li>发送相应的请求。</li>
</ol>
<h3 id="分区的状态转移"><a href="#分区的状态转移" class="headerlink" title="分区的状态转移"></a>分区的状态转移</h3><p>这里以要转移的 TargetState 区分做详细详细讲解，当 TargetState 分别是 NewPartition、OfflinePartition、NonExistentPartition 或者 OnlinePartition 时，副本状态机所做的事情。</p>
<h4 id="TargetState-NewPartition"><a href="#TargetState-NewPartition" class="headerlink" title="TargetState: NewPartition"></a>TargetState: NewPartition</h4><p>NewPartition 是 Partition 刚创建时的一个状态，其处理逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果该 Partition 的状态不存在,默认为 NonExistentPartition</span></div><div class="line"><span class="keyword">val</span> currState = partitionState.getOrElseUpdate(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line"><span class="comment">// pre: partition did not exist before this</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NonExistentPartition</span>), <span class="type">NewPartition</span>)</div><div class="line">partitionState.put(topicAndPartition, <span class="type">NewPartition</span>) <span class="comment">//note: 缓存 partition 的状态</span></div><div class="line"><span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(<span class="string">","</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,</div><div class="line">                                  assignedReplicas))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NonExistentPartition；</li>
<li>将该 Partition 的状态转移为 NewPartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-OnlinePartition"><a href="#TargetState-OnlinePartition" class="headerlink" title="TargetState: OnlinePartition"></a>TargetState: OnlinePartition</h4><p>OnlinePartition 是一个 Partition 正常工作时的状态，这个状态下的 Partition 已经成功选举出了 leader 和 isr 信息，其实现逻辑如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 判断 Partition 之前的状态是否可以转换为目的状态</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OnlinePartition</span>)</div><div class="line">partitionState(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">  <span class="keyword">case</span> <span class="type">NewPartition</span> =&gt; <span class="comment">//note: 新建的 Partition</span></div><div class="line">    <span class="comment">//note: 选举 leader 和 isr,更新到 zk 和 controller 中,如果没有存活的 replica,抛出异常</span></div><div class="line">    <span class="comment">// initialize leader and isr path for new partition</span></div><div class="line">    initializeLeaderAndIsrForPartition(topicAndPartition)</div><div class="line">  <span class="keyword">case</span> <span class="type">OfflinePartition</span> =&gt; <span class="comment">//note: leader 挂掉的 Partition</span></div><div class="line">    <span class="comment">//note: 进行 leader 选举,更新到 zk 及 controller 缓存中,失败的抛出异常</span></div><div class="line">    electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">  <span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt; <span class="comment">// invoked when the leader needs to be re-elected</span></div><div class="line">    <span class="comment">//note:这种只有在 leader 需要重新选举时才会触发</span></div><div class="line">    electLeaderForPartition(topic, partition, leaderSelector)</div><div class="line">  <span class="keyword">case</span> _ =&gt; <span class="comment">// should never come here since illegal previous states are checked above</span></div><div class="line">&#125;</div><div class="line">partitionState.put(topicAndPartition, <span class="type">OnlinePartition</span>)</div><div class="line"><span class="keyword">val</span> leader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s from %s to %s with leader %d"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState, leader))</div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验这个 Partition 的前置状态，有效的前置状态是：NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>如果前置状态是 NewPartition，那么为该 Partition 选举 leader 和 isr，更新到 zk 和 controller 的缓存中，如果副本没有处于 alive 状态的话，就抛出异常；</li>
<li>如果前置状态是 OnlinePartition，那么只是触发 leader 选举，在 OnlinePartition –&gt; OnlinePartition 这种状态转移时，需要传入 leader 选举的方法，触发该 Partition 的 leader 选举；</li>
<li>如果前置状态是 OfflinePartition，同上，也是触发 leader 选举。</li>
<li>更新 Partition 的状态为 OnlinePartition。</li>
</ol>
<p>对于以上这几种情况，无论前置状态是什么，最后都会触发这个 Partition 的 leader 选举，leader 成功后，都会触发向这个 Partition 的所有 replica 发送 LeaderAndIsr 请求。</p>
<h4 id="TargetState-OfflinePartition"><a href="#TargetState-OfflinePartition" class="headerlink" title="TargetState: OfflinePartition"></a>TargetState: OfflinePartition</h4><p>OfflinePartition 是这个 Partition 的 leader 挂掉时转移的一个状态，如果 Partition 转移到这个状态，那么就意味着这个 Partition 没有了可用 leader。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// pre: partition should be in New or Online state</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">NewPartition</span>, <span class="type">OnlinePartition</span>, <span class="type">OfflinePartition</span>), <span class="type">OfflinePartition</span>)</div><div class="line"><span class="comment">// should be called when the leader for a partition is no longer alive</span></div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">partitionState.put(topicAndPartition, <span class="type">OfflinePartition</span>)</div><div class="line"><span class="comment">// post: partition has no alive leader</span></div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 NewPartition、OnlinePartition 或者 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 OfflinePartition 状态，并且更新到缓存中。</li>
</ol>
<h4 id="TargetState-NonExistentPartition"><a href="#TargetState-NonExistentPartition" class="headerlink" title="TargetState: NonExistentPartition"></a>TargetState: NonExistentPartition</h4><p>NonExistentPartition 代表了已经处于 OfflinePartition 状态的 Partition 已经从 metadata 和 zk 中删除后进入的状态。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">// pre: partition should be in Offline state</span></div><div class="line">assertValidPreviousStates(topicAndPartition, <span class="type">List</span>(<span class="type">OfflinePartition</span>), <span class="type">NonExistentPartition</span>)</div><div class="line">stateChangeLogger.trace(<span class="string">"Controller %d epoch %d changed partition %s state from %s to %s"</span></div><div class="line">                          .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))</div><div class="line">partitionState.put(topicAndPartition, <span class="type">NonExistentPartition</span>)</div><div class="line"><span class="comment">// post: partition state is deleted from all brokers and zookeeper</span></div></pre></td></tr></table></figure>
<p>实现逻辑：</p>
<ol>
<li>校验其前置状态，它有效的前置状态为 OfflinePartition；</li>
<li>将该 Partition 的状态转移为 NonExistentPartition 状态，并且更新到缓存中。</li>
</ol>
<h3 id="状态转移触发的条件-1"><a href="#状态转移触发的条件-1" class="headerlink" title="状态转移触发的条件"></a>状态转移触发的条件</h3><p>这里主要是看一下上面 Partition   各种转移的触发的条件，整理的结果如下表所示，部分内容会在后续文章讲解。</p>
<table>
<thead>
<tr>
<th>TargetState</th>
<th>触发方法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 shutdownBroker()</td>
<td>优雅关闭 Broker 时调用，因为要下线的节点是 leader，所以需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 新建时，这个是在 Replica 已经变为 NewPartition 状态后进行的，为新建的 Partition 初始化 leader 和 isr</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 onPreferredReplicaElection()</td>
<td>对 Partition 进行最优 leader 选举，目的是触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>controller 的 moveReassignedPartitionLeaderIfRequired()</td>
<td>分区副本迁移完成后，1. 当前的 leader 不在 RAR 中，需要触发 leader 选举；2. 当前 leader 在 RAR 但是掉线了，也需要触发 leader 选举</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 triggerOnlinePartitionStateChange()</td>
<td>当 Controller 重新选举出来或 broker 有变化时，目的为了那些状态为 NewPartition/OfflinePartition 的 Partition 重新选举 leader，选举成功后状态变为 OnlinePartition</td>
</tr>
<tr>
<td>OnlinePartition</td>
<td>PartitionStateMachine 的 initializePartitionState()</td>
<td>Controller 初始化时，遍历 zk 的所有的分区，如果有 LeaderAndIsr 信息并且 leader 在 alive broker 上，那么就将状态转为 OnlinePartition。</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>controller 的 onBrokerFailure()</td>
<td>当有 broker 掉线时，将 leader 在这个机器上的 Partition 设置为 OfflinePartition</td>
</tr>
<tr>
<td>OfflinePartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，中间会将该 Partition 的状态先转变为 OfflinePartition</td>
</tr>
<tr>
<td>NonExistentPartition</td>
<td>TopicDeletionManager 的 completeDeleteTopic()</td>
<td>Topic 删除成功后，最后会将该 Partition 的状态转移为 NonExistentPartition</td>
</tr>
<tr>
<td>NewPartition</td>
<td>Controller 的 onNewPartitionCreation()</td>
<td>Partition 刚创建时的一个中间状态 ，此时还没选举 leader 和设置 isr 信息</td>
</tr>
</tbody>
</table>
<p>上面就是副本状态机与分区状态机的所有内容，这里只是单纯地讲述了一下这两种状态机，后续文章会开始介绍 Controller 一些其他内容，包括 Partition 迁移、Topic 新建、Topic 下线等，这些内容都会用到这篇文章讲述的内容。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇讲述了 KafkaController 的启动流程，但是关于分区状态机和副本状态机的初始化并没有触及，分区状态机和副本状态机的内容将在本篇文章深入讲述。分区状态机记录着当前集群所有 Partition 的状态信息以及如何对 Partition 状态转移进行相应的处理；副
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Controller 选举及服务启动流程（十六）</title>
    <link href="http://matt33.com/2018/06/15/kafka-controller-start/"/>
    <id>http://matt33.com/2018/06/15/kafka-controller-start/</id>
    <published>2018-06-15T03:04:14.000Z</published>
    <updated>2018-06-23T03:28:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>从本篇文章开始，Kafka 源码解析就正式进入了 Controller 部分，Controller 作为 Kafka Server 端一个重要的组件，它的角色类似于其他分布式系统 Master 的角色，跟其他系统不一样的是，Kafka 集群的任何一台 Broker 都可以作为 Controller，但是在一个集群中同时只会有一个 Controller 是 alive 状态。Controller 在集群中负责的事务很多，比如：集群 meta 信息的一致性保证、Partition leader 的选举、broker 上下线等都是由 Controller 来具体负责。Controller 部分的内容还是比较多的，计划分5篇左右的文章讲述，本文先来看下 Controller 的简介、Controller 的选举、Controller 选举后服务的启动流程以及 Controller 的四种不同 leader 选举机制。分区状态机、副本副本状态机以及对各种 listener 的处理将在后续的文章中展开。</p>
<h2 id="Controller-简介"><a href="#Controller-简介" class="headerlink" title="Controller 简介"></a>Controller 简介</h2><p>在于分布式系统中，总会有一个地方需要对全局 meta 做一个统一的维护，Kafka 的 Controller 就是充当这个角色的。Kafka 简单的框架图如下所示</p>
<p><img src="/images/kafka/kafka-framwoker.png" alt="Kafka架构简图"></p>
<p>Controller 是运行在 Broker 上的，任何一台 Broker 都可以作为 Controller，但是一个集群同时只能存在一个 Controller，也就意味着 Controller 与数据节点是在一起的，Controller 做的主要事情如下：</p>
<ol>
<li>Broker 的上线、下线处理；</li>
<li>新创建的 topic 或已有 topic 的分区扩容，处理分区副本的分配、leader 选举；</li>
<li>管理所有副本的状态机和分区的状态机，处理状态机的变化事件；</li>
<li>topic 删除、副本迁移、leader 切换等处理。</li>
</ol>
<h2 id="Controller-选举过程"><a href="#Controller-选举过程" class="headerlink" title="Controller 选举过程"></a>Controller 选举过程</h2><p>Kafka 的每台 Broker 在启动过程中，都会启动 Controller 服务，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  info(<span class="string">"starting"</span>)</div><div class="line">  <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">  <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">    <span class="comment">/* start kafka controller */</span></div><div class="line">    <span class="comment">//note: 启动 controller</span></div><div class="line">    kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkUtils, brokerState, time, metrics, threadNamePrefix)</div><div class="line">    kafkaController.startup()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Controller-启动"><a href="#Controller-启动" class="headerlink" title="Controller 启动"></a>Controller 启动</h3><p>Kafka Server 在启动的过程中，都会去启动 Controller 服务，Controller 启动方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 当 broker 的 controller 模块启动时触发,它比并不保证当前 broker 是 controller,它仅仅是注册 registerSessionExpirationListener 和启动 controllerElector</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() = &#123;</div><div class="line">  inLock(controllerContext.controllerLock) &#123;</div><div class="line">    info(<span class="string">"Controller starting up"</span>)</div><div class="line">    registerSessionExpirationListener() <span class="comment">// note: 注册回话失效的监听器</span></div><div class="line">    isRunning = <span class="literal">true</span></div><div class="line">    controllerElector.startup <span class="comment">//note: 启动选举过程</span></div><div class="line">    info(<span class="string">"Controller startup complete"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Controller 在 <code>startup()</code> 方法中主要实现以下两部分功能：</p>
<ol>
<li><code>registerSessionExpirationListener()</code> 方法注册连接 zk 的超时监听器；</li>
<li><code>controllerElector.startup()</code> 方法，监听 zk 上 controller 节点的变化，并触发 controller 选举方法。</li>
</ol>
<h3 id="Controller-选举"><a href="#Controller-选举" class="headerlink" title="Controller 选举"></a>Controller 选举</h3><p>Controller 在启动时，会初始化 ZookeeperLeaderElector 对象，并调用其 <code>startup()</code> 启动相应的流程，具体过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span> </span>&#123;</div><div class="line">  inLock(controllerContext.controllerLock) &#123;</div><div class="line">    controllerContext.zkUtils.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)</div><div class="line">    elect</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>startup()</code> 方法中，主要做了下面两件事情：</p>
<ol>
<li>监听 zk 的 <code>/controller</code> 节点的数据变化，一旦节点有变化，立刻通过 LeaderChangeListener 的方法进行相应的处理；</li>
<li><code>elect</code> 在 controller 不存在的情况下选举 controller，存在的话，就是从 zk 获取当前的 controller 节点信息。</li>
</ol>
<h4 id="Controller-选举方法-elect"><a href="#Controller-选举方法-elect" class="headerlink" title="Controller 选举方法 elect"></a>Controller 选举方法 elect</h4><p>ZookeeperLeaderElector 的 <code>elect</code> 方法实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 从 zk 获取当前的 controller 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getControllerID</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  controllerContext.zkUtils.readDataMaybeNull(electionPath)._1 <span class="keyword">match</span> &#123;</div><div class="line">     <span class="keyword">case</span> <span class="type">Some</span>(controller) =&gt; <span class="type">KafkaController</span>.parseControllerId(controller)</div><div class="line">     <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="number">-1</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 进行 controller 选举</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">elect</span></span>: <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="keyword">val</span> timestamp = time.milliseconds.toString</div><div class="line">  <span class="keyword">val</span> electString = <span class="type">Json</span>.encode(<span class="type">Map</span>(<span class="string">"version"</span> -&gt; <span class="number">1</span>, <span class="string">"brokerid"</span> -&gt; brokerId, <span class="string">"timestamp"</span> -&gt; timestamp))</div><div class="line"></div><div class="line"> leaderId = getControllerID</div><div class="line">  <span class="comment">/*</span></div><div class="line">   * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,</div><div class="line">   * it's possible that the controller has already been elected when we get here. This check will prevent the following</div><div class="line">   * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.</div><div class="line">   */</div><div class="line">  <span class="keyword">if</span>(leaderId != <span class="number">-1</span>) &#123;</div><div class="line">     debug(<span class="string">"Broker %d has been elected as leader, so stopping the election process."</span>.format(leaderId))</div><div class="line">     <span class="keyword">return</span> amILeader</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> zkCheckedEphemeral = <span class="keyword">new</span> <span class="type">ZKCheckedEphemeral</span>(electionPath,</div><div class="line">                                                    electString,</div><div class="line">                                                    controllerContext.zkUtils.zkConnection.getZookeeper,</div><div class="line">                                                    <span class="type">JaasUtils</span>.isZkSecurityEnabled())</div><div class="line">    zkCheckedEphemeral.create() <span class="comment">//note: 没有异常的话就是创建成功了</span></div><div class="line">    info(brokerId + <span class="string">" successfully elected as leader"</span>)</div><div class="line">    leaderId = brokerId</div><div class="line">    onBecomingLeader() <span class="comment">//note: 成为了 controller</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> _: <span class="type">ZkNodeExistsException</span> =&gt; <span class="comment">//note: 在创建时,发现已经有 broker 提前注册成功</span></div><div class="line">      <span class="comment">// If someone else has written the path, then</span></div><div class="line">      leaderId = getControllerID</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (leaderId != <span class="number">-1</span>)</div><div class="line">        debug(<span class="string">"Broker %d was elected as leader instead of broker %d"</span>.format(leaderId, brokerId))</div><div class="line">      <span class="keyword">else</span></div><div class="line">        warn(<span class="string">"A leader has been elected but just resigned, this will result in another round of election"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">case</span> e2: <span class="type">Throwable</span> =&gt; <span class="comment">//note: 抛出了其他异常，那么重新选举 controller</span></div><div class="line">      error(<span class="string">"Error while electing or becoming leader on broker %d"</span>.format(brokerId), e2)</div><div class="line">      resign()</div><div class="line">  &#125;</div><div class="line">  amILeader</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">amILeader</span> </span>: <span class="type">Boolean</span> = leaderId == brokerId</div></pre></td></tr></table></figure>
<p>其实现逻辑如下：</p>
<ol>
<li>先获取 zk 的 <code>/cotroller</code> 节点的信息，获取 controller 的 broker id，如果该节点不存在（比如集群刚创建时），那么获取的 controller id 为-1；</li>
<li>如果 controller id 不为-1，即 controller 已经存在，直接结束流程；</li>
<li>如果 controller id 为-1，证明 controller 还不存在，这时候当前 broker 开始在 zk 注册 controller；</li>
<li>如果注册成功，那么当前 broker 就成为了 controller，这时候开始调用 <code>onBecomingLeader()</code> 方法，正式初始化 controller（注意：<strong>controller 节点是临时节点</strong>，如果当前 controller 与 zk 的 session 断开，那么 controller 的临时节点会消失，会触发 controller 的重新选举）；</li>
<li>如果注册失败（刚好 controller 被其他 broker 创建了、抛出异常等），那么直接返回。</li>
</ol>
<p>在这里 controller 算是成功被选举出来了，controller 选举过程实际上就是各个 Broker 抢占式注册该节点，注册成功的便为 Controller。</p>
<h4 id="controller-节点监听-LeaderChangeListener"><a href="#controller-节点监听-LeaderChangeListener" class="headerlink" title="controller 节点监听 LeaderChangeListener"></a>controller 节点监听 LeaderChangeListener</h4><p>LeaderChangeListener 主要是监听 zk 上的 Controller 节点变化，如果该节点内容变化或者节点被删除，那么会触发 <code>handleDataChange()</code> 和 <code>handleDataDeleted()</code> 方法，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 监控 controller 内容的变化</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeaderChangeListener</span> <span class="keyword">extends</span> <span class="title">IZkDataListener</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Called when the leader information stored in zookeeper has changed. Record the new leader in memory</div><div class="line">   * @throws Exception On any error.</div><div class="line">   */</div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(dataPath: <span class="type">String</span>, data: <span class="type">Object</span>) &#123;</div><div class="line">    <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      <span class="keyword">val</span> amILeaderBeforeDataChange = amILeader</div><div class="line">      leaderId = <span class="type">KafkaController</span>.parseControllerId(data.toString)</div><div class="line">      info(<span class="string">"New leader is %d"</span>.format(leaderId))</div><div class="line">      <span class="comment">// The old leader needs to resign leadership if it is no longer the leader</span></div><div class="line">      amILeaderBeforeDataChange &amp;&amp; !amILeader</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 之前是 controller,现在不是了</span></div><div class="line">    <span class="keyword">if</span> (shouldResign)</div><div class="line">      onResigningAsLeader() <span class="comment">//note: 关闭 controller 服务</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader</div><div class="line">   * @throws Exception</div><div class="line">   *             On any error.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 如果之前是 controller,现在这个节点被删除了,那么首先退出 controller 进程,然后开始重新选举 controller</span></div><div class="line">  <span class="meta">@throws</span>[<span class="type">Exception</span>]</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">handleDataDeleted</span></span>(dataPath: <span class="type">String</span>) &#123;</div><div class="line">    <span class="keyword">val</span> shouldResign = inLock(controllerContext.controllerLock) &#123;</div><div class="line">      debug(<span class="string">"%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"</span></div><div class="line">        .format(brokerId, dataPath))</div><div class="line">      amILeader</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (shouldResign)</div><div class="line">      onResigningAsLeader()</div><div class="line"></div><div class="line">    inLock(controllerContext.controllerLock) &#123;</div><div class="line">      elect</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>处理过程如下：</p>
<ol>
<li>如果 <code>/controller</code> 节点内容变化，那么更新一下 controller 最新的节点信息，如果该节点刚好之前是 controller，现在不是了，那么需要执行 controller 关闭操作，即 <code>onResigningAsLeader()</code> 方法；</li>
<li>如果 <code>/controller</code> 节点被删除，如果该节点刚好之前是 controller，那么需要执行 controller 关闭操作，即 <code>onResigningAsLeader()</code> 方法，然后再执行 <code>elect</code> 方法重新去选举 controller；</li>
</ol>
<h2 id="Controller-服务启动流程"><a href="#Controller-服务启动流程" class="headerlink" title="Controller 服务启动流程"></a>Controller 服务启动流程</h2><p>Controller 节点选举出来之后，ZookeeperLeaderElector 就会调用 <code>onBecomingLeader()</code> 方法初始化 KafkaController 的相关内容，在 KafkaController 对 ZookeeperLeaderElector 的初始化中可以看到 <code>onBecomingLeader()</code> 这个方法实际上是 KafkaController 的 <code>onControllerFailover()</code> 方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaController</span></span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controllerElector = <span class="keyword">new</span> <span class="type">ZookeeperLeaderElector</span>(controllerContext, <span class="type">ZkUtils</span>.<span class="type">ControllerPath</span>, onControllerFailover,</div><div class="line">                                                               onControllerResignation, config.brokerId, time) <span class="comment">//note: controller 通过 zk 选举</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: controller 临时节点监控及 controller 选举</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZookeeperLeaderElector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>,</span></span></div><div class="line">                             electionPath: <span class="type">String</span>, //note: 路径是 /controller</div><div class="line">                             onBecomingLeader: () <span class="title">=&gt;</span> <span class="title">Unit</span>, <span class="title">//note</span>: onControllerFailover() 方法</div><div class="line">                             onResigningAsLeader: () =&gt; <span class="type">Unit</span>, <span class="comment">//note: onControllerResignation() 方法</span></div><div class="line">                             brokerId: <span class="type">Int</span>,</div><div class="line">                             time: <span class="type">Time</span>)</div></pre></td></tr></table></figure>
<h3 id="onControllerFailover-启动及初始化"><a href="#onControllerFailover-启动及初始化" class="headerlink" title="onControllerFailover 启动及初始化"></a>onControllerFailover 启动及初始化</h3><p>下面开始进入 KafkaController 正式初始化的讲解过程中，<code>onControllerFailover()</code> 方法实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 如果当前 Broker 被选为 controller 时, 当被选为 controller,它将会做以下操作</span></div><div class="line"><span class="comment">//note: 1. 注册 controller epoch changed listener;</span></div><div class="line"><span class="comment">//note: 2. controller epoch 自增加1;</span></div><div class="line"><span class="comment">//note: 3. 初始化 KafkaController 的上下文信息 ControllerContext,它包含了当前的 topic、存活的 broker 以及已经存在的 partition 的 leader;</span></div><div class="line"><span class="comment">//note: 4. 启动 controller 的 channel 管理: 建立与其他 broker 的连接的,负责与其他 broker 之间的通信;</span></div><div class="line"><span class="comment">//note: 5. 启动 ReplicaStateMachine（副本状态机,管理副本的状态）;</span></div><div class="line"><span class="comment">//note: 6. 启动 PartitionStateMachine（分区状态机,管理分区的状态）;</span></div><div class="line"><span class="comment">//note: 如果在 Controller 服务初始化的过程中，出现了任何不可预期的 异常/错误，它将会退出当前的进程，这确保了可以再次触发 controller 的选举</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onControllerFailover</span></span>() &#123;</div><div class="line">  <span class="keyword">if</span>(isRunning) &#123;</div><div class="line">    info(<span class="string">"Broker %d starting become controller state transition"</span>.format(config.brokerId))</div><div class="line">    readControllerEpochFromZookeeper() <span class="comment">//note: 从 zk 获取 controllrt 的 epoch 和 zkVersion 值</span></div><div class="line">    incrementControllerEpoch(zkUtils.zkClient) <span class="comment">//note: 更新 Controller 的 epoch 和 zkVersion 值，可能会抛出异常</span></div><div class="line"></div><div class="line">    <span class="comment">// before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks</span></div><div class="line">    <span class="comment">//note: 再从 zk 获取数据初始化前，注册一些关于 broker/topic 的回调监听器</span></div><div class="line">    registerReassignedPartitionsListener() <span class="comment">//note: 监控路径【/admin/reassign_partitions】，分区迁移监听</span></div><div class="line">    registerIsrChangeNotificationListener() <span class="comment">//note: 监控路径【/isr_change_notification】，isr 变动监听</span></div><div class="line">    registerPreferredReplicaElectionListener() <span class="comment">//note: 监听路径【/admin/preferred_replica_election】，最优 leader 选举</span></div><div class="line">    partitionStateMachine.registerListeners()<span class="comment">//note: 监听 Topic 的创建与删除</span></div><div class="line">    replicaStateMachine.registerListeners() <span class="comment">//note: 监听 broker 的上下线</span></div><div class="line"></div><div class="line">    <span class="comment">//note: 初始化 controller 相关的变量信息:包括 alive broker 列表、partition 的详细信息等</span></div><div class="line">    initializeControllerContext() <span class="comment">//note: 初始化 controller 相关的变量信息</span></div><div class="line"></div><div class="line">    <span class="comment">// We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines</span></div><div class="line">    <span class="comment">// are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before</span></div><div class="line">    <span class="comment">// they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and</span></div><div class="line">    <span class="comment">// partitionStateMachine.startup().</span></div><div class="line">    <span class="comment">//note: 在 controller contest 初始化之后,我们需要发送 UpdateMetadata 请求在状态机启动之前,这是因为 broker 需要从 UpdateMetadata 请求</span></div><div class="line">    <span class="comment">//note: 获取当前存活的 broker list, 因为它们需要处理来自副本状态机或分区状态机启动发送的 LeaderAndIsr 请求</span></div><div class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</div><div class="line"></div><div class="line">    <span class="comment">//note: 初始化 replica 的状态信息: replica 是存活状态时是 OnlineReplica, 否则是 ReplicaDeletionIneligible</span></div><div class="line">    replicaStateMachine.startup() <span class="comment">//note: 初始化 replica 的状态信息</span></div><div class="line">    <span class="comment">//note: 初始化 partition 的状态信息:如果 leader 所在 broker 是 alive 的,那么状态为 OnlinePartition,否则为 OfflinePartition</span></div><div class="line">    <span class="comment">//note: 并状态为 OfflinePartition 的 topic 选举 leader</span></div><div class="line">    partitionStateMachine.startup() <span class="comment">//note: 初始化 partition 的状态信息</span></div><div class="line"></div><div class="line">    <span class="comment">// register the partition change listeners for all existing topics on failover</span></div><div class="line">    <span class="comment">//note: 为所有的 topic 注册 partition change 监听器</span></div><div class="line">    controllerContext.allTopics.foreach(topic =&gt; partitionStateMachine.registerPartitionChangeListener(topic))</div><div class="line">    info(<span class="string">"Broker %d is ready to serve as the new controller with epoch %d"</span>.format(config.brokerId, epoch))</div><div class="line">    maybeTriggerPartitionReassignment() <span class="comment">//note: 触发一次分区副本迁移的操作</span></div><div class="line">    maybeTriggerPreferredReplicaElection() <span class="comment">//note: 触发一次分区的最优 leader 选举操作</span></div><div class="line">    <span class="keyword">if</span> (config.autoLeaderRebalanceEnable) &#123; <span class="comment">//note: 如果开启自动均衡</span></div><div class="line">      info(<span class="string">"starting the partition rebalance scheduler"</span>)</div><div class="line">      autoRebalanceScheduler.startup()</div><div class="line">      autoRebalanceScheduler.schedule(<span class="string">"partition-rebalance-thread"</span>, checkAndTriggerPartitionRebalance,</div><div class="line">        <span class="number">5</span>, config.leaderImbalanceCheckIntervalSeconds.toLong, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">//note: 发送最新的 meta 信息</span></div><div class="line">    &#125;</div><div class="line">    deleteTopicManager.start() <span class="comment">//note: topic 删除线程启动</span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span></div><div class="line">    info(<span class="string">"Controller has been shut down, aborting startup/failover"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>onControllerFailover()</code> 所做的事情如下：</p>
<ol>
<li><code>readControllerEpochFromZookeeper()</code> 方法更新 controller 的 epoch 及 zkVersion 信息，<code>incrementControllerEpoch()</code> 方法将 controller 的 epoch 字增加1，并更新到 zk 中；</li>
<li>在控制器中注册相关的监听器，主要有6类类型，如下面表格中所列；</li>
<li>通过 <code>initializeControllerContext()</code> 方法初始化 Controller 的上下文信息，更新 Controller 的相关缓存信息、并启动 ControllerChannelManager 等；</li>
<li>向所有 alive 的 broker 发送 Update-Metadata 请求，broker 通过这个请求获取当前集群中 alive 的 broker 列表；</li>
<li>启动副本状态机，初始化所有 Replica 的状态信息，如果 Replica 所在节点是 alive 的，那么状态更新为 OnlineReplica, 否则更新为 ReplicaDeletionIneligible；</li>
<li>启动分区状态机，初始化所有 Partition 的状态信息，如果 leader 所在 broker 是 alive 的，那么状态更新为 OnlinePartition，否则更新为 OfflinePartition；</li>
<li>为当前所有 topic 注册一个 PartitionModificationsListener 监听器，监听所有 Topic 分区数的变化；</li>
<li>KafkaController 初始化完成，正式启动；</li>
<li>KafkaController 启动后，触发一次副本迁移，如果需要的情况下；</li>
<li>KafkaController 启动后，触发一次最优 leader 选举操作，如果需要的情况下；</li>
<li>KafkaController 启动后，如果开启了自动 leader 均衡，启动自动 leader 均衡线程，它会根据配置的信息定期运行。</li>
</ol>
<p>KafkaController 需要监听的 zk 节点、触发的监听方法及作用如下：</p>
<table>
<thead>
<tr>
<th>监听方法</th>
<th>监听路径</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>registerReassignedPartitionsListener</td>
<td>/admin/reassign_partitions</td>
<td>用于分区副本迁移</td>
</tr>
<tr>
<td>registerIsrChangeNotificationListener</td>
<td>/isr_change_notification</td>
<td>用于 Partition ISR 变动</td>
</tr>
<tr>
<td>registerPreferredReplicaElectionListener</td>
<td>/admin/preferred_replica_election</td>
<td>用于 Partition 最优 leader 选举</td>
</tr>
<tr>
<td>partitionStateMachine.registerTopicChangeListener()</td>
<td>/brokers/topics</td>
<td>用于 Topic 新建的监听</td>
</tr>
<tr>
<td>partitionStateMachine.registerDeleteTopicListener()</td>
<td>/admin/delete_topics</td>
<td>用于 Topic 删除的监听</td>
</tr>
<tr>
<td>replicaStateMachine.registerBrokerChangeListener()</td>
<td>/brokers/ids</td>
<td>用于 broker 上下线的监听</td>
</tr>
<tr>
<td>partitionStateMachine.registerPartitionChangeListener(topic)</td>
<td>/brokers/topics/TOPIC_NAME</td>
<td>用于 Topic Partition 扩容的监听</td>
</tr>
</tbody>
</table>
<p>在 KafkaController 中</p>
<ul>
<li>有两个状态机：分区状态机和副本状态机；</li>
<li>一个管理器：Channel 管理器，负责管理所有的 Broker 通信；</li>
<li>相关缓存：Partition 信息、Topic 信息、broker id 信息等；</li>
<li>四种 leader 选举机制：分别是用 leader offline、broker 掉线、partition reassign、最优 leader 选举时触发；</li>
</ul>
<p>如下图所示：</p>
<p><img src="/images/kafka/controller-cache.png" alt="Kafka Controller 的重要内容"></p>
<h3 id="initializeControllerContext-初始化-Controller-上下文信息"><a href="#initializeControllerContext-初始化-Controller-上下文信息" class="headerlink" title="initializeControllerContext 初始化 Controller 上下文信息"></a>initializeControllerContext 初始化 Controller 上下文信息</h3><p>在 <code>initializeControllerContext()</code> 初始化 KafkaController 上下文信息的方法中，主要做了以下事情：</p>
<ol>
<li>从 zk 获取所有 alive broker 列表，记录到 <code>liveBrokers</code>；</li>
<li>从 zk 获取所有的 topic 列表，记录到 <code>allTopic</code> 中；</li>
<li>从 zk 获取所有 Partition 的 replica 信息，更新到 <code>partitionReplicaAssignment</code> 中；</li>
<li>从 zk 获取所有 Partition 的 LeaderAndIsr 信息，更新到 <code>partitionLeadershipInfo</code> 中；</li>
<li>调用 <code>startChannelManager()</code> 启动 Controller 的 Channel Manager；</li>
<li>通过 <code>initializePreferredReplicaElection()</code> 初始化需要最优 leader 选举的 Partition 列表，记录到 <code>partitionsUndergoingPreferredReplicaElection</code> 中；</li>
<li>通过 <code>initializePartitionReassignment()</code> 方法初始化需要进行副本迁移的 Partition 列表，记录到 <code>partitionsBeingReassigned</code> 中；</li>
<li>通过 <code>initializeTopicDeletion()</code> 方法初始化需要删除的 topic 列表及 TopicDeletionManager 对象；</li>
</ol>
<p>综上，这个方法最主要的作用就是相关的 meta 信息及启动 Channel 管理器，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 初始化 KafkaController 的上下文数据</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeControllerContext</span></span>() &#123;</div><div class="line">  <span class="comment">// update controller cache with delete topic information</span></div><div class="line">  controllerContext.liveBrokers = zkUtils.getAllBrokersInCluster().toSet <span class="comment">//note: 初始化 zk 的 broker_list 信息</span></div><div class="line">  controllerContext.allTopics = zkUtils.getAllTopics().toSet <span class="comment">//note: 初始化所有的 topic 信息</span></div><div class="line">  <span class="comment">//note: 初始化所有 topic 的所有 partition 的 replica 分配</span></div><div class="line">  controllerContext.partitionReplicaAssignment = zkUtils.getReplicaAssignmentForTopics(controllerContext.allTopics.toSeq)</div><div class="line">  <span class="comment">//note: 下面两个都是新创建的空集合</span></div><div class="line">  controllerContext.partitionLeadershipInfo = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicAndPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>]</div><div class="line">  controllerContext.shuttingDownBrokerIds = mutable.<span class="type">Set</span>.empty[<span class="type">Int</span>]</div><div class="line">  <span class="comment">// update the leader and isr cache for all existing partitions from Zookeeper</span></div><div class="line">  updateLeaderAndIsrCache() <span class="comment">//note: 获取 topic-partition 的详细信息,更新到 partitionLeadershipInfo 中</span></div><div class="line">  <span class="comment">// start the channel manager</span></div><div class="line">  startChannelManager() <span class="comment">//note: 启动连接所有的 broker 的线程, 根据 broker/ids 的临时去判断要连接哪些 broker</span></div><div class="line">  initializePreferredReplicaElection() <span class="comment">//note: 初始化需要进行最优 leader 选举的 partition</span></div><div class="line">  initializePartitionReassignment() <span class="comment">//note: 初始化需要进行分区副本迁移的 partition</span></div><div class="line">  initializeTopicDeletion() <span class="comment">//note: 初始化要删除的 topic 及后台的 topic 删除线程,还有不能删除的 topic 集合</span></div><div class="line">  info(<span class="string">"Currently active brokers in the cluster: %s"</span>.format(controllerContext.liveBrokerIds))</div><div class="line">  info(<span class="string">"Currently shutting brokers in the cluster: %s"</span>.format(controllerContext.shuttingDownBrokerIds))</div><div class="line">  info(<span class="string">"Current list of topics in the cluster: %s"</span>.format(controllerContext.allTopics))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>最优 leader 选举：就是默认选择 Replica 分配中第一个 replica 作为 leader，为什么叫做最优 leader 选举呢？因为 Kafka 在给每个 Partition 分配副本时，它会保证分区的主副本会均匀分布在所有的 broker 上，这样的话只要保证第一个 replica 被选举为 leader，读写流量就会均匀分布在所有的 Broker 上，当然这是有一个前提的，那就是每个 Partition 的读写流量相差不多，但是在实际的生产环境，这是不太可能的，所以一般情况下，大集群是不建议开自动 leader 均衡的，可以通过额外的算法计算、手动去触发最优 leader 选举。</p>
</blockquote>
<h3 id="Controller-Channel-Manager"><a href="#Controller-Channel-Manager" class="headerlink" title="Controller Channel Manager"></a>Controller Channel Manager</h3><p><code>initializeControllerContext()</code> 方法会通过 <code>startChannelManager()</code> 方法初始化 ControllerChannelManager 对象，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 启动 ChannelManager 线程</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startChannelManager</span></span>() &#123;</div><div class="line">  controllerContext.controllerChannelManager = <span class="keyword">new</span> <span class="type">ControllerChannelManager</span>(controllerContext, config, time, metrics, threadNamePrefix)</div><div class="line">  controllerContext.controllerChannelManager.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ControllerChannelManager 在初始化时，会为集群中的每个节点初始化一个 ControllerBrokerStateInfo 对象，该对象包含四个部分：</p>
<ol>
<li>NetworkClient：网络连接对象；</li>
<li>Node：节点信息；</li>
<li>BlockingQueue：请求队列；</li>
<li>RequestSendThread：请求的发送线程。</li>
</ol>
<p>其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 控制所有已经存活 broker 的网络连接</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControllerChannelManager</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>, config: <span class="type">KafkaConfig</span>, time: <span class="type">Time</span>, metrics: <span class="type">Metrics</span>, threadNamePrefix: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">protected</span> <span class="keyword">val</span> brokerStateInfo = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ControllerBrokerStateInfo</span>]</div><div class="line">  controllerContext.liveBrokers.foreach(addNewBroker) <span class="comment">//note: 获取目前已经存活的所有 broker</span></div><div class="line">  <span class="comment">//note: 添加一个新的 broker（初始化时,这个方法相当于连接当前存活的所有 broker）</span></div><div class="line">  <span class="comment">//note: 建立网络连接、启动请求发送线程</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addNewBroker</span></span>(broker: <span class="type">Broker</span>) &#123;</div><div class="line">    <span class="keyword">val</span> messageQueue = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">QueueItem</span>]</div><div class="line">    debug(<span class="string">"Controller %d trying to connect to broker %d"</span>.format(config.brokerId, broker.id))</div><div class="line">    <span class="keyword">val</span> brokerEndPoint = broker.getBrokerEndPoint(config.interBrokerListenerName)</div><div class="line">    <span class="keyword">val</span> brokerNode = <span class="keyword">new</span> <span class="type">Node</span>(broker.id, brokerEndPoint.host, brokerEndPoint.port)</div><div class="line">    <span class="keyword">val</span> networkClient = &#123; <span class="comment">//note: 初始化 NetworkClient</span></div><div class="line">      <span class="keyword">val</span> channelBuilder = <span class="type">ChannelBuilders</span>.clientChannelBuilder(</div><div class="line">        config.interBrokerSecurityProtocol,</div><div class="line">        <span class="type">LoginType</span>.<span class="type">SERVER</span>,</div><div class="line">        config.values,</div><div class="line">        config.saslMechanismInterBrokerProtocol,</div><div class="line">        config.saslInterBrokerHandshakeRequestEnable</div><div class="line">      )</div><div class="line">      <span class="keyword">val</span> selector = <span class="keyword">new</span> <span class="type">Selector</span>(</div><div class="line">        <span class="type">NetworkReceive</span>.<span class="type">UNLIMITED</span>,</div><div class="line">        <span class="type">Selector</span>.<span class="type">NO_IDLE_TIMEOUT_MS</span>,</div><div class="line">        metrics,</div><div class="line">        time,</div><div class="line">        <span class="string">"controller-channel"</span>,</div><div class="line">        <span class="type">Map</span>(<span class="string">"broker-id"</span> -&gt; broker.id.toString).asJava,</div><div class="line">        <span class="literal">false</span>,</div><div class="line">        channelBuilder</div><div class="line">      )</div><div class="line">      <span class="keyword">new</span> <span class="type">NetworkClient</span>(</div><div class="line">        selector,</div><div class="line">        <span class="keyword">new</span> <span class="type">ManualMetadataUpdater</span>(<span class="type">Seq</span>(brokerNode).asJava),</div><div class="line">        config.brokerId.toString,</div><div class="line">        <span class="number">1</span>,</div><div class="line">        <span class="number">0</span>,</div><div class="line">        <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>,</div><div class="line">        <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>,</div><div class="line">        config.requestTimeoutMs,</div><div class="line">        time,</div><div class="line">        <span class="literal">false</span></div><div class="line">      )</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="string">"Controller-%d-to-broker-%d-send-thread"</span>.format(config.brokerId, broker.id)</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(name) =&gt; <span class="string">"%s:Controller-%d-to-broker-%d-send-thread"</span>.format(name, config.brokerId, broker.id)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> requestThread = <span class="keyword">new</span> <span class="type">RequestSendThread</span>(config.brokerId, controllerContext, messageQueue, networkClient,</div><div class="line">      brokerNode, config, time, threadName) <span class="comment">//note: 初始化 requestThread</span></div><div class="line">    requestThread.setDaemon(<span class="literal">false</span>) <span class="comment">//note: 非守护进程</span></div><div class="line">    brokerStateInfo.put(broker.id, <span class="keyword">new</span> <span class="type">ControllerBrokerStateInfo</span>(networkClient, brokerNode, messageQueue, requestThread))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清楚了上面的逻辑，再来看 KafkaController 部分是如何向 Broker 发送请求的？</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sendRequest(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</div><div class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) = &#123;</div><div class="line">  controllerContext.controllerChannelManager.sendRequest(brokerId, apiKey, request, callback)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>KafkaController 实际上是调用的 ControllerChannelManager 的 <code>sendRequest()</code> 方法向 Broker 发送请求信息，其实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向 broker 发送请求（并没有真正发送,只是添加到对应的 queue 中）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</div><div class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</div><div class="line">  brokerLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> stateInfoOpt = brokerStateInfo.get(brokerId)</div><div class="line">    stateInfoOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(stateInfo) =&gt;</div><div class="line">        stateInfo.messageQueue.put(<span class="type">QueueItem</span>(apiKey, request, callback))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"Not sending request %s to broker %d, since it is offline."</span>.format(request, brokerId))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它实际上只是把对应的请求添加到该 Broker 对应的 MessageQueue 中，并没有真正的去发送请求，请求的的发送是在 每台 Broker 对应的 RequestSendThread 中处理的。</p>
<h2 id="Controller-原生的四种-leader-选举机制"><a href="#Controller-原生的四种-leader-选举机制" class="headerlink" title="Controller 原生的四种 leader 选举机制"></a>Controller 原生的四种 leader 选举机制</h2><p>KafkaController 在初始化时，也会初始化四种不同的 leader 选举机制，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: partition leader 挂掉时，选举 leader</span></div><div class="line"><span class="keyword">val</span> offlinePartitionSelector = <span class="keyword">new</span> <span class="type">OfflinePartitionLeaderSelector</span>(controllerContext, config)</div><div class="line"><span class="comment">//note: 重新分配分区时，leader 选举</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> reassignedPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ReassignedPartitionLeaderSelector</span>(controllerContext)</div><div class="line"><span class="comment">//note: 使用最优的副本作为 leader</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">PreferredReplicaPartitionLeaderSelector</span>(controllerContext)</div><div class="line"><span class="comment">//note: broker 掉线时，重新选举 leader</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">val</span> controlledShutdownPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ControlledShutdownLeaderSelector</span>(controllerContext)</div></pre></td></tr></table></figure>
<p>四种 leader 选举实现类及对应触发条件如下所示：</p>
<table>
<thead>
<tr>
<th>实现</th>
<th>触发条件</th>
</tr>
</thead>
<tbody>
<tr>
<td>OfflinePartitionLeaderSelector</td>
<td>leader 掉线时触发</td>
</tr>
<tr>
<td>ReassignedPartitionLeaderSelector</td>
<td>分区的副本重新分配数据同步完成后触发的</td>
</tr>
<tr>
<td>PreferredReplicaPartitionLeaderSelector</td>
<td>最优 leader 选举，手动触发或自动 leader 均衡调度时触发</td>
</tr>
<tr>
<td>ControlledShutdownLeaderSelector</td>
<td>broker 发送 ShutDown 请求主动关闭服务时触发</td>
</tr>
</tbody>
</table>
<h3 id="OfflinePartitionLeaderSelector"><a href="#OfflinePartitionLeaderSelector" class="headerlink" title="OfflinePartitionLeaderSelector"></a>OfflinePartitionLeaderSelector</h3><p>OfflinePartitionLeaderSelector Partition leader 选举的逻辑是：</p>
<ol>
<li>如果 isr 中至少有一个副本是存活的，那么从该 Partition 存活的 isr 中选举第一个副本作为新的 leader，存活的 isr 作为新的 isr；</li>
<li>否则，如果脏选举（unclear elect）是禁止的，那么就抛出 NoReplicaOnlineException 异常；</li>
<li>否则，即允许脏选举的情况下，从存活的、所分配的副本（不在 isr 中的副本）中选出一个副本作为新的 leader 和新的 isr 集合；</li>
<li>否则，即是 Partition 分配的副本没有存活的，抛出 NoReplicaOnlineException 异常；</li>
</ol>
<p>一旦 leader 被成功注册到 zk 中，它将会更新到 KafkaController 缓存中的 allLeaders 中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于 LeaderAndIsrRequest， 选举一个新的 leader、isr 和 receiving replicas</span></div><div class="line"><span class="comment">//note: 1.如果 isr 中至少有一个副本是存活的，那么存活的 isr 中选举一个副本作为新的 leader，存活的 isr 作为新的 isr；</span></div><div class="line"><span class="comment">//note: 2.否则，如果脏选举（unclear elect）是禁止的，那么就抛出 NoReplicaOnlineException 异常；</span></div><div class="line"><span class="comment">//note: 3.否则，从存活的、所分配的副本中选出一个副本作为新的 leader 和新的 isr 集合；</span></div><div class="line"><span class="comment">//note: 4.否则，partition 分配的副本没有存活的，抛出 NoReplicaOnlineException 异常；</span></div><div class="line"><span class="comment">//note: 一旦 leader 被成功注册到 zk 中，它将更新缓存中的 allLeaders。</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">OfflinePartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span>, config: <span class="type">KafkaConfig</span></span>)</span></div><div class="line">  <span class="keyword">extends</span> <span class="type">PartitionLeaderSelector</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[OfflinePartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="comment">//note: leader 选举，过程如上面所述</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    controllerContext.partitionReplicaAssignment.get(topicAndPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(assignedReplicas) =&gt;</div><div class="line">        <span class="comment">//note: AR 中还存活的副本</span></div><div class="line">        <span class="keyword">val</span> liveAssignedReplicas = assignedReplicas.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">        <span class="comment">//note: 当前 isr 中还存活的副本</span></div><div class="line">        <span class="keyword">val</span> liveBrokersInIsr = currentLeaderAndIsr.isr.filter(r =&gt; controllerContext.liveBrokerIds.contains(r))</div><div class="line">        <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch <span class="comment">//note: epoch</span></div><div class="line">        <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion <span class="comment">//note: zkVersion</span></div><div class="line">        <span class="comment">//note: 选取新的 leader 和 isr</span></div><div class="line">        <span class="keyword">val</span> newLeaderAndIsr =</div><div class="line">          <span class="keyword">if</span> (liveBrokersInIsr.isEmpty) &#123; <span class="comment">//note: 当前 isr 中副本都挂了</span></div><div class="line">            <span class="comment">// Prior to electing an unclean (i.e. non-ISR) leader, ensure that doing so is not disallowed by the configuration</span></div><div class="line">            <span class="comment">// for unclean leader election.</span></div><div class="line">            <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(config.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(controllerContext.zkUtils,</div><div class="line">              <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicAndPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许脏选举的话，抛异常</span></div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>((<span class="string">"No broker in ISR for partition "</span> +</div><div class="line">                <span class="string">"%s is alive. Live brokers are: [%s],"</span>.format(topicAndPartition, controllerContext.liveBrokerIds)) +</div><div class="line">                <span class="string">" ISR brokers are: [%s]"</span>.format(currentLeaderAndIsr.isr.mkString(<span class="string">","</span>)))</div><div class="line">            &#125;</div><div class="line">            debug(<span class="string">"No broker in ISR is alive for %s. Pick the leader from the alive assigned replicas: %s"</span></div><div class="line">              .format(topicAndPartition, liveAssignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="keyword">if</span> (liveAssignedReplicas.isEmpty) &#123; <span class="comment">//note: 副本全挂了，抛异常</span></div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>((<span class="string">"No replica for partition "</span> +</div><div class="line">                <span class="string">"%s is alive. Live brokers are: [%s],"</span>.format(topicAndPartition, controllerContext.liveBrokerIds)) +</div><div class="line">                <span class="string">" Assigned replicas are: [%s]"</span>.format(assignedReplicas))</div><div class="line">            &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 从存活的副本中选举 leader（不能保证选举的是 LEO 最大的副本），并将该副本作为 isr</span></div><div class="line">              <span class="type">ControllerStats</span>.uncleanLeaderElectionRate.mark()</div><div class="line">              <span class="keyword">val</span> newLeader = liveAssignedReplicas.head <span class="comment">//note: 选择第一个作为 leader</span></div><div class="line">              warn(<span class="string">"No broker in ISR is alive for %s. Elect leader %d from live brokers %s. There's potential data loss."</span></div><div class="line">                .format(topicAndPartition, newLeader, liveAssignedReplicas.mkString(<span class="string">","</span>)))</div><div class="line">              <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, <span class="type">List</span>(newLeader), currentLeaderIsrZkPathVersion + <span class="number">1</span>)</div><div class="line">            &#125;</div><div class="line">          &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 isr 中还有副本存活</span></div><div class="line">            <span class="keyword">val</span> liveReplicasInIsr = liveAssignedReplicas.filter(r =&gt; liveBrokersInIsr.contains(r))</div><div class="line">            <span class="keyword">val</span> newLeader = liveReplicasInIsr.head <span class="comment">//note: 第一个作为 leader</span></div><div class="line">            debug(<span class="string">"Some broker in ISR is alive for %s. Select %d from ISR %s to be the leader."</span></div><div class="line">              .format(topicAndPartition, newLeader, liveBrokersInIsr.mkString(<span class="string">","</span>)))</div><div class="line">            <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, liveBrokersInIsr.toList, currentLeaderIsrZkPathVersion + <span class="number">1</span>)</div><div class="line">          &#125;</div><div class="line">        info(<span class="string">"Selected new leader and ISR %s for offline partition %s"</span>.format(newLeaderAndIsr.toString(), topicAndPartition))</div><div class="line">        (newLeaderAndIsr, liveAssignedReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"Partition %s doesn't have replicas assigned to it"</span>.format(topicAndPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">`</div></pre></td></tr></table></figure>
<h3 id="ReassignedPartitionLeaderSelector"><a href="#ReassignedPartitionLeaderSelector" class="headerlink" title="ReassignedPartitionLeaderSelector"></a>ReassignedPartitionLeaderSelector</h3><p>ReassignedPartitionLeaderSelector 是在 Partition 副本迁移后，副本同步完成（RAR 都处在 isr 中，RAR 指的是该 Partition 新分配的副本）后触发的，其 leader 选举逻辑如下：</p>
<ol>
<li>leader 选择存活的 RAR 中的第一个副本，此时 RAR 都在 isr 中了；</li>
<li>new isr 是所有存活的 RAR 副本列表；</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 重新分配分区时，partition 的 leader 选举策略</span></div><div class="line"><span class="comment">//note: new leader = 新分配并且在 isr 中的一个副本</span></div><div class="line"><span class="comment">//note: new isr = 当前的 isr</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的副本 = reassigned replicas</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReassignedPartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>) <span class="keyword">extends</span> <span class="title">PartitionLeaderSelector</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[ReassignedPartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="comment">/**</span></div><div class="line">   * The reassigned replicas are already in the ISR when selectLeader is called.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 当这个方法被调用时，要求新分配的副本已经在 isr 中了</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="comment">//note: 新分配的 replica 列表</span></div><div class="line">    <span class="keyword">val</span> reassignedInSyncReplicas = controllerContext.partitionsBeingReassigned(topicAndPartition).newReplicas</div><div class="line">    <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch</div><div class="line">    <span class="comment">//note: 当前的 zk version</span></div><div class="line">    <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion</div><div class="line">    <span class="comment">//note: 新分配的 replica 列表，并且其 broker 存活、且在 isr 中</span></div><div class="line">    <span class="keyword">val</span> aliveReassignedInSyncReplicas = reassignedInSyncReplicas.filter(r =&gt; controllerContext.liveBrokerIds.contains(r) &amp;&amp;</div><div class="line">                                                                             currentLeaderAndIsr.isr.contains(r))</div><div class="line">    <span class="comment">//note: 选择第一个作为新的 leader</span></div><div class="line">    <span class="keyword">val</span> newLeaderOpt = aliveReassignedInSyncReplicas.headOption</div><div class="line">    newLeaderOpt <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(newLeader) =&gt; (<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, currentLeaderAndIsr.isr,</div><div class="line">        currentLeaderIsrZkPathVersion + <span class="number">1</span>), reassignedInSyncReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        reassignedInSyncReplicas.size <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"List of reassigned replicas for partition "</span> +</div><div class="line">              <span class="string">" %s is empty. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">          <span class="keyword">case</span> _ =&gt;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoReplicaOnlineException</span>(<span class="string">"None of the reassigned replicas for partition "</span> +</div><div class="line">              <span class="string">"%s are in-sync with the leader. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="PreferredReplicaPartitionLeaderSelector"><a href="#PreferredReplicaPartitionLeaderSelector" class="headerlink" title="PreferredReplicaPartitionLeaderSelector"></a>PreferredReplicaPartitionLeaderSelector</h3><p>PreferredReplicaPartitionLeaderSelector 是最优 leader 选举，选择 AR（assign replica）中的第一个副本作为 leader，前提是该 replica 在是存活的、并且在 isr 中，否则会抛出 StateChangeFailedException 的异常。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 最优的 leader 选举策略（主要用于自动 leader 均衡，选择 AR 中第一个为 leader，前提是它在 isr 中，这样整个集群的 leader 是均衡的,否则抛出异常）</span></div><div class="line"><span class="comment">//note: new leader = 第一个 replica（alive and in isr）</span></div><div class="line"><span class="comment">//note: new isr = 当前 isr</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的 replica = AR</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">PreferredReplicaPartitionLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>) <span class="keyword">extends</span> <span class="title">PartitionLeaderSelector</span></span></div><div class="line"><span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[PreferredReplicaPartitionLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="comment">//note: Partition 的 AR</span></div><div class="line">    <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="comment">//note: preferredReplica，第一个 replica</span></div><div class="line">    <span class="keyword">val</span> preferredReplica = assignedReplicas.head</div><div class="line">    <span class="comment">// check if preferred replica is the current leader</span></div><div class="line">    <span class="comment">//note: 当前的 leader</span></div><div class="line">    <span class="keyword">val</span> currentLeader = controllerContext.partitionLeadershipInfo(topicAndPartition).leaderAndIsr.leader</div><div class="line">    <span class="keyword">if</span> (currentLeader == preferredReplica) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">LeaderElectionNotNeededException</span>(<span class="string">"Preferred replica %d is already the current leader for partition %s"</span></div><div class="line">                                                   .format(preferredReplica, topicAndPartition))</div><div class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 当前 leader 不是 preferredReplica 的情况</span></div><div class="line">      info(<span class="string">"Current leader %d for partition %s is not the preferred replica."</span>.format(currentLeader, topicAndPartition) +</div><div class="line">        <span class="string">" Triggering preferred replica leader election"</span>)</div><div class="line">      <span class="comment">// check if preferred replica is not the current leader and is alive and in the isr</span></div><div class="line">      <span class="comment">//note: preferredReplica 是 alive 并且在 isr 中</span></div><div class="line">      <span class="keyword">if</span> (controllerContext.liveBrokerIds.contains(preferredReplica) &amp;&amp; currentLeaderAndIsr.isr.contains(preferredReplica)) &#123;</div><div class="line">        (<span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(preferredReplica, currentLeaderAndIsr.leaderEpoch + <span class="number">1</span>, currentLeaderAndIsr.isr,</div><div class="line">          currentLeaderAndIsr.zkVersion + <span class="number">1</span>), assignedReplicas)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">"Preferred replica %d for partition "</span>.format(preferredReplica) +</div><div class="line">          <span class="string">"%s is either not alive or not in the isr. Current leader and ISR: [%s]"</span>.format(topicAndPartition, currentLeaderAndIsr))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ControlledShutdownLeaderSelector"><a href="#ControlledShutdownLeaderSelector" class="headerlink" title="ControlledShutdownLeaderSelector"></a>ControlledShutdownLeaderSelector</h3><p>ControlledShutdownLeaderSelector 是在处理 broker 下线时调用的 leader 选举方法，它会选举 isr 中第一个没有正在关闭的 replica 作为 leader，否则抛出 StateChangeFailedException 异常。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Broker 掉线时，重新选举 leader 调用的 leader 选举方法</span></div><div class="line"><span class="comment">//note: new leader = 在 isr 中，并且没有正在 shutdown 的 replica</span></div><div class="line"><span class="comment">//note: new isr = 当前 isr 除去关闭的 replica</span></div><div class="line"><span class="comment">//note: 接收 LeaderAndIsr request 的 replica = 存活的 AR</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControlledShutdownLeaderSelector</span>(<span class="params">controllerContext: <span class="type">ControllerContext</span></span>)</span></div><div class="line">        <span class="keyword">extends</span> <span class="type">PartitionLeaderSelector</span></div><div class="line">        <span class="keyword">with</span> <span class="type">Logging</span> &#123;</div><div class="line"></div><div class="line">  <span class="keyword">this</span>.logIdent = <span class="string">"[ControlledShutdownLeaderSelector]: "</span></div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">selectLeader</span></span>(topicAndPartition: <span class="type">TopicAndPartition</span>, currentLeaderAndIsr: <span class="type">LeaderAndIsr</span>): (<span class="type">LeaderAndIsr</span>, <span class="type">Seq</span>[<span class="type">Int</span>]) = &#123;</div><div class="line">    <span class="keyword">val</span> currentLeaderEpoch = currentLeaderAndIsr.leaderEpoch</div><div class="line">    <span class="keyword">val</span> currentLeaderIsrZkPathVersion = currentLeaderAndIsr.zkVersion</div><div class="line"></div><div class="line">    <span class="keyword">val</span> currentLeader = currentLeaderAndIsr.leader</div><div class="line"></div><div class="line">    <span class="keyword">val</span> assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition)</div><div class="line">    <span class="keyword">val</span> liveOrShuttingDownBrokerIds = controllerContext.liveOrShuttingDownBrokerIds</div><div class="line">    <span class="comment">//note: 存活的 AR</span></div><div class="line">    <span class="keyword">val</span> liveAssignedReplicas = assignedReplicas.filter(r =&gt; liveOrShuttingDownBrokerIds.contains(r))</div><div class="line"></div><div class="line">    <span class="comment">//note: 从当前 isr 中过滤掉正在 shutdown 的 broker</span></div><div class="line">    <span class="keyword">val</span> newIsr = currentLeaderAndIsr.isr.filter(brokerId =&gt; !controllerContext.shuttingDownBrokerIds.contains(brokerId))</div><div class="line">    liveAssignedReplicas.find(newIsr.contains) <span class="keyword">match</span> &#123; <span class="comment">//note: find 方法返回的是第一满足条件的元素，AR 中第一个在 newIsr 集合中的元素被选为 leader</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(newLeader) =&gt;</div><div class="line">        debug(<span class="string">"Partition %s : current leader = %d, new leader = %d"</span>.format(topicAndPartition, currentLeader, newLeader))</div><div class="line">        (<span class="type">LeaderAndIsr</span>(newLeader, currentLeaderEpoch + <span class="number">1</span>, newIsr, currentLeaderIsrZkPathVersion + <span class="number">1</span>), liveAssignedReplicas)</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>((<span class="string">"No other replicas in ISR %s for %s besides"</span> +</div><div class="line">          <span class="string">" shutting down brokers %s"</span>).format(currentLeaderAndIsr.isr.mkString(<span class="string">","</span>), topicAndPartition, controllerContext.shuttingDownBrokerIds.mkString(<span class="string">","</span>)))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从本篇文章开始，Kafka 源码解析就正式进入了 Controller 部分，Controller 作为 Kafka Server 端一个重要的组件，它的角色类似于其他分布式系统 Master 的角色，跟其他系统不一样的是，Kafka 集群的任何一台 Broker 都可以作
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 ReplicaManager 详解（十五）</title>
    <link href="http://matt33.com/2018/05/01/kafka-replica-manager/"/>
    <id>http://matt33.com/2018/05/01/kafka-replica-manager/</id>
    <published>2018-05-01T04:07:01.000Z</published>
    <updated>2018-05-01T04:35:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类为入口，通过对 ReplicaManager 的详解，顺便再把 Kafka 存储层的内容做一个简单的总结。</p>
<h2 id="ReplicaManager-简介"><a href="#ReplicaManager-简介" class="headerlink" title="ReplicaManager 简介"></a>ReplicaManager 简介</h2><p>前面三篇文章，关于 Produce 请求、Fetch 请求以及副本同步流程的启动都是由 ReplicaManager 来控制的，ReplicaManager 可以说是 Server 端重要的组成部分，回头再仔细看下 KafkaApi 这个类，就会发现 Server 端要处理的多种类型的请求都是 ReplicaManager 来处理的，ReplicaManager 需要处理的请求的有以下六种：</p>
<ol>
<li>LeaderAndIsr 请求；</li>
<li>StopReplica 请求；</li>
<li>UpdateMetadata 请求；</li>
<li>Produce 请求；</li>
<li>Fetch 请求；</li>
<li>ListOffset 请求；</li>
</ol>
<p>其中后面三个已经在前面的文章中介绍过，前面三个都是 Controller 发送的请求，虽然是由 ReplicaManager 中处理的，也会在 Controller 部分展开详细的介绍。</p>
<p>这里先看下面这张图，这张图把 ReplicaManager、Partition、Replica、LogManager、Log、logSegment 这几个抽象的类之间的调用关系简单地串了起来，也算是对存储层这几个重要的部分简单总结了一下：</p>
<p><img src="/images/kafka/replica-manager.png" alt="存储层各个类之间关系"></p>
<p>对着上面的图，简单总结一下：</p>
<ol>
<li>ReplicaManager 是最外层暴露的一个实例，前面说的那几种类型的请求都是由这个实例来处理的；</li>
<li>LogManager 负责管理本节点上所有的日志（Log）实例，它作为 ReplicaManager 的变量传入到了 ReplicaManager 中，ReplicaManager 通过 LogManager 可以对相应的日志实例进行操作；</li>
<li>在 ReplicaManager 中有一个变量：allPartitions，它负责管理本节点所有的 Partition 实例（只要本节点有这个 partition 的日志实例，就会有一个对应的 Partition 对对象实例）；</li>
<li>在创建 Partition 实例时，ReplicaManager 也会作为成员变量传入到 Partition 实例中，Partition 通过 ReplicaManager 可以获得 LogManager 实例、brokerId 等；</li>
<li>Partition 会为它的每一个副本创建一个 Replica 对象实例，但只会为那个在本地副本创建 Log 对象实例（LogManager 不存在这个 Log 对象的情况下，有的话直接引用），这样的话，本地的 Replica 也就与 Log 实例建立了一个联系。</li>
</ol>
<p>关于 ReplicaManager 的 allPartitions 变量可以看下面这张图（假设 Partition 设置的是3副本）：</p>
<p><img src="/images/kafka/all-partition.png" alt="ReplicaManager 的 allPartitions 变量"></p>
<p>allPartitions 管理的 Partition 实例，因为是 3 副本，所以每个 Partition 实例又会管理着三个 Replica，其中只有本地副本（对于上图，就是值 replica.id = 1 的副本）才有对应的 Log 实例对象（HW 和 LEO 的介绍参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B">Offset 那些事</a>）。</p>
<h2 id="ReplicaManager-启动"><a href="#ReplicaManager-启动" class="headerlink" title="ReplicaManager 启动"></a>ReplicaManager 启动</h2><p>KafkaServer 在启动时，就初始化了 ReplicaManager 实例，如下所示，KafkaServer 在初始化 logManager 后，将 logManager 作为参数传递给了 ReplicaManager。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(isShuttingDown.get)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"Kafka server is still shutting down, cannot re-start!"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(startupComplete.get)</div><div class="line">      <span class="keyword">return</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> canStartup = isStartingUp.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">    <span class="keyword">if</span> (canStartup) &#123;</div><div class="line">      brokerState.newState(<span class="type">Starting</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* start scheduler */</span></div><div class="line">      kafkaScheduler.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* setup zookeeper */</span></div><div class="line">      zkUtils = initZk()</div><div class="line"></div><div class="line">      <span class="comment">/* Get or create cluster_id */</span></div><div class="line">      _clusterId = getOrGenerateClusterId(zkUtils)</div><div class="line">      info(<span class="string">s"Cluster ID = <span class="subst">$clusterId</span>"</span>)</div><div class="line"></div><div class="line">      <span class="comment">/* generate brokerId */</span></div><div class="line">      config.brokerId =  getBrokerId</div><div class="line">      <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Server "</span> + config.brokerId + <span class="string">"], "</span></div><div class="line"></div><div class="line">      <span class="comment">/* start log manager */</span></div><div class="line">      <span class="comment">//note: 启动日志管理线程</span></div><div class="line">      logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">      logManager.startup()</div><div class="line"></div><div class="line">      <span class="comment">/* start replica manager */</span></div><div class="line">      <span class="comment">//note: 启动 replica manager</span></div><div class="line">      replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager,</div><div class="line">        isShuttingDown, quotaManagers.follower)</div><div class="line">      replicaManager.startup()</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">      isStartingUp.set(<span class="literal">false</span>)</div><div class="line">      shutdown()</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">    &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h3 id="startup"><a href="#startup" class="headerlink" title="startup"></a>startup</h3><p>ReplicaManager <code>startup()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">// start ISR expiration thread</span></div><div class="line">  <span class="comment">// A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR</span></div><div class="line">  <span class="comment">//note: 周期性检查 isr 是否有 replica 过期需要从 isr 中移除</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-expiration"</span>, maybeShrinkIsr, period = config.replicaLagTimeMaxMs / <span class="number">2</span>, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  <span class="comment">//note: 周期性检查是不是有 topic-partition 的 isr 需要变动,如果需要,就更新到 zk 上,来触发 controller</span></div><div class="line">  scheduler.schedule(<span class="string">"isr-change-propagation"</span>, maybePropagateIsrChanges, period = <span class="number">2500</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法与 LogManager 的 <code>startup()</code> 方法类似，也是启动了相应的定时任务，这里，ReplicaManger 启动了两个周期性的任务：</p>
<ol>
<li>maybeShrinkIsr: 判断 topic-partition 的 isr 是否有 replica 因为延迟或 hang 住需要从 isr 中移除；</li>
<li>maybePropagateIsrChanges：判断是不是需要对 isr 进行更新，如果有 topic-partition 的 isr 发生了变动需要进行更新，那么这个方法就会被调用，它会触发 zk 的相应节点，进而触发 controller 进行相应的操作。</li>
</ol>
<p>关于 ReplicaManager 这两个方法的处理过程及 topic-partition isr 变动情况的触发，下面这张流程图做了简单的说明，如下所示：</p>
<p><img src="/images/kafka/replica-manager-startup.png" alt="ReplicaManager 的 Startup 方法启动两个周期性任务及 isr 扩充的情况"></p>
<h3 id="maybeShrinkIsr"><a href="#maybeShrinkIsr" class="headerlink" title="maybeShrinkIsr"></a>maybeShrinkIsr</h3><p>如前面流程图所示， ReplicaManager 的 <code>maybeShrinkIsr()</code> 实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 遍历所有的 partition 对象,检查其 isr 是否需要抖动</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  trace(<span class="string">"Evaluating ISR list of partitions to see which replicas can be removed from the ISR"</span>)</div><div class="line">  allPartitions.values.foreach(partition =&gt; partition.maybeShrinkIsr(config.replicaLagTimeMaxMs))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code>  会遍历本节点所有的 Partition 实例，来检查它们 isr 中的 replica 是否需要从 isr 中移除，Partition 中这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查这个 isr 中的每个 replcia</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(replicaMaxLagTimeMs: <span class="type">Long</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123; <span class="comment">//note: 只有本地副本是 leader, 才会做这个操作</span></div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="comment">//note: 检查当前 isr 的副本是否需要从 isr 中移除</span></div><div class="line">        <span class="keyword">val</span> outOfSyncReplicas = getOutOfSyncReplicas(leaderReplica, replicaMaxLagTimeMs)</div><div class="line">        <span class="keyword">if</span>(outOfSyncReplicas.nonEmpty) &#123;</div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas <span class="comment">//note: new isr</span></div><div class="line">          assert(newInSyncReplicas.nonEmpty)</div><div class="line">          info(<span class="string">"Shrinking ISR for partition [%s,%d] from %s to %s"</span>.format(topic, partitionId,</div><div class="line">            inSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>), newInSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line">          <span class="comment">// update ISR in zk and in cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新 isr 到 zk</span></div><div class="line">          <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line"></div><div class="line">          replicaManager.isrShrinkRate.mark() <span class="comment">//note: 更新 metrics</span></div><div class="line">          maybeIncrementLeaderHW(leaderReplica) <span class="comment">//note: isr 变动了,判断是否需要更新 partition 的 hw</span></div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="literal">false</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// do nothing if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 检查 isr 中的副本是否需要从 isr 中移除</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOutOfSyncReplicas</span></span>(leaderReplica: <span class="type">Replica</span>, maxLagMs: <span class="type">Long</span>): <span class="type">Set</span>[<span class="type">Replica</span>] = &#123;</div><div class="line">  <span class="comment">//note: 获取那些不应该咋 isr 中副本的列表</span></div><div class="line">  <span class="comment">//note: 1. hang 住的 replica: replica 的 LEO 超过 maxLagMs 没有更新, 那么这个 replica 将会被从 isr 中移除;</span></div><div class="line">  <span class="comment">//note: 2. 数据同步慢的 replica: 副本在 maxLagMs 内没有追上 leader 当前的 LEO, 那么这个 replica 讲会从 ist 中移除;</span></div><div class="line">  <span class="comment">//note: 都是通过 lastCaughtUpTimeMs 来判断的</span></div><div class="line">  <span class="keyword">val</span> candidateReplicas = inSyncReplicas - leaderReplica</div><div class="line"></div><div class="line">  <span class="keyword">val</span> laggingReplicas = candidateReplicas.filter(r =&gt; (time.milliseconds - r.lastCaughtUpTimeMs) &gt; maxLagMs)</div><div class="line">  <span class="keyword">if</span> (laggingReplicas.nonEmpty)</div><div class="line">    debug(<span class="string">"Lagging replicas for partition %s are %s"</span>.format(topicPartition, laggingReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</div><div class="line"></div><div class="line">  laggingReplicas</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>maybeShrinkIsr()</code> 这个方法的实现可以简单总结为以下几步：</p>
<ol>
<li>先判断本地副本是不是这个 partition 的 leader，<strong>这个操作只会在 leader 上进行</strong>，如果不是 leader 直接跳过；</li>
<li>通过 <code>getOutOfSyncReplicas()</code> 方法遍历除 leader 外 isr 的所有 replica，找到那些满足条件（<strong>落后超过 maxLagMs 时间的副本</strong>）需要从 isr 中移除的 replica；</li>
<li>得到了新的 isr 列表，调用 <code>updateIsr()</code> 将新的 isr 更新到 zk 上，并且这个方法内部又调用了 ReplicaManager 的 <code>recordIsrChange()</code> 方法来告诉 ReplicaManager 当前这个 topic-partition 的 isr 发生了变化（<strong>可以看出，zk 上这个 topic-partition 的 isr 信息虽然变化了，但是实际上 controller 还是无法感知的</strong>）；</li>
<li>因为 isr 发生了变动，所以这里会通过 <code>maybeIncrementLeaderHW()</code> 方法来检查一下这个 partition 的 HW 是否需要更新。</li>
</ol>
<p><code>updateIsr()</code> 和 <code>maybeIncrementLeaderHW()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查是否需要更新 partition 的 HW,这个方法将在两种情况下触发:</span></div><div class="line"><span class="comment">//note: 1.Partition ISR 变动; 2. 任何副本的 LEO 改变;</span></div><div class="line"><span class="comment">//note: 在获取 HW 时,是从 isr 和认为能追得上的副本中选择最小的 LEO,之所以也要从能追得上的副本中选择,是为了等待 follower 追上 HW,否则可能没机会追上了</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeIncrementLeaderHW</span></span>(leaderReplica: <span class="type">Replica</span>, curTime: <span class="type">Long</span> = time.milliseconds): <span class="type">Boolean</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取 isr 以及能够追上 isr （认为最近一次 fetch 的时间在 replica.lag.time.max.time 之内） 副本的 LEO 信息。</span></div><div class="line">  <span class="keyword">val</span> allLogEndOffsets = assignedReplicas.filter &#123; replica =&gt;</div><div class="line">    curTime - replica.lastCaughtUpTimeMs &lt;= replicaManager.config.replicaLagTimeMaxMs || inSyncReplicas.contains(replica)</div><div class="line">  &#125;.map(_.logEndOffset)</div><div class="line">  <span class="keyword">val</span> newHighWatermark = allLogEndOffsets.min(<span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>.<span class="type">OffsetOrdering</span>) <span class="comment">//note: 新的 HW</span></div><div class="line">  <span class="keyword">val</span> oldHighWatermark = leaderReplica.highWatermark</div><div class="line">  <span class="keyword">if</span> (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset || oldHighWatermark.onOlderSegment(newHighWatermark)) &#123;</div><div class="line">    leaderReplica.highWatermark = newHighWatermark</div><div class="line">    debug(<span class="string">"High watermark for partition [%s,%d] updated to %s"</span>.format(topic, partitionId, newHighWatermark))</div><div class="line">    <span class="literal">true</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    debug(<span class="string">"Skipping update high watermark since Old hw %s is larger than new hw %s for partition [%s,%d]. All leo's are %s"</span></div><div class="line">      .format(oldHighWatermark, newHighWatermark, topic, partitionId, allLogEndOffsets.mkString(<span class="string">","</span>)))</div><div class="line">    <span class="literal">false</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateIsr</span></span>(newIsr: <span class="type">Set</span>[<span class="type">Replica</span>]) &#123;</div><div class="line">  <span class="keyword">val</span> newLeaderAndIsr = <span class="keyword">new</span> <span class="type">LeaderAndIsr</span>(localBrokerId, leaderEpoch, newIsr.map(r =&gt; r.brokerId).toList, zkVersion)</div><div class="line">  <span class="keyword">val</span> (updateSucceeded,newVersion) = <span class="type">ReplicationUtils</span>.updateLeaderAndIsr(zkUtils, topic, partitionId,</div><div class="line">    newLeaderAndIsr, controllerEpoch, zkVersion) <span class="comment">//note: 执行更新操作</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span>(updateSucceeded) &#123; <span class="comment">//note: 成功更新到 zk 上</span></div><div class="line">    replicaManager.recordIsrChange(topicPartition) <span class="comment">//note: 告诉 replicaManager 这个 partition 的 isr 需要更新</span></div><div class="line">    inSyncReplicas = newIsr</div><div class="line">    zkVersion = newVersion</div><div class="line">    trace(<span class="string">"ISR updated to [%s] and zkVersion updated to [%d]"</span>.format(newIsr.mkString(<span class="string">","</span>), zkVersion))</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    info(<span class="string">"Cached zkVersion [%d] not equal to that in zookeeper, skip updating ISR"</span>.format(zkVersion))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="maybePropagateIsrChanges"><a href="#maybePropagateIsrChanges" class="headerlink" title="maybePropagateIsrChanges"></a>maybePropagateIsrChanges</h3><p>ReplicaManager <code>maybePropagateIsrChanges()</code> 方法的作用是将那些 isr 变动的 topic-partition 列表（<code>isrChangeSet</code>）通过 ReplicationUtils 的 <code>propagateIsrChanges()</code> 方法更新 zk 上，这时候 Controller 才能知道哪些 topic-partition 的 isr 发生了变动。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 这个方法是周期性的运行,来判断 partition 的 isr 是否需要更新,</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybePropagateIsrChanges</span></span>() &#123;</div><div class="line">  <span class="keyword">val</span> now = <span class="type">System</span>.currentTimeMillis()</div><div class="line">  isrChangeSet synchronized &#123;</div><div class="line">    <span class="keyword">if</span> (isrChangeSet.nonEmpty &amp;&amp; <span class="comment">//note:  有 topic-partition 的 isr 需要更新</span></div><div class="line">      (lastIsrChangeMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationBlackOut</span> &lt; now || <span class="comment">//note: 5s 内没有触发过</span></div><div class="line">        lastIsrPropagationMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationInterval</span> &lt; now)) &#123; <span class="comment">//note: 距离上次触发有60s</span></div><div class="line">      <span class="type">ReplicationUtils</span>.propagateIsrChanges(zkUtils, isrChangeSet) <span class="comment">//note: 在 zk 创建 isr 变动的提醒</span></div><div class="line">      isrChangeSet.clear() <span class="comment">//note: 清空 isrChangeSet,它记录着 isr 变动的 topic-partition 信息</span></div><div class="line">      lastIsrPropagationMs.set(now) <span class="comment">//note: 最近一次触发这个方法的时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Partition-ISR-变化"><a href="#Partition-ISR-变化" class="headerlink" title="Partition ISR 变化"></a>Partition ISR 变化</h2><p>前面讲述了 ReplicaManager 周期性调度的两个方法：<code>maybeShrinkIsr()</code> 和 <code>maybePropagateIsrChanges()</code> ，其中 <code>maybeShrinkIsr()</code> 是来检查 isr 中是否有 replica 需要从 isr 中移除，也就是说这个方法只会减少 isr 中的副本数，那么 isr 中副本数的增加是在哪里触发的呢？</p>
<p>观察上面流程图的第三部分，ReplicaManager 在处理来自 replica 的 Fetch 请求时，会将 Fetch 的相关信息到更新 Partition 中，Partition 调用 <code>maybeExpandIsr()</code> 方法来判断 isr 是否需要更新。</p>
<p>举一个例子，一个 topic 的 partition 1有三个副本，其中 replica 1 为 leader replica，那么这个副本之间关系图如下所示：</p>
<p><img src="/images/kafka/partition_replica.png" alt="Leader replica 与 follower replica"></p>
<p>简单分析一下上面的图：</p>
<ol>
<li>对于 replica 1 而言，它是 leader，首先 replica 1 有对应的 Log 实例对象，而且它会记录其他远程副本的 LEO，以便更新这个 Partition 的 HW；</li>
<li>对于 replica 2 而言，它是 follower，replica 2 有对应的 Log 实例对象，它只会有本地的 LEO 和 HW 记录，没有其他副本的 LEO 记录。</li>
<li>replica 2 和 replica 3 从 replica 1 上拉取数据，进行数据同步。</li>
</ol>
<p>再来看前面的流程图，ReplicaManager 在 <code>FetchMessages()</code> 方法对来自副本的 Fetch 请求进行处理的，实际上是会更新相应 replica 的 LEO 信息的，这时候 leader 可以根据副本 LEO 信息的变动来判断 这个副本是否满足加入 isr 的条件，下面详细来看下这个过程。</p>
<h3 id="updateFollowerLogReadResults"><a href="#updateFollowerLogReadResults" class="headerlink" title="updateFollowerLogReadResults"></a>updateFollowerLogReadResults</h3><p>在 ReplicaManager 的 <code>FetchMessages()</code> 方法中，如果 Fetch 请求是来自副本，那么会调用 <code>updateFollowerLogReadResults()</code> 更新远程副本的信息，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) &#123;</div><div class="line">  debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</div><div class="line">  readResults.foreach &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</div><div class="line">    getPartition(topicPartition) <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">        <span class="comment">//note: 更新副本的相关信息</span></div><div class="line">        partition.updateReplicaLogReadResult(replicaId, readResult)</div><div class="line"></div><div class="line">        <span class="comment">// for producer requests with ack &gt; 1, we need to check</span></div><div class="line">        <span class="comment">// if they can be unblocked after some follower's log end offsets have moved</span></div><div class="line">        tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法的作用就是找到本节点这个 Partition 对象，然后调用其 <code>updateReplicaLogReadResult()</code> 方法更新副本的 LEO 信息和拉取时间信息。</p>
<h3 id="updateReplicaLogReadResult"><a href="#updateReplicaLogReadResult" class="headerlink" title="updateReplicaLogReadResult"></a>updateReplicaLogReadResult</h3><p>这个方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新这个 partition replica 的 the end offset</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateReplicaLogReadResult</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  getReplica(replicaId) <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(replica) =&gt;</div><div class="line">      <span class="comment">//note: 更新副本的信息</span></div><div class="line">      replica.updateLogReadResult(logReadResult)</div><div class="line">      <span class="comment">// check if we need to expand ISR to include this replica</span></div><div class="line">      <span class="comment">// if it is not in the ISR yet</span></div><div class="line">      <span class="comment">//note: 如果该副本不在 isr 中,检查是否需要进行更新</span></div><div class="line">      maybeExpandIsr(replicaId, logReadResult)</div><div class="line"></div><div class="line">      debug(<span class="string">"Recorded replica %d log end offset (LEO) position %d for partition %s."</span></div><div class="line">        .format(replicaId, logReadResult.info.fetchOffsetMetadata.messageOffset, topicPartition))</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotAssignedReplicaException</span>((<span class="string">"Leader %d failed to record follower %d's position %d since the replica"</span> +</div><div class="line">        <span class="string">" is not recognized to be one of the assigned replicas %s for partition %s."</span>)</div><div class="line">        .format(localBrokerId,</div><div class="line">                replicaId,</div><div class="line">                logReadResult.info.fetchOffsetMetadata.messageOffset,</div><div class="line">                assignedReplicas.map(_.brokerId).mkString(<span class="string">","</span>),</div><div class="line">                topicPartition))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法分为以下两步：</p>
<ol>
<li><code>updateLogReadResult()</code>：更新副本的相关信息，这里是更新该副本的 LEO、lastFetchLeaderLogEndOffset 和 lastFetchTimeMs；</li>
<li><code>maybeExpandIsr()</code>：判断 isr 是否需要扩充，即是否有不在 isr 内的副本满足进入 isr 的条件。</li>
</ol>
<h3 id="maybeExpandIsr"><a href="#maybeExpandIsr" class="headerlink" title="maybeExpandIsr"></a>maybeExpandIsr</h3><p><code>maybeExpandIsr()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 检查当前 Partition 是否需要扩充 ISR, 副本的 LEO 大于等于 hw 的副本将会被添加到 isr 中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeExpandIsr</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) &#123;</div><div class="line">  <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) &#123;</div><div class="line">    <span class="comment">// check if this replica needs to be added to the ISR</span></div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> replica = getReplica(replicaId).get</div><div class="line">        <span class="keyword">val</span> leaderHW = leaderReplica.highWatermark</div><div class="line">        <span class="keyword">if</span>(!inSyncReplicas.contains(replica) &amp;&amp;</div><div class="line">           assignedReplicas.map(_.brokerId).contains(replicaId) &amp;&amp;</div><div class="line">           replica.logEndOffset.offsetDiff(leaderHW) &gt;= <span class="number">0</span>) &#123; <span class="comment">//note: replica LEO 大于 HW 的情况下,加入 isr 列表</span></div><div class="line">          <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas + replica</div><div class="line">          info(<span class="string">s"Expanding ISR for partition <span class="subst">$topicPartition</span> from <span class="subst">$&#123;inSyncReplicas.map(_.brokerId).mkString(",")&#125;</span> "</span> +</div><div class="line">            <span class="string">s"to <span class="subst">$&#123;newInSyncReplicas.map(_.brokerId).mkString(",")&#125;</span>"</span>)</div><div class="line">          <span class="comment">// update ISR in ZK and cache</span></div><div class="line">          updateIsr(newInSyncReplicas) <span class="comment">//note: 更新到 zk</span></div><div class="line">          replicaManager.isrExpandRate.mark()</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">// check if the HW of the partition can now be incremented</span></div><div class="line">        <span class="comment">// since the replica may already be in the ISR and its LEO has just incremented</span></div><div class="line">        <span class="comment">//note: 检查 HW 是否需要更新</span></div><div class="line">        maybeIncrementLeaderHW(leaderReplica, logReadResult.fetchTimeMs)</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// nothing to do if no longer leader</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法会根据这个 replica 的 LEO 来判断它是否满足进入 ISR 的条件，如果满足的话，就添加到 ISR 中（前提是这个 replica 在 AR：assign replica 中，并且不在 ISR 中），之后再调用 <code>updateIsr()</code> 更新这个 topic-partition 的 isr 信息和更新 HW 信息。</p>
<h2 id="Updata-Metadata-请求的处理"><a href="#Updata-Metadata-请求的处理" class="headerlink" title="Updata-Metadata 请求的处理"></a>Updata-Metadata 请求的处理</h2><p>这里顺便讲述一下 Update-Metadata 请求的处理流程，先看下在 KafkaApis 中对 Update-Metadata 请求的处理流程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 处理 update-metadata 请求</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleUpdateMetadataRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> correlationId = request.header.correlationId</div><div class="line">  <span class="keyword">val</span> updateMetadataRequest = request.body.asInstanceOf[<span class="type">UpdateMetadataRequest</span>]</div><div class="line"></div><div class="line">  <span class="keyword">val</span> updateMetadataResponse =</div><div class="line">    <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata, 并返回需要删除的 Partition</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = replicaManager.maybeUpdateMetadataCache(correlationId, updateMetadataRequest, metadataCache)</div><div class="line">      <span class="keyword">if</span> (deletedPartitions.nonEmpty)</div><div class="line">        coordinator.handleDeletedPartitions(deletedPartitions) <span class="comment">//note: GroupCoordinator 会清除相关 partition 的信息</span></div><div class="line"></div><div class="line">      <span class="keyword">if</span> (adminManager.hasDelayedTopicOperations) &#123;</div><div class="line">        updateMetadataRequest.partitionStates.keySet.asScala.map(_.topic).foreach &#123; topic =&gt;</div><div class="line">          adminManager.tryCompleteDelayedTopicOperations(topic)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.code)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">Response</span>(request, updateMetadataResponse))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个请求的处理还是调用 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法进行处理的，这个方法会先更新相关的 meta 信息，然后返回需要删除的 topic-partition 信息，GroupCoordinator 再从它的 meta 删除这个 topic-partition 的相关信息。</p>
<h3 id="maybeUpdateMetadataCache"><a href="#maybeUpdateMetadataCache" class="headerlink" title="maybeUpdateMetadataCache"></a>maybeUpdateMetadataCache</h3><p>先看下 ReplicaManager 的 <code>maybeUpdateMetadataCache()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: Controller 向所有的 Broker 发送请求,让它们去更新各自的 meta 信息</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeUpdateMetadataCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>, metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Seq</span>[<span class="type">TopicPartition</span>] =  &#123;</div><div class="line">  replicaStateChangeLock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(updateMetadataRequest.controllerEpoch &lt; controllerEpoch) &#123; <span class="comment">//note: 来自过期的 controller</span></div><div class="line">      <span class="keyword">val</span> stateControllerEpochErrorMessage = (<span class="string">"Broker %d received update metadata request with correlation id %d from an "</span> +</div><div class="line">        <span class="string">"old controller %d with epoch %d. Latest known controller epoch is %d"</span>).format(localBrokerId,</div><div class="line">        correlationId, updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch,</div><div class="line">        controllerEpoch)</div><div class="line">      stateChangeLogger.warn(stateControllerEpochErrorMessage)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(stateControllerEpochErrorMessage)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">//note: 更新 metadata 信息,并返回需要删除的 Partition 信息</span></div><div class="line">      <span class="keyword">val</span> deletedPartitions = metadataCache.updateCache(correlationId, updateMetadataRequest)</div><div class="line">      controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      deletedPartitions</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法就是：调用 <code>metadataCache.updateCache()</code> 方法更新 meta 缓存，然后返回需要删除的 topic-partition 列表。</p>
<h3 id="updateCache"><a href="#updateCache" class="headerlink" title="updateCache"></a>updateCache</h3><p>MetadataCache 的 <code>updateCache()</code> 的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 更新本地的 meta,并返回要删除的 topic-partition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = &#123;</div><div class="line">  inWriteLock(partitionMetadataLock) &#123;</div><div class="line">    controllerId = updateMetadataRequest.controllerId <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> id <span class="keyword">if</span> id &lt; <span class="number">0</span> =&gt; <span class="type">None</span></div><div class="line">        <span class="keyword">case</span> id =&gt; <span class="type">Some</span>(id)</div><div class="line">      &#125;</div><div class="line">    <span class="comment">//note: 清空 aliveNodes 和 aliveBrokers 记录,并更新成最新的记录</span></div><div class="line">    aliveNodes.clear()</div><div class="line">    aliveBrokers.clear()</div><div class="line">    updateMetadataRequest.liveBrokers.asScala.foreach &#123; broker =&gt;</div><div class="line">      <span class="comment">// `aliveNodes` is a hot path for metadata requests for large clusters, so we use java.util.HashMap which</span></div><div class="line">      <span class="comment">// is a bit faster than scala.collection.mutable.HashMap. When we drop support for Scala 2.10, we could</span></div><div class="line">      <span class="comment">// move to `AnyRefMap`, which has comparable performance.</span></div><div class="line">      <span class="keyword">val</span> nodes = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]</div><div class="line">      <span class="keyword">val</span> endPoints = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">EndPoint</span>]</div><div class="line">      broker.endPoints.asScala.foreach &#123; ep =&gt;</div><div class="line">        endPoints += <span class="type">EndPoint</span>(ep.host, ep.port, ep.listenerName, ep.securityProtocol)</div><div class="line">        nodes.put(ep.listenerName, <span class="keyword">new</span> <span class="type">Node</span>(broker.id, ep.host, ep.port))</div><div class="line">      &#125;</div><div class="line">      aliveBrokers(broker.id) = <span class="type">Broker</span>(broker.id, endPoints, <span class="type">Option</span>(broker.rack))</div><div class="line">      aliveNodes(broker.id) = nodes.asScala</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> deletedPartitions = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">TopicPartition</span>] <span class="comment">//note:</span></div><div class="line">    updateMetadataRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (tp, info) =&gt;</div><div class="line">      <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId</div><div class="line">      <span class="keyword">val</span> controllerEpoch = updateMetadataRequest.controllerEpoch</div><div class="line">      <span class="keyword">if</span> (info.leader == <span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>) &#123; <span class="comment">//note: partition 被标记为了删除</span></div><div class="line">        removePartitionInfo(tp.topic, tp.partition) <span class="comment">//note: 从 cache 中删除</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> deleted partition <span class="subst">$tp</span> from metadata cache in response to UpdateMetadata "</span> +</div><div class="line">          <span class="string">s"request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">        deletedPartitions += tp</div><div class="line">      &#125; <span class="keyword">else</span> &#123;<span class="comment">//note: 更新</span></div><div class="line">        <span class="keyword">val</span> partitionInfo = partitionStateToPartitionStateInfo(info)</div><div class="line">        addOrUpdatePartitionInfo(tp.topic, tp.partition, partitionInfo) <span class="comment">//note: 更新 topic-partition meta</span></div><div class="line">        stateChangeLogger.trace(<span class="string">s"Broker <span class="subst">$brokerId</span> cached leader info <span class="subst">$partitionInfo</span> for partition <span class="subst">$tp</span> in response to "</span> +</div><div class="line">          <span class="string">s"UpdateMetadata request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    deletedPartitions</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>它的处理流程如下：</p>
<ol>
<li>清空本节点的 aliveNodes 和 aliveBrokers 记录，并更新为最新的记录；</li>
<li>对于要删除的 topic-partition，从缓存中删除，并记录下来作为这个方法的返回；</li>
<li>对于其他的 topic-partition，执行 updateOrCreate 操作。</li>
</ol>
<p>到这里 ReplicaManager 算是讲述完了，Kafka 存储层的内容基本也介绍完了，后面会开始讲述 Kafka Controller 部分的内容，争取这部分能够在一个半月内总结完。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面几篇文章讲述了 LogManager 的实现、Produce 请求、Fetch 请求的处理以及副本同步机制的实现，Kafka 存储层的主要内容基本上算是讲完了（还有几个小块的内容后面会结合 Controller 再详细介绍）。本篇文章以 ReplicaManager 类
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之副本同步机制实现（十四）</title>
    <link href="http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/"/>
    <id>http://matt33.com/2018/04/29/kafka-replica-fetcher-thread/</id>
    <published>2018-04-29T10:36:52.000Z</published>
    <updated>2018-06-24T23:30:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。本篇文章我们就来看一下 Kafka 副本同步这块的内容，对于每个 broker 来说，它上面的 replica 对象，除了 leader 就是 follower，只要这台 broker 有 follower replica，broker 就会启动副本同步流程从 leader 同步数据，副本同步机制的实现是 Kafka Server 端非常重要的内容，在这篇文章中，主要会从以下几块来讲解：</p>
<ol>
<li>Kafka 在什么情况下会启动副本同步线程？</li>
<li>Kafka 副本同步线程启动流程及付副本同步流程的处理逻辑；</li>
<li>Kafka 副本同步需要解决的问题以及 Kafka 是如何解决这些问题的？</li>
<li>Kafka 在什么情况下会关闭一个副本同步线程。</li>
</ol>
<blockquote>
<p>小插曲：本来想先介绍一下与 LeaderAndIsr 请求相关的，因为副本同步线程的启动与这部分是息息相关的，但是发现涉及到了很多 controller 端的内容，而 controller 这部分还没开始涉及，所以本篇文章涉及到 LeaderAndIsr 请求的部分先简单讲述一下其处理逻辑，在 controller 这块再详细介绍。</p>
</blockquote>
<h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p>Kafka Server 端的副本同步，是由 replica fetcher 线程来负责的，而它又是由 ReplicaManager 来控制的。关于 ReplicaManger，不知道大家还记不记得在 <a href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a> 有一个简单的表格，如下所示。ReplicaManager 通过对 Partition 对象的管理，来控制着 Partition 对应的 Replica 实例，而 Replica 实例又是通过 Log 对象实例来管理着其底层的存储内容。</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 的内容准备专门写一篇文章来介绍，刚好也作为对 Kafka 存储层内容的一个总结。</p>
<p>下面回到这篇文章的主题 —— 副本同步机制，在 ReplicaManager 中有一个实例变量 <code>replicaFetcherManager</code>，它负责管理所有副本同步线程，副本同步线程的启动和关闭都是由这个实例来操作的，关于副本同步相关处理逻辑，下面这张图可以作为一个整体流程，包括了 replica fetcher 线程的启动、工作流程、关闭三个部分，如下图所示：</p>
<p><img src="/images/kafka/fetcher_thread.png" alt="副本同步机制"></p>
<p>后面的讲述会围绕着这张图开始，这里看不懂或不理解也没有关系，后面会一一讲解。</p>
<h2 id="replica-fetcher-线程何时启动"><a href="#replica-fetcher-线程何时启动" class="headerlink" title="replica fetcher 线程何时启动"></a>replica fetcher 线程何时启动</h2><p>Broker 会在什么情况下启动副本同步线程呢？简单想一下这部分的逻辑：首先 broker 分配的任何一个 partition 都是以 Replica 对象实例的形式存在，而 Replica 在 Kafka 上是有两个角色： leader 和 follower，只要这个 Replica 是 follower，它便会向 leader 进行数据同步。</p>
<p>反应在 ReplicaManager 上就是如果 Broker 的本地副本被选举为 follower，那么它将会启动副本同步线程，其具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 对于给定的这些副本，将本地副本设置为 follower</span></div><div class="line"><span class="comment">//note: 1. 从 leader partition 集合移除这些 partition；</span></div><div class="line"><span class="comment">//note: 2. 将这些 partition 标记为 follower，之后这些 partition 就不会再接收 produce 的请求了；</span></div><div class="line"><span class="comment">//note: 3. 停止对这些 partition 的副本同步，这样这些副本就不会再有（来自副本请求线程）的数据进行追加了；</span></div><div class="line"><span class="comment">//note: 4. 对这些 partition 的 offset 进行 checkpoint，如果日志需要截断就进行截断操作；</span></div><div class="line"><span class="comment">//note: 5. 清空 purgatory 中的 produce 和 fetch 请求；</span></div><div class="line"><span class="comment">//note: 6. 如果 broker 没有掉线，向这些 partition 的新 leader 启动副本同步线程；</span></div><div class="line"><span class="comment">//note: 上面这些操作的顺序性，保证了这些副本在 offset checkpoint 之前将不会接收新的数据，这样的话，在 checkpoint 之前这些数据都可以保证刷到磁盘</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                          epoch: <span class="type">Int</span>,</div><div class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                          correlationId: <span class="type">Int</span>,</div><div class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</div><div class="line">                          metadataCache: <span class="type">MetadataCache</span>) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="comment">//note: 统计 follower 的集合</span></div><div class="line">  <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">    <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</div><div class="line">      metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123; <span class="comment">//note: leader 是可用的</span></div><div class="line">        <span class="comment">// Only change partition state when the leader is available</span></div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">//note: partition 的本地副本设置为 follower</span></div><div class="line">          <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</div><div class="line">            partitionsToMakeFollower += partition</div><div class="line">          <span class="keyword">else</span> <span class="comment">//note: 这个 partition 的本地副本已经是 follower 了</span></div><div class="line">            stateChangeLogger.info((<span class="string">"Broker %d skipped the become-follower state change after marking its partition as follower with correlation id %d from "</span> +</div><div class="line">              <span class="string">"controller %d epoch %d for partition %s since the new leader %d is the same as the old leader"</span>)</div><div class="line">              .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">              partition.topicPartition, newLeaderBrokerId))</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">// The leader broker should always be present in the metadata cache.</span></div><div class="line">          <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></div><div class="line">          stateChangeLogger.error((<span class="string">"Broker %d received LeaderAndIsrRequest with correlation id %d from controller"</span> +</div><div class="line">            <span class="string">" %d epoch %d for partition %s but cannot become follower since the new leader %d is unavailable."</span>)</div><div class="line">            .format(localBrokerId, correlationId, controllerId, partitionStateInfo.controllerEpoch,</div><div class="line">            partition.topicPartition, newLeaderBrokerId))</div><div class="line">          <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></div><div class="line">          <span class="comment">// the partition's high watermark in the checkpoint file (see KAFKA-1647)</span></div><div class="line">          partition.getOrCreateReplica()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: 删除对这些 partition 的副本同步线程</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-follower request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: Truncate the partition logs to the specified offsets and checkpoint the recovery point to this offset</span></div><div class="line">    logManager.truncateTo(partitionsToMakeFollower.map &#123; partition =&gt;</div><div class="line">      (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</div><div class="line">    &#125;.toMap)</div><div class="line">    <span class="comment">//note: 完成那些延迟请求的处理</span></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</div><div class="line">      tryCompleteDelayedProduce(topicPartitionOperationKey)</div><div class="line">      tryCompleteDelayedFetch(topicPartitionOperationKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d truncated logs and checkpointed recovery boundaries for partition %s as part of "</span> +</div><div class="line">        <span class="string">"become-follower request with correlation id %d from controller %d epoch %d"</span>).format(localBrokerId,</div><div class="line">        partition.topicPartition, correlationId, controllerId, epoch))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (isShuttingDown.get()) &#123;</div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d skipped the adding-fetcher step of the become-follower state change with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is shutting down"</span>).format(localBrokerId, correlationId,</div><div class="line">          controllerId, epoch, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></div><div class="line">      <span class="comment">//note: 启动副本同步线程</span></div><div class="line">      <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</div><div class="line">        partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</div><div class="line">          metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</div><div class="line">          partition.getReplica().get.logEndOffset.messageOffset)).toMap <span class="comment">//note: leader 信息+本地 replica 的 offset</span></div><div class="line">      replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</div><div class="line"></div><div class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</div><div class="line">        stateChangeLogger.trace((<span class="string">"Broker %d started fetcher to new leader as part of become-follower request from controller "</span> +</div><div class="line">          <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">          .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request with correlationId %d received from controller %d "</span> +</div><div class="line">        <span class="string">"epoch %d"</span>).format(localBrokerId, correlationId, controllerId, epoch)</div><div class="line">      stateChangeLogger.error(errorMsg, e)</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-follower transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeFollower</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，<code>makeFollowers()</code> 的处理过程如下：</p>
<ol>
<li>先从本地记录 leader partition 的集合中将这些 partition 移除，因为这些 partition 已经被选举为了 follower；</li>
<li>将这些 partition 的本地副本设置为 follower，后面就不会接收关于这个 partition 的 Produce 请求了，如果依然有 client 在向这台 broker 发送数据，那么它将会返回相应的错误；</li>
<li>先停止关于这些 partition 的副本同步线程（如果本地副本之前是 follower 现在还是 follower，先关闭的原因是：这个 partition 的 leader 发生了变化，如果 leader 没有发生变化，那么 <code>makeFollower</code> 方法返回的是 False，这个 Partition 就不会被添加到 partitionsToMakeFollower 集合中），这样的话可以保证这些 partition 的本地副本将不会再有新的数据追加；</li>
<li>对这些 partition 本地副本日志文件进行截断操作并进行 checkpoint 操作；</li>
<li>完成那些延迟处理的 Produce 和 Fetch 请求；</li>
<li>如果本地的 broker 没有掉线，那么向这些 partition 新选举出来的 leader 启动副本同步线程。</li>
</ol>
<p>关于第6步，并不一定会为每一个 partition 都启动一个 fetcher 线程，对于一个目的 broker，只会启动 <code>num.replica.fetchers</code> 个线程，具体这个 topic-partition 会分配到哪个 fetcher 线程上，是根据 topic 名和 partition id 进行计算得到，实现所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取分配到这个 topic-partition 的 fetcher 线程 id</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFetcherId</span></span>(topic: <span class="type">String</span>, partitionId: <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="type">Utils</span>.abs(<span class="number">31</span> * topic.hashCode() + partitionId) % numFetchers</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="replica-fetcher-线程参数设置"><a href="#replica-fetcher-线程参数设置" class="headerlink" title="replica fetcher 线程参数设置"></a>replica fetcher 线程参数设置</h3><p>关于副本同步线程有一些参数配置，具体如下表所示：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>num.replica.fetchers</td>
<td>从一个 broker 同步数据的 fetcher 线程数，增加这个值时也会增加该 broker 的 Io 并行度（也就是说：从一台 broker 同步数据，最多能开这么大的线程数）</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.wait.max.ms</td>
<td>对于 follower replica 而言，每个 Fetch 请求的最大等待时间，这个值应该比 <code>replica.lag.time.max.ms</code> 要小，否则对于那些吞吐量特别低的 topic 可能会导致 isr 频繁抖动</td>
<td>500</td>
</tr>
<tr>
<td>replica.high.watermark.checkpoint.interval.ms</td>
<td>hw 刷到磁盘频率</td>
<td>500</td>
</tr>
<tr>
<td>replica.lag.time.max.ms</td>
<td>如果一个 follower 在这个时间内没有发送任何 fetch 请求或者在这个时间内没有追上 leader 当前的 log end offset，那么将会从 isr 中移除</td>
<td>10000</td>
</tr>
<tr>
<td>replica.fetch.min.bytes</td>
<td>每次 fetch 请求最少拉取的数据量，如果不满足这个条件，那么要等待 replicaMaxWaitTimeMs</td>
<td>1</td>
</tr>
<tr>
<td>replica.fetch.backoff.ms</td>
<td>拉取时，如果遇到错误，下次拉取等待的时间</td>
<td>1000</td>
</tr>
<tr>
<td>replica.fetch.max.bytes</td>
<td>在对每个 partition 拉取时，最大的拉取数量，这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>1048576</td>
</tr>
<tr>
<td>replica.fetch.response.max.bytes</td>
<td>对于一个 fetch 请求，返回的最大数据量（可能会涉及多个 partition），这并不是一个绝对值，如果拉取的第一条 msg 的大小超过了这个值，只要不超过这个 topic 设置（defined via message.max.bytes (broker config) or max.message.bytes (topic config)）的单条大小限制，依然会返回。</td>
<td>10MB</td>
</tr>
</tbody>
</table>
<h2 id="replica-fetcher-线程启动"><a href="#replica-fetcher-线程启动" class="headerlink" title="replica fetcher 线程启动"></a>replica fetcher 线程启动</h2><p>如上面的图所示，在 ReplicaManager 调用 <code>makeFollowers()</code> 启动 replica fetcher 线程后，它实际上是通过 ReplicaFetcherManager 实例进行相关 topic-partition 同步线程的启动和关闭，其启动过程分为下面两步：</p>
<ol>
<li>ReplicaFetcherManager 调用 <code>addFetcherForPartitions()</code> 添加对这些 topic-partition 的数据同步流程；</li>
<li>ReplicaFetcherManager 调用 <code>createFetcherThread()</code> 初始化相应的 ReplicaFetcherThread 线程。</li>
</ol>
<h3 id="addFetcherForPartitions"><a href="#addFetcherForPartitions" class="headerlink" title="addFetcherForPartitions"></a>addFetcherForPartitions</h3><p><code>addFetcherForPartitions()</code> 的具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 为一个 topic-partition 添加 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">addFetcherForPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">BrokerAndInitialOffset</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="comment">//note: 为这些 topic-partition 分配相应的 fetch 线程 id</span></div><div class="line">    <span class="keyword">val</span> partitionsPerFetcher = partitionAndOffsets.groupBy &#123; <span class="keyword">case</span>(topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">      <span class="type">BrokerAndFetcherId</span>(brokerAndInitialOffset.broker, getFetcherId(topicPartition.topic, topicPartition.partition))&#125;</div><div class="line">    <span class="keyword">for</span> ((brokerAndFetcherId, partitionAndOffsets) &lt;- partitionsPerFetcher) &#123;</div><div class="line">      <span class="comment">//note: 为 BrokerAndFetcherId 构造 fetcherThread 线程</span></div><div class="line">      <span class="keyword">var</span> fetcherThread: <span class="type">AbstractFetcherThread</span> = <span class="literal">null</span></div><div class="line">      fetcherThreadMap.get(brokerAndFetcherId) <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(f) =&gt; fetcherThread = f</div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="comment">//note: 创建 fetcher 线程</span></div><div class="line">          fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)</div><div class="line">          fetcherThreadMap.put(brokerAndFetcherId, fetcherThread)</div><div class="line">          fetcherThread.start</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 添加 topic-partition 列表</span></div><div class="line">      fetcherThreadMap(brokerAndFetcherId).addPartitions(partitionAndOffsets.map &#123; <span class="keyword">case</span> (tp, brokerAndInitOffset) =&gt;</div><div class="line">        tp -&gt; brokerAndInitOffset.initOffset</div><div class="line">      &#125;)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  info(<span class="string">"Added fetcher for partitions %s"</span>.format(partitionAndOffsets.map &#123; <span class="keyword">case</span> (topicPartition, brokerAndInitialOffset) =&gt;</div><div class="line">    <span class="string">"["</span> + topicPartition + <span class="string">", initOffset "</span> + brokerAndInitialOffset.initOffset + <span class="string">" to broker "</span> + brokerAndInitialOffset.broker + <span class="string">"] "</span>&#125;))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个方法其实是做了下面这几件事：</p>
<ol>
<li>先计算这个 topic-partition 对应的 fetcher id；</li>
<li>根据 leader 和 fetcher id 获取对应的 replica fetcher 线程，如果没有找到，就调用 <code>createFetcherThread()</code> 创建一个新的 fetcher 线程；</li>
<li>如果是新启动的 replica fetcher 线程，那么就启动这个线程；</li>
<li>将 topic-partition 记录到 <code>fetcherThreadMap</code> 中，这个变量记录每个 replica fetcher 线程要同步的 topic-partition 列表。</li>
</ol>
<h3 id="createFetcherThread"><a href="#createFetcherThread" class="headerlink" title="createFetcherThread"></a>createFetcherThread</h3><p>ReplicaFetcherManager 创建 replica Fetcher 线程的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 创建 replica-fetch 线程</span></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createFetcherThread</span></span>(fetcherId: <span class="type">Int</span>, sourceBroker: <span class="type">BrokerEndPoint</span>): <span class="type">AbstractFetcherThread</span> = &#123;</div><div class="line">  <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="string">"ReplicaFetcherThread-%d-%d"</span>.format(fetcherId, sourceBroker.id)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt;</div><div class="line">      <span class="string">"%s:ReplicaFetcherThread-%d-%d"</span>.format(p, fetcherId, sourceBroker.id)</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">new</span> <span class="type">ReplicaFetcherThread</span>(threadName, fetcherId, sourceBroker, brokerConfig,</div><div class="line">    replicaMgr, metrics, time, quotaManager) <span class="comment">//note: replica-fetch 线程</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="replica-fetcher-线程处理过程"><a href="#replica-fetcher-线程处理过程" class="headerlink" title="replica fetcher 线程处理过程"></a>replica fetcher 线程处理过程</h2><p>replica fetcher 线程在启动之后就开始进行正常数据同步流程了，在文章最开始流程图中的第二部分（线程处理过程）已经给出了大概的处理过程，这节会详细介绍一下，这个过程都是在 ReplicaFetcherThread 线程中实现的。</p>
<h3 id="doWoker"><a href="#doWoker" class="headerlink" title="doWoker"></a>doWoker</h3><p>ReplicaFetcherThread 的 <code>doWork()</code> 方法是一直在这个线程中的 <code>run()</code> 中调用的，实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">  info(<span class="string">"Starting "</span>)</div><div class="line">  <span class="keyword">try</span>&#123;</div><div class="line">    <span class="keyword">while</span>(isRunning.get())&#123;</div><div class="line">      doWork()</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span>&#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span>(isRunning.get())</div><div class="line">        error(<span class="string">"Error due to "</span>, e)</div><div class="line">  &#125;</div><div class="line">  shutdownLatch.countDown()</div><div class="line">  info(<span class="string">"Stopped "</span>)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() &#123;</div><div class="line">  <span class="comment">//note: 构造 fetch request</span></div><div class="line">  <span class="keyword">val</span> fetchRequest = inLock(partitionMapLock) &#123;</div><div class="line">    <span class="keyword">val</span> fetchRequest = buildFetchRequest(partitionStates.partitionStates.asScala.map &#123; state =&gt;</div><div class="line">      state.topicPartition -&gt; state.value</div><div class="line">    &#125;)</div><div class="line">    <span class="keyword">if</span> (fetchRequest.isEmpty) &#123; <span class="comment">//note: 如果没有活跃的 partition，在下次调用之前，sleep fetchBackOffMs 时间</span></div><div class="line">      trace(<span class="string">"There are no active partitions. Back off for %d ms before sending a fetch request"</span>.format(fetchBackOffMs))</div><div class="line">      partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    &#125;</div><div class="line">    fetchRequest</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!fetchRequest.isEmpty)</div><div class="line">    processFetchRequest(fetchRequest) <span class="comment">//note: 发送 fetch 请求，处理 fetch 的结果</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在 <code>doWork()</code> 方法中主要做了两件事：</p>
<ol>
<li>构造相应的 Fetch 请求（<code>buildFetchRequest()</code>）；</li>
<li>通过 <code>processFetchRequest()</code> 方法发送 Fetch 请求，并对其结果进行相应的处理。</li>
</ol>
<h3 id="buildFetchRequest"><a href="#buildFetchRequest" class="headerlink" title="buildFetchRequest"></a>buildFetchRequest</h3><p>通过 <code>buildFetchRequest()</code> 方法构造相应的 Fetcher 请求时，会设置 replicaId，该值会代表了这个 Fetch 请求是来自副本同步，而不是来自 consumer。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, 对于 consumer, 该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="processFetchRequest"><a href="#processFetchRequest" class="headerlink" title="processFetchRequest"></a>processFetchRequest</h3><p><code>processFetchRequest()</code> 这个方法的作用是发送 Fetch 请求，并对返回的结果进行处理，最终写入到本地副本的 Log 实例中，其具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processFetchRequest</span></span>(fetchRequest: <span class="type">REQ</span>) &#123;</div><div class="line">  <span class="keyword">val</span> partitionsWithError = mutable.<span class="type">Set</span>[<span class="type">TopicPartition</span>]()</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updatePartitionsWithError</span></span>(partition: <span class="type">TopicPartition</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    partitionsWithError += partition</div><div class="line">    partitionStates.moveToEnd(partition)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> responseData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PD</span>)] = <span class="type">Seq</span>.empty</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    trace(<span class="string">"Issuing to broker %d of fetch request %s"</span>.format(sourceBroker.id, fetchRequest))</div><div class="line">    responseData = fetch(fetchRequest) <span class="comment">//note: 发送 fetch 请求，获取 fetch 结果</span></div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">      <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">        warn(<span class="string">s"Error in fetch <span class="subst">$fetchRequest</span>"</span>, t)</div><div class="line">        inLock(partitionMapLock) &#123; <span class="comment">//note: fetch 时发生错误，sleep 一会</span></div><div class="line">          partitionStates.partitionSet.asScala.foreach(updatePartitionsWithError)</div><div class="line">          <span class="comment">// there is an error occurred while fetching partitions, sleep a while</span></div><div class="line">          <span class="comment">// note that `ReplicaFetcherThread.handlePartitionsWithError` will also introduce the same delay for every</span></div><div class="line">          <span class="comment">// partition with error effectively doubling the delay. It would be good to improve this.</span></div><div class="line">          partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">  fetcherStats.requestRate.mark()</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (responseData.nonEmpty) &#123; <span class="comment">//note: fetch 结果不为空</span></div><div class="line">    <span class="comment">// process fetched data</span></div><div class="line">    inLock(partitionMapLock) &#123;</div><div class="line"></div><div class="line">      responseData.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionData) =&gt;</div><div class="line">        <span class="keyword">val</span> topic = topicPartition.topic</div><div class="line">        <span class="keyword">val</span> partitionId = topicPartition.partition</div><div class="line">        <span class="type">Option</span>(partitionStates.stateValue(topicPartition)).foreach(currentPartitionFetchState =&gt;</div><div class="line">          <span class="comment">// we append to the log if the current offset is defined and it is the same as the offset requested during fetch</span></div><div class="line">          <span class="comment">//note: 如果 fetch 的 offset 与返回结果的 offset 相同，并且返回没有异常，那么就将拉取的数据追加到对应的 partition 上</span></div><div class="line">          <span class="keyword">if</span> (fetchRequest.offset(topicPartition) == currentPartitionFetchState.offset) &#123;</div><div class="line">            <span class="type">Errors</span>.forCode(partitionData.errorCode) <span class="keyword">match</span> &#123;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NONE</span> =&gt;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line">                  <span class="keyword">val</span> newOffset = records.shallowEntries.asScala.lastOption.map(_.nextOffset).getOrElse(</div><div class="line">                    currentPartitionFetchState.offset)</div><div class="line"></div><div class="line">                  fetcherLagStats.getAndMaybePut(topic, partitionId).lag = <span class="type">Math</span>.max(<span class="number">0</span>L, partitionData.highWatermark - newOffset)</div><div class="line">                  <span class="comment">// Once we hand off the partition data to the subclass, we can't mess with it any more in this thread</span></div><div class="line">                  <span class="comment">//note: 将 fetch 的数据追加到日志文件中</span></div><div class="line">                  processPartitionData(topicPartition, currentPartitionFetchState.offset, partitionData)</div><div class="line"></div><div class="line">                  <span class="keyword">val</span> validBytes = records.validBytes</div><div class="line">                  <span class="keyword">if</span> (validBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">                    <span class="comment">// Update partitionStates only if there is no exception during processPartitionData</span></div><div class="line">                    <span class="comment">//note: 更新 fetch 的 offset 位置</span></div><div class="line">                    partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                    fetcherStats.byteRate.mark(validBytes) <span class="comment">//note: 更新 metrics</span></div><div class="line">                  &#125;</div><div class="line">                &#125; <span class="keyword">catch</span> &#123;</div><div class="line">                  <span class="keyword">case</span> ime: <span class="type">CorruptRecordException</span> =&gt;</div><div class="line">                    <span class="comment">// we log the error and continue. This ensures two things</span></div><div class="line">                    <span class="comment">// 1. If there is a corrupt message in a topic partition, it does not bring the fetcher thread down and cause other topic partition to also lag</span></div><div class="line">                    <span class="comment">// 2. If the message is corrupt due to a transient state in the log (truncation, partial writes can cause this), we simply continue and</span></div><div class="line">                    <span class="comment">// should get fixed in the subsequent fetches</span></div><div class="line">                    <span class="comment">//note: CRC 验证失败时，打印日志，并继续进行（这个线程还会有其他的 tp 拉取，防止影响其他副本同步）</span></div><div class="line">                    logger.error(<span class="string">"Found invalid messages during fetch for partition ["</span> + topic + <span class="string">","</span> + partitionId + <span class="string">"] offset "</span> + currentPartitionFetchState.offset  + <span class="string">" error "</span> + ime.getMessage)</div><div class="line">                    updatePartitionsWithError(topicPartition);</div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    <span class="comment">//note: 这里还会抛出异常，是 RUNTimeException</span></div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"error processing data for partition [%s,%d] offset %d"</span></div><div class="line">                      .format(topic, partitionId, currentPartitionFetchState.offset), e)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">OFFSET_OUT_OF_RANGE</span> =&gt; <span class="comment">//note: Out-of-range 的情况处理</span></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                  <span class="keyword">val</span> newOffset = handleOffsetOutOfRange(topicPartition)</div><div class="line">                  partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</div><div class="line">                  error(<span class="string">"Current offset %d for partition [%s,%d] out of range; reset offset to %d"</span></div><div class="line">                    .format(currentPartitionFetchState.offset, topic, partitionId, newOffset))</div><div class="line">                &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理 out-of-range 是抛出的异常</span></div><div class="line">                  <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">                    error(<span class="string">"Error getting offset for partition [%s,%d] to broker %d"</span>.format(topic, partitionId, sourceBroker.id), e)</div><div class="line">                    updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">              <span class="keyword">case</span> _ =&gt; <span class="comment">//note: 其他的异常情况</span></div><div class="line">                <span class="keyword">if</span> (isRunning.get) &#123;</div><div class="line">                  error(<span class="string">"Error for partition [%s,%d] to broker %d:%s"</span>.format(topic, partitionId, sourceBroker.id,</div><div class="line">                    partitionData.exception.get))</div><div class="line">                  updatePartitionsWithError(topicPartition)</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 处理拉取遇到的错误读的 tp</span></div><div class="line">  <span class="keyword">if</span> (partitionsWithError.nonEmpty) &#123;</div><div class="line">    debug(<span class="string">"handling partitions with error for %s"</span>.format(partitionsWithError))</div><div class="line">    handlePartitionsWithErrors(partitionsWithError)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其处理过程简单总结一下：</p>
<ol>
<li>通过 <code>fetch()</code> 方法，发送 Fetch 请求，获取相应的 response（如果遇到异常，那么在下次发送 Fetch 请求之前，会 sleep 一段时间再发）；</li>
<li>如果返回的结果 不为空，并且 Fetch 请求的 offset 信息与返回结果的 offset 信息对得上，那么就会调用 <code>processPartitionData()</code> 方法将拉取到的数据追加本地副本的日志文件中，如果返回结果有错误信息，那么就对相应错误进行相应的处理；</li>
<li>对在 Fetch 过程中遇到异常或返回错误的 topic-partition，会进行 delay 操作，下次 Fetch 请求的发生至少要间隔 <code>replica.fetch.backoff.ms</code> 时间。</li>
</ol>
<h4 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h4><p><code>fetch()</code> 方法作用是发送 Fetch 请求，并返回相应的结果，其具体的实现，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 发送 fetch 请求，获取拉取结果</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">fetch</span></span>(fetchRequest: <span class="type">FetchRequest</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)] = &#123;</div><div class="line">  <span class="keyword">val</span> clientResponse = sendRequest(fetchRequest.underlying)</div><div class="line">  <span class="keyword">val</span> fetchResponse = clientResponse.responseBody.asInstanceOf[<span class="type">FetchResponse</span>]</div><div class="line">  fetchResponse.responseData.asScala.toSeq.map &#123; <span class="keyword">case</span> (key, value) =&gt;</div><div class="line">    key -&gt; <span class="keyword">new</span> <span class="type">PartitionData</span>(value)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 发送请求</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(requestBuilder: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>]): <span class="type">ClientResponse</span> = &#123;</div><div class="line">  <span class="keyword">import</span> kafka.utils.<span class="type">NetworkClientBlockingOps</span>._</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">if</span> (!networkClient.blockingReady(sourceNode, socketTimeout)(time))</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SocketTimeoutException</span>(<span class="string">s"Failed to connect within <span class="subst">$socketTimeout</span> ms"</span>)</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">val</span> clientRequest = networkClient.newClientRequest(sourceBroker.id.toString, requestBuilder,</div><div class="line">        time.milliseconds(), <span class="literal">true</span>)</div><div class="line">      networkClient.blockingSendAndReceive(clientRequest)(time) <span class="comment">//note: 阻塞直到获取返回结果</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      networkClient.close(sourceBroker.id.toString)</div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="processPartitionData"><a href="#processPartitionData" class="headerlink" title="processPartitionData"></a>processPartitionData</h4><p>这个方法的作用是，处理 Fetch 请求的具体数据内容，简单来说就是：检查一下数据大小是否超过限制、将数据追加到本地副本的日志文件中、更新本地副本的 hw 值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// process fetched data</span></div><div class="line"><span class="comment">//note: 处理 fetch 的数据，将 fetch 的数据追加的日志文件中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">processPartitionData</span></span>(topicPartition: <span class="type">TopicPartition</span>, fetchOffset: <span class="type">Long</span>, partitionData: <span class="type">PartitionData</span>) &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</div><div class="line">    <span class="keyword">val</span> records = partitionData.toRecords</div><div class="line"></div><div class="line">    <span class="comment">//note: 检查 records</span></div><div class="line">    maybeWarnIfOversizedRecords(records, topicPartition)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (fetchOffset != replica.logEndOffset.messageOffset)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Offset mismatch for partition %s: fetched offset = %d, log end offset = %d."</span>.format(topicPartition, fetchOffset, replica.logEndOffset.messageOffset))</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d for partition %s. Received %d messages and leader hw %d"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))</div><div class="line">    replica.log.get.append(records, assignOffsets = <span class="literal">false</span>) <span class="comment">//note: 将 fetch 的数据追加到 log 中</span></div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">"Follower %d has replica log end offset %d after appending %d bytes of messages for partition %s"</span></div><div class="line">        .format(replica.brokerId, replica.logEndOffset.messageOffset, records.sizeInBytes, topicPartition))</div><div class="line">    <span class="comment">//note: 更新 replica 的 hw（logEndOffset 在追加数据后也会立马进行修改)</span></div><div class="line">    <span class="keyword">val</span> followerHighWatermark = replica.logEndOffset.messageOffset.min(partitionData.highWatermark)</div><div class="line">    <span class="comment">// for the follower replica, we do not need to keep</span></div><div class="line">    <span class="comment">// its segment base offset the physical position,</span></div><div class="line">    <span class="comment">// these values will be computed upon making the leader</span></div><div class="line">    <span class="comment">//note: 这个值主要是用在 leader replica 上的</span></div><div class="line">    replica.highWatermark = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(followerHighWatermark)</div><div class="line">    <span class="keyword">if</span> (logger.isTraceEnabled)</div><div class="line">      trace(<span class="string">s"Follower <span class="subst">$&#123;replica.brokerId&#125;</span> set replica high watermark for partition <span class="subst">$topicPartition</span> to <span class="subst">$followerHighWatermark</span>"</span>)</div><div class="line">    <span class="keyword">if</span> (quota.isThrottled(topicPartition))</div><div class="line">      quota.record(records.sizeInBytes)</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">      fatal(<span class="string">s"Disk error while replicating data for <span class="subst">$topicPartition</span>"</span>, e)</div><div class="line">      <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="副本同步异常情况的处理"><a href="#副本同步异常情况的处理" class="headerlink" title="副本同步异常情况的处理"></a>副本同步异常情况的处理</h2><p>在副本同步的过程中，会遇到哪些异常情况呢？</p>
<p>大家一定会想到关于 offset 的问题，在 Kafka 中，关于 offset 的处理，无论是 producer 端、consumer 端还是其他地方，offset 似乎都是一个形影不离的问题。在副本同步时，关于 offset，会遇到什么问题呢？下面举两个异常的场景：</p>
<ol>
<li>假如当前本地（id：1）的副本现在是 leader，其 LEO 假设为1000，而另一个在 isr 中的副本（id：2）其 LEO 为800，此时出现网络抖动，id 为1 的机器掉线后又上线了，但是此时副本的 leader 实际上已经变成了 2，而2的 LEO 为800，这时候1启动副本同步线程去2上拉取数据，希望从 offset=1000 的地方开始拉取，但是2上最大的 offset 才是800，这种情况该如何处理呢？</li>
<li>假设一个 replica （id：1）其 LEO 是10，它已经掉线好几天，这个 partition leader 的 offset 范围是 [100, 800]，那么 1 重启启动时，它希望从 offset=10 的地方开始拉取数据时，这时候发生了 OutOfRange，不过跟上面不同的是这里是小于了 leader offset 的范围，这种情况又该怎么处理？</li>
</ol>
<p>以上两种情况都是 offset OutOfRange 的情况，只不过：一是 Fetch Offset 超过了 leader 的 LEO，二是 Fetch Offset 小于 leader 最小的 offset，在介绍 Kafka 解决方案之前，我们先来自己思考一下这两种情况应该怎么处理？</p>
<ol>
<li>如果 fetch offset 超过 leader 的 offset，这时候副本应该是回溯到 leader 的 LEO 位置（超过这个值的数据删除），然后再去进行副本同步，当然这种解决方案其实是无法保证 leader 与 follower 数据的完全一致，再次发生 leader 切换时，可能会导致数据的可见性不一致，但既然用户允许了脏选举的发生，其实我们是可以认为用户是可以接收这种情况发生的；</li>
<li>这种就比较容易处理，首先清空本地的数据，因为本地的数据都已经过期了，然后从 leader 的最小 offset 位置开始拉取数据。</li>
</ol>
<p>上面是我们比较容易想出的解决方案，而在 Kafka 中，其解决方案也很类似，不过遇到情况比上面我们列出的两种情况多了一些复杂，其解决方案如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">  <span class="comment">/**</span></div><div class="line">   * Unclean leader election: A follower goes down, in the meanwhile the leader keeps appending messages. The follower comes back up</div><div class="line">   * and before it has completely caught up with the leader's logs, all replicas in the ISR go down. The follower is now uncleanly</div><div class="line">   * elected as the new leader, and it starts appending messages from the client. The old leader comes back up, becomes a follower</div><div class="line">   * and it may discover that the current leader's end offset is behind its own end offset.</div><div class="line">   *</div><div class="line">   * In such a case, truncate the current follower's log to the current leader's end offset and continue fetching.</div><div class="line">   *</div><div class="line">   * There is a potential for a mismatch between the logs of the two replicas here. We don't fix this mismatch as of now.</div><div class="line">   */</div><div class="line">  <span class="comment">//note: 脏选举的发生</span></div><div class="line">  <span class="comment">//note: 获取最新的 offset</span></div><div class="line">  <span class="keyword">val</span> leaderEndOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">LATEST_TIMESTAMP</span>,</div><div class="line">    brokerConfig.brokerId)</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (leaderEndOffset &lt; replica.logEndOffset.messageOffset) &#123; <span class="comment">//note: leaderEndOffset 小于 副本 LEO 的情况</span></div><div class="line">    <span class="comment">// Prior to truncating the follower's log, ensure that doing so is not disallowed by the configuration for unclean leader election.</span></div><div class="line">    <span class="comment">// This situation could only happen if the unclean election configuration for a topic changes while a replica is down. Otherwise,</span></div><div class="line">    <span class="comment">// we should never encounter this situation since a non-ISR leader cannot be elected if disallowed by the broker configuration.</span></div><div class="line">    <span class="comment">//note: 这种情况只是发生在 unclear election 的情况下</span></div><div class="line">    <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(brokerConfig.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(replicaMgr.zkUtils,</div><div class="line">      <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicPartition.topic)).uncleanLeaderElectionEnable) &#123; <span class="comment">//note: 不允许 unclear elect 时,直接退出进程</span></div><div class="line">      <span class="comment">// Log a fatal error and shutdown the broker to ensure that data loss does not unexpectedly occur.</span></div><div class="line">      fatal(<span class="string">"Exiting because log truncation is not allowed for partition %s,"</span>.format(topicPartition) +</div><div class="line">        <span class="string">" Current leader %d's latest offset %d is less than replica %d's latest offset %d"</span></div><div class="line">        .format(sourceBroker.id, leaderEndOffset, brokerConfig.brokerId, replica.logEndOffset.messageOffset))</div><div class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//note: warn 日志信息</span></div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's latest offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderEndOffset))</div><div class="line">    <span class="comment">//note: 进行截断操作,将offset 大于等于targetOffset 的数据和索引删除</span></div><div class="line">    replicaMgr.logManager.truncateTo(<span class="type">Map</span>(topicPartition -&gt; leaderEndOffset))</div><div class="line">    leaderEndOffset</div><div class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: leader 的 LEO 大于 follower 的 LEO 的情况下,还发生了 OutOfRange</span></div><div class="line">    <span class="comment">//note: 1. follower 下线了很久,其 LEO 已经小于了 leader 的 StartOffset;</span></div><div class="line">    <span class="comment">//note: 2. 脏选举发生时, 如果 old leader 的 HW 大于 new leader 的 LEO,此时 old leader 回溯到 HW,并且这个位置开始拉取数据发生了 Out of range</span></div><div class="line">    <span class="comment">//note:    当这个方法调用时,随着 produce 持续产生数据,可能出现 leader LEO 大于 Follower LEO 的情况（不做任何处理,重试即可解决,但</span></div><div class="line">    <span class="comment">//note:    无法保证数据的一致性）。</span></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * If the leader's log end offset is greater than the follower's log end offset, there are two possibilities:</div><div class="line">     * 1. The follower could have been down for a long time and when it starts up, its end offset could be smaller than the leader's</div><div class="line">     * start offset because the leader has deleted old logs (log.logEndOffset &lt; leaderStartOffset).</div><div class="line">     * 2. When unclean leader election occurs, it is possible that the old leader's high watermark is greater than</div><div class="line">     * the new leader's log end offset. So when the old leader truncates its offset to its high watermark and starts</div><div class="line">     * to fetch from the new leader, an OffsetOutOfRangeException will be thrown. After that some more messages are</div><div class="line">     * produced to the new leader. While the old leader is trying to handle the OffsetOutOfRangeException and query</div><div class="line">     * the log end offset of the new leader, the new leader's log end offset becomes higher than the follower's log end offset.</div><div class="line">     *</div><div class="line">     * In the first case, the follower's current log end offset is smaller than the leader's log start offset. So the</div><div class="line">     * follower should truncate all its logs, roll out a new segment and start to fetch from the current leader's log</div><div class="line">     * start offset.</div><div class="line">     * In the second case, the follower should just keep the current log segments and retry the fetch. In the second</div><div class="line">     * case, there will be some inconsistency of data between old and new leader. We are not solving it here.</div><div class="line">     * If users want to have strong consistency guarantees, appropriate configurations needs to be set for both</div><div class="line">     * brokers and producers.</div><div class="line">     *</div><div class="line">     * Putting the two cases together, the follower should fetch from the higher one of its replica log end offset</div><div class="line">     * and the current leader's log start offset.</div><div class="line">     *</div><div class="line">     */</div><div class="line">    <span class="keyword">val</span> leaderStartOffset: <span class="type">Long</span> = earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">EARLIEST_TIMESTAMP</span>,</div><div class="line">      brokerConfig.brokerId)</div><div class="line">    warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's start offset %d"</span></div><div class="line">      .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderStartOffset))</div><div class="line">    <span class="keyword">val</span> offsetToFetch = <span class="type">Math</span>.max(leaderStartOffset, replica.logEndOffset.messageOffset)</div><div class="line">    <span class="comment">// Only truncate log when current leader's log start offset is greater than follower's log end offset.</span></div><div class="line">    <span class="keyword">if</span> (leaderStartOffset &gt; replica.logEndOffset.messageOffset) <span class="comment">//note: 如果 leader 的 startOffset 大于副本的最大 offset</span></div><div class="line">      <span class="comment">//note: 将这个 log 的数据全部清空,并且从 leaderStartOffset 开始拉取数据</span></div><div class="line">      replicaMgr.logManager.truncateFullyAndStartAt(topicPartition, leaderStartOffset)</div><div class="line">    offsetToFetch</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>针对第一种情况，在 Kafka 中，实际上还会发生这样一种情况，1 在收到 OutOfRange 错误时，这时去 leader 上获取的 LEO 值与最小的 offset 值，这时候却发现 leader 的 LEO 已经从 800 变成了 1100（这个 topic-partition 的数据量增长得比较快），再按照上面的解决方案就不太合理，Kafka 这边的解决方案是：遇到这种情况，进行重试就可以了，下次同步时就会正常了，但是依然会有上面说的那个问题。</p>
<h2 id="replica-fetcher-线程的关闭"><a href="#replica-fetcher-线程的关闭" class="headerlink" title="replica fetcher 线程的关闭"></a>replica fetcher 线程的关闭</h2><p>最后我们再来介绍一下 replica fetcher 线程在什么情况下会关闭，同样，看一下最开始那张图的第三部分，图中已经比较清晰地列出了 replica fetcher 线程关闭的条件，在三种情况下会关闭对这个 topic-partition 的拉取操作（<code>becomeLeaderOrFollower()</code> 这个方法会在对 LeaderAndIsr 请求处理的文章中讲解，这里先忽略）：</p>
<ol>
<li><code>stopReplica()</code>：broker 收到了 controller 发来的 StopReplica 请求，这时会开始关闭对指定 topic-partition 的同步线程；</li>
<li><code>makeLeaders</code>：这些 partition 的本地副本被选举成了 leader，这时候就会先停止对这些 topic-partition 副本同步线程；</li>
<li><code>makeFollowers()</code>：前面已经介绍过，这里实际上停止副本同步，然后再开启副本同步线程，因为这些 topic-partition 的 leader 可能发生了切换。</li>
</ol>
<blockquote>
<p>这里直接说线程关闭，其实不是很准确，因为每个 replica fetcher 线程操作的是多个 topic-partition，而在关闭的粒度是 partition 级别，只有这个线程分配的 partition 全部关闭后，这个线程才会真正被关闭。</p>
</blockquote>
<h3 id="关闭副本同步"><a href="#关闭副本同步" class="headerlink" title="关闭副本同步"></a>关闭副本同步</h3><p>看下 ReplicaManager 中触发 replica fetcher 线程关闭的三个方法。</p>
<h4 id="stopReplica"><a href="#stopReplica" class="headerlink" title="stopReplica"></a>stopReplica</h4><p>StopReplica 的请求实际上是 Controller 发送过来的，这个在 controller 部分会讲述，它触发的条件有多种，比如：broker 下线、partition replica 迁移等等，ReplicaManager 这里的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 获取 tp 的 leader replica</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLeaderReplicaIfLocal</span></span>(topicPartition: <span class="type">TopicPartition</span>): <span class="type">Replica</span> =  &#123;</div><div class="line">  <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取对应的 Partiion 对象</span></div><div class="line">  partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">s"Partition <span class="subst">$topicPartition</span> doesn't exist on <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">      partition.leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt; leaderReplica <span class="comment">//note: 返回 leader 对应的副本</span></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">s"Leader not local for partition <span class="subst">$topicPartition</span> on broker <span class="subst">$localBrokerId</span>"</span>)</div><div class="line">      &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h4><p><code>makeLeaders()</code> 方法的调用是在 broker 上这个 partition 的副本被设置为 leader 时触发的，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> * Make the current broker to become leader for a given set of partitions by:</div><div class="line"> *</div><div class="line"> * 1. Stop fetchers for these partitions</div><div class="line"> * 2. Update the partition metadata in cache</div><div class="line"> * 3. Add these partitions to the leader partitions set</div><div class="line"> *</div><div class="line"> * If an unexpected error is thrown in this function, it will be propagated to KafkaApis where</div><div class="line"> * the error message will be set on each partition since we do not know which partition caused it. Otherwise,</div><div class="line"> * return the set of partitions that are made leader due to this method</div><div class="line"> *</div><div class="line"> *  <span class="doctag">TODO:</span> the above may need to be fixed later</div><div class="line"> */</div><div class="line"><span class="comment">//note: 选举当前副本作为 partition 的 leader，处理过程：</span></div><div class="line"><span class="comment">//note: 1. 停止这些 partition 的 副本同步请求；</span></div><div class="line"><span class="comment">//note: 2. 更新缓存中的 partition metadata；</span></div><div class="line"><span class="comment">//note: 3. 将这些 partition 添加到 leader partition 集合中。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</div><div class="line">                        epoch: <span class="type">Int</span>,</div><div class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</div><div class="line">                        correlationId: <span class="type">Int</span>,</div><div class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d handling LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"starting the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">for</span> (partition &lt;- partitionState.keys)</div><div class="line">    responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// First stop fetchers for all the partitions</span></div><div class="line">    <span class="comment">//note: 停止这些副本同步请求</span></div><div class="line">    replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</div><div class="line">    <span class="comment">// Update the partition information to be the leader</span></div><div class="line">    <span class="comment">//note: 更新这些 partition 的信息（这些 partition 成为 leader 了）</span></div><div class="line">    partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</div><div class="line">      <span class="comment">//note: 在 partition 对象将本地副本设置为 leader</span></div><div class="line">      <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</div><div class="line">        partitionsToMakeLeaders += partition <span class="comment">//note: 成功选为 leader 的 partition 集合</span></div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="comment">//note: 本地 replica 已经是 leader replica，可能是接收了重试的请求</span></div><div class="line">        stateChangeLogger.info((<span class="string">"Broker %d skipped the become-leader state change after marking its partition as leader with correlation id %d from "</span> +</div><div class="line">          <span class="string">"controller %d epoch %d for partition %s since it is already the leader for the partition."</span>)</div><div class="line">          .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">    partitionsToMakeLeaders.foreach &#123; partition =&gt;</div><div class="line">      stateChangeLogger.trace((<span class="string">"Broker %d stopped fetchers as part of become-leader request from controller "</span> +</div><div class="line">        <span class="string">"%d epoch %d with correlation id %d for partition %s"</span>)</div><div class="line">        .format(localBrokerId, controllerId, epoch, correlationId, partition.topicPartition))</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">      partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">        <span class="keyword">val</span> errorMsg = (<span class="string">"Error on broker %d while processing LeaderAndIsr request correlationId %d received from controller %d"</span> +</div><div class="line">          <span class="string">" epoch %d for partition %s"</span>).format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition)</div><div class="line">        stateChangeLogger.error(errorMsg, e)</div><div class="line">      &#125;</div><div class="line">      <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></div><div class="line">      <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: LeaderAndIsr 请求处理完成</span></div><div class="line">  partitionState.keys.foreach &#123; partition =&gt;</div><div class="line">    stateChangeLogger.trace((<span class="string">"Broker %d completed LeaderAndIsr request correlationId %d from controller %d epoch %d "</span> +</div><div class="line">      <span class="string">"for the become-leader transition for partition %s"</span>)</div><div class="line">      .format(localBrokerId, correlationId, controllerId, epoch, partition.topicPartition))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  partitionsToMakeLeaders</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>简单来说，这个方法的过程逻辑如下：</p>
<ol>
<li>先停止对这些 partition 的副本同步流程，因为这些 partition 的本地副本已经被选举成为了 leader；</li>
<li>将这些 partition 的本地副本设置为 leader，并且开始更新相应 meta 信息（主要是记录其他 follower 副本的相关信息）；</li>
<li>将这些 partition 添加到本地记录的 leader partition 集合中。</li>
</ol>
<h4 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h4><p>这个在前面已经讲述过了，参考前面的讲述。</p>
<h3 id="removeFetcherForPartitions"><a href="#removeFetcherForPartitions" class="headerlink" title="removeFetcherForPartitions"></a>removeFetcherForPartitions</h3><p>调用 ReplicaFetcherManager 的 <code>removeFetcherForPartitions()</code> 删除对这些 topic-partition 的副本同步设置，这里在实现时，会遍历所有的 replica fetcher 线程，都执行 <code>removePartitions()</code> 方法来移除对应的 topic-partition 集合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 删除一个 partition 的 replica-fetch 线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeFetcherForPartitions</span></span>(partitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">for</span> (fetcher &lt;- fetcherThreadMap.values) <span class="comment">//note: 遍历所有的 fetchThread 去移除这个 topic-partition 集合</span></div><div class="line">      fetcher.removePartitions(partitions)</div><div class="line">  &#125;</div><div class="line">  info(<span class="string">"Removed fetcher for partitions %s"</span>.format(partitions.mkString(<span class="string">","</span>)))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="removePartitions"><a href="#removePartitions" class="headerlink" title="removePartitions"></a>removePartitions</h3><p>这个方法的作用是：ReplicaFetcherThread 将这些 topic-partition 从自己要拉取的 partition 列表中移除。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">removePartitions</span></span>(topicPartitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</div><div class="line">  partitionMapLock.lockInterruptibly()</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    topicPartitions.foreach &#123; topicPartition =&gt;</div><div class="line">      partitionStates.remove(topicPartition)</div><div class="line">      fetcherLagStats.unregister(topicPartition.topic, topicPartition.partition)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">finally</span> partitionMapLock.unlock()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="ReplicaFetcherThread-的关闭"><a href="#ReplicaFetcherThread-的关闭" class="headerlink" title="ReplicaFetcherThread 的关闭"></a>ReplicaFetcherThread 的关闭</h3><p>前面介绍那么多，似乎还是没有真正去关闭，那么 ReplicaFetcherThread 真正关闭是哪里操作的呢？</p>
<p>实际上 ReplicaManager 每次处理完 LeaderAndIsr 请求后，都会调用 ReplicaFetcherManager 的 <code>shutdownIdleFetcherThreads()</code> 方法，如果 fetcher 线程要拉取的 topic-partition 集合为空，那么就会关闭掉对应的 fetcher 线程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 关闭没有拉取 topic-partition 任务的拉取线程</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownIdleFetcherThreads</span></span>() &#123;</div><div class="line">  mapLock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> keysToBeRemoved = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">BrokerAndFetcherId</span>]</div><div class="line">    <span class="keyword">for</span> ((key, fetcher) &lt;- fetcherThreadMap) &#123;</div><div class="line">      <span class="keyword">if</span> (fetcher.partitionCount &lt;= <span class="number">0</span>) &#123; <span class="comment">//note: 如果该线程拉取的 partition 数小于 0</span></div><div class="line">        fetcher.shutdown()</div><div class="line">        keysToBeRemoved += key</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    fetcherThreadMap --= keysToBeRemoved</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>关于 Replica Fetcher 线程这部分的内容终于讲解完了，希望能对大家有所帮助，有问题欢迎通过留言、微博或邮件进行交流。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上篇文章中讲述了 Fetch 请求是如何处理的，其中包括来自副本同步的 Fetch 请求和 Consumer 的 Fetch 请求，副本同步是 Kafka 多副本机制（可靠性）实现的基础，它也是通过向 leader replica 发送 Fetch 请求来实现数据同步的。
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</title>
    <link href="http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/"/>
    <id>http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/</id>
    <published>2018-04-15T15:21:16.000Z</published>
    <updated>2018-04-17T12:40:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底层的 Log 实例部分。与 Produce 请求不一样的地方是，对于 Fetch 请求，是有两种不同的来源：consumer 和 follower，consumer 读取数据与副本同步数据都是通过向 leader 发送 Fetch 请求来实现的，在对这两种不同情况处理过程中，其底层的实现是统一的，只是实现方法的参数不同而已，在本文中会详细讲述对这两种不同情况的处理。</p>
<h2 id="Fetch-请求处理的整体流程"><a href="#Fetch-请求处理的整体流程" class="headerlink" title="Fetch 请求处理的整体流程"></a>Fetch 请求处理的整体流程</h2><p>Fetch 请求（读请求）的处理与 Produce 请求（写请求）的整体流程非常类似，读和写由最上面的抽象层做入口，最终还是在存储层的 Log 对象实例进行真正的读写操作，在这一点上，Kafka 封装的非常清晰，这样的系统设计是非常值得学习的，甚至可以作为分布式系统的模范系统来学习。</p>
<p>Fetch 请求处理的整体流程如下图所示，与 Produce 请求的处理流程非常相似。</p>
<p><img src="/images/kafka/kafka_fetch_request.png" alt="Server 端处理 Fetch 请求的总体过程"></p>
<h3 id="Fetch-请求的来源"><a href="#Fetch-请求的来源" class="headerlink" title="Fetch 请求的来源"></a>Fetch 请求的来源</h3><p>那 Server 要处理的 Fetch 请求有几种类型呢？来自于哪里呢？第一个来源肯定是 Consumer，Consumer 在消费数据时会向 Server 端发送 Fetch 请求，那么是不是还没有其他的类型，对 Kafka 比较熟悉的同学大概会猜到，还有一种就是：副本同步，follower 在从 leader 同步数据时，也是发送的 Fetch 请求，下面看下这两种情况的具体实现（代码会进行简化，并不完全与源码一致，便于理解）。</p>
<h4 id="Consumer-Fetch-请求"><a href="#Consumer-Fetch-请求" class="headerlink" title="Consumer Fetch 请求"></a>Consumer Fetch 请求</h4><p>Consumer 的 Fetch 请求是在 poll 方法中调用的，Fetcher 请求的构造过程及发送如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Set-up a fetch request for any node that we have assigned partitions for which doesn't already have</div><div class="line"> * an in-flight fetch or pending fetch data.</div><div class="line"> * <span class="doctag">@return</span> number of fetches sent</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向订阅的所有 partition （只要该 leader 暂时没有拉取请求）所在 leader 发送 fetch请求</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="comment">//note: 1 创建 Fetch Request</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; fetchRequestMap = createFetchRequests();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchRequest.Builder&gt; fetchEntry : fetchRequestMap.entrySet()) &#123;</div><div class="line">        <span class="keyword">final</span> FetchRequest.Builder request = fetchEntry.getValue();</div><div class="line">        <span class="keyword">final</span> Node fetchTarget = fetchEntry.getKey();</div><div class="line"></div><div class="line">        log.debug(<span class="string">"Sending fetch for partitions &#123;&#125; to broker &#123;&#125;"</span>, request.fetchData().keySet(), fetchTarget);</div><div class="line">        <span class="comment">//note: 2 发送 Fetch Request</span></div><div class="line">        client.send(fetchTarget, request)</div><div class="line">                .addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</div><div class="line">                        ...</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="meta">@Override</span></div><div class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</div><div class="line">                        ...</div><div class="line">                    &#125;</div><div class="line">                &#125;);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> fetchRequestMap.size();</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Create fetch requests for all nodes for which we have assigned partitions</div><div class="line"> * that have no existing requests in flight.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 为所有 node 创建 fetch request</span></div><div class="line"><span class="keyword">private</span> Map&lt;Node, FetchRequest.Builder&gt; createFetchRequests() &#123;</div><div class="line">    <span class="comment">// create the fetch info</span></div><div class="line">    Cluster cluster = metadata.fetch();</div><div class="line">    Map&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; fetchable = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (TopicPartition partition : fetchablePartitions()) &#123;</div><div class="line">        Node node = cluster.leaderFor(partition);</div><div class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</div><div class="line">            metadata.requestUpdate();</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.client.pendingRequestCount(node) == <span class="number">0</span>) &#123;</div><div class="line">            <span class="comment">// if there is a leader and no in-flight requests, issue a new fetch</span></div><div class="line">            LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt; fetch = fetchable.get(node);</div><div class="line">            <span class="keyword">if</span> (fetch == <span class="keyword">null</span>) &#123;</div><div class="line">                fetch = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</div><div class="line">                fetchable.put(node, fetch);</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            <span class="keyword">long</span> position = <span class="keyword">this</span>.subscriptions.position(partition);</div><div class="line">            <span class="comment">//note: 要 fetch 的 position 以及 fetch 的大小</span></div><div class="line">            fetch.put(partition, <span class="keyword">new</span> FetchRequest.PartitionData(position, <span class="keyword">this</span>.fetchSize));</div><div class="line">            log.trace(<span class="string">"Added fetch request for partition &#123;&#125; at offset &#123;&#125; to node &#123;&#125;"</span>, partition, position, node);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            log.trace(<span class="string">"Skipping fetch for partition &#123;&#125; because there is an in-flight request to &#123;&#125;"</span>, partition, node);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// create the fetches</span></div><div class="line">    Map&lt;Node, FetchRequest.Builder&gt; requests = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; entry : fetchable.entrySet()) &#123;</div><div class="line">        Node node = entry.getKey();</div><div class="line">        <span class="comment">// 构造 Fetch 请求</span></div><div class="line">        FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</div><div class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></div><div class="line">        requests.put(node, fetch);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> requests;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出，Consumer 的 Fetcher 请求构造为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</div><div class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></div></pre></td></tr></table></figure>
<h4 id="Replica-同步-Fetch-请求"><a href="#Replica-同步-Fetch-请求" class="headerlink" title="Replica 同步 Fetch 请求"></a>Replica 同步 Fetch 请求</h4><p>在 Replica 同步（Replica 同步流程的讲解将会在下篇文章中详细展开）的 Fetch 请求中，其 Fetch 请求的构造如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 构造 Fetch 请求</span></div><div class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</div><div class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</div><div class="line"></div><div class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</div><div class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></div><div class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</div><div class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, consumer 的该值为 CONSUMER_REPLICA_ID（-1）</span></div><div class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</div><div class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</div><div class="line">  requestBuilder.setVersion(fetchRequestVersion)</div><div class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>与 Consumer Fetch 请求进行对比，这里区别仅在于在构造 FetchRequest 时，调用了 <code>setReplicaId()</code> 方法设置了对应的 replicaId，而 Consumer 在构造时则没有进行设置，该值默认为 <code>CONSUMER_REPLICA_ID</code>，即 <strong>-1</strong>，这个值是作为 Consumer 的 Fetch 请求与 Replica 同步的 Fetch 请求的区分。</p>
<h2 id="Server-端的处理"><a href="#Server-端的处理" class="headerlink" title="Server 端的处理"></a>Server 端的处理</h2><p>这里开始真正讲解 Fetch 请求的处理过程，会按照前面图中的处理流程开始讲解，本节主要是 Server 端抽象层的内容。</p>
<h3 id="KafkaApis-如何处理-Fetch-请求"><a href="#KafkaApis-如何处理-Fetch-请求" class="headerlink" title="KafkaApis 如何处理 Fetch 请求"></a>KafkaApis 如何处理 Fetch 请求</h3><p>关于 Fetch 请求的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle a fetch request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> fetchRequest = request.body.asInstanceOf[<span class="type">FetchRequest</span>]</div><div class="line">  <span class="keyword">val</span> versionId = request.header.apiVersion</div><div class="line">  <span class="keyword">val</span> clientId = request.header.clientId</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断 tp 是否存在以及是否有 Describe 权限</span></div><div class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = fetchRequest.fetchData.asScala.toSeq.partition &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic)) &amp;&amp; metadataCache.contains(tp.topic)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断 tp 是否有 Read 权限</span></div><div class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForReadRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Read</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 不存在或没有 Describe 权限的 topic 返回 UNKNOWN_TOPIC_OR_PARTITION 错误</span></div><div class="line">  <span class="keyword">val</span> nonExistingOrUnauthorizedForDescribePartitionData = nonExistingOrUnauthorizedForDescribeTopics.map &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 没有 Read 权限的 topic 返回 TOPIC_AUTHORIZATION_FAILED 错误</span></div><div class="line">  <span class="keyword">val</span> unauthorizedForReadPartitionData = unauthorizedForReadRequestInfo.map &#123;</div><div class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// the callback for sending a fetch response</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responsePartitionData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)]) &#123;</div><div class="line">    ....</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetchResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</div><div class="line">      trace(<span class="string">s"Sending fetch response to client <span class="subst">$clientId</span> of "</span> +</div><div class="line">        <span class="string">s"<span class="subst">$&#123;convertedPartitionData.map &#123; case (_, v) =&gt; v.records.sizeInBytes &#125;</span>.sum&#125; bytes"</span>)</div><div class="line">      <span class="keyword">val</span> fetchResponse = <span class="keyword">if</span> (delayTimeMs &gt; <span class="number">0</span>) <span class="keyword">new</span> <span class="type">FetchResponse</span>(versionId, fetchedPartitionData, delayTimeMs) <span class="keyword">else</span> response</div><div class="line">      requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, fetchResponse))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></div><div class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</div><div class="line"></div><div class="line">    <span class="comment">//note: 配额情况的处理</span></div><div class="line">    <span class="keyword">if</span> (fetchRequest.isFromFollower) &#123;</div><div class="line">      <span class="comment">// We've already evaluated against the quota and are good to go. Just need to record it now.</span></div><div class="line">      <span class="keyword">val</span> responseSize = sizeOfThrottledPartitions(versionId, fetchRequest, mergedPartitionData, quotas.leader)</div><div class="line">      quotas.leader.record(responseSize)</div><div class="line">      fetchResponseCallback(<span class="number">0</span>)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      quotas.fetch.recordAndMaybeThrottle(request.session.sanitizedUser, clientId, response.sizeOf, fetchResponseCallback)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</div><div class="line">    sendResponseCallback(<span class="type">Seq</span>.empty)</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// call the replica manager to fetch messages from the local replica</span></div><div class="line">    <span class="comment">//note: 从 replica 上拉取数据,满足条件后调用回调函数进行返回</span></div><div class="line">    replicaManager.fetchMessages(</div><div class="line">      fetchRequest.maxWait.toLong, <span class="comment">//note: 拉取请求最长的等待时间</span></div><div class="line">      fetchRequest.replicaId, <span class="comment">//note: Replica 编号，Consumer 的为 -1</span></div><div class="line">      fetchRequest.minBytes, <span class="comment">//note: 拉取请求设置的最小拉取字节</span></div><div class="line">      fetchRequest.maxBytes, <span class="comment">//note: 拉取请求设置的最大拉取字节</span></div><div class="line">      versionId &lt;= <span class="number">2</span>,</div><div class="line">      authorizedRequestInfo,</div><div class="line">      replicationQuota(fetchRequest),</div><div class="line">      sendResponseCallback)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Fetch 请求处理的真正实现是在 replicaManager 的 <code>fetchMessages()</code> 方法中，在这里，可以看出，无论是 Fetch 请求还是 Produce 请求，都是通过副本管理器来实现的，副本管理器（ReplicaManager）管理的对象是分区实例（Partition），而每个分区都会与相应的副本实例对应（Replica），在这个节点上的副本又会与唯一的 Log 实例对应，正如流程图的上半部分一样，Server 就是通过这几部分抽象概念来管理真正存储层的内容。</p>
<h3 id="ReplicaManager-如何处理-Fetch-请求"><a href="#ReplicaManager-如何处理-Fetch-请求" class="headerlink" title="ReplicaManager 如何处理 Fetch 请求"></a>ReplicaManager 如何处理 Fetch 请求</h3><p>ReplicaManger 处理 Fetch 请求的入口在 <code>fetchMessages()</code> 方法。</p>
<h4 id="fetchMessages"><a href="#fetchMessages" class="headerlink" title="fetchMessages"></a>fetchMessages</h4><p><code>fetchMessages()</code> 方法的具体如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Fetch messages from the leader replica, and wait until enough data can be fetched and return;</div><div class="line"> * the callback function will be triggered either when timeout or required fetch info is satisfied</div><div class="line"> */</div><div class="line"><span class="comment">//note: 从 leader 拉取数据,等待拉取到足够的数据或者达到 timeout 时间后返回拉取的结果</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetchMessages</span></span>(timeout: <span class="type">Long</span>,</div><div class="line">                  replicaId: <span class="type">Int</span>,</div><div class="line">                  fetchMinBytes: <span class="type">Int</span>,</div><div class="line">                  fetchMaxBytes: <span class="type">Int</span>,</div><div class="line">                  hardMaxBytesLimit: <span class="type">Boolean</span>,</div><div class="line">                  fetchInfos: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</div><div class="line">                  quota: <span class="type">ReplicaQuota</span> = <span class="type">UnboundedQuota</span>,</div><div class="line">                  responseCallback: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line">  <span class="keyword">val</span> isFromFollower = replicaId &gt;= <span class="number">0</span> <span class="comment">//note: 判断请求是来自 consumer （这个值为 -1）还是副本同步</span></div><div class="line">  <span class="comment">//note: 默认都是从 leader 拉取，推测这个值只是为了后续能从 follower 消费数据而设置的</span></div><div class="line">  <span class="keyword">val</span> fetchOnlyFromLeader: <span class="type">Boolean</span> = replicaId != <span class="type">Request</span>.<span class="type">DebuggingConsumerId</span></div><div class="line">  <span class="comment">//note: 如果拉取请求来自 consumer（true）,只拉取 HW 以内的数据,如果是来自 Replica 同步,则没有该限制（false）。</span></div><div class="line">  <span class="keyword">val</span> fetchOnlyCommitted: <span class="type">Boolean</span> = ! <span class="type">Request</span>.isValidBrokerId(replicaId)</div><div class="line"></div><div class="line">  <span class="comment">// read from local logs</span></div><div class="line">  <span class="comment">//note：获取本地日志</span></div><div class="line">  <span class="keyword">val</span> logReadResults = readFromLocalLog(</div><div class="line">    replicaId = replicaId,</div><div class="line">    fetchOnlyFromLeader = fetchOnlyFromLeader,</div><div class="line">    readOnlyCommitted = fetchOnlyCommitted,</div><div class="line">    fetchMaxBytes = fetchMaxBytes,</div><div class="line">    hardMaxBytesLimit = hardMaxBytesLimit,</div><div class="line">    readPartitionInfo = fetchInfos,</div><div class="line">    quota = quota)</div><div class="line"></div><div class="line">  <span class="comment">// if the fetch comes from the follower,</span></div><div class="line">  <span class="comment">// update its corresponding log end offset</span></div><div class="line">  <span class="comment">//note: 如果 fetch 来自 broker 的副本同步,那么就更新相关的 log end offset</span></div><div class="line">  <span class="keyword">if</span>(<span class="type">Request</span>.isValidBrokerId(replicaId))</div><div class="line">    updateFollowerLogReadResults(replicaId, logReadResults)</div><div class="line"></div><div class="line">  <span class="comment">// check if this fetch request can be satisfied right away</span></div><div class="line">  <span class="keyword">val</span> logReadResultValues = logReadResults.map &#123; <span class="keyword">case</span> (_, v) =&gt; v &#125;</div><div class="line">  <span class="keyword">val</span> bytesReadable = logReadResultValues.map(_.info.records.sizeInBytes).sum</div><div class="line">  <span class="keyword">val</span> errorReadingData = logReadResultValues.foldLeft(<span class="literal">false</span>) ((errorIncurred, readResult) =&gt;</div><div class="line">    errorIncurred || (readResult.error != <span class="type">Errors</span>.<span class="type">NONE</span>))</div><div class="line"></div><div class="line">  <span class="comment">// respond immediately if 1) fetch request does not want to wait</span></div><div class="line">  <span class="comment">//                        2) fetch request does not require any data</span></div><div class="line">  <span class="comment">//                        3) has enough data to respond</span></div><div class="line">  <span class="comment">//                        4) some error happens while reading data</span></div><div class="line">  <span class="comment">//note: 如果满足以下条件的其中一个,将会立马返回结果:</span></div><div class="line">  <span class="comment">//note: 1. timeout 达到; 2. 拉取结果为空; 3. 拉取到足够的数据; 4. 拉取是遇到 error</span></div><div class="line">  <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span> || fetchInfos.isEmpty || bytesReadable &gt;= fetchMinBytes || errorReadingData) &#123;</div><div class="line">    <span class="keyword">val</span> fetchPartitionData = logReadResults.map &#123; <span class="keyword">case</span> (tp, result) =&gt;</div><div class="line">      tp -&gt; <span class="type">FetchPartitionData</span>(result.error, result.hw, result.info.records)</div><div class="line">    &#125;</div><div class="line">    responseCallback(fetchPartitionData)</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">//note： 其他情况下,延迟发送结果</span></div><div class="line">    <span class="comment">// construct the fetch results from the read results</span></div><div class="line">    <span class="keyword">val</span> fetchPartitionStatus = logReadResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</div><div class="line">      <span class="keyword">val</span> fetchInfo = fetchInfos.collectFirst &#123;</div><div class="line">        <span class="keyword">case</span> (tp, v) <span class="keyword">if</span> tp == topicPartition =&gt; v</div><div class="line">      &#125;.getOrElse(sys.error(<span class="string">s"Partition <span class="subst">$topicPartition</span> not found in fetchInfos"</span>))</div><div class="line">      (topicPartition, <span class="type">FetchPartitionStatus</span>(result.info.fetchOffsetMetadata, fetchInfo))</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> fetchMetadata = <span class="type">FetchMetadata</span>(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, fetchOnlyFromLeader,</div><div class="line">      fetchOnlyCommitted, isFromFollower, replicaId, fetchPartitionStatus)</div><div class="line">    <span class="keyword">val</span> delayedFetch = <span class="keyword">new</span> <span class="type">DelayedFetch</span>(timeout, fetchMetadata, <span class="keyword">this</span>, quota, responseCallback)</div><div class="line"></div><div class="line">    <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed fetch operation</span></div><div class="line">    <span class="keyword">val</span> delayedFetchKeys = fetchPartitionStatus.map &#123; <span class="keyword">case</span> (tp, _) =&gt; <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(tp) &#125;</div><div class="line"></div><div class="line">    <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory;</span></div><div class="line">    <span class="comment">// this is because while the delayed fetch operation is being created, new requests</span></div><div class="line">    <span class="comment">// may arrive and hence make this operation completable.</span></div><div class="line">    delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>整体来说，分为以下几步：</p>
<ol>
<li><code>readFromLocalLog()</code>：调用该方法，从本地日志拉取相应的数据；</li>
<li>判断 Fetch 请求来源，如果来自副本同步，那么更新该副本的 the end offset 记录，如果该副本不在 isr 中，并判断是否需要更新 isr；</li>
<li>返回结果，满足条件的话立马返回，否则的话，通过延迟操作，延迟返回结果。</li>
</ol>
<h4 id="readFromLocalLog"><a href="#readFromLocalLog" class="headerlink" title="readFromLocalLog"></a>readFromLocalLog</h4><p><code>readFromLocalLog()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Read from multiple topic partitions at the given offset up to maxSize bytes</div><div class="line"> */</div><div class="line"><span class="comment">//note: 按 offset 从 tp 列表中读取相应的数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">readFromLocalLog</span></span>(replicaId: <span class="type">Int</span>,</div><div class="line">                     fetchOnlyFromLeader: <span class="type">Boolean</span>,</div><div class="line">                     readOnlyCommitted: <span class="type">Boolean</span>,</div><div class="line">                     fetchMaxBytes: <span class="type">Int</span>,</div><div class="line">                     hardMaxBytesLimit: <span class="type">Boolean</span>,</div><div class="line">                     readPartitionInfo: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</div><div class="line">                     quota: <span class="type">ReplicaQuota</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = &#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(tp: <span class="type">TopicPartition</span>, fetchInfo: <span class="type">PartitionData</span>, limitBytes: <span class="type">Int</span>, minOneMessage: <span class="type">Boolean</span>): <span class="type">LogReadResult</span> = &#123;</div><div class="line">    <span class="keyword">val</span> offset = fetchInfo.offset</div><div class="line">    <span class="keyword">val</span> partitionFetchSize = fetchInfo.maxBytes</div><div class="line"></div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).totalFetchRequestRate.mark()</div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalFetchRequestRate.mark()</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      trace(<span class="string">s"Fetching log segment for partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>, partition fetch size <span class="subst">$partitionFetchSize</span>, "</span> +</div><div class="line">        <span class="string">s"remaining response limit <span class="subst">$limitBytes</span>"</span> +</div><div class="line">        (<span class="keyword">if</span> (minOneMessage) <span class="string">s", ignoring response/partition size limits"</span> <span class="keyword">else</span> <span class="string">""</span>))</div><div class="line"></div><div class="line">      <span class="comment">// decide whether to only fetch from leader</span></div><div class="line">      <span class="comment">//note: 根据决定 [是否只从 leader 读取数据] 来获取相应的副本</span></div><div class="line">      <span class="comment">//note: 根据 tp 获取 Partition 对象, 在获取相应的 Replica 对象</span></div><div class="line">      <span class="keyword">val</span> localReplica = <span class="keyword">if</span> (fetchOnlyFromLeader)</div><div class="line">        getLeaderReplicaIfLocal(tp)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        getReplicaOrException(tp)</div><div class="line"></div><div class="line">      <span class="comment">// decide whether to only fetch committed data (i.e. messages below high watermark)</span></div><div class="line">      <span class="comment">//note: 获取 hw 位置，副本同步不设置这个值</span></div><div class="line">      <span class="keyword">val</span> maxOffsetOpt = <span class="keyword">if</span> (readOnlyCommitted)</div><div class="line">        <span class="type">Some</span>(localReplica.highWatermark.messageOffset)</div><div class="line">      <span class="keyword">else</span></div><div class="line">        <span class="type">None</span></div><div class="line"></div><div class="line">      <span class="comment">/* Read the LogOffsetMetadata prior to performing the read from the log.</span></div><div class="line">       * We use the LogOffsetMetadata to determine if a particular replica is in-sync or not.</div><div class="line">       * Using the log end offset after performing the read can lead to a race condition</div><div class="line">       * where data gets appended to the log immediately after the replica has consumed from it</div><div class="line">       * This can cause a replica to always be out of sync.</div><div class="line">       */</div><div class="line">      <span class="keyword">val</span> initialLogEndOffset = localReplica.logEndOffset.messageOffset <span class="comment">//note: the end offset</span></div><div class="line">      <span class="keyword">val</span> initialHighWatermark = localReplica.highWatermark.messageOffset <span class="comment">//note: hw</span></div><div class="line">      <span class="keyword">val</span> fetchTimeMs = time.milliseconds</div><div class="line">      <span class="keyword">val</span> logReadInfo = localReplica.log <span class="keyword">match</span> &#123;</div><div class="line">        <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</div><div class="line">          <span class="keyword">val</span> adjustedFetchSize = math.min(partitionFetchSize, limitBytes)</div><div class="line"></div><div class="line">          <span class="comment">// Try the read first, this tells us whether we need all of adjustedFetchSize for this partition</span></div><div class="line">          <span class="comment">//note: 从指定的 offset 位置开始读取数据，副本同步不需要 maxOffsetOpt</span></div><div class="line">          <span class="keyword">val</span> fetch = log.read(offset, adjustedFetchSize, maxOffsetOpt, minOneMessage)</div><div class="line"></div><div class="line">          <span class="comment">// If the partition is being throttled, simply return an empty set.</span></div><div class="line">          <span class="keyword">if</span> (shouldLeaderThrottle(quota, tp, replicaId)) <span class="comment">//note: 如果被限速了,那么返回 空 集合</span></div><div class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">          <span class="comment">// For FetchRequest version 3, we replace incomplete message sets with an empty one as consumers can make</span></div><div class="line">          <span class="comment">// progress in such cases and don't need to report a `RecordTooLargeException`</span></div><div class="line">          <span class="keyword">else</span> <span class="keyword">if</span> (!hardMaxBytesLimit &amp;&amp; fetch.firstEntryIncomplete)</div><div class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">          <span class="keyword">else</span> fetch</div><div class="line"></div><div class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">          error(<span class="string">s"Leader for partition <span class="subst">$tp</span> does not have a local log"</span>)</div><div class="line">          <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">//note: 返回最后的结果,返回的都是 LogReadResult 对象</span></div><div class="line">      <span class="type">LogReadResult</span>(info = logReadInfo,</div><div class="line">                    hw = initialHighWatermark,</div><div class="line">                    leaderLogEndOffset = initialLogEndOffset,</div><div class="line">                    fetchTimeMs = fetchTimeMs,</div><div class="line">                    readSize = partitionFetchSize,</div><div class="line">                    exception = <span class="type">None</span>)</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="comment">// <span class="doctag">NOTE:</span> Failed fetch requests metric is not incremented for known exceptions since it</span></div><div class="line">      <span class="comment">// is supposed to indicate un-expected failure of a broker in handling a fetch request</span></div><div class="line">      <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</div><div class="line">               _: <span class="type">NotLeaderForPartitionException</span> |</div><div class="line">               _: <span class="type">ReplicaNotAvailableException</span> |</div><div class="line">               _: <span class="type">OffsetOutOfRangeException</span>) =&gt;</div><div class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</div><div class="line">                      hw = <span class="number">-1</span>L,</div><div class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</div><div class="line">                      fetchTimeMs = <span class="number">-1</span>L,</div><div class="line">                      readSize = partitionFetchSize,</div><div class="line">                      exception = <span class="type">Some</span>(e))</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).failedFetchRequestRate.mark()</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().failedFetchRequestRate.mark()</div><div class="line">        error(<span class="string">s"Error processing fetch operation on partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>"</span>, e)</div><div class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</div><div class="line">                      hw = <span class="number">-1</span>L,</div><div class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</div><div class="line">                      fetchTimeMs = <span class="number">-1</span>L,</div><div class="line">                      readSize = partitionFetchSize,</div><div class="line">                      exception = <span class="type">Some</span>(e))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> limitBytes = fetchMaxBytes</div><div class="line">  <span class="keyword">val</span> result = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]</div><div class="line">  <span class="keyword">var</span> minOneMessage = !hardMaxBytesLimit</div><div class="line">  readPartitionInfo.foreach &#123; <span class="keyword">case</span> (tp, fetchInfo) =&gt;</div><div class="line">    <span class="keyword">val</span> readResult = read(tp, fetchInfo, limitBytes, minOneMessage) <span class="comment">//note: 读取该 tp 的数据</span></div><div class="line">    <span class="keyword">val</span> messageSetSize = readResult.info.records.sizeInBytes</div><div class="line">    <span class="comment">// Once we read from a non-empty partition, we stop ignoring request and partition level size limits</span></div><div class="line">    <span class="keyword">if</span> (messageSetSize &gt; <span class="number">0</span>)</div><div class="line">      minOneMessage = <span class="literal">false</span></div><div class="line">    limitBytes = math.max(<span class="number">0</span>, limitBytes - messageSetSize)</div><div class="line">    result += (tp -&gt; readResult)</div><div class="line">  &#125;</div><div class="line">  result</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>readFromLocalLog()</code> 方法的处理过程：</p>
<ol>
<li>先根据要拉取的 topic-partition 获取对应的 Partition 对象，根据 Partition 对象获取对应的 Replica 对象；</li>
<li>根据 Replica 对象找到对应的 Log 对象，然后调用其 <code>read()</code> 方法从指定的位置读取数据。</li>
</ol>
<h2 id="存储层对-Fetch-请求的处理"><a href="#存储层对-Fetch-请求的处理" class="headerlink" title="存储层对 Fetch 请求的处理"></a>存储层对 Fetch 请求的处理</h2><p>接着前面的流程开始往下走。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>每个 Replica 会对应一个 log 对象，而每个 log 对象会管理相应的 LogSegment 实例。</p>
<h4 id="read"><a href="#read" class="headerlink" title="read()"></a>read()</h4><p>Log 对象的 <code>read()</code> 方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 从指定 offset 开始读取数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxLength: <span class="type">Int</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>] = <span class="type">None</span>, minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</div><div class="line">  trace(<span class="string">"Reading %d bytes from offset %d in log %s of length %d bytes"</span>.format(maxLength, startOffset, name, size))</div><div class="line"></div><div class="line">  <span class="comment">// Because we don't use lock for reading, the synchronization is a little bit tricky.</span></div><div class="line">  <span class="comment">// We create the local variables to avoid race conditions with updates to the log.</span></div><div class="line">  <span class="keyword">val</span> currentNextOffsetMetadata = nextOffsetMetadata</div><div class="line">  <span class="keyword">val</span> next = currentNextOffsetMetadata.messageOffset</div><div class="line">  <span class="keyword">if</span>(startOffset == next)</div><div class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(currentNextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line"></div><div class="line">  <span class="comment">//note: 先查找对应的日志分段（segment）</span></div><div class="line">  <span class="keyword">var</span> entry = segments.floorEntry(startOffset)</div><div class="line"></div><div class="line">  <span class="comment">// attempt to read beyond the log end offset is an error</span></div><div class="line">  <span class="keyword">if</span>(startOffset &gt; next || entry == <span class="literal">null</span>)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(<span class="string">"Request for offset %d but we only have log segments in the range %d to %d."</span>.format(startOffset, segments.firstKey, next))</div><div class="line"></div><div class="line">  <span class="comment">// Do the read on the segment with a base offset less than the target offset</span></div><div class="line">  <span class="comment">// but if that segment doesn't contain any messages with an offset greater than that</span></div><div class="line">  <span class="comment">// continue to read from successive segments until we get some messages or we reach the end of the log</span></div><div class="line">  <span class="keyword">while</span>(entry != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">// If the fetch occurs on the active segment, there might be a race condition where two fetch requests occur after</span></div><div class="line">    <span class="comment">// the message is appended but before the nextOffsetMetadata is updated. In that case the second fetch may</span></div><div class="line">    <span class="comment">// cause OffsetOutOfRangeException. To solve that, we cap the reading up to exposed position instead of the log</span></div><div class="line">    <span class="comment">// end of the active segment.</span></div><div class="line">    <span class="comment">//note: 如果 Fetch 请求刚好发生在 the active segment 上,当多个 Fetch 请求同时处理,如果 nextOffsetMetadata 更新不及时,可能会导致</span></div><div class="line">    <span class="comment">//note: 发送 OffsetOutOfRangeException 异常; 为了解决这个问题, 这里能读取的最大位置是对应的物理位置（exposedPos）</span></div><div class="line">    <span class="comment">//note: 而不是 the log end of the active segment.</span></div><div class="line">    <span class="keyword">val</span> maxPosition = &#123;</div><div class="line">      <span class="keyword">if</span> (entry == segments.lastEntry) &#123;</div><div class="line">        <span class="comment">//note: nextOffsetMetadata 对应的实际物理位置</span></div><div class="line">        <span class="keyword">val</span> exposedPos = nextOffsetMetadata.relativePositionInSegment.toLong</div><div class="line">        <span class="comment">// Check the segment again in case a new segment has just rolled out.</span></div><div class="line">        <span class="keyword">if</span> (entry != segments.lastEntry) <span class="comment">//note: 可能会有新的 segment 产生,所以需要再次判断</span></div><div class="line">          <span class="comment">// New log segment has rolled out, we can read up to the file end.</span></div><div class="line">          entry.getValue.size</div><div class="line">        <span class="keyword">else</span></div><div class="line">          exposedPos</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        entry.getValue.size</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 从 segment 中读取相应的数据</span></div><div class="line">    <span class="keyword">val</span> fetchInfo = entry.getValue.read(startOffset, maxOffset, maxLength, maxPosition, minOneMessage)</div><div class="line">    <span class="keyword">if</span>(fetchInfo == <span class="literal">null</span>) &#123; <span class="comment">//note: 如果该日志分段没有读取到数据,则读取更高的日志分段</span></div><div class="line">      entry = segments.higherEntry(entry.getKey)</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">return</span> fetchInfo</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// okay we are beyond the end of the last segment with no data fetched although the start offset is in range,</span></div><div class="line">  <span class="comment">// this can happen when all messages with offset larger than start offsets have been deleted.</span></div><div class="line">  <span class="comment">// In this case, we will return the empty set with log end offset metadata</span></div><div class="line">  <span class="type">FetchDataInfo</span>(nextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从实现可以看出，该方法会先查找对应的 Segment 对象（日志分段），然后循环直到读取到数据结束，如果当前的日志分段没有读取到相应的数据，那么会更新日志分段及对应的最大位置。</p>
<p>日志分段实际上是逻辑概念，它管理了物理概念的一个数据文件、一个时间索引文件和一个 offset 索引文件，读取日志分段时，会先读取 offset 索引文件再读取数据文件，具体步骤如下：</p>
<ol>
<li>根据要读取的起始偏移量（startOffset）读取 offset 索引文件中对应的物理位置；</li>
<li>查找 offset 索引文件最后返回：起始偏移量对应的最近物理位置（startPosition）；</li>
<li>根据 startPosition 直接定位到数据文件，然后读取数据文件内容；</li>
<li>最多能读到数据文件的结束位置（maxPosition）。</li>
</ol>
<h3 id="LogSegment"><a href="#LogSegment" class="headerlink" title="LogSegment"></a>LogSegment</h3><p>关乎 数据文件、offset 索引文件和时间索引文件真正的操作都是在 LogSegment 对象中的，日志读取也与这个方法息息相关。</p>
<h4 id="read-1"><a href="#read-1" class="headerlink" title="read()"></a>read()</h4><p><code>read()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 读取日志分段（副本同步不会设置 maxSize）</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>], maxSize: <span class="type">Int</span>, maxPosition: <span class="type">Long</span> = size,</div><div class="line">         minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (maxSize &lt; <span class="number">0</span>)</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Invalid max size for log read (%d)"</span>.format(maxSize))</div><div class="line"></div><div class="line">  <span class="comment">//note: log 文件物理长度</span></div><div class="line">  <span class="keyword">val</span> logSize = log.sizeInBytes <span class="comment">// this may change, need to save a consistent copy</span></div><div class="line">  <span class="comment">//note: 将起始的 offset 转换为起始的实际物理位置</span></div><div class="line">  <span class="keyword">val</span> startOffsetAndSize = translateOffset(startOffset)</div><div class="line"></div><div class="line">  <span class="comment">// if the start position is already off the end of the log, return null</span></div><div class="line">  <span class="keyword">if</span> (startOffsetAndSize == <span class="literal">null</span>)</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span></div><div class="line"></div><div class="line">  <span class="keyword">val</span> startPosition = startOffsetAndSize.position.toInt</div><div class="line">  <span class="keyword">val</span> offsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(startOffset, <span class="keyword">this</span>.baseOffset, startPosition)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> adjustedMaxSize =</div><div class="line">    <span class="keyword">if</span> (minOneMessage) math.max(maxSize, startOffsetAndSize.size)</div><div class="line">    <span class="keyword">else</span> maxSize</div><div class="line"></div><div class="line">  <span class="comment">// return a log segment but with zero size in the case below</span></div><div class="line">  <span class="keyword">if</span> (adjustedMaxSize == <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</div><div class="line"></div><div class="line">  <span class="comment">// calculate the length of the message set to read based on whether or not they gave us a maxOffset</span></div><div class="line">  <span class="comment">//note: 计算读取的长度</span></div><div class="line">  <span class="keyword">val</span> length = maxOffset <span class="keyword">match</span> &#123;</div><div class="line">    <span class="comment">//note: 副本同步时的计算方式</span></div><div class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">      <span class="comment">// no max offset, just read until the max position</span></div><div class="line">      min((maxPosition - startPosition).toInt, adjustedMaxSize) <span class="comment">//note: 直接读取到最大的位置</span></div><div class="line">    <span class="comment">//note: consumer 拉取时,计算方式</span></div><div class="line">    <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt;</div><div class="line">      <span class="comment">// there is a max offset, translate it to a file position and use that to calculate the max read size;</span></div><div class="line">      <span class="comment">// when the leader of a partition changes, it's possible for the new leader's high watermark to be less than the</span></div><div class="line">      <span class="comment">// true high watermark in the previous leader for a short window. In this window, if a consumer fetches on an</span></div><div class="line">      <span class="comment">// offset between new leader's high watermark and the log end offset, we want to return an empty response.</span></div><div class="line">      <span class="keyword">if</span> (offset &lt; startOffset)</div><div class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>, firstEntryIncomplete = <span class="literal">false</span>)</div><div class="line">      <span class="keyword">val</span> mapping = translateOffset(offset, startPosition)</div><div class="line">      <span class="keyword">val</span> endPosition =</div><div class="line">        <span class="keyword">if</span> (mapping == <span class="literal">null</span>)</div><div class="line">          logSize <span class="comment">// the max offset is off the end of the log, use the end of the file</span></div><div class="line">        <span class="keyword">else</span></div><div class="line">          mapping.position</div><div class="line">      min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 根据起始的物理位置和读取长度读取数据文件</span></div><div class="line">  <span class="type">FetchDataInfo</span>(offsetMetadata, log.read(startPosition, length),</div><div class="line">    firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的实现来看，上述过程分为以下三部分：</p>
<ol>
<li>根据 startOffset 得到实际的物理位置（<code>translateOffset()</code>）；</li>
<li>计算要读取的实际物理长度；</li>
<li>根据实际起始物理位置和要读取实际物理长度读取数据文件。</li>
</ol>
<h4 id="translateOffset"><a href="#translateOffset" class="headerlink" title="translateOffset()"></a>translateOffset()</h4><p><code>translateOffset()</code> 方法的实现过程主要分为两部分：</p>
<ol>
<li>查找 offset 索引文件：调用 offset 索引文件的 <code>lookup()</code> 查找方法，获取离 startOffset 最接近的物理位置；</li>
<li>调用数据文件的 <code>searchFor()</code> 方法，从指定的物理位置开始读取每条数据，知道找到对应 offset 的物理位置。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">translateOffset</span></span>(offset: <span class="type">Long</span>, startingFilePosition: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogEntryPosition</span> = &#123;</div><div class="line">  <span class="comment">//note: 获取离 offset 最新的物理位置,返回包括 offset 和物理位置（不是准确值）</span></div><div class="line">  <span class="keyword">val</span> mapping = index.lookup(offset)</div><div class="line">  <span class="comment">//note: 从指定的位置开始消费,直到找到 offset 对应的实际物理位置,返回包括 offset 和物理位置（准确值）</span></div><div class="line">  log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="查找-offset-索引文件"><a href="#查找-offset-索引文件" class="headerlink" title="查找 offset 索引文件"></a>查找 offset 索引文件</h5><p>offset 索引文件是使用内存映射（不了解的，可以阅读 <a href="http://matt33.com/2018/02/04/linux-mmap/">操作系统之共享对象学习</a>）的方式加载到内存中的，在查询的过程中，内存映射是会发生变化，所以在 <code>lookup()</code> 中先拷贝出来了一个（idx），然后再进行查询，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 查找小于等于指定 offset 的最大 offset,并且返回对应的 offset 和实际物理位置</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(targetOffset: <span class="type">Long</span>): <span class="type">OffsetPosition</span> = &#123;</div><div class="line">  maybeLock(lock) &#123;</div><div class="line">    <span class="keyword">val</span> idx = mmap.duplicate <span class="comment">//note: 查询时,mmap 会发生变化,先复制出来一个</span></div><div class="line">    <span class="keyword">val</span> slot = indexSlotFor(idx, targetOffset, <span class="type">IndexSearchType</span>.<span class="type">KEY</span>) <span class="comment">//note: 二分查找</span></div><div class="line">    <span class="keyword">if</span>(slot == <span class="number">-1</span>)</div><div class="line">      <span class="type">OffsetPosition</span>(baseOffset, <span class="number">0</span>)</div><div class="line">    <span class="keyword">else</span></div><div class="line">      <span class="comment">//note: 先计算绝对偏移量,再计算物理位置</span></div><div class="line">      parseEntry(idx, slot).asInstanceOf[<span class="type">OffsetPosition</span>]</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">parseEntry</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">IndexEntry</span> = &#123;</div><div class="line">    <span class="type">OffsetPosition</span>(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">relativeOffset</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize)</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">physical</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize + <span class="number">4</span>)</div></pre></td></tr></table></figure>
<p>关于 relativeOffset 和 physical 的计算方法，可以参考下面这张图（来自《Kafka 计算内幕》）：</p>
<p><img src="/images/kafka/offset-physical.png" alt="根据索引条目编号查找偏移量的值和物理位置的值"></p>
<h5 id="搜索数据文件获取准确的物理位置"><a href="#搜索数据文件获取准确的物理位置" class="headerlink" title="搜索数据文件获取准确的物理位置"></a>搜索数据文件获取准确的物理位置</h5><p>前面通过 offset 索引文件获取的物理位置是一个接近值，下面通过实际读取数据文件将会得到一个真正的准确值，它是通过遍历数据文件实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Search forward for the file position of the last offset that is greater than or equal to the target offset</div><div class="line"> * and return its physical position and the size of the message (including log overhead) at the returned offset. If</div><div class="line"> * no such offsets are found, return null.</div><div class="line"> *</div><div class="line"> * @param targetOffset The offset to search for.</div><div class="line"> * @param startingPosition The starting position in the file to begin searching from.</div><div class="line"> */</div><div class="line">public <span class="type">LogEntryPosition</span> searchForOffsetWithSize(long targetOffset, int startingPosition) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="type">FileChannelLogEntry</span> entry : shallowEntriesFrom(startingPosition)) &#123;</div><div class="line">        long offset = entry.offset();</div><div class="line">        <span class="keyword">if</span> (offset &gt;= targetOffset)</div><div class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">LogEntryPosition</span>(offset, entry.position(), entry.sizeInBytes());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里，一个 Fetch 请求的处理过程算是完成了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</title>
    <link href="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/"/>
    <id>http://matt33.com/2018/03/18/kafka-server-handle-produce-request/</id>
    <published>2018-03-18T08:32:01.000Z</published>
    <updated>2018-03-18T08:45:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer Client 的 Produce 请求为入口。但是 Server 端的内容较多，本篇文章并不能全部涵盖，涉及到其他内容，在本篇文章暂时先不详细讲述，后面会再分析，本篇文章会以 Server 处理 produce 为主线，主要详细讲解 Kafka 存储层的内容。</p>
<h2 id="produce-请求处理整体流程"><a href="#produce-请求处理整体流程" class="headerlink" title="produce 请求处理整体流程"></a>produce 请求处理整体流程</h2><p>根据在这篇 <a href="http://matt33.com/2017/06/25/kafka-producer-send-module/">Kafka 源码解析之 Producer 发送模型（一）</a> 中的讲解，在 Producer Client 端，Producer 会维护一个 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches</code> 的变量，然后会根据 topic-partition 的 leader 信息，将 leader 在同一台机器上的 batch 放在一个 request 中，发送到 server，这样可以节省很多网络开销，提高发送效率。</p>
<p>Producer Client 发送请求的方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 produce 请求</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;RecordBatch&gt; batches)</span> </span>&#123;</div><div class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</div><div class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</div><div class="line">    <span class="keyword">for</span> (RecordBatch batch : batches) &#123;</div><div class="line">        TopicPartition tp = batch.topicPartition;</div><div class="line">        produceRecordsByPartition.put(tp, batch.records());</div><div class="line">        recordsByPartition.put(tp, batch);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    ProduceRequest.Builder requestBuilder =</div><div class="line">            <span class="keyword">new</span> ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</div><div class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</div><div class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</div><div class="line">        &#125;</div><div class="line">    &#125;;</div><div class="line"></div><div class="line">    String nodeId = Integer.toString(destination);</div><div class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, callback);</div><div class="line">    client.send(clientRequest, now);</div><div class="line">    log.trace(<span class="string">"Sent produce request to &#123;&#125;: &#123;&#125;"</span>, nodeId, requestBuilder);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在发送 Produce 的请求里，Client 是把一个 <code>Map&lt;TopicPartition, MemoryRecords&gt;</code> 类型的 <code>produceRecordsByPartition</code> 作为内容发送给了 Server 端，那么 Server 端是如何处理这个请求的呢？这就是本篇文章要讲述的内容，Server 处理这个请求的总体逻辑如下图所示：</p>
<p><img src="/images/kafka/kafka_produce_process.png" alt="Server 端处理 produce 请求的总体过程"></p>
<p>Broker 在收到 Produce 请求后，会有一个 KafkaApis 进行处理，KafkaApis 是 Server 端处理所有请求的入口，它会负责将请求的具体处理交给相应的组件进行处理，从上图可以看到 Produce 请求是交给了 ReplicaManager 对象进行处理了。</p>
<h2 id="Server-端处理"><a href="#Server-端处理" class="headerlink" title="Server 端处理"></a>Server 端处理</h2><p>Server 端的处理过程会按照上图的流程一块一块去介绍。</p>
<h3 id="KafkaApis-处理-Produce-请求"><a href="#KafkaApis-处理-Produce-请求" class="headerlink" title="KafkaApis 处理 Produce 请求"></a>KafkaApis 处理 Produce 请求</h3><p>KafkaApis 处理 produce 请求是在 <code>handleProducerRequest()</code> 方法中完成，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Handle a produce request</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleProducerRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</div><div class="line">  <span class="keyword">val</span> produceRequest = request.body.asInstanceOf[<span class="type">ProduceRequest</span>]</div><div class="line">  <span class="keyword">val</span> numBytesAppended = request.header.sizeOf + produceRequest.sizeOf</div><div class="line"></div><div class="line">  <span class="comment">//note: 按 exist 和有 Describe 权限进行筛选</span></div><div class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = produceRequest.partitionRecords.asScala.partition &#123;</div><div class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic)) &amp;&amp; metadataCache.contains(topicPartition.topic)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 判断有没有 Write 权限</span></div><div class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForWriteRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</div><div class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Write</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// the callback for sending a produce response</span></div><div class="line">  <span class="comment">//note: 回调函数</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> mergedResponseStatus = responseStatus ++</div><div class="line">      unauthorizedForWriteRequestInfo.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)) ++</div><div class="line">      nonExistingOrUnauthorizedForDescribeTopics.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>))</div><div class="line"></div><div class="line">    <span class="keyword">var</span> errorInResponse = <span class="literal">false</span></div><div class="line"></div><div class="line">    mergedResponseStatus.foreach &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</div><div class="line">      <span class="keyword">if</span> (status.error != <span class="type">Errors</span>.<span class="type">NONE</span>) &#123;</div><div class="line">        errorInResponse = <span class="literal">true</span></div><div class="line">        debug(<span class="string">"Produce request with correlation id %d from client %s on partition %s failed due to %s"</span>.format(</div><div class="line">          request.header.correlationId,</div><div class="line">          request.header.clientId,</div><div class="line">          topicPartition,</div><div class="line">          status.error.exceptionName))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">produceResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</div><div class="line">      <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) &#123;</div><div class="line">        <span class="comment">// no operation needed if producer request.required.acks = 0; however, if there is any error in handling</span></div><div class="line">        <span class="comment">// the request, since no response is expected by the producer, the server will close socket server so that</span></div><div class="line">        <span class="comment">// the producer client will know that some error has happened and will refresh its metadata</span></div><div class="line">        <span class="comment">//note: 因为设置的 ack=0, 相当于 client 会默认发送成功了,如果 server 在处理的过程出现了错误,那么就会关闭 socket 连接来间接地通知 client</span></div><div class="line">        <span class="comment">//note: client 会重新刷新 meta,重新建立相应的连接</span></div><div class="line">        <span class="keyword">if</span> (errorInResponse) &#123;</div><div class="line">          <span class="keyword">val</span> exceptionsSummary = mergedResponseStatus.map &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</div><div class="line">            topicPartition -&gt; status.error.exceptionName</div><div class="line">          &#125;.mkString(<span class="string">", "</span>)</div><div class="line">          info(</div><div class="line">            <span class="string">s"Closing connection due to error during produce request with correlation id <span class="subst">$&#123;request.header.correlationId&#125;</span> "</span> +</div><div class="line">              <span class="string">s"from client id <span class="subst">$&#123;request.header.clientId&#125;</span> with ack=0\n"</span> +</div><div class="line">              <span class="string">s"Topic and partition to exceptions: <span class="subst">$exceptionsSummary</span>"</span></div><div class="line">          )</div><div class="line">          requestChannel.closeConnection(request.processor, request)</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          requestChannel.noOperation(request.processor, request)</div><div class="line">        &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">val</span> respBody = request.header.apiVersion <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava)</div><div class="line">          <span class="keyword">case</span> version@(<span class="number">1</span> | <span class="number">2</span>) =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava, delayTimeMs, version)</div><div class="line">          <span class="comment">// This case shouldn't happen unless a new version of ProducerRequest is added without</span></div><div class="line">          <span class="comment">// updating this part of the code to handle it properly.</span></div><div class="line">          <span class="keyword">case</span> version =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Version `<span class="subst">$version</span>` of ProduceRequest is not handled. Code must be updated."</span>)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, respBody))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></div><div class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</div><div class="line"></div><div class="line">    quotas.produce.recordAndMaybeThrottle(</div><div class="line">      request.session.sanitizedUser,</div><div class="line">      request.header.clientId,</div><div class="line">      numBytesAppended,</div><div class="line">      produceResponseCallback)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</div><div class="line">    sendResponseCallback(<span class="type">Map</span>.empty)</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">val</span> internalTopicsAllowed = request.header.clientId == <span class="type">AdminUtils</span>.<span class="type">AdminClientId</span></div><div class="line"></div><div class="line">    <span class="comment">// call the replica manager to append messages to the replicas</span></div><div class="line">    <span class="comment">//note: 追加 Record</span></div><div class="line">    replicaManager.appendRecords(</div><div class="line">      produceRequest.timeout.toLong,</div><div class="line">      produceRequest.acks,</div><div class="line">      internalTopicsAllowed,</div><div class="line">      authorizedRequestInfo,</div><div class="line">      sendResponseCallback)</div><div class="line"></div><div class="line">    <span class="comment">// if the request is put into the purgatory, it will have a held reference</span></div><div class="line">    <span class="comment">// and hence cannot be garbage collected; hence we clear its data here in</span></div><div class="line">    <span class="comment">// order to let GC re-claim its memory since it is already appended to log</span></div><div class="line">    produceRequest.clearPartitionRecords()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>总体来说，处理过程是（在权限系统的情况下）：</p>
<ol>
<li>查看 topic 是否存在，以及 client 是否有相应的 Desribe 权限；</li>
<li>对于已经有 Describe 权限的 topic 查看是否有 Write 权限；</li>
<li>调用 <code>replicaManager.appendRecords()</code> 方法向有 Write 权限的 topic-partition 追加相应的 record。</li>
</ol>
<h3 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h3><p>ReplicaManager 顾名思义，它就是副本管理器，副本管理器的作用是管理这台 broker 上的所有副本（replica）。在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。</p>
<p>Kafka Server 在启动的时候，会创建 ReplicaManager 对象，如下所示。在 ReplicaManager 的构造方法中，它需要 LogManager 作为成员变量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//kafka.server.KafkaServer</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line">    <span class="comment">/* start replica manager */</span></div><div class="line">    replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)</div><div class="line">    replicaManager.startup()</div><div class="line">  &#125;<span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">    isStartingUp.set(<span class="literal">false</span>)</div><div class="line">    shutdown()</div><div class="line">    <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ReplicaManager 的<strong>并不负责具体的日志创建，它只是管理 Broker 上的所有分区</strong>（也就是图中下一步的那个 Partition 对象）。在创建 Partition 对象时，它需要 ReplicaManager 的 logManager 对象，Partition 会通过这个 logManager 对象为每个 replica 创建对应的日志。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR</div><div class="line"> */</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Partition</span>(<span class="params">val topic: <span class="type">String</span>,</span></span></div><div class="line">                val partitionId: <span class="type">Int</span>,</div><div class="line">                time: <span class="type">Time</span>,</div><div class="line">                replicaManager: <span class="type">ReplicaManager</span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> &#123;</div><div class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partitionId)</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> localBrokerId = replicaManager.config.brokerId</div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logManager = replicaManager.logManager <span class="comment">//note: 日志管理器</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ReplicaManager 与 LogManger 对比如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 后面还会介绍，这篇文章先不详细展开。</p>
<h4 id="appendRecords-实现"><a href="#appendRecords-实现" class="headerlink" title="appendRecords() 实现"></a><code>appendRecords()</code> 实现</h4><p>下面我们来看 <code>appendRecords()</code> 方法的具体实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 向 partition 的 leader 写入数据</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecords</span></span>(timeout: <span class="type">Long</span>,</div><div class="line">                  requiredAcks: <span class="type">Short</span>,</div><div class="line">                  internalTopicsAllowed: <span class="type">Boolean</span>,</div><div class="line">                  entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</div><div class="line">                  responseCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>] =&gt; <span class="type">Unit</span>) &#123;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (isValidRequiredAcks(requiredAcks)) &#123; <span class="comment">//note: acks 设置有效</span></div><div class="line">    <span class="keyword">val</span> sTime = time.milliseconds</div><div class="line">    <span class="comment">//note: 向本地的副本 log 追加数据</span></div><div class="line">    <span class="keyword">val</span> localProduceResults = appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)</div><div class="line">    debug(<span class="string">"Produce to local log in %d ms"</span>.format(time.milliseconds - sTime))</div><div class="line"></div><div class="line">    <span class="keyword">val</span> produceStatus = localProduceResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</div><div class="line">      topicPartition -&gt;</div><div class="line">              <span class="type">ProducePartitionStatus</span>(</div><div class="line">                result.info.lastOffset + <span class="number">1</span>, <span class="comment">// required offset</span></div><div class="line">                <span class="keyword">new</span> <span class="type">PartitionResponse</span>(result.error, result.info.firstOffset, result.info.logAppendTime)) <span class="comment">// response status</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</div><div class="line">      <span class="comment">//note: 处理 ack=-1 的情况,需要等到 isr 的 follower 都写入成功的话,才能返回最后结果</span></div><div class="line">      <span class="comment">// create delayed produce operation</span></div><div class="line">      <span class="keyword">val</span> produceMetadata = <span class="type">ProduceMetadata</span>(requiredAcks, produceStatus)</div><div class="line">      <span class="comment">//note: 延迟 produce 请求</span></div><div class="line">      <span class="keyword">val</span> delayedProduce = <span class="keyword">new</span> <span class="type">DelayedProduce</span>(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback)</div><div class="line"></div><div class="line">      <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed produce operation</span></div><div class="line">      <span class="keyword">val</span> producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(_)).toSeq</div><div class="line"></div><div class="line">      <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory</span></div><div class="line">      <span class="comment">// this is because while the delayed produce operation is being created, new</span></div><div class="line">      <span class="comment">// requests may arrive and hence make this operation completable.</span></div><div class="line">      delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</div><div class="line"></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// we can respond immediately</span></div><div class="line">      <span class="comment">//note: 通过回调函数直接返回结果</span></div><div class="line">      <span class="keyword">val</span> produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</div><div class="line">      responseCallback(produceResponseStatus)</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// If required.acks is outside accepted range, something is wrong with the client</span></div><div class="line">    <span class="comment">// Just return an error and don't handle the request at all</span></div><div class="line">    <span class="comment">//note: 返回 INVALID_REQUIRED_ACKS 错误</span></div><div class="line">    <span class="keyword">val</span> responseStatus = entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, _) =&gt;</div><div class="line">      topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">INVALID_REQUIRED_ACKS</span>,</div><div class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.firstOffset, <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span>)</div><div class="line">    &#125;</div><div class="line">    responseCallback(responseStatus)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的实现来看，<code>appendRecords()</code> 的实现主要分为以下几步：</p>
<ol>
<li>首先判断 acks 设置是否有效（-1，0，1三个值有效），无效的话直接返回异常，不再处理；</li>
<li>acks 设置有效的话，调用 <code>appendToLocalLog()</code> 方法将 records 追加到本地对应的 log 对象中；</li>
<li><code>appendToLocalLog()</code> 处理完后，如果发现 clients 设置的 acks=-1，即需要 isr 的其他的副本同步完成才能返回 response，那么就会创建一个 DelayedProduce 对象，等待 isr 的其他副本进行同步，否则的话直接返回追加的结果。</li>
</ol>
<h4 id="appendToLocalLog-的实现"><a href="#appendToLocalLog-的实现" class="headerlink" title="appendToLocalLog() 的实现"></a><code>appendToLocalLog()</code> 的实现</h4><p>追加日志的实际操作是在 <code>appendToLocalLog()</code>  中完成的，这里看下它的具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Append the messages to the local replica logs</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向本地的 replica 写入数据</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">appendToLocalLog</span></span>(internalTopicsAllowed: <span class="type">Boolean</span>,</div><div class="line">                             entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</div><div class="line">                             requiredAcks: <span class="type">Short</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">LogAppendResult</span>] = &#123;</div><div class="line">  trace(<span class="string">"Append [%s] to local log "</span>.format(entriesPerPartition))</div><div class="line">  entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, records) =&gt; <span class="comment">//note: 遍历要写的所有 topic-partition</span></div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).totalProduceRequestRate.mark()</div><div class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalProduceRequestRate.mark()</div><div class="line"></div><div class="line">    <span class="comment">// reject appending to internal topics if it is not allowed</span></div><div class="line">    <span class="comment">//note: 不能向 kafka 内部使用的 topic 追加数据</span></div><div class="line">    <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topicPartition.topic) &amp;&amp; !internalTopicsAllowed) &#123;</div><div class="line">      (topicPartition, <span class="type">LogAppendResult</span>(</div><div class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>,</div><div class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">InvalidTopicException</span>(<span class="string">s"Cannot append to internal topic <span class="subst">$&#123;topicPartition.topic&#125;</span>"</span>))))</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        <span class="comment">//note: 查找对应的 Partition,并向分区对应的副本写入数据文件</span></div><div class="line">        <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取 topic-partition 的 Partition 对象</span></div><div class="line">        <span class="keyword">val</span> info = partitionOpt <span class="keyword">match</span> &#123;</div><div class="line">          <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</div><div class="line">            partition.appendRecordsToLeader(records, requiredAcks) <span class="comment">//note: 如果找到了这个对象,就开始追加日志</span></div><div class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">"Partition %s doesn't exist on %d"</span></div><div class="line">            .format(topicPartition, localBrokerId)) <span class="comment">//note: 没有找到的话,返回异常</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 追加的 msg 数</span></div><div class="line">        <span class="keyword">val</span> numAppendedMessages =</div><div class="line">          <span class="keyword">if</span> (info.firstOffset == <span class="number">-1</span>L || info.lastOffset == <span class="number">-1</span>L)</div><div class="line">            <span class="number">0</span></div><div class="line">          <span class="keyword">else</span></div><div class="line">            info.lastOffset - info.firstOffset + <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment">// update stats for successfully appended bytes and messages as bytesInRate and messageInRate</span></div><div class="line">        <span class="comment">//note:  更新 metrics</span></div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesInRate.mark(records.sizeInBytes)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesInRate.mark(records.sizeInBytes)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).messagesInRate.mark(numAppendedMessages)</div><div class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.messagesInRate.mark(numAppendedMessages)</div><div class="line"></div><div class="line">        trace(<span class="string">"%d bytes written to log %s-%d beginning at offset %d and ending at offset %d"</span></div><div class="line">          .format(records.sizeInBytes, topicPartition.topic, topicPartition.partition, info.firstOffset, info.lastOffset))</div><div class="line">        (topicPartition, <span class="type">LogAppendResult</span>(info))</div><div class="line">      &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理追加过程中出现的异常</span></div><div class="line">        <span class="comment">// <span class="doctag">NOTE:</span> Failed produce requests metric is not incremented for known exceptions</span></div><div class="line">        <span class="comment">// it is supposed to indicate un-expected failures of a broker in handling a produce request</span></div><div class="line">        <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</div><div class="line">          fatal(<span class="string">"Halting due to unrecoverable I/O error while handling produce request: "</span>, e)</div><div class="line">          <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</div><div class="line">          (topicPartition, <span class="literal">null</span>)</div><div class="line">        <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</div><div class="line">                 _: <span class="type">NotLeaderForPartitionException</span> |</div><div class="line">                 _: <span class="type">RecordTooLargeException</span> |</div><div class="line">                 _: <span class="type">RecordBatchTooLargeException</span> |</div><div class="line">                 _: <span class="type">CorruptRecordException</span> |</div><div class="line">                 _: <span class="type">InvalidTimestampException</span>) =&gt;</div><div class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(e)))</div><div class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</div><div class="line">          <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).failedProduceRequestRate.mark()</div><div class="line">          <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.failedProduceRequestRate.mark()</div><div class="line">          error(<span class="string">"Error processing append operation on partition %s"</span>.format(topicPartition), t)</div><div class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(t)))</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看到 <code>appendToLocalLog()</code> 的实现如下：</p>
<ol>
<li>首先判断要写的 topic 是不是 Kafka 内置的 topic，内置的 topic 是不允许 Producer 写入的；</li>
<li>先查找 topic-partition 对应的 Partition 对象，如果在 <code>allPartitions</code> 中查找到了对应的 partition，那么直接调用 <code>partition.appendRecordsToLeader()</code> 方法追加相应的 records，否则会向 client 抛出异常。</li>
</ol>
<h3 id="Partition-appendRecordsToLeader-方法"><a href="#Partition-appendRecordsToLeader-方法" class="headerlink" title="Partition.appendRecordsToLeader() 方法"></a>Partition.appendRecordsToLeader() 方法</h3><p>ReplicaManager 在追加 records 时，调用的是 Partition 的 <code>appendRecordsToLeader()</code> 方法，其具体的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecordsToLeader</span></span>(records: <span class="type">MemoryRecords</span>, requiredAcks: <span class="type">Int</span> = <span class="number">0</span>) = &#123;</div><div class="line">  <span class="keyword">val</span> (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) &#123;</div><div class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</div><div class="line">        <span class="keyword">val</span> log = leaderReplica.log.get <span class="comment">//note: 获取对应的 Log 对象</span></div><div class="line">        <span class="keyword">val</span> minIsr = log.config.minInSyncReplicas</div><div class="line">        <span class="keyword">val</span> inSyncSize = inSyncReplicas.size</div><div class="line"></div><div class="line">        <span class="comment">// Avoid writing to leader if there are not enough insync replicas to make it safe</span></div><div class="line">        <span class="comment">//note: 如果 ack 设置为-1, isr 数小于设置的 min.isr 时,就会向 producer 抛出相应的异常</span></div><div class="line">        <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == <span class="number">-1</span>) &#123;</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotEnoughReplicasException</span>(<span class="string">"Number of insync replicas for partition %s is [%d], below required minimum [%d]"</span></div><div class="line">            .format(topicPartition, inSyncSize, minIsr))</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//note: 向副本对应的 log 追加响应的数据</span></div><div class="line">        <span class="keyword">val</span> info = log.append(records, assignOffsets = <span class="literal">true</span>)</div><div class="line">        <span class="comment">// probably unblock some follower fetch requests since log end offset has been updated</span></div><div class="line">        replicaManager.tryCompleteDelayedFetch(<span class="type">TopicPartitionOperationKey</span>(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</div><div class="line">        <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></div><div class="line">        <span class="comment">//note: 判断是否需要增加 HHW（追加日志后会进行一次判断）</span></div><div class="line">        (info, maybeIncrementLeaderHW(leaderReplica))</div><div class="line"></div><div class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</div><div class="line">        <span class="comment">//note: leader 不在本台机器上</span></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">"Leader not local for partition %s on broker %d"</span></div><div class="line">          .format(topicPartition, localBrokerId))</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></div><div class="line">  <span class="keyword">if</span> (leaderHWIncremented)</div><div class="line">    tryCompleteDelayedRequests()</div><div class="line"></div><div class="line">  info</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这个方法里，会根据 topic 的 <code>min.isrs</code> 配置以及当前这个 partition 的 isr 情况判断是否可以写入，如果不满足条件，就会抛出 <code>NotEnoughReplicasException</code> 的异常，如果满足条件，就会调用 <code>log.append()</code> 向 replica 追加日志。</p>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><p>跟着最开始图中的流程及代码分析，走到这里，才算是到了 Kafka 的存储层部分，在这里会详细讲述在存储层 Kafka 如何写入日志。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>在上面有过一些介绍，每个 replica 会对应一个 log 对象，log 对象是管理当前分区的一个单位，它会包含这个分区的所有 segment 文件（包括对应的 offset 索引和时间戳索引文件），它会提供一些增删查的方法。</p>
<p>在 Log 对象的初始化时，有三个变量是比较重要的：</p>
<ol>
<li><code>nextOffsetMetadata</code>：可以叫做下一个偏移量元数据，它包括 activeSegment 的下一条消息的偏移量，该 activeSegment 的基准偏移量及日志分段的大小；</li>
<li><code>activeSegment</code>：指的是该 Log 管理的 segments 中那个最新的 segment（这里叫做活跃的 segment），一个 Log 中只会有一个活跃的 segment，其他的 segment 都已经被持久化到磁盘了；</li>
<li><code>logEndOffset</code>：表示下一条消息的 offset，它取自 <code>nextOffsetMetadata</code> 的 offset，实际上就是活动日志分段的下一个偏移量。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: nextOffsetMetadata 声明为 volatile，如果该值被修改，其他使用此变量的线程就可以立刻见到变化后的值，在生产和消费都会使用到这个值</span></div><div class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> nextOffsetMetadata: <span class="type">LogOffsetMetadata</span> = _</div><div class="line"></div><div class="line"><span class="comment">/* Calculate the offset of the next message */</span></div><div class="line"><span class="comment">//note: 下一个偏移量元数据</span></div><div class="line"><span class="comment">//note: 第一个参数：下一条消息的偏移量；第二个参数：日志分段的基准偏移量；第三个参数：日志分段大小</span></div><div class="line">nextOffsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(activeSegment.nextOffset(), activeSegment.baseOffset, activeSegment.size.toInt)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">* The active segment that is currently taking appends</div><div class="line">*/</div><div class="line"><span class="comment">//note: 任何时刻，只会有一个活动的日志分段</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">activeSegment</span> </span>= segments.lastEntry.getValue</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line">*  The offset of the next message that will be appended to the log</div><div class="line">*/</div><div class="line"><span class="comment">//note: 下一条消息的 offset，从 nextOffsetMetadata 中获取的</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logEndOffset</span></span>: <span class="type">Long</span> = nextOffsetMetadata.messageOffset</div></pre></td></tr></table></figure>
<h4 id="日志写入"><a href="#日志写入" class="headerlink" title="日志写入"></a>日志写入</h4><p>在 Log 中一个重要的方法就是日志的写入方法，下面来看下这个方法的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Append this message set to the active segment of the log, rolling over to a fresh segment if necessary.</div><div class="line"> *</div><div class="line"> * This method will generally be responsible for assigning offsets to the messages,</div><div class="line"> * however if the assignOffsets=false flag is passed we will only check that the existing offsets are valid.</div><div class="line"> *</div><div class="line"> * @param records The log records to append</div><div class="line"> * @param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given</div><div class="line"> * @throws KafkaStorageException If the append fails due to an I/O error.</div><div class="line"> * @return Information about the appended messages including the first and last offset.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 向 active segment 追加 log,必要的情况下,滚动创建新的 segment</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</div><div class="line">  <span class="keyword">val</span> appendInfo = analyzeAndValidateRecords(records) <span class="comment">//note: 返回这批消息的该要信息,并对这批 msg 进行校验</span></div><div class="line"></div><div class="line">  <span class="comment">// if we have any valid messages, append them to the log</span></div><div class="line">  <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> appendInfo</div><div class="line"></div><div class="line">  <span class="comment">// trim any invalid bytes or partial messages before appending it to the on-disk log</span></div><div class="line">  <span class="comment">//note: 删除这批消息中无效的消息</span></div><div class="line">  <span class="keyword">var</span> validRecords = trimInvalidBytes(records, appendInfo)</div><div class="line"></div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// they are valid, insert them in the log</span></div><div class="line">    lock synchronized &#123;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (assignOffsets) &#123;</div><div class="line">        <span class="comment">// assign offsets to the message set</span></div><div class="line">        <span class="comment">//note: 计算这个消息集起始 offset，对 offset 的操作是一个原子操作</span></div><div class="line">        <span class="keyword">val</span> offset = <span class="keyword">new</span> <span class="type">LongRef</span>(nextOffsetMetadata.messageOffset)</div><div class="line">        appendInfo.firstOffset = offset.value <span class="comment">//note: 作为消息集的第一个 offset</span></div><div class="line">        <span class="keyword">val</span> now = time.milliseconds <span class="comment">//note: 设置的时间错以 server 收到的时间戳为准</span></div><div class="line">        <span class="comment">//note: 验证消息,并为没条 record 设置相应的 offset 和 timestrap</span></div><div class="line">        <span class="keyword">val</span> validateAndOffsetAssignResult = <span class="keyword">try</span> &#123;</div><div class="line">          <span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(validRecords,</div><div class="line">                                                        offset,</div><div class="line">                                                        now,</div><div class="line">                                                        appendInfo.sourceCodec,</div><div class="line">                                                        appendInfo.targetCodec,</div><div class="line">                                                        config.compact,</div><div class="line">                                                        config.messageFormatVersion.messageFormatVersion,</div><div class="line">                                                        config.messageTimestampType,</div><div class="line">                                                        config.messageTimestampDifferenceMaxMs)</div><div class="line">        &#125; <span class="keyword">catch</span> &#123;</div><div class="line">          <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Error in validating messages while appending to log '%s'"</span>.format(name), e)</div><div class="line">        &#125;</div><div class="line">        <span class="comment">//note: 返回已经计算好 offset 和 timestrap 的 MemoryRecords</span></div><div class="line">        validRecords = validateAndOffsetAssignResult.validatedRecords</div><div class="line">        appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</div><div class="line">        appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</div><div class="line">        appendInfo.lastOffset = offset.value - <span class="number">1</span> <span class="comment">//note: 最后一条消息的 offset</span></div><div class="line">        <span class="keyword">if</span> (config.messageTimestampType == <span class="type">TimestampType</span>.<span class="type">LOG_APPEND_TIME</span>)</div><div class="line">          appendInfo.logAppendTime = now</div><div class="line"></div><div class="line">        <span class="comment">// re-validate message sizes if there's a possibility that they have changed (due to re-compression or message</span></div><div class="line">        <span class="comment">// format conversion)</span></div><div class="line">        <span class="comment">//note: 更新 metrics 的记录</span></div><div class="line">        <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) &#123;</div><div class="line">          <span class="keyword">for</span> (logEntry &lt;- validRecords.shallowEntries.asScala) &#123;</div><div class="line">            <span class="keyword">if</span> (logEntry.sizeInBytes &gt; config.maxMessageSize) &#123;</div><div class="line">              <span class="comment">// we record the original message set size instead of the trimmed size</span></div><div class="line">              <span class="comment">// to be consistent with pre-compression bytesRejectedRate recording</span></div><div class="line">              <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</div><div class="line">              <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(<span class="string">"Message size is %d bytes which exceeds the maximum configured message size of %d."</span></div><div class="line">                .format(logEntry.sizeInBytes, config.maxMessageSize))</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// we are taking the offsets we are given</span></div><div class="line">        <span class="keyword">if</span> (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &lt; nextOffsetMetadata.messageOffset)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Out of order offsets found in "</span> + records.deepEntries.asScala.map(_.offset))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// check messages set size may be exceed config.segmentSize</span></div><div class="line">      <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordBatchTooLargeException</span>(<span class="string">"Message set size is %d bytes which exceeds the maximum configured segment size of %d."</span></div><div class="line">          .format(validRecords.sizeInBytes, config.segmentSize))</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="comment">// maybe roll the log if this segment is full</span></div><div class="line">      <span class="comment">//note: 如果当前 segment 满了，就需要重新新建一个 segment</span></div><div class="line">      <span class="keyword">val</span> segment = maybeRoll(messagesSize = validRecords.sizeInBytes,</div><div class="line">        maxTimestampInMessages = appendInfo.maxTimestamp,</div><div class="line">        maxOffsetInMessages = appendInfo.lastOffset)</div><div class="line"></div><div class="line"></div><div class="line">      <span class="comment">// now append to the log</span></div><div class="line">      <span class="comment">//note: 追加消息到当前 segment</span></div><div class="line">      segment.append(firstOffset = appendInfo.firstOffset,</div><div class="line">        largestOffset = appendInfo.lastOffset,</div><div class="line">        largestTimestamp = appendInfo.maxTimestamp,</div><div class="line">        shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</div><div class="line">        records = validRecords)</div><div class="line"></div><div class="line">      <span class="comment">// increment the log end offset</span></div><div class="line">      <span class="comment">//note: 修改最新的 next_offset</span></div><div class="line">      updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</div><div class="line"></div><div class="line">      trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></div><div class="line">        .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)<span class="comment">//note: 满足条件的话，刷新磁盘</span></div><div class="line">        flush()</div><div class="line"></div><div class="line">      appendInfo</div><div class="line">    &#125;</div><div class="line">  &#125; <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaStorageException</span>(<span class="string">"I/O exception in append to log '%s'"</span>.format(name), e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Server 将每个分区的消息追加到日志中时，是以 segment 为单位的，当 segment 的大小到达阈值大小之后，会滚动新建一个日志分段（segment）保存新的消息，而分区的消息总是追加到最新的日志分段（也就是 activeSegment）中。每个日志分段都会有一个基准偏移量（segmentBaseOffset，或者叫做 baseOffset），这个基准偏移量就是分区级别的绝对偏移量，而且这个值在日志分段是固定的。有了这个基准偏移量，就可以计算出来每条消息在分区中的绝对偏移量，最后把数据以及对应的绝对偏移量写到日志文件中。<code>append()</code> 方法的过程可以总结如下：</p>
<ol>
<li><code>analyzeAndValidateRecords()</code>：对这批要写入的消息进行检测，主要是检查消息的大小及 crc 校验；</li>
<li><code>trimInvalidBytes()</code>：会将这批消息中无效的消息删除，返回一个都是有效消息的 MemoryRecords；</li>
<li><code>LogValidator.validateMessagesAndAssignOffsets()</code>：为每条消息设置相应的 offset（绝对偏移量） 和 timestrap；</li>
<li><code>maybeRoll()</code>：判断是否需要新建一个 segment 的，如果当前的 segment 放不下这批消息的话，需要新建一个 segment；</li>
<li><code>segment.append()</code>：向 segment 中添加消息；</li>
<li>更新 logEndOffset 和判断是否需要刷新磁盘（如果需要的话，调用 <code>flush()</code> 方法刷到磁盘）。</li>
</ol>
<p>关于 timestrap 的设置，这里也顺便介绍一下，在新版的 Kafka 中，每条 msg 都会有一个对应的时间戳记录，producer 端可以设置这个字段 <code>message.timestamp.type</code> 来选择 timestrap 的类型，默认是按照创建时间，只能选择从下面的选择中二选一：</p>
<ol>
<li><code>CreateTime</code>，默认值；</li>
<li><code>LogAppendTime</code>。</li>
</ol>
<h4 id="日志分段"><a href="#日志分段" class="headerlink" title="日志分段"></a>日志分段</h4><p>在 Log 的 <code>append()</code> 方法中，会调用 <code>maybeRoll()</code> 方法来判断是否需要进行相应日志分段操作，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Roll the log over to a new empty log segment if necessary.</div><div class="line"> *</div><div class="line"> * @param messagesSize The messages set size in bytes</div><div class="line"> * @param maxTimestampInMessages The maximum timestamp in the messages.</div><div class="line"> * logSegment will be rolled if one of the following conditions met</div><div class="line"> * &lt;ol&gt;</div><div class="line"> * &lt;li&gt; The logSegment is full</div><div class="line"> * &lt;li&gt; The maxTime has elapsed since the timestamp of first message in the segment (or since the create time if</div><div class="line"> * the first message does not have a timestamp)</div><div class="line"> * &lt;li&gt; The index is full</div><div class="line"> * &lt;/ol&gt;</div><div class="line"> * @return The currently active segment after (perhaps) rolling to a new segment</div><div class="line"> */</div><div class="line"><span class="comment">//note: 判断是否需要创建日志分段,如果不需要返回当前分段,需要的话,返回新创建的日志分段</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeRoll</span></span>(messagesSize: <span class="type">Int</span>, maxTimestampInMessages: <span class="type">Long</span>, maxOffsetInMessages: <span class="type">Long</span>): <span class="type">LogSegment</span> = &#123;</div><div class="line">  <span class="keyword">val</span> segment = activeSegment <span class="comment">//note: 对活跃的日志分段进行判断,它也是最新的一个日志分段</span></div><div class="line">  <span class="keyword">val</span> now = time.milliseconds</div><div class="line">  <span class="comment">//note: 距离上次日志分段的时间是否达到了设置的阈值（log.roll.hours）</span></div><div class="line">  <span class="keyword">val</span> reachedRollMs = segment.timeWaitedForRoll(now, maxTimestampInMessages) &gt; config.segmentMs - segment.rollJitterMs</div><div class="line">  <span class="comment">//note: 这是五个条件: 1. 文件满了,不足以放心这么大的 messageSet; 2. 文件有数据,并且到分段的时间阈值; 3. 索引文件满了;</span></div><div class="line">  <span class="comment">//note: 4. 时间索引文件满了; 5. 最大的 offset，其相对偏移量超过了正整数的阈值</span></div><div class="line">  <span class="keyword">if</span> (segment.size &gt; config.segmentSize - messagesSize ||</div><div class="line">      (segment.size &gt; <span class="number">0</span> &amp;&amp; reachedRollMs) ||</div><div class="line">      segment.index.isFull || segment.timeIndex.isFull || !segment.canConvertToRelativeOffset(maxOffsetInMessages)) &#123;</div><div class="line">    debug(<span class="string">s"Rolling new log segment in <span class="subst">$name</span> (log_size = <span class="subst">$&#123;segment.size&#125;</span>/<span class="subst">$&#123;config.segmentSize&#125;</span>&#125;, "</span> +</div><div class="line">        <span class="string">s"index_size = <span class="subst">$&#123;segment.index.entries&#125;</span>/<span class="subst">$&#123;segment.index.maxEntries&#125;</span>, "</span> +</div><div class="line">        <span class="string">s"time_index_size = <span class="subst">$&#123;segment.timeIndex.entries&#125;</span>/<span class="subst">$&#123;segment.timeIndex.maxEntries&#125;</span>, "</span> +</div><div class="line">        <span class="string">s"inactive_time_ms = <span class="subst">$&#123;segment.timeWaitedForRoll(now, maxTimestampInMessages)&#125;</span>/<span class="subst">$&#123;config.segmentMs - segment.rollJitterMs&#125;</span>)."</span>)</div><div class="line">    roll(maxOffsetInMessages - <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>) <span class="comment">//note: 创建新的日志分段</span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    segment <span class="comment">//note: 使用当前的日志分段</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从 <code>maybeRoll()</code> 的实现可以看到，是否需要创建新的日志分段，有下面几种情况：</p>
<ol>
<li>当前日志分段的大小加上消息的大小超过了日志分段的阈值（<code>log.segment.bytes</code>）；</li>
<li>距离上次创建日志分段的时间达到了一定的阈值（<code>log.roll.hours</code>），并且数据文件有数据；</li>
<li>索引文件满了；</li>
<li>时间索引文件满了；</li>
<li>最大的 offset，其相对偏移量超过了正整数的阈值。</li>
</ol>
<p>如果上面的其中一个条件，就会创建新的 segment 文件，见 <code>roll()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Roll the log over to a new active segment starting with the current logEndOffset.</div><div class="line"> * This will trim the index to the exact size of the number of entries it currently contains.</div><div class="line"> *</div><div class="line"> * @return The newly rolled segment</div><div class="line"> */</div><div class="line"><span class="comment">//note: 滚动创建日志,并添加到日志管理的映射表中</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">roll</span></span>(expectedNextOffset: <span class="type">Long</span> = <span class="number">0</span>): <span class="type">LogSegment</span> = &#123;</div><div class="line">  <span class="keyword">val</span> start = time.nanoseconds</div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> newOffset = <span class="type">Math</span>.max(expectedNextOffset, logEndOffset) <span class="comment">//note: 选择最新的 offset 作为基准偏移量</span></div><div class="line">    <span class="keyword">val</span> logFile = logFilename(dir, newOffset) <span class="comment">//note: 创建数据文件</span></div><div class="line">    <span class="keyword">val</span> indexFile = indexFilename(dir, newOffset) <span class="comment">//note: 创建 offset 索引文件</span></div><div class="line">    <span class="keyword">val</span> timeIndexFile = timeIndexFilename(dir, newOffset) <span class="comment">//note: 创建 time 索引文件</span></div><div class="line">    <span class="keyword">for</span>(file &lt;- <span class="type">List</span>(logFile, indexFile, timeIndexFile); <span class="keyword">if</span> file.exists) &#123;</div><div class="line">      warn(<span class="string">"Newly rolled segment file "</span> + file.getName + <span class="string">" already exists; deleting it first"</span>)</div><div class="line">      file.delete()</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    segments.lastEntry() <span class="keyword">match</span> &#123;</div><div class="line">      <span class="keyword">case</span> <span class="literal">null</span> =&gt;</div><div class="line">      <span class="keyword">case</span> entry =&gt; &#123;</div><div class="line">        <span class="keyword">val</span> seg = entry.getValue</div><div class="line">        seg.onBecomeInactiveSegment()</div><div class="line">        seg.index.trimToValidSize()</div><div class="line">        seg.timeIndex.trimToValidSize()</div><div class="line">        seg.log.trim()</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">//note: 创建一个 segment 对象</span></div><div class="line">    <span class="keyword">val</span> segment = <span class="keyword">new</span> <span class="type">LogSegment</span>(dir,</div><div class="line">                                 startOffset = newOffset,</div><div class="line">                                 indexIntervalBytes = config.indexInterval,</div><div class="line">                                 maxIndexSize = config.maxIndexSize,</div><div class="line">                                 rollJitterMs = config.randomSegmentJitter,</div><div class="line">                                 time = time,</div><div class="line">                                 fileAlreadyExists = <span class="literal">false</span>,</div><div class="line">                                 initFileSize = initFileSize,</div><div class="line">                                 preallocate = config.preallocate)</div><div class="line">    <span class="keyword">val</span> prev = addSegment(segment) <span class="comment">//note: 添加到日志管理中</span></div><div class="line">    <span class="keyword">if</span>(prev != <span class="literal">null</span>)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Trying to roll a new log segment for topic partition %s with start offset %d while it already exists."</span>.format(name, newOffset))</div><div class="line">    <span class="comment">// We need to update the segment base offset and append position data of the metadata when log rolls.</span></div><div class="line">    <span class="comment">// The next offset should not change.</span></div><div class="line">    updateLogEndOffset(nextOffsetMetadata.messageOffset) <span class="comment">//note: 更新 offset</span></div><div class="line">    <span class="comment">// schedule an asynchronous flush of the old segment</span></div><div class="line">    scheduler.schedule(<span class="string">"flush-log"</span>, () =&gt; flush(newOffset), delay = <span class="number">0</span>L)</div><div class="line"></div><div class="line">    info(<span class="string">"Rolled new log segment for '"</span> + name + <span class="string">"' in %.0f ms."</span>.format((<span class="type">System</span>.nanoTime - start) / (<span class="number">1000.0</span>*<span class="number">1000.0</span>)))</div><div class="line"></div><div class="line">    segment</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>创建一个 segment 对象，真正的实现是在 Log 的 <code>roll()</code> 方法中，也就是上面的方法中，创建 segment 对象，主要包括三部分：数据文件、offset 索引文件和 time 索引文件。</p>
<h4 id="offset-索引文件"><a href="#offset-索引文件" class="headerlink" title="offset 索引文件"></a>offset 索引文件</h4><p>这里顺便讲述一下 offset 索引文件，Kafka 的索引文件有下面一个特点：</p>
<ol>
<li>采用 <strong>绝对偏移量+相对偏移量</strong> 的方式进行存储的，每个 segment 最开始绝对偏移量也是其基准偏移量；</li>
<li>数据文件每隔一定的大小创建一个索引条目，而不是每条消息会创建索引条目，通过 <code>index.interval.bytes</code> 来配置，默认是 4096，也就是4KB；</li>
</ol>
<p>这样做的好处也非常明显：</p>
<ol>
<li>因为不是每条消息都创建相应的索引条目，所以索引条目是稀疏的；</li>
<li>索引的相对偏移量占据4个字节，而绝对偏移量占据8个字节，加上物理位置的4个字节，使用相对索引可以将每条索引条目的大小从12字节减少到8个字节；</li>
<li>因为偏移量有序的，再读取数据时，可以按照二分查找的方式去快速定位偏移量的位置；</li>
<li>这样的稀疏索引是可以完全放到内存中，加快偏移量的查找。</li>
</ol>
<h3 id="LogSegment-写入"><a href="#LogSegment-写入" class="headerlink" title="LogSegment 写入"></a>LogSegment 写入</h3><p>真正的日志写入，还是在 LogSegment 的 <code>append()</code> 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line"> * Append the given messages starting with the given offset. Add</div><div class="line"> * an entry to the index if needed.</div><div class="line"> *</div><div class="line"> * It is assumed this method is being called from within a lock.</div><div class="line"> *</div><div class="line"> * @param firstOffset The first offset in the message set.</div><div class="line"> * @param largestTimestamp The largest timestamp in the message set.</div><div class="line"> * @param shallowOffsetOfMaxTimestamp The offset of the message that has the largest timestamp in the messages to append.</div><div class="line"> * @param records The log entries to append.</div><div class="line"> */</div><div class="line"> <span class="comment">//note: 在指定的 offset 处追加指定的 msgs, 需要的情况下追加相应的索引</span></div><div class="line"><span class="meta">@nonthreadsafe</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(firstOffset: <span class="type">Long</span>, largestOffset: <span class="type">Long</span>, largestTimestamp: <span class="type">Long</span>, shallowOffsetOfMaxTimestamp: <span class="type">Long</span>, records: <span class="type">MemoryRecords</span>) &#123;</div><div class="line">  <span class="keyword">if</span> (records.sizeInBytes &gt; <span class="number">0</span>) &#123;</div><div class="line">    trace(<span class="string">"Inserting %d bytes at offset %d at position %d with largest timestamp %d at shallow offset %d"</span></div><div class="line">        .format(records.sizeInBytes, firstOffset, log.sizeInBytes(), largestTimestamp, shallowOffsetOfMaxTimestamp))</div><div class="line">    <span class="keyword">val</span> physicalPosition = log.sizeInBytes()</div><div class="line">    <span class="keyword">if</span> (physicalPosition == <span class="number">0</span>)</div><div class="line">      rollingBasedTimestamp = <span class="type">Some</span>(largestTimestamp)</div><div class="line">    <span class="comment">// append the messages</span></div><div class="line">    require(canConvertToRelativeOffset(largestOffset), <span class="string">"largest offset in message set can not be safely converted to relative offset."</span>)</div><div class="line">    <span class="keyword">val</span> appendedBytes = log.append(records) <span class="comment">//note: 追加到数据文件中</span></div><div class="line">    trace(<span class="string">s"Appended <span class="subst">$appendedBytes</span> to <span class="subst">$&#123;log.file()&#125;</span> at offset <span class="subst">$firstOffset</span>"</span>)</div><div class="line">    <span class="comment">// Update the in memory max timestamp and corresponding offset.</span></div><div class="line">    <span class="keyword">if</span> (largestTimestamp &gt; maxTimestampSoFar) &#123;</div><div class="line">      maxTimestampSoFar = largestTimestamp</div><div class="line">      offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// append an entry to the index (if needed)</span></div><div class="line">    <span class="comment">//note: 判断是否需要追加索引（数据每次都会添加到数据文件中,但不是每次都会添加索引的,间隔 indexIntervalBytes 大小才会写入一个索引文件）</span></div><div class="line">    <span class="keyword">if</span>(bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123;</div><div class="line">      index.append(firstOffset, physicalPosition) <span class="comment">//note: 添加索引</span></div><div class="line">      timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</div><div class="line">      bytesSinceLastIndexEntry = <span class="number">0</span> <span class="comment">//note: 重置为0</span></div><div class="line">    &#125;</div><div class="line">    bytesSinceLastIndexEntry += records.sizeInBytes</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>经过上面的分析，一个消息集（MemoryRecords）在 Kafka 存储层的调用情况如下图所示：</p>
<p><img src="/images/kafka/log_append.png" alt="MemoryRecords 追加过程"></p>
<p>最后还是利用底层的 Java NIO 实现。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer 
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 源码解析之日志管理（十一）</title>
    <link href="http://matt33.com/2018/03/12/kafka-log-manager/"/>
    <id>http://matt33.com/2018/03/12/kafka-log-manager/</id>
    <published>2018-03-11T16:48:13.000Z</published>
    <updated>2018-03-11T16:26:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是比较多的，会分为三篇左右的文章介绍，本篇先介绍最简单的部分，主要是日志的基本概念、日志管理、日志刷新和日志清理四部分（后两个其实也属于日志管理，为便于讲解，这里分开讲述），日志的读写和分段将在下一篇讲述。</p>
<p>本篇主要的内容如下：</p>
<ol>
<li>Kafka 中 Log 的基本概念；</li>
<li>日志管理；</li>
<li>日志刷新；</li>
<li>日志清理；</li>
</ol>
<h2 id="日志的基本概念"><a href="#日志的基本概念" class="headerlink" title="日志的基本概念"></a>日志的基本概念</h2><p>在 Kafka 的官方文档中，最开始介绍 Kafka 的一句话是：</p>
<blockquote>
<p>Kafka is a distributed, partitioned, replicated commit log service. （0.10.0 之前）</p>
<p>Apache Kafka is a distributed streaming platform. （0.10.0 及之后）</p>
</blockquote>
<p>可以说在 KafkaStream 之前，Kafka 最开始的应用场景就是日志场景或 mq 场景，更多的扮演着一个存储系统，这是 Kafka 立家之本。</p>
<p>Kafka 是一个分布式的（distributed）、可分区的（partitioned）、支持多副本（replicated）的日志提交系统，分布式这个概念很好理解，Kafka 本身就是一个分布式系统，那另外两个概念什么意思呢？</p>
<ul>
<li>可分区的：一个 topic 是可以设置多个分区的，可分区解决了单 topic 线性扩展的问题（也解决了负载均衡的问题）；</li>
<li>支持多副本的：使得 topic 可以做到更多容错性，牺牲性能与空间去换取更高的可靠性。</li>
</ul>
<p>一个 Topic 基本结果如下：</p>
<p><img src="/images/2016-03-07-KafkaMessage/topic.png" alt="Topic"></p>
<p>图中的 topic 由三个 partition 组成，topic 在创建开始，每个 partition 在写入时，其 offset 值是从0开始逐渐增加。topic 的 partition 是可以分配到 Kafka 集群的任何节点上，在实际存储时，每个 partition 是按 segment 文件去存储的（segment 的大小是在 server 端配置的，这就是日志的分段），如下图所示：</p>
<p><img src="/images/2016-03-07-KafkaMessage/segment.png" alt="Segment"></p>
<blockquote>
<p>注：上图是 0.8.2.1 版的 segment 的结构，0.10.2.0 版每个 segment 还会有一个对应的 timestrap 文件。</p>
</blockquote>
<p>再简单介绍一下 topic 的副本的概念，kafka 中为了保证一定可靠性，一般会为设置多个副本，假设一个 topic 设置了三个副本：</p>
<ul>
<li>每个 partition 都会有三个副本，这个三个副本需要分配在不同的 broker 上，在同一台 broker 上的话，就没有什么意义了；</li>
<li>这个三个副本中，会有选举出来了一个 leader，另外两个就是 follower，topic 的读写都是在 leader 上进行的，follower 从 leader 同步 partition 的数据。</li>
</ul>
<blockquote>
<p>follower 不支持读的原因，个人感觉是对于流式系统而言，如果允许 follower 也可以读的话，数据一致性、可见性将会很难保证，对最初 Kafka 的设计将会带来很大的复杂性。</p>
</blockquote>
<p>有了对 topic、partition、副本（replica）、segment、leader、follower 概念的理解之后，下面再看 Kafka 存储层的内容，就不会那么云里雾里了。 </p>
<h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><p>Kafka 的日志管理（LogManager）主要的作用是负责日志的创建、检索、清理，日志相关的读写操作实际上是由日志实例对象（Log）来处理的。</p>
<h3 id="KafkaServer-启动-LogManager-线程"><a href="#KafkaServer-启动-LogManager-线程" class="headerlink" title="KafkaServer 启动 LogManager 线程"></a>KafkaServer 启动 LogManager 线程</h3><p>LogManager 线程是在节点的 Kafka 服务启动时启动的，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//kafka.server.KafkaServer</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="keyword">try</span> &#123;</div><div class="line">    info(<span class="string">"starting"</span>)</div><div class="line">    <span class="comment">/* start log manager */</span></div><div class="line">    <span class="comment">//note: 启动日志管理线程</span></div><div class="line">    logManager = createLogManager(zkUtils.zkClient, brokerState)</div><div class="line">    logManager.startup()</div><div class="line">    &#125;</div><div class="line">  <span class="keyword">catch</span> &#123;</div><div class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</div><div class="line">    isStartingUp.set(<span class="literal">false</span>)</div><div class="line">    shutdown()</div><div class="line">    <span class="keyword">throw</span> e</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createLogManager</span></span>(zkClient: <span class="type">ZkClient</span>, brokerState: <span class="type">BrokerState</span>): <span class="type">LogManager</span> = &#123;</div><div class="line">  <span class="keyword">val</span> defaultProps = <span class="type">KafkaServer</span>.copyKafkaConfigToLog(config)</div><div class="line">  <span class="keyword">val</span> defaultLogConfig = <span class="type">LogConfig</span>(defaultProps)</div><div class="line"></div><div class="line">  <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchAllTopicConfigs(zkUtils).map &#123; <span class="keyword">case</span> (topic, configs) =&gt;</div><div class="line">    topic -&gt; <span class="type">LogConfig</span>.fromProps(defaultProps, configs)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// read the log configurations from zookeeper</span></div><div class="line">  <span class="keyword">val</span> cleanerConfig = <span class="type">CleanerConfig</span>(numThreads = config.logCleanerThreads, <span class="comment">//note: 日志清理线程数,默认是1</span></div><div class="line">                                    dedupeBufferSize = config.logCleanerDedupeBufferSize, <span class="comment">//note: 日志清理使用的总内容,默认128MB</span></div><div class="line">                                    dedupeBufferLoadFactor = config.logCleanerDedupeBufferLoadFactor, <span class="comment">//note:  buffer load factor</span></div><div class="line">                                    ioBufferSize = config.logCleanerIoBufferSize, <span class="comment">//note:</span></div><div class="line">                                    maxMessageSize = config.messageMaxBytes, <span class="comment">//note:</span></div><div class="line">                                    maxIoBytesPerSecond = config.logCleanerIoMaxBytesPerSecond, <span class="comment">//note:</span></div><div class="line">                                    backOffMs = config.logCleanerBackoffMs, <span class="comment">//note: 没有日志清理时的 sleep 时间,默认 15s</span></div><div class="line">                                    enableCleaner = config.logCleanerEnable) <span class="comment">//note: 是否允许对 compact 日志进行清理</span></div><div class="line">  <span class="keyword">new</span> <span class="type">LogManager</span>(logDirs = config.logDirs.map(<span class="keyword">new</span> <span class="type">File</span>(_)).toArray, <span class="comment">//note: 日志目录列表</span></div><div class="line">                 topicConfigs = configs,</div><div class="line">                 defaultConfig = defaultLogConfig,</div><div class="line">                 cleanerConfig = cleanerConfig,</div><div class="line">                 ioThreads = config.numRecoveryThreadsPerDataDir,<span class="comment">//note: 每个日志目录在开始时用日志恢复以及关闭时日志flush的线程数,默认1</span></div><div class="line">                 flushCheckMs = config.logFlushSchedulerIntervalMs,</div><div class="line">                 flushCheckpointMs = config.logFlushOffsetCheckpointIntervalMs, <span class="comment">//note: 更新 check-point 的频率,默认是60s</span></div><div class="line">                 retentionCheckMs = config.logCleanupIntervalMs, <span class="comment">//note: log-cleaner 检查 topic 是否需要删除的频率,默认是5min</span></div><div class="line">                 scheduler = kafkaScheduler,</div><div class="line">                 brokerState = brokerState,</div><div class="line">                 time = time)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="LogManager-初始化"><a href="#LogManager-初始化" class="headerlink" title="LogManager 初始化"></a>LogManager 初始化</h3><p>LogManager 在初始化时，首先会检查 server 端配置的日志目录信息，然后会加载日志目录下的所有分区日志，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogManager</span>(<span class="params"></span>)</span>&#123;</div><div class="line">  <span class="comment">//note: 检查点表示日志已经刷新到磁盘的位置，主要是用于数据恢复</span></div><div class="line">  <span class="keyword">val</span> <span class="type">RecoveryPointCheckpointFile</span> = <span class="string">"recovery-point-offset-checkpoint"</span> <span class="comment">//note: 检查点文件</span></div><div class="line">  </div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logs = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">TopicPartition</span>, <span class="type">Log</span>]() <span class="comment">//note: 分区与日志实例的对应关系</span></div><div class="line"></div><div class="line">  createAndValidateLogDirs(logDirs) <span class="comment">//note: 检查日志目录</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dirLocks = lockLogDirs(logDirs)</div><div class="line">  <span class="comment">//note: 每个数据目录都有一个检查点文件,存储这个数据目录下所有分区的检查点信息</span></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> recoveryPointCheckpoints = logDirs.map(dir =&gt; (dir, <span class="keyword">new</span> <span class="type">OffsetCheckpoint</span>(<span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">RecoveryPointCheckpointFile</span>)))).toMap</div><div class="line">  loadLogs()</div><div class="line">  </div><div class="line">  <span class="comment">//note: 创建指定的数据目录,并做相应的检查:</span></div><div class="line">  <span class="comment">//note: 1.确保数据目录中没有重复的数据目录;</span></div><div class="line">  <span class="comment">//note: 2.数据不存在的话就创建相应的目录;</span></div><div class="line">  <span class="comment">//note: 3.检查每个目录路径是否是可读的。</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAndValidateLogDirs</span></span>(dirs: <span class="type">Seq</span>[<span class="type">File</span>]) &#123;</div><div class="line">    <span class="keyword">if</span>(dirs.map(_.getCanonicalPath).toSet.size &lt; dirs.size)</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Duplicate log directory found: "</span> + logDirs.mkString(<span class="string">", "</span>))</div><div class="line">    <span class="keyword">for</span>(dir &lt;- dirs) &#123;</div><div class="line">      <span class="keyword">if</span>(!dir.exists) &#123;</div><div class="line">        info(<span class="string">"Log directory '"</span> + dir.getAbsolutePath + <span class="string">"' not found, creating it."</span>)</div><div class="line">        <span class="keyword">val</span> created = dir.mkdirs()</div><div class="line">        <span class="keyword">if</span>(!created)</div><div class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Failed to create data directory "</span> + dir.getAbsolutePath)</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span>(!dir.isDirectory || !dir.canRead)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(dir.getAbsolutePath + <span class="string">" is not a readable log directory."</span>)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">//note: 加载所有的日志,而每个日志也会调用 loadSegments() 方法加载所有的分段,过程比较慢,所有每个日志都会创建一个单独的线程</span></div><div class="line">  <span class="comment">//note: 日志管理器采用线程池提交任务,标识不用的任务可以同时运行</span></div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">loadLogs</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    info(<span class="string">"Loading logs."</span>)</div><div class="line">    <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">    <span class="keyword">val</span> threadPools = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">ExecutorService</span>]</div><div class="line">    <span class="keyword">val</span> jobs = mutable.<span class="type">Map</span>.empty[<span class="type">File</span>, <span class="type">Seq</span>[<span class="type">Future</span>[_]]]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> (dir &lt;- <span class="keyword">this</span>.logDirs) &#123; <span class="comment">//note: 处理每一个日志目录</span></div><div class="line">      <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(ioThreads) <span class="comment">//note: 默认为 1</span></div><div class="line">      threadPools.append(pool) <span class="comment">//note: 每个对应的数据目录都有一个线程池</span></div><div class="line"></div><div class="line">      <span class="keyword">val</span> cleanShutdownFile = <span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">Log</span>.<span class="type">CleanShutdownFile</span>)</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (cleanShutdownFile.exists) &#123;</div><div class="line">        debug(</div><div class="line">          <span class="string">"Found clean shutdown file. "</span> +</div><div class="line">          <span class="string">"Skipping recovery for all logs in data directory: "</span> +</div><div class="line">          dir.getAbsolutePath)</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// log recovery itself is being performed by `Log` class during initialization</span></div><div class="line">        brokerState.newState(<span class="type">RecoveringFromUncleanShutdown</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">var</span> recoveryPoints = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read <span class="comment">//note: 读取检查点文件</span></div><div class="line">      &#125; <span class="keyword">catch</span> &#123;</div><div class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</div><div class="line">          warn(<span class="string">"Error occured while reading recovery-point-offset-checkpoint file of directory "</span> + dir, e)</div><div class="line">          warn(<span class="string">"Resetting the recovery checkpoint to 0"</span>)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">val</span> jobsForDir = <span class="keyword">for</span> &#123;</div><div class="line">        dirContent &lt;- <span class="type">Option</span>(dir.listFiles).toList <span class="comment">//note: 数据目录下的所有日志目录</span></div><div class="line">        logDir &lt;- dirContent <span class="keyword">if</span> logDir.isDirectory <span class="comment">//note: 日志目录下每个分区目录</span></div><div class="line">      &#125; <span class="keyword">yield</span> &#123;</div><div class="line">        <span class="type">CoreUtils</span>.runnable &#123; <span class="comment">//note: 每个分区的目录都对应了一个线程</span></div><div class="line">          debug(<span class="string">"Loading log '"</span> + logDir.getName + <span class="string">"'"</span>)</div><div class="line"></div><div class="line">          <span class="keyword">val</span> topicPartition = <span class="type">Log</span>.parseTopicPartitionName(logDir)</div><div class="line">          <span class="keyword">val</span> config = topicConfigs.getOrElse(topicPartition.topic, defaultConfig)</div><div class="line">          <span class="keyword">val</span> logRecoveryPoint = recoveryPoints.getOrElse(topicPartition, <span class="number">0</span>L)</div><div class="line"></div><div class="line">          <span class="keyword">val</span> current = <span class="keyword">new</span> <span class="type">Log</span>(logDir, config, logRecoveryPoint, scheduler, time)<span class="comment">//note: 创建 Log 对象后，初始化时会加载所有的 segment</span></div><div class="line">          <span class="keyword">if</span> (logDir.getName.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) &#123; <span class="comment">//note: 该目录被标记为删除</span></div><div class="line">            <span class="keyword">this</span>.logsToBeDeleted.add(current)</div><div class="line">          &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">val</span> previous = <span class="keyword">this</span>.logs.put(topicPartition, current) <span class="comment">//note: 创建日志后,加入日志管理的映射表</span></div><div class="line">            <span class="keyword">if</span> (previous != <span class="literal">null</span>) &#123;</div><div class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</div><div class="line">                <span class="string">"Duplicate log directories found: %s, %s!"</span>.format(</div><div class="line">                  current.dir.getAbsolutePath, previous.dir.getAbsolutePath))</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      jobs(cleanShutdownFile) = jobsForDir.map(pool.submit).toSeq <span class="comment">//note: 提交任务</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="keyword">for</span> ((cleanShutdownFile, dirJobs) &lt;- jobs) &#123;</div><div class="line">        dirJobs.foreach(_.get)</div><div class="line">        cleanShutdownFile.delete()</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">ExecutionException</span> =&gt; &#123;</div><div class="line">        error(<span class="string">"There was an error in one of the threads during logs loading: "</span> + e.getCause)</div><div class="line">        <span class="keyword">throw</span> e.getCause</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      threadPools.foreach(_.shutdown())</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    info(<span class="string">s"Logs loading complete in <span class="subst">$&#123;time.milliseconds - startMs&#125;</span> ms."</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>初始化 LogManger 代码有两个主要方法：</p>
<ol>
<li><code>createAndValidateLogDirs()</code>：创建指定的数据目录，并做相应的检查： 1.确保数据目录中没有重复的数据目录、2.数据目录不存在的话就创建相应的目录；3. 检查每个目录路径是否是可读的；</li>
<li><code>loadLogs()</code>：加载所有的日志分区，而每个日志也会调用 <code>loadSegments()</code> 方法加载该分区所有的 segment 文件，过程比较慢，所以 LogManager 使用线程池的方式，为每个日志的加载都会创建一个单独的线程。</li>
</ol>
<p>虽然使用的是线程池提交任务，并发进行 load 分区日志，但这个任务本身是阻塞式的，只有当所有的分区日志加载完成，才能调用 <code>startup()</code> 启动 LogManager 线程。</p>
<h3 id="LogManager-启动"><a href="#LogManager-启动" class="headerlink" title="LogManager 启动"></a>LogManager 启动</h3><p>在日志目录的所有分区日志都加载完成后，KafkaServer 调用 <code>startup()</code> 方法启动 LogManager 线程，LogManager 启动后，后台会运行四个定时任务，代码实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</div><div class="line">  <span class="comment">/* Schedule the cleanup task to delete old logs */</span></div><div class="line">  <span class="keyword">if</span>(scheduler != <span class="literal">null</span>) &#123;</div><div class="line">    <span class="comment">//note: 定时清理过期的日志 segment,并维护日志的大小</span></div><div class="line">    info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</div><div class="line">    scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</div><div class="line">                       cleanupLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = retentionCheckMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时刷新还没有写到磁盘上日志</span></div><div class="line">    info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</div><div class="line">    scheduler.schedule(<span class="string">"kafka-log-flusher"</span>,</div><div class="line">                       flushDirtyLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = flushCheckMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时将所有数据目录所有日志的检查点写到检查点文件中</span></div><div class="line">    scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</div><div class="line">                       checkpointRecoveryPointOffsets,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = flushCheckpointMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">    <span class="comment">//note: 定时删除标记为 delete 的日志文件</span></div><div class="line">    scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</div><div class="line">                       deleteLogs,</div><div class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</div><div class="line">                       period = defaultConfig.fileDeleteDelayMs,</div><div class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//note: 如果设置为 true， 自动清理 compaction 类型的 topic</span></div><div class="line">  <span class="keyword">if</span>(cleanerConfig.enableCleaner)</div><div class="line">    cleaner.startup()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>四个后台定时线程的作用：</p>
<ol>
<li><code>cleanupLogs</code>：定时清理过期的日志 segment，并维护日志的大小（默认5min）；</li>
<li><code>flushDirtyLogs</code>：定时刷新将还没有写到磁盘上日志刷新到磁盘（默认 无限大）；</li>
<li><code>checkpointRecoveryPointOffsets</code>：定时将所有数据目录所有日志的检查点写到检查点文件中（默认 60s）；</li>
<li><code>deleteLogs</code>：定时删除标记为 delete 的日志文件（默认 30s）。</li>
</ol>
<h3 id="检查点文件"><a href="#检查点文件" class="headerlink" title="检查点文件"></a>检查点文件</h3><p>在 LogManager 中有一个非常重要的文件——检查点文件：</p>
<ol>
<li>Kafka 启动时创建 LogManager，读取检查点文件，并把每个分区对应的检查点（checkPoint）作为日志的恢复点（recoveryPoint），最后创建分区对应的日志实例；</li>
<li>消息追加到分区对应的日志，在刷新日志时，将最新的偏移量作为日志的检查点（也即是刷新日志时，会更新检查点位置）；</li>
<li>LogManager 会启动一个定时任务，读取所有日志的检查点，并写入全局的检查点文件（定时将检查点的位置更新到检查点文件中）。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note：通常所有数据目录都会一起执行，不会专门操作某一个数据目录的检查点文件</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpointRecoveryPointOffsets</span></span>() &#123;</div><div class="line">  <span class="keyword">this</span>.logDirs.foreach(checkpointLogsInDir)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Make a checkpoint for all logs in provided directory.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 对数据目录下的所有日志（即所有分区），将其检查点写入检查点文件</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkpointLogsInDir</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">val</span> recoveryPoints = <span class="keyword">this</span>.logsByDir.get(dir.toString)</div><div class="line">  <span class="keyword">if</span> (recoveryPoints.isDefined) &#123;</div><div class="line">    <span class="keyword">this</span>.recoveryPointCheckpoints(dir).write(recoveryPoints.get.mapValues(_.recoveryPoint))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>这里留一个问题：启动时，如果发现检查点文件的 offset 比 segment 中最大的 offset 小时（最新的检查点在更新到文件前机器宕机了），应该怎么处理？答案将在下一篇文章中讲述。</p>
</blockquote>
<h2 id="日志刷新"><a href="#日志刷新" class="headerlink" title="日志刷新"></a>日志刷新</h2><p>日志管理器会定时调度 <code>flushDirtyLogs()</code> 方法，定期将页面缓存中的数据真正刷新到磁盘文件中。如果缓存中的数据（在 pagecache 中）在 flush 到磁盘之前，Broker 宕机了，那么会导致数据丢失（多副本减少了这个风险）。</p>
<p>在 Kafka 中有两种策略，将日志刷新到磁盘上：</p>
<ul>
<li>时间策略，（<code>log.flush.interval.ms</code> 中配置调度周期，默认为无限大，即选择大小策略）：</li>
<li>大小策略，（<code>log.flush.interval.messages</code> 中配置当未刷新的 msg 数超过这个值后，进行刷新）。</li>
</ul>
<p>LogManager 刷新日志的实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: LogManager 启动时，会启动一个周期性调度任务，调度这个方法，定时刷新日志。</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">flushDirtyLogs</span></span>() = &#123;</div><div class="line">  debug(<span class="string">"Checking for dirty logs to flush..."</span>)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> ((topicPartition, log) &lt;- logs) &#123;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">//note: 每个日志的刷新时间并不相同</span></div><div class="line">      <span class="keyword">val</span> timeSinceLastFlush = time.milliseconds - log.lastFlushTime</div><div class="line">      debug(<span class="string">"Checking if flush is needed on "</span> + topicPartition.topic + <span class="string">" flush interval  "</span> + log.config.flushMs +</div><div class="line">            <span class="string">" last flushed "</span> + log.lastFlushTime + <span class="string">" time since last flush: "</span> + timeSinceLastFlush)</div><div class="line">      <span class="keyword">if</span>(timeSinceLastFlush &gt;= log.config.flushMs)</div><div class="line">        log.flush</div><div class="line">    &#125; <span class="keyword">catch</span> &#123;</div><div class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</div><div class="line">        error(<span class="string">"Error flushing topic "</span> + topicPartition.topic, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>LogManager 这个方法最后的结果还是调用了 <code>log.flush()</code> 进行刷新操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Flush all log segments</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(): <span class="type">Unit</span> = flush(<span class="keyword">this</span>.logEndOffset)</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Flush log segments for all offsets up to offset-1</div><div class="line"> *</div><div class="line"> * @param offset The offset to flush up to (non-inclusive); the new recovery point</div><div class="line"> */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(offset: <span class="type">Long</span>) : <span class="type">Unit</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (offset &lt;= <span class="keyword">this</span>.recoveryPoint)</div><div class="line">    <span class="keyword">return</span></div><div class="line">  debug(<span class="string">"Flushing log '"</span> + name + <span class="string">" up to offset "</span> + offset + <span class="string">", last flushed: "</span> + lastFlushTime + <span class="string">" current time: "</span> +</div><div class="line">        time.milliseconds + <span class="string">" unflushed = "</span> + unflushedMessages)</div><div class="line">  <span class="comment">//note: 刷新检查点到最新偏移量之间的所有日志分段</span></div><div class="line">  <span class="keyword">for</span>(segment &lt;- logSegments(<span class="keyword">this</span>.recoveryPoint, offset))</div><div class="line">    segment.flush()<span class="comment">//note: 刷新数据文件和索引文件（调用操作系统的 fsync）</span></div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">if</span>(offset &gt; <span class="keyword">this</span>.recoveryPoint) &#123;</div><div class="line">      <span class="keyword">this</span>.recoveryPoint = offset</div><div class="line">      lastflushedTime.set(time.milliseconds)<span class="comment">//note: 更新刷新时间</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的内容实际上只是按 <code>log.flush.interval.ms</code> 设置去 flush 日志到磁盘，那么 <code>log.flush.interval.messages</code> 策略是在什么地方生效的呢？用心想一下，大家应该能猜出来，是在数据追加到 Log 中的时候，这时候会判断没有 flush 的数据大小是否达到阈值，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 其他部分这里暂时忽略了</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</div><div class="line">  <span class="comment">// now append to the log</span></div><div class="line">  segment.append(firstOffset = appendInfo.firstOffset,</div><div class="line">    largestOffset = appendInfo.lastOffset,</div><div class="line">    largestTimestamp = appendInfo.maxTimestamp,</div><div class="line">    shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</div><div class="line">    records = validRecords)</div><div class="line"></div><div class="line">  <span class="comment">// increment the log end offset</span></div><div class="line">  updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</div><div class="line"></div><div class="line">  trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></div><div class="line">    .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</div><div class="line">    flush()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h2><p>为了保证分区的总大小不超过阈值（<code>log.retention.bytes</code>），日志管理器会定时清理旧的数据。</p>
<blockquote>
<p>不过一般情况下，都是通过配置 <code>log.retention.hours</code> 来配置 segment 的保存时间，而不是通过单日志的总大小配置，因为不同的 topic，其 partition 大小相差很大，导致最后的保存时间可能也不一致，不利于管理。</p>
</blockquote>
<p>清理旧日志分段方法，主要有两种：</p>
<ol>
<li>删除：超过时间或大小阈值的旧 segment，直接进行删除；</li>
<li>压缩：不是直接删除日志分段，而是采用合并压缩的方式进行。</li>
</ol>
<p>这里主要讲述第一种方法，第二种将会后续文章介绍。</p>
<p>先看下 LogManager 中日志清除任务的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * Delete any eligible logs. Return the number of segments deleted.</div><div class="line"> * Only consider logs that are not compacted.</div><div class="line"> */</div><div class="line"><span class="comment">//note: 日志清除任务</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanupLogs</span></span>() &#123;</div><div class="line">  debug(<span class="string">"Beginning log cleanup..."</span>)</div><div class="line">  <span class="keyword">var</span> total = <span class="number">0</span></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">  <span class="keyword">for</span>(log &lt;- allLogs; <span class="keyword">if</span> !log.config.compact) &#123;</div><div class="line">    debug(<span class="string">"Garbage collecting '"</span> + log.name + <span class="string">"'"</span>)</div><div class="line">    total += log.deleteOldSegments() <span class="comment">//note: 清理过期的 segment</span></div><div class="line">  &#125;</div><div class="line">  debug(<span class="string">"Log cleanup completed. "</span> + total + <span class="string">" files deleted in "</span> +</div><div class="line">                (time.milliseconds - startMs) / <span class="number">1000</span> + <span class="string">" seconds"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>日志清除任务的实现还是在 Log 的 <code>deleteOldSegments()</code> 中实现的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">  * Delete any log segments that have either expired due to time based retention</div><div class="line">  * or because the log size is &gt; retentionSize</div><div class="line">  */</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(): <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (!config.delete) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  deleteRetenionMsBreachedSegments() + deleteRetentionSizeBreachedSegments()</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 清除保存时间满足条件的 segment</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetenionMsBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (config.retentionMs &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">val</span> startMs = time.milliseconds</div><div class="line">  deleteOldSegments(startMs - _.largestTimestamp &gt; config.retentionMs)</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//note: 清除保存大小满足条件的 segment</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetentionSizeBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</div><div class="line">  <span class="keyword">if</span> (config.retentionSize &lt; <span class="number">0</span> || size &lt; config.retentionSize) <span class="keyword">return</span> <span class="number">0</span></div><div class="line">  <span class="keyword">var</span> diff = size - config.retentionSize</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shouldDelete</span></span>(segment: <span class="type">LogSegment</span>) = &#123;</div><div class="line">    <span class="keyword">if</span> (diff - segment.size &gt;= <span class="number">0</span>) &#123;</div><div class="line">      diff -= segment.size</div><div class="line">      <span class="literal">true</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="literal">false</span></div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  deleteOldSegments(shouldDelete)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清除日志的两个方法：</p>
<ol>
<li><code>deleteRetenionMsBreachedSegments()</code>：如果 segment 保存时间超过设置的时间，那么进行删除；</li>
<li><code>deleteRetentionSizeBreachedSegments()</code>：如果当前最新的日志大小减少下一个即将删除的 segment 分段的大小超过阈值，那么就允许删除该 segment，否则就不允许。</li>
</ol>
<p>调用 <code>deleteOldSegments()</code> 方法删除日志数据文件及索引文件的具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//note: 清除相应的 segment 及相应的索引文件</span></div><div class="line"><span class="comment">//note: 其中 predicate 是一个高阶函数，只有返回值为 true 该 segment 才会被删除</span></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(predicate: <span class="type">LogSegment</span> =&gt; <span class="type">Boolean</span>): <span class="type">Int</span> = &#123;</div><div class="line">  lock synchronized &#123;</div><div class="line">    <span class="keyword">val</span> deletable = deletableSegments(predicate)</div><div class="line">    <span class="keyword">val</span> numToDelete = deletable.size</div><div class="line">    <span class="keyword">if</span> (numToDelete &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// we must always have at least one segment, so if we are going to delete all the segments, create a new one first</span></div><div class="line">      <span class="keyword">if</span> (segments.size == numToDelete)</div><div class="line">        roll()</div><div class="line">      <span class="comment">// remove the segments for lookups</span></div><div class="line">      deletable.foreach(deleteSegment) <span class="comment">//note: 删除 segment</span></div><div class="line">    &#125;</div><div class="line">    numToDelete</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</div><div class="line">  info(<span class="string">"Scheduling log segment %d for log %s for deletion."</span>.format(segment.baseOffset, name))</div><div class="line">  lock synchronized &#123;</div><div class="line">    segments.remove(segment.baseOffset) <span class="comment">//note:  从映射关系表中删除数据</span></div><div class="line">    asyncDeleteSegment(segment) <span class="comment">//note: 异步删除日志 segment</span></div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Perform an asynchronous delete on the given file if it exists (otherwise do nothing)</div><div class="line"> *</div><div class="line"> * @throws KafkaStorageException if the file can't be renamed and still exists</div><div class="line"> */</div><div class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">asyncDeleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</div><div class="line">  segment.changeFileSuffixes(<span class="string">""</span>, <span class="type">Log</span>.<span class="type">DeletedFileSuffix</span>) <span class="comment">//note: 先将 segment 的数据文件和索引文件后缀添加 `.deleted`</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteSeg</span></span>() &#123;</div><div class="line">    info(<span class="string">"Deleting segment %d from log %s."</span>.format(segment.baseOffset, name))</div><div class="line">    segment.delete()</div><div class="line">  &#125;</div><div class="line">  scheduler.schedule(<span class="string">"delete-file"</span>, deleteSeg, delay = config.fileDeleteDelayMs) <span class="comment">//note: 异步调度进行删除</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面的讲解来看，Kafka LogManager 线程工作还是比较清晰简洁的，它的作用就是负责日志的创建、检索、清理，并不负责日志的读写等实际操作。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是
    
    </summary>
    
      <category term="技术" scheme="http://matt33.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="kafka" scheme="http://matt33.com/tags/kafka/"/>
    
  </entry>
  
</feed>
