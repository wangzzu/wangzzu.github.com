<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="与一群有趣的人，做一些有趣的事."><title>Kafka 源码解析之 Producer 发送模型（一） | Matt's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafka 源码解析之 Producer 发送模型（一）</h1><a id="logo" href="/.">Matt's Blog</a><p class="description">王蒙</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kafka 源码解析之 Producer 发送模型（一）</h1><div class="post-meta">Jun 25, 2017<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><span> | </span><span class="post-count">3,771</span><span> 字</span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Times</span></span></div><a data-disqus-identifier="2017/06/25/kafka-producer-send-module/" href="/2017/06/25/kafka-producer-send-module/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Producer-使用"><span class="toc-number">2.</span> <span class="toc-text">Producer 使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Producer-数据发送流程"><span class="toc-number">3.</span> <span class="toc-text">Producer 数据发送流程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Producer-的-send-实现"><span class="toc-number">3.1.</span> <span class="toc-text">Producer 的 send 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Producer-的-doSend-实现"><span class="toc-number">3.2.</span> <span class="toc-text">Producer 的 doSend 实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#发送过程详解"><span class="toc-number">4.</span> <span class="toc-text">发送过程详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#获取-topic-的-metadata-信息"><span class="toc-number">4.1.</span> <span class="toc-text">获取 topic 的 metadata 信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#key-和-value-的序列化"><span class="toc-number">4.2.</span> <span class="toc-text">key 和 value 的序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#获取-partition-值"><span class="toc-number">4.3.</span> <span class="toc-text">获取 partition 值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#向-accumulator-写数据"><span class="toc-number">4.4.</span> <span class="toc-text">向 accumulator 写数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#发送-RecordBatch"><span class="toc-number">4.5.</span> <span class="toc-text">发送 RecordBatch</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#最后"><span class="toc-number">5.</span> <span class="toc-text">最后</span></a></li></ol></div></div><div class="post-content"><p>早就开始计划写 Kafka 源码分析的文章，但却一直迟迟没有动手，直到看到一位同事的博客 <a href="http://blog.bcmeng.com/" target="_blank" rel="external">编程小梦</a>，彻底受到了打击，这位同事是去年本科毕业，年龄算起来应该比我小两岁，但是非常厉害，在刚工作半年的时候就成为了 Apache Kylin 的 commiter，看到身边同事这么优秀，而且还这么努力 （<a href="http://blog.bcmeng.com/post/booklist.html" target="_blank" rel="external">编程小梦-我的书单</a>），自己实在没有理由不努力了，因此，在 github 上给自己提了一个 issue <a href="https://github.com/wangzzu/awesome/issues/7" target="_blank" rel="external">Kafka 源码分析系列</a>，希望自己能够在未来半年里，至少每两周输出一篇 Kafka 源码分析的文章，本文是这个系列的第一篇 —— Producer 的发送模型（以 <strong>Kafka 0.10.2</strong> 为例）。</p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Kafka，作为目前在大数据领域应用最为广泛的消息队列，其内部实现和设计有很多值得深入研究和分析的地方。</p>
<p>再 0.10.2 的 Kafka 中，其 Client 端是由 Java 实现，Server 端是由 Scala 来实现的，在使用 Kafka 时，Client 是用户最先接触到部分，因此，计划写的源码分析也会从 Client 端开始，会先从 Producer 端开始，今天讲的是 Producer 端的发送模型的实现。</p>
<h1 id="Producer-使用"><a href="#Producer-使用" class="headerlink" title="Producer 使用"></a>Producer 使用</h1><p>在分析 Producer 发送模型之前，先看一下用户是如何使用 Producer 向 Kafka 写数据的，下面是一个关于 Producer 最简单的应用示例。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by matt on 16/7/26.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerTest</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String topicName;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> msgNum;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> key;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"127.0.0.1:9092,127.0.0.2:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        topicName = <span class="string">"test"</span>;</span><br><span class="line">        msgNum = <span class="number">10</span>; <span class="comment">// 发送的消息数</span></span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; msgNum; i++) &#123;</span><br><span class="line">            String msg = i + <span class="string">" This is matt's blog."</span>;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topicName, msg));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的代码可以看出 Kafka 为用户提供了非常简单的 API，在使用时，只需要如下两步：</p>
<ol>
<li>初始化 <code>KafkaProducer</code> 实例；</li>
<li>调用 <code>send</code> 接口发送数据。</li>
</ol>
<p>本文主要是围绕着 Producer 在内部是如何实现 <code>send</code> 接口而展开的。</p>
<h1 id="Producer-数据发送流程"><a href="#Producer-数据发送流程" class="headerlink" title="Producer 数据发送流程"></a>Producer 数据发送流程</h1><p>下面通过对 <code>send</code> 源码分析来一步步剖析 Producer 数据的发送流程。</p>
<h2 id="Producer-的-send-实现"><a href="#Producer-的-send-实现" class="headerlink" title="Producer 的 send 实现"></a>Producer 的 send 实现</h2><p>用户是直接使用 <code>producer.send()</code> 发送的数据，先看一下 <code>send()</code> 接口的实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 异步向一个 topic 发送数据</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> send(record, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向 topic 异步地发送数据，当发送确认后唤起回调函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// intercept the record, which can be potentially modified; this method does not throw exceptions</span></span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = <span class="keyword">this</span>.interceptors == <span class="keyword">null</span> ? record : <span class="keyword">this</span>.interceptors.onSend(record);</span><br><span class="line">    <span class="keyword">return</span> doSend(interceptedRecord, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>数据发送的最终实现还是调用了 Producer 的 <code>doSend()</code> 接口。</p>
<h2 id="Producer-的-doSend-实现"><a href="#Producer-的-doSend-实现" class="headerlink" title="Producer 的 doSend 实现"></a>Producer 的 doSend 实现</h2><p>下面是 <code>doSend()</code> 的具体实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">       TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">       <span class="keyword">try</span> &#123;</span><br><span class="line">           <span class="comment">// 1.确认数据要发送到的 topic 的 metadata 是可用的</span></span><br><span class="line">           ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">           <span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">           Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">           <span class="comment">// 2.序列化 record 的 key 和 value</span></span><br><span class="line">           <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">           <span class="keyword">try</span> &#123;</span><br><span class="line">               serializedKey = keySerializer.serialize(record.topic(), record.key());</span><br><span class="line">           &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert key of class "</span> + record.key().getClass().getName() +</span><br><span class="line">                       <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                       <span class="string">" specified in key.serializer"</span>);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">byte</span>[] serializedValue;</span><br><span class="line">           <span class="keyword">try</span> &#123;</span><br><span class="line">               serializedValue = valueSerializer.serialize(record.topic(), record.value());</span><br><span class="line">           &#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">               <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert value of class "</span> + record.value().getClass().getName() +</span><br><span class="line">                       <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                       <span class="string">" specified in value.serializer"</span>);</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           <span class="comment">// 3. 获取该 record 的 partition 的值（可以指定,也可以根据算法计算）</span></span><br><span class="line">           <span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">           <span class="keyword">int</span> serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);</span><br><span class="line">           ensureValidRecordSize(serializedSize); <span class="comment">// record 的字节超出限制或大于内存限制时,就会抛出 RecordTooLargeException 异常</span></span><br><span class="line">           tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line">           <span class="keyword">long</span> timestamp = record.timestamp() == <span class="keyword">null</span> ? time.milliseconds() : record.timestamp(); <span class="comment">// 时间戳</span></span><br><span class="line">           log.trace(<span class="string">"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;"</span>, record, callback, record.topic(), partition);</span><br><span class="line">           Callback interceptCallback = <span class="keyword">this</span>.interceptors == <span class="keyword">null</span> ? callback : <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line">           <span class="comment">// 4. 向 accumulator 中追加数据</span></span><br><span class="line">           RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);</span><br><span class="line">           <span class="comment">// 5. 如果 batch 已经满了,唤醒 sender 线程发送数据</span></span><br><span class="line">           <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">               log.trace(<span class="string">"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">               <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">return</span> result.future;</span><br><span class="line">       &#125; <span class="keyword">catch</span> (ApiException e) &#123;</span><br><span class="line">           log.debug(<span class="string">"Exception occurred during message send:"</span>, e);</span><br><span class="line">           <span class="keyword">if</span> (callback != <span class="keyword">null</span>)</span><br><span class="line">               callback.onCompletion(<span class="keyword">null</span>, e);</span><br><span class="line">           <span class="keyword">this</span>.errors.record();</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">               <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">new</span> FutureFailure(e);</span><br><span class="line">       &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">           <span class="keyword">this</span>.errors.record();</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">               <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> InterruptException(e);</span><br><span class="line">       &#125; <span class="keyword">catch</span> (BufferExhaustedException e) &#123;</span><br><span class="line">           <span class="keyword">this</span>.errors.record();</span><br><span class="line">           <span class="keyword">this</span>.metrics.sensor(<span class="string">"buffer-exhausted-records"</span>).record();</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">               <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">           <span class="keyword">throw</span> e;</span><br><span class="line">       &#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">           <span class="keyword">this</span>.errors.record();</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">               <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">           <span class="keyword">throw</span> e;</span><br><span class="line">       &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.interceptors != <span class="keyword">null</span>)</span><br><span class="line">               <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">           <span class="keyword">throw</span> e;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>dosend()</code> 方法的实现上，一条 Record 数据的发送，可以分为以下五步：</p>
<ol>
<li>确认数据要发送到的 topic 的 metadata 是可用的（如果该 partition 的 leader 存在则是可用的，如果开启权限时，client 有相应的权限），如果没有 topic 的 metadata 信息，就需要获取相应的 metadata；</li>
<li>序列化 record 的 key 和 value；</li>
<li>获取该 record 要发送到的 partition（可以指定，也可以根据算法计算）；</li>
<li>向 accumulator 中追加 record 数据，数据会先进行缓存；</li>
<li>如果追加完数据后，对应的 RecordBatch 已经达到了 batch.size 的大小（或者batch 的剩余空间不足以添加下一条 Record），则唤醒 <code>sender</code> 线程发送数据。</li>
</ol>
<p>数据的发送过程，可以简单总结为以上五点，下面会这几部分的具体实现进行详细分析。</p>
<h1 id="发送过程详解"><a href="#发送过程详解" class="headerlink" title="发送过程详解"></a>发送过程详解</h1><h2 id="获取-topic-的-metadata-信息"><a href="#获取-topic-的-metadata-信息" class="headerlink" title="获取 topic 的 metadata 信息"></a>获取 topic 的 metadata 信息</h2><p>Producer 通过 <code>waitOnMetadata()</code> 方法来获取对应 topic 的 metadata 信息，这部分后面会单独抽出一篇文章来介绍，这里就不再详述，总结起来就是：在数据发送前，需要先该 topic 是可用的。</p>
<h2 id="key-和-value-的序列化"><a href="#key-和-value-的序列化" class="headerlink" title="key 和 value 的序列化"></a>key 和 value 的序列化</h2><p>Producer 端对 record 的 <code>key</code> 和 <code>value</code> 值进行序列化操作，在 Consumer 端再进行相应的反序列化，Kafka 内部提供的序列化和反序列化算法如下图所示：</p>
<p><img src="/images/kafka/serialize.png" alt="Kafka serialize &amp; deserialize"></p>
<p>当然我们也是可以自定义序列化的具体实现，不过一般情况下，Kafka 内部提供的这些方法已经足够使用。</p>
<h2 id="获取-partition-值"><a href="#获取-partition-值" class="headerlink" title="获取 partition 值"></a>获取 partition 值</h2><p>关于 partition 值的计算，分为三种情况：</p>
<ol>
<li>指明 partition 的情况下，直接将指明的值直接作为 partiton 值；</li>
<li>没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值；</li>
<li>既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 <code>round-robin</code> 算法。</li>
</ol>
<p>具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 当 record 中有 partition 值时，直接返回，没有的情况下调用 partitioner 的类的 partition 方法去计算（KafkaProducer.class）</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Producer 默认使用的 <code>partitioner</code> 是 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code>，用户也可以自定义 partition 的策略，下面是这个类两个方法的具体实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;<span class="comment">// 没有指定 key 的情况下</span></span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic); <span class="comment">// 第一次的时候产生一个随机整数,后面每次调用在之前的基础上自增;</span></span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="comment">// leader 不为 null,即为可用的 partition</span></span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;<span class="comment">// 有 key 的情况下,使用 key 的 hash 值进行计算</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; <span class="comment">// 选择 key 的 hash 值</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据 topic 获取对应的整数变量</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123; <span class="comment">// 第一次调用时，随机产生</span></span><br><span class="line">            counter = <span class="keyword">new</span> AtomicInteger(<span class="keyword">new</span> Random().nextInt());</span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> counter.getAndIncrement(); <span class="comment">// 后面再调用时，根据之前的结果自增</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这就是 Producer 中默认的 partitioner 实现。</p>
<h2 id="向-accumulator-写数据"><a href="#向-accumulator-写数据" class="headerlink" title="向 accumulator 写数据"></a>向 accumulator 写数据</h2><p>Producer 会先将 record 写入到 buffer 中，当达到一个 <code>batch.size</code> 的大小时，再唤起 <code>sender</code> 线程去发送 <code>RecordBatch</code>（第五步），这里先详细分析一下 Producer 是如何向 buffer 中写入数据的。</p>
<p>Producer 是通过 <code>RecordAccumulator</code> 实例追加数据，<code>RecordAccumulator</code> 模型如下图所示，一个重要的变量就是 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches</code>，每个 <code>TopicPartition</code> 都会对应一个 <code>Deque&lt;RecordBatch&gt;</code>，当添加数据时，会向其 topic-partition 对应的这个 queue 最新创建的一个 <code>RecordBatch</code> 中添加 record，而发送数据时，则会先从 queue 中最老的那个 <code>RecordBatch</code> 开始发送。</p>
<p><img src="/images/kafka/recordbatch.png" alt="Producer RecordAccumulator 模型"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// org.apache.kafka.clients.producer.internals.RecordAccumulator</span></span><br><span class="line">     <span class="comment">// 向 accumulator 添加一条 record，并返回添加后的结果（结果主要包含: future metadata、batch 是否满的标志以及新 batch 是否创建）其中， maxTimeToBlock 是 buffer.memory 的 block 的最大时间</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> maxTimeToBlock)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        appendsInProgress.incrementAndGet();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp);<span class="comment">// 每个 topicPartition 对应一个 queue</span></span><br><span class="line">            <span class="keyword">synchronized</span> (dq) &#123;<span class="comment">// 在对一个 queue 进行操作时,会保证线程安全</span></span><br><span class="line">                <span class="keyword">if</span> (closed)</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Cannot send after the producer is closed."</span>);</span><br><span class="line">                RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); <span class="comment">// 追加数据</span></span><br><span class="line">                <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)<span class="comment">// 这个 topic-partition 已经有记录了</span></span><br><span class="line">                    <span class="keyword">return</span> appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 为 topic-partition 创建一个新的 RecordBatch, 需要初始化相应的 RecordBatch，要为其分配的大小是: max（batch.size, 加上头文件的本条消息的大小）</span></span><br><span class="line">            <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, Records.LOG_OVERHEAD + Record.recordSize(key, value));</span><br><span class="line">            log.trace(<span class="string">"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;"</span>, size, tp.topic(), tp.partition());</span><br><span class="line">            ByteBuffer buffer = free.allocate(size, maxTimeToBlock);<span class="comment">// 给这个 RecordBatch 初始化一个 buffer</span></span><br><span class="line">            <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">                <span class="keyword">if</span> (closed)</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Cannot send after the producer is closed."</span>);</span><br><span class="line"></span><br><span class="line">                RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq);</span><br><span class="line">                <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123;<span class="comment">// 如果突然发现这个 queue 已经存在，那么就释放这个已经分配的空间</span></span><br><span class="line">                    free.deallocate(buffer);</span><br><span class="line">                    <span class="keyword">return</span> appendResult;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 给 topic-partition 创建一个 RecordBatch</span></span><br><span class="line">                MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, <span class="keyword">this</span>.batchSize);</span><br><span class="line">                RecordBatch batch = <span class="keyword">new</span> RecordBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">                <span class="comment">// 向新的 RecordBatch 中追加数据</span></span><br><span class="line">                FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">                dq.addLast(batch);<span class="comment">// 将 RecordBatch 添加到对应的 queue 中</span></span><br><span class="line">                incomplete.add(batch);<span class="comment">// 向未 ack 的 batch 集合添加这个 batch</span></span><br><span class="line">                <span class="comment">// 如果 dp.size()&gt;1 就证明这个 queue 有一个 batch 是可以发送了</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            appendsInProgress.decrementAndGet();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>总结一下其 record 写入的具体流程如下图所示：</p>
<p><img src="/images/kafka/RecordBatch_append.png" alt="Producer RecordAccumulator record 写入流程"></p>
<ol>
<li>获取该 topic-partition 对应的 queue，没有的话会创建一个空的 queue；</li>
<li>向 queue 中追加数据，先获取 queue 中最新加入的那个 <code>RecordBatch</code>，如果不存在或者存在但剩余空余不足以添加本条 record 则返回 null，成功写入的话直接返回结果，写入成功；</li>
<li>创建一个新的 <code>RecordBatch</code>，初始化内存大小根据 <code>max(batch.size, Records.LOG_OVERHEAD + Record.recordSize(key, value))</code> 来确定（防止单条 record 过大的情况）；</li>
<li>向新建的 <code>RecordBatch</code> 写入 record，并将 <code>RecordBatch</code> 添加到 queue 中，返回结果，写入成功。</li>
</ol>
<h2 id="发送-RecordBatch"><a href="#发送-RecordBatch" class="headerlink" title="发送 RecordBatch"></a>发送 RecordBatch</h2><p>当 record 写入成功后，如果发现 <code>RecordBatch</code> 已满足发送的条件（通常是 queue 中有多个 batch，那么最先添加的那些 batch 肯定是可以发送了），那么就会唤醒 <code>sender</code> 线程，发送 <code>RecordBatch</code>。</p>
<p><code>sender</code> 线程对 <code>RecordBatch</code> 的处理是在 <code>run()</code> 方法中进行的，该方法具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">        Cluster cluster = metadata.fetch();</span><br><span class="line">        <span class="comment">// 获取那些已经可以发送的 RecordBatch 对应的 nodes</span></span><br><span class="line">        RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果有 topic-partition 的 leader 是未知的,就强制 metadata 更新</span></span><br><span class="line">        <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class="line">                <span class="keyword">this</span>.metadata.add(topic);</span><br><span class="line">            <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果与node 没有连接（如果可以连接,同时初始化该连接）,就证明该 node 暂时不能发送数据,暂时移除该 node</span></span><br><span class="line">        Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">        <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">            Node node = iter.next();</span><br><span class="line">            <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) &#123;</span><br><span class="line">                iter.remove();</span><br><span class="line">                notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.connectionDelay(node, now));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回该 node 对应的所有可以发送的 RecordBatch 组成的 batches（key 是 node.id）,并将 RecordBatch 从对应的 queue 中移除</span></span><br><span class="line">        Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line">        <span class="keyword">if</span> (guaranteeMessageOrder) &#123;</span><br><span class="line">            <span class="comment">//记录将要发送的 RecordBatch</span></span><br><span class="line">            <span class="keyword">for</span> (List&lt;RecordBatch&gt; batchList : batches.values()) &#123;</span><br><span class="line">                <span class="keyword">for</span> (RecordBatch batch : batchList)</span><br><span class="line">                    <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将由于元数据不可用而导致发送超时的 RecordBatch 移除</span></span><br><span class="line">        List&lt;RecordBatch&gt; expiredBatches = <span class="keyword">this</span>.accumulator.abortExpiredBatches(<span class="keyword">this</span>.requestTimeout, now);</span><br><span class="line">        <span class="keyword">for</span> (RecordBatch expiredBatch : expiredBatches)</span><br><span class="line">            <span class="keyword">this</span>.sensors.recordErrors(expiredBatch.topicPartition.topic(), expiredBatch.recordCount);</span><br><span class="line"></span><br><span class="line">        sensors.updateProduceRequestMetrics(batches);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">        <span class="keyword">if</span> (!result.readyNodes.isEmpty()) &#123;</span><br><span class="line">            log.trace(<span class="string">"Nodes with data ready to send: &#123;&#125;"</span>, result.readyNodes);</span><br><span class="line">            pollTimeout = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 发送 RecordBatch</span></span><br><span class="line">        sendProduceRequests(batches, now);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.client.poll(pollTimeout, now); <span class="comment">// 关于 socket 的一些实际的读写操作（其中包括 meta 信息的更新）</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这段代码前面有很多是其他的逻辑处理，如：移除暂时不可用的 node、处理由于元数据不可用导致的超时 <code>RecordBatch</code>，真正进行发送发送 <code>RecordBatch</code> 的是 <code>sendProduceRequests(batches, now)</code> 这个方法，具体是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transfer the record batches into a list of produce requests on a per-node basis</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequests</span><span class="params">(Map&lt;Integer, List&lt;RecordBatch&gt;&gt; collated, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Integer, List&lt;RecordBatch&gt;&gt; entry : collated.entrySet())</span><br><span class="line">        sendProduceRequest(now, entry.getKey(), acks, requestTimeout, entry.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a produce request from the given record batches</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 发送 produce 请求</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;RecordBatch&gt; batches)</span> </span>&#123;</span><br><span class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">for</span> (RecordBatch batch : batches) &#123;</span><br><span class="line">        TopicPartition tp = batch.topicPartition;</span><br><span class="line">        produceRecordsByPartition.put(tp, batch.records());</span><br><span class="line">        recordsByPartition.put(tp, batch);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ProduceRequest.Builder requestBuilder =</span><br><span class="line">            <span class="keyword">new</span> ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</span><br><span class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</span><br><span class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    String nodeId = Integer.toString(destination);</span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, callback);</span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">    log.trace(<span class="string">"Sent produce request to &#123;&#125;: &#123;&#125;"</span>, nodeId, requestBuilder);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码就简单很多，总来起来就是，将 <code>batches</code> 中 leader 为同一个 node 的所有 RecordBatch 放在一个请求中进行发送。</p>
<h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>本文是对 Kafka Producer 端发送模型的一个简单分析，下一篇文章将会详细介绍 metadata 相关的内容，包括 metadata 的内容以及在 Producer 端 metadata 的更新机制。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matt33.com/2017/06/25/kafka-producer-send-module/" data-id="ck657ycc40072mbjb217cz4ji" class="article-share-link">分享到</a><div class="copyright"><a href="http://matt33.com/copyright/">博客版权说明</a></div><div class="tags"><a href="/tags/kafka/">kafka</a></div><div class="post-nav"><a href="/2017/07/02/buy-little-room/" class="pre">买房感想</a><a href="/2017/06/21/css-summary/" class="next">CSS 一些常用方法的总结</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button></div><script>var disqus_shortname = 'http-matt33-com';
var disqus_identifier = '2017/06/25/kafka-producer-send-module/';
var disqus_title = 'Kafka 源码解析之 Producer 发送模型（一）';
var disqus_url = 'http://matt33.com/2017/06/25/kafka-producer-send-module/';
$('.btn_click_load').click(function() {
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('.btn_click_load').css('display','none');
});
$.ajax({
  url: 'https://disqus.com/favicon.ico',
  timeout: 3000,
  type: 'GET',
  success: (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    $('.btn_click_load').css('display','none');
  })(),
  error: function() {
    $('.btn_click_load').css('display','block');
  }
});</script><script id="dsq-count-scr" src="//http-matt33-com.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-weibo"> 微博</i></div><iframe width="100%" height="90" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=100&fansRow=1&ptype=1&speed=0&skin=1&isTitle=0&noborder=1&isWeibo=0&isFans=0&uid=2650396571&verifier=f2f0e397&colors=D8D8D8,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书屋/">书屋</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影如人生/">影如人生</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/旅行/">旅行</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/转载/">转载</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/database/" style="font-size: 15px;">database</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/storm/" style="font-size: 15px;">storm</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/learn/" style="font-size: 15px;">learn</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/travel/" style="font-size: 15px;">travel</a> <a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/tcp/" style="font-size: 15px;">tcp</a> <a href="/tags/电影随想/" style="font-size: 15px;">电影随想</a> <a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/转载/" style="font-size: 15px;">转载</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/jvm/" style="font-size: 15px;">jvm</a> <a href="/tags/web/" style="font-size: 15px;">web</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a> <a href="/tags/bk/" style="font-size: 15px;">bk</a> <a href="/tags/rpc/" style="font-size: 15px;">rpc</a> <a href="/tags/thrift/" style="font-size: 15px;">thrift</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/calcite/" style="font-size: 15px;">calcite</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/02/02/2019-summary/">2019年终总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/27/flink-jobmanager-6/">Flink JobManager 详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/23/flink-master-5/">Flink Master 详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/20/flink-execution-graph-4/">Flink 如何生成 ExecutionGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/09/flink-job-graph-3/">Flink Streaming 作业如何转化为 JobGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/08/flink-stream-graph-2/">Flink DataStream API 概述及作业如何转换为 StreamGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/flink-learn-start-1/">Apache Flink 初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/paper-chandy-lamport/">Paper 阅读: Distributed Snapshots: Determining Global States of Distributed Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/20/paper-flink-snapshot/">Paper 阅读: Lightweight Asynchronous Snapshots for Distributed Dataflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/19/paper-ray1/">Paper 阅读: Real-Time Machine Learning: The Missing Pieces</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://matt33.com/about/" title="个人公众号：柳年思水" target="_blank">个人公众号：柳年思水</a><ul></ul><a href="http://tech.meituan.com/" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://jm.taobao.org/" title="阿里中间件团队博客" target="_blank">阿里中间件团队博客</a><ul></ul><a href="http://www.jianshu.com/" title="简书" target="_blank">简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matt's Blog 柳年思水.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><div class="analytics"><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1256517224'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1256517224%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script></div><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-64518924-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5cf44757fa0d23bc7637935e44a9104a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>