<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="与一群有趣的人，做一些有趣的事."><title>Apache Kafka 0.10.0 new Consumer使用 | Matt's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Apache Kafka 0.10.0 new Consumer使用</h1><a id="logo" href="/.">Matt's Blog</a><p class="description">王蒙</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Apache Kafka 0.10.0 new Consumer使用</h1><div class="post-meta">Jul 22, 2016<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><span> | </span><span class="post-count">2,610</span><span> 字</span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Times</span></span></div><a data-disqus-identifier="2016/07/22/kafak-new-consumer-use/" href="/2016/07/22/kafak-new-consumer-use/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Consumer-Client"><span class="toc-number">1.</span> <span class="toc-text">Consumer Client</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#配置"><span class="toc-number">1.1.</span> <span class="toc-text">配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化与配置"><span class="toc-number">1.2.</span> <span class="toc-text">初始化与配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Consumer-Auto-Offset-Commit"><span class="toc-number">1.3.</span> <span class="toc-text">Consumer Auto Offset Commit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Consumer-Manual-Offset-Control"><span class="toc-number">1.4.</span> <span class="toc-text">Consumer Manual Offset Control</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Consumer-Manual-Partition-Assign"><span class="toc-number">1.5.</span> <span class="toc-text">Consumer Manual Partition Assign</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#KafkaStream使用"><span class="toc-number">2.</span> <span class="toc-text">KafkaStream使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#配置-1"><span class="toc-number">2.1.</span> <span class="toc-text">配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化与配置-1"><span class="toc-number">2.2.</span> <span class="toc-text">初始化与配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#小示例"><span class="toc-number">2.3.</span> <span class="toc-text">小示例</span></a></li></ol></li></ol></div></div><div class="post-content"><p>本文主要介绍一下Kafka new Consumer的使用，关于new Consumer的基本概念可以参考上一篇博文<a href="http://matt33.com/2016/07/21/kafka-new-consumer/">Apache Kafka 0.9 Consumer Client 介绍【译】</a>，这篇对于Kafka的new Consumer介绍得比较清楚。本文的一部分内容也来自上一篇文章。</p>
<h1 id="Consumer-Client"><a href="#Consumer-Client" class="headerlink" title="Consumer Client"></a>Consumer Client</h1><p>本节主要介绍Kafka从一些topic消费数据的示例。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>使用新版的Consumer，需要先在工程中添加kafka-clients依赖，添加的配置信息如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="初始化与配置"><a href="#初始化与配置" class="headerlink" title="初始化与配置"></a>初始化与配置</h2><p>Consumer的创建过程与之前旧的API创建方法一样，一个Consumer必备的最小配置项如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>); <span class="comment">// 通过其中的一台broker来找到group的coordinator，并不需要列出所有的broker</span></span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"consumer-tutorial"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, StringDeserializer.class.getName());</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, StringDeserializer.class.getName());</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props); <span class="comment">// consumer实例</span></span><br></pre></td></tr></table></figure>
<p>Consumer的其他配置项可以参考<a href="http://kafka.apache.org/documentation.html#newconsumerconfigs" target="_blank" rel="external">New Consumer Configs</a>，除了上面的这几个配置之外，其他的几个比较常用的配置信息如下表所示</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>heartbeat.interval.ms</td>
<td>3000</td>
<td>当使用Kafka的group管理机制时，consumer向coordinator发送心跳的间隔，这个值要比session.timeout.ms小，最好不要超过session.timeout.ms的\frac{1}{3}</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>30000</td>
<td>当使用Kafka的group管理机制时用于检测到consumer失败的时长，如果在这个时间内没有收到consumer的心跳信息，就认为Consumer失败了</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>latest</td>
<td>group首次开始消费数据时的offset，有以下几个值可以选择：earliest、latest、none、anything else.</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>true</td>
<td>设置为true时，Consumer的offset将会被周期性地自动commit</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>5000</td>
<td>Consumer的offset自动commit时的周期</td>
</tr>
</tbody>
</table>
<h2 id="Consumer-Auto-Offset-Commit"><a href="#Consumer-Auto-Offset-Commit" class="headerlink" title="Consumer Auto Offset Commit"></a>Consumer Auto Offset Commit</h2><p>本例使用Kafka的自动commit机制，每隔一段时间（可通过<code>auto.commit.interval.ms</code>来设置）就会自动进行commit offset。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"group"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>); <span class="comment">// 自动commit</span></span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>); <span class="comment">// 自动commit的间隔</span></span><br><span class="line">props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"test1"</span>, <span class="string">"test2"</span>)); <span class="comment">// 可消费多个topic,组成一个list</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s \n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里有几点需要注意：</p>
<ol>
<li>在使用自动commit时，系统是保证at least once，因为offset是在这些messages被应用处理成功后才进行commit的；</li>
<li>subscribe方法需要传入所有topic的列表，一个group所消费的topic是不能动态增加的，但是可以在任何时间改变这个列表，它会把前面的设置覆盖掉；</li>
<li>poll中的参数就是设置一个时长，Consumer在进行拉取数据进行block的最大时间限制；</li>
</ol>
<h2 id="Consumer-Manual-Offset-Control"><a href="#Consumer-Manual-Offset-Control" class="headerlink" title="Consumer Manual Offset Control"></a>Consumer Manual Offset Control</h2><p>要进行手动commit，需要在配置文件中将enable.auto.commit设置为false，来禁止自动commit，本例以手动同步commit为例</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"group"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>); <span class="comment">//关闭自动commit</span></span><br><span class="line">props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"test1"</span>, <span class="string">"test2"</span>));</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s \n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= minBatchSize) &#123;</span><br><span class="line">        consumer.commitSync(); <span class="comment">//批量完成写入后，手工同步commit offset</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>在本例中，我们调用了commitSync方法，这是同步commit的方式，同时Kafka还提供了commitAsync方法，它们的区别是：使用同步提交时，consumer会进行block知道commit的结果返回，这样的话如果commit失败就可以今早地发现错误，而当使用异步commit时，commit的结果还未返回，Consumer就会开始拉取下一批的数据，但是使用异步commit可以系统的吞吐量，具体使用哪种方式需要开发者自己权衡；</li>
<li>本例中的实现依然是保证at least once，但是如果每次拉取到数据之后，就进行commit，最后再处理数据，就可以保证at last once。</li>
</ol>
<h2 id="Consumer-Manual-Partition-Assign"><a href="#Consumer-Manual-Partition-Assign" class="headerlink" title="Consumer Manual Partition Assign"></a>Consumer Manual Partition Assign</h2><p>Kafka在进行消费数据时，可以指定消费某个topic的某个partition，这种使用情况比较特殊，并不需要coordinator进行rebalance，也就意味着这种模式虽然需要设置group id，但是它跟前面的group的机制并不一样，它与旧的Consumer中的Simple Consumer相似，这是Kafka在新的Consumer API中对这种情况的支持。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"group"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>); <span class="comment">//关闭自动commit</span></span><br><span class="line">props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"earliest"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer consumer = <span class="keyword">new</span> KafkaConsumer(props);</span><br><span class="line">TopicPartition partition0 = <span class="keyword">new</span> TopicPartition(<span class="string">"test"</span>, <span class="number">0</span>);</span><br><span class="line">TopicPartition partition1 = <span class="keyword">new</span> TopicPartition(<span class="string">"test"</span>, <span class="number">2</span>);</span><br><span class="line">consumer.assign(Arrays.asList(partition0, partition1));</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s \n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= minBatchSize) &#123;</span><br><span class="line">        consumer.commitSync(); <span class="comment">//批量完成写入后，手工sync offset</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<ol>
<li>与前面的subscribe方法一样，在调用assign方法时，需要传入这个Consumer要消费的所有TopicPartition的列表；</li>
<li>不管对于simple consumer还是consumer group，所有offset的commit都必须经过group coordinator；</li>
<li>在进行commit时，必须设置一个合适的group.id，避免与其他的group产生冲突。如果一个simple consumer试图使用一个与一个active group相同的id进行commit offset，coordinator将会拒绝这个commit请求，会返回一个CommitFailedException异常，但是，如果一个simple consumer与另一个simple consumer使用同一个id，系统就不会报任何错误。</li>
</ol>
<h1 id="KafkaStream使用"><a href="#KafkaStream使用" class="headerlink" title="KafkaStream使用"></a>KafkaStream使用</h1><p>KafkaStream是在Kafka 0.10.0版中新提出的内容，Kafka官方也说了设计这个feature的原因——为了简单，之前在流处理方面，一般情况下都会使用Kafka作为消息队列，然后再搭建一个流处理环境做流处理，而现在我们可以直接在Kafka中进行流处理，不需要再搭建另外一个环境（加了这个feature之后会使得Kafka变得更加复杂，不过官网说，在使用时我们只需要在工程中添加一个外部依赖包即可使用这个功能）。</p>
<h2 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h2><p>需要在pom文件中添加如下依赖，KafkaStream在实际运行时也是依赖这个外部的jar包运行。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="初始化与配置-1"><a href="#初始化与配置-1" class="headerlink" title="初始化与配置"></a>初始化与配置</h2><p>KafkaStream使用的一个基本初始化部分如下所示（代码来自<a href="http://kafka.apache.org/0100/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html" target="_blank" rel="external">Javadoc</a>）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, Object&gt; props = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-stream-processing-application"</span>);</span><br><span class="line">props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(StreamsConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class="line">props.put(StreamsConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);</span><br><span class="line">props.put(StreamsConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">props.put(StreamsConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);</span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(props);</span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">builder.from(<span class="string">"my-input-topic"</span>).mapValue(value -&gt; value.length().toString()).to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, config);</span><br><span class="line">streams.start();</span><br></pre></td></tr></table></figure>
<p>完整的配置选项如下表所示，也可以参考<a href="http://kafka.apache.org/documentation.html#streamsconfigs" target="_blank" rel="external">Streams Configs</a></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
<th>类型</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>application.id</td>
<td>流处理应用的标识，对同一个应用需要一致，因为它是作为消费的group_id的</td>
<td>string</td>
<td></td>
</tr>
<tr>
<td>bootstrap.servers</td>
<td>host1:port1,host2:port2 这样的列表，是用来发现所有Kafka节点的种子，因此不需要配上所有的Kafka节点</td>
<td>list</td>
<td></td>
</tr>
<tr>
<td>client.id</td>
<td>应用的一个客户端的逻辑名称，设定后可以区分是哪个客户端在请求</td>
<td>string</td>
<td>“”</td>
</tr>
<tr>
<td>zookeeper.connect</td>
<td>zookeeper</td>
<td>string</td>
<td>“”</td>
</tr>
<tr>
<td>key.serde</td>
<td>键的序列化/反序列化类</td>
<td>class</td>
<td>org.apache.kafka.common.serialization.Serdes$ByteArraySerde</td>
</tr>
<tr>
<td>partition.grouper</td>
<td>用于分区组织的类，需要实现PartitionGrouper接口</td>
<td>class</td>
<td>org.apache.kafka.streams.processor.DefaultPartitionGrouper</td>
</tr>
<tr>
<td>replication.factor</td>
<td>流处理应用会创建change log topic和repartition topic用于管理内部状态，这个参数设定这些topic的副本数</td>
<td>int</td>
<td>1</td>
</tr>
<tr>
<td>state.dir</td>
<td>状态仓库的存储路径</td>
<td>string</td>
<td>/tmp/kafka-streams</td>
</tr>
<tr>
<td>timestamp.extractor</td>
<td>时间戳抽取类，需要实现TimestampExtractor接口</td>
<td>class</td>
<td>org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor</td>
</tr>
<tr>
<td>value.serde</td>
<td>值的序列化/反序列化类</td>
<td>class</td>
<td>org.apache.kafka.common.serialization.Serdes$ByteArraySerde</td>
</tr>
<tr>
<td>buffered.records.per.partition</td>
<td>每个分区缓存的最大记录数</td>
<td>int</td>
<td>1000</td>
</tr>
<tr>
<td>commit.interval.ms</td>
<td>存储处理器当前位置的间隔毫秒数</td>
<td>long</td>
<td>30000</td>
</tr>
<tr>
<td>metric.reporters</td>
<td>用于性能报告的类列表。需要实现MetricReporter接口。JmxReporter会永远开启不需要指定</td>
<td>list</td>
<td>[]</td>
</tr>
<tr>
<td>metric.num.samples</td>
<td>计算性能需要的采样数</td>
<td>int</td>
<td>2</td>
</tr>
<tr>
<td>metric.sample.window.ms</td>
<td>性能采样的时间间隔</td>
<td>long</td>
<td>30000</td>
</tr>
<tr>
<td>num.standby.replicas</td>
<td>每个任务的后备副本数</td>
<td>int</td>
<td>0</td>
</tr>
<tr>
<td>num.stream.threads</td>
<td>执行流处理的线程数</td>
<td>int</td>
<td>1</td>
</tr>
<tr>
<td>poll.ms</td>
<td>等待输入的毫秒数</td>
<td>long</td>
<td>100</td>
</tr>
<tr>
<td>state.cleanup.delay.ms</td>
<td>一个分区迁移后，在删除状态前等待的毫秒数</td>
<td>long</td>
<td>60000</td>
</tr>
</tbody>
</table>
<h2 id="小示例"><a href="#小示例" class="headerlink" title="小示例"></a>小示例</h2><p>这是个将一个topic的事件进行过滤的示例，处理很简单，下面给出了这个例子的完整代码。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.Predicate;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by matt on 16/7/22.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventFilter</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"test-filter"</span>);</span><br><span class="line">        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.4.232.70:9091,10.4.232.77:2181"</span>);</span><br><span class="line">        props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// setting offset reset to earliest so that we can re-run the demo code with the same pre-loaded data</span></span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line"></span><br><span class="line">        KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line"></span><br><span class="line">        KStream&lt;String, String&gt; source = builder.stream(<span class="string">"test"</span>);</span><br><span class="line"></span><br><span class="line">        source.filter(<span class="keyword">new</span> Predicate&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">test</span><span class="params">(String key, String value)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> (value.split(<span class="string">","</span>)[<span class="number">3</span>]).equals(<span class="string">"food"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).to(<span class="string">"food"</span>);</span><br><span class="line"></span><br><span class="line">        KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, props);</span><br><span class="line">        streams.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// usually the stream application would be running forever,</span></span><br><span class="line">        <span class="comment">// in this example we just let it run for some time and stop since the input data is finite.</span></span><br><span class="line">        Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">        streams.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<p>本文只是介绍这两个重要feature的使用方法，而KafkaStream并没有深入去讨论，后面会对本文再进行更新，并且还会增加Producer和Consumer使用安全机制的方法。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matt33.com/2016/07/22/kafak-new-consumer-use/" data-id="ck4imvxlt003f5zza8o15r2zm" class="article-share-link">分享到</a><div class="copyright"><a href="http://matt33.com/copyright/">博客版权说明</a></div><div class="tags"><a href="/tags/kafka/">kafka</a></div><div class="post-nav"><a href="/2016/07/29/sasl-plain-kafka/" class="pre">Kafka 0.10.0 SASL/PLAIN身份认证及权限实现</a><a href="/2016/07/21/kafka-new-consumer/" class="next">Apache Kafka 0.9 Consumer Client 介绍【译】</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button></div><script>var disqus_shortname = 'http-matt33-com';
var disqus_identifier = '2016/07/22/kafak-new-consumer-use/';
var disqus_title = 'Apache Kafka 0.10.0 new Consumer使用';
var disqus_url = 'http://matt33.com/2016/07/22/kafak-new-consumer-use/';
$('.btn_click_load').click(function() {
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('.btn_click_load').css('display','none');
});
$.ajax({
  url: 'https://disqus.com/favicon.ico',
  timeout: 3000,
  type: 'GET',
  success: (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    $('.btn_click_load').css('display','none');
  })(),
  error: function() {
    $('.btn_click_load').css('display','block');
  }
});</script><script id="dsq-count-scr" src="//http-matt33-com.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-weibo"> 微博</i></div><iframe width="100%" height="90" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=100&fansRow=1&ptype=1&speed=0&skin=1&isTitle=0&noborder=1&isWeibo=0&isFans=0&uid=2650396571&verifier=f2f0e397&colors=D8D8D8,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书屋/">书屋</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影如人生/">影如人生</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/旅行/">旅行</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/转载/">转载</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tcp/" style="font-size: 15px;">tcp</a> <a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/storm/" style="font-size: 15px;">storm</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/learn/" style="font-size: 15px;">learn</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/travel/" style="font-size: 15px;">travel</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/database/" style="font-size: 15px;">database</a> <a href="/tags/电影随想/" style="font-size: 15px;">电影随想</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/转载/" style="font-size: 15px;">转载</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/jvm/" style="font-size: 15px;">jvm</a> <a href="/tags/web/" style="font-size: 15px;">web</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a> <a href="/tags/bk/" style="font-size: 15px;">bk</a> <a href="/tags/rpc/" style="font-size: 15px;">rpc</a> <a href="/tags/thrift/" style="font-size: 15px;">thrift</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/calcite/" style="font-size: 15px;">calcite</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/12/23/flink-master-5/">Flink Master 详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/19/flink-execution-graph-4/">Flink 如何生成 ExecutionGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/09/flink-job-graph-3/">Flink Streaming 作业如何转化为 JobGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/08/flink-stream-graph-2/">Flink DataStream API 概述及作业如何转换为 StreamGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/flink-learn-start-1/">Apache Flink 初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/paper-chandy-lamport/">Paper 阅读: Distributed Snapshots: Determining Global States of Distributed Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/20/paper-flink-snapshot/">Paper 阅读: Lightweight Asynchronous Snapshots for Distributed Dataflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/19/paper-ray1/">Paper 阅读: Real-Time Machine Learning: The Missing Pieces</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/17/apache-calcite-planner/">Apache Calcite 优化器详解（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://matt33.com/about/" title="个人公众号：柳年思水" target="_blank">个人公众号：柳年思水</a><ul></ul><a href="http://tech.meituan.com/" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://jm.taobao.org/" title="阿里中间件团队博客" target="_blank">阿里中间件团队博客</a><ul></ul><a href="http://www.jianshu.com/" title="简书" target="_blank">简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matt's Blog 柳年思水.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><div class="analytics"><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1256517224'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1256517224%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script></div><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-64518924-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5cf44757fa0d23bc7637935e44a9104a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>