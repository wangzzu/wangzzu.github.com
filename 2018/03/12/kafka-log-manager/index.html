<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="与一群有趣的人，做一些有趣的事."><title>Kafka 源码解析之日志管理（十一） | Matt's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafka 源码解析之日志管理（十一）</h1><a id="logo" href="/.">Matt's Blog</a><p class="description">王蒙</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kafka 源码解析之日志管理（十一）</h1><div class="post-meta">Mar 12, 2018<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><span> | </span><span class="post-count">4,581</span><span> 字</span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Times</span></span></div><a data-disqus-identifier="2018/03/12/kafka-log-manager/" href="/2018/03/12/kafka-log-manager/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#日志的基本概念"><span class="toc-number">1.</span> <span class="toc-text">日志的基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#日志管理"><span class="toc-number">2.</span> <span class="toc-text">日志管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KafkaServer-启动-LogManager-线程"><span class="toc-number">2.1.</span> <span class="toc-text">KafkaServer 启动 LogManager 线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogManager-初始化"><span class="toc-number">2.2.</span> <span class="toc-text">LogManager 初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogManager-启动"><span class="toc-number">2.3.</span> <span class="toc-text">LogManager 启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#检查点文件"><span class="toc-number">2.4.</span> <span class="toc-text">检查点文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#日志刷新"><span class="toc-number">3.</span> <span class="toc-text">日志刷新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#日志清理"><span class="toc-number">4.</span> <span class="toc-text">日志清理</span></a></li></ol></div></div><div class="post-content"><p>上篇文章在介绍完 Kafka 的 GroupCoordinator 之后，下面开始介绍 Kafka 存储层的内容，也就是 Kafka Server 端 Log 部分的内容，Log 部分是 Kafka 比较底层的代码，日志的读写、分段、清理和管理都是在这一部分完成的，内容还是比较多的，会分为三篇左右的文章介绍，本篇先介绍最简单的部分，主要是日志的基本概念、日志管理、日志刷新和日志清理四部分（后两个其实也属于日志管理，为便于讲解，这里分开讲述），日志的读写和分段将在下一篇讲述。</p>
<p>本篇主要的内容如下：</p>
<ol>
<li>Kafka 中 Log 的基本概念；</li>
<li>日志管理；</li>
<li>日志刷新；</li>
<li>日志清理；</li>
</ol>
<h2 id="日志的基本概念"><a href="#日志的基本概念" class="headerlink" title="日志的基本概念"></a>日志的基本概念</h2><p>在 Kafka 的官方文档中，最开始介绍 Kafka 的一句话是：</p>
<blockquote>
<p>Kafka is a distributed, partitioned, replicated commit log service. （0.10.0 之前）</p>
<p>Apache Kafka is a distributed streaming platform. （0.10.0 及之后）</p>
</blockquote>
<p>可以说在 KafkaStream 之前，Kafka 最开始的应用场景就是日志场景或 mq 场景，更多的扮演着一个存储系统，这是 Kafka 立家之本。</p>
<p>Kafka 是一个分布式的（distributed）、可分区的（partitioned）、支持多副本（replicated）的日志提交系统，分布式这个概念很好理解，Kafka 本身就是一个分布式系统，那另外两个概念什么意思呢？</p>
<ul>
<li>可分区的：一个 topic 是可以设置多个分区的，可分区解决了单 topic 线性扩展的问题（也解决了负载均衡的问题）；</li>
<li>支持多副本的：使得 topic 可以做到更多容错性，牺牲性能与空间去换取更高的可靠性。</li>
</ul>
<p>一个 Topic 基本结果如下：</p>
<p><img src="/images/2016-03-07-KafkaMessage/topic.png" alt="Topic"></p>
<p>图中的 topic 由三个 partition 组成，topic 在创建开始，每个 partition 在写入时，其 offset 值是从0开始逐渐增加。topic 的 partition 是可以分配到 Kafka 集群的任何节点上，在实际存储时，每个 partition 是按 segment 文件去存储的（segment 的大小是在 server 端配置的，这就是日志的分段），如下图所示：</p>
<p><img src="/images/2016-03-07-KafkaMessage/segment.png" alt="Segment"></p>
<blockquote>
<p>注：上图是 0.8.2.1 版的 segment 的结构，0.10.2.0 版每个 segment 还会有一个对应的 timestrap 文件。</p>
</blockquote>
<p>再简单介绍一下 topic 的副本的概念，kafka 中为了保证一定可靠性，一般会为设置多个副本，假设一个 topic 设置了三个副本：</p>
<ul>
<li>每个 partition 都会有三个副本，这个三个副本需要分配在不同的 broker 上，在同一台 broker 上的话，就没有什么意义了；</li>
<li>这个三个副本中，会有选举出来了一个 leader，另外两个就是 follower，topic 的读写都是在 leader 上进行的，follower 从 leader 同步 partition 的数据。</li>
</ul>
<blockquote>
<p>follower 不支持读的原因，个人感觉是对于流式系统而言，如果允许 follower 也可以读的话，数据一致性、可见性将会很难保证，对最初 Kafka 的设计将会带来很大的复杂性。</p>
</blockquote>
<p>有了对 topic、partition、副本（replica）、segment、leader、follower 概念的理解之后，下面再看 Kafka 存储层的内容，就不会那么云里雾里了。 </p>
<h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><p>Kafka 的日志管理（LogManager）主要的作用是负责日志的创建、检索、清理，日志相关的读写操作实际上是由日志实例对象（Log）来处理的。</p>
<h3 id="KafkaServer-启动-LogManager-线程"><a href="#KafkaServer-启动-LogManager-线程" class="headerlink" title="KafkaServer 启动 LogManager 线程"></a>KafkaServer 启动 LogManager 线程</h3><p>LogManager 线程是在节点的 Kafka 服务启动时启动的，相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//kafka.server.KafkaServer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    info(<span class="string">"starting"</span>)</span><br><span class="line">    <span class="comment">/* start log manager */</span></span><br><span class="line">    <span class="comment">//note: 启动日志管理线程</span></span><br><span class="line">    logManager = createLogManager(zkUtils.zkClient, brokerState)</span><br><span class="line">    logManager.startup()</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</span><br><span class="line">    isStartingUp.set(<span class="literal">false</span>)</span><br><span class="line">    shutdown()</span><br><span class="line">    <span class="keyword">throw</span> e</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createLogManager</span></span>(zkClient: <span class="type">ZkClient</span>, brokerState: <span class="type">BrokerState</span>): <span class="type">LogManager</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> defaultProps = <span class="type">KafkaServer</span>.copyKafkaConfigToLog(config)</span><br><span class="line">  <span class="keyword">val</span> defaultLogConfig = <span class="type">LogConfig</span>(defaultProps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> configs = <span class="type">AdminUtils</span>.fetchAllTopicConfigs(zkUtils).map &#123; <span class="keyword">case</span> (topic, configs) =&gt;</span><br><span class="line">    topic -&gt; <span class="type">LogConfig</span>.fromProps(defaultProps, configs)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// read the log configurations from zookeeper</span></span><br><span class="line">  <span class="keyword">val</span> cleanerConfig = <span class="type">CleanerConfig</span>(numThreads = config.logCleanerThreads, <span class="comment">//note: 日志清理线程数,默认是1</span></span><br><span class="line">                                    dedupeBufferSize = config.logCleanerDedupeBufferSize, <span class="comment">//note: 日志清理使用的总内容,默认128MB</span></span><br><span class="line">                                    dedupeBufferLoadFactor = config.logCleanerDedupeBufferLoadFactor, <span class="comment">//note:  buffer load factor</span></span><br><span class="line">                                    ioBufferSize = config.logCleanerIoBufferSize, <span class="comment">//note:</span></span><br><span class="line">                                    maxMessageSize = config.messageMaxBytes, <span class="comment">//note:</span></span><br><span class="line">                                    maxIoBytesPerSecond = config.logCleanerIoMaxBytesPerSecond, <span class="comment">//note:</span></span><br><span class="line">                                    backOffMs = config.logCleanerBackoffMs, <span class="comment">//note: 没有日志清理时的 sleep 时间,默认 15s</span></span><br><span class="line">                                    enableCleaner = config.logCleanerEnable) <span class="comment">//note: 是否允许对 compact 日志进行清理</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">LogManager</span>(logDirs = config.logDirs.map(<span class="keyword">new</span> <span class="type">File</span>(_)).toArray, <span class="comment">//note: 日志目录列表</span></span><br><span class="line">                 topicConfigs = configs,</span><br><span class="line">                 defaultConfig = defaultLogConfig,</span><br><span class="line">                 cleanerConfig = cleanerConfig,</span><br><span class="line">                 ioThreads = config.numRecoveryThreadsPerDataDir,<span class="comment">//note: 每个日志目录在开始时用日志恢复以及关闭时日志flush的线程数,默认1</span></span><br><span class="line">                 flushCheckMs = config.logFlushSchedulerIntervalMs,</span><br><span class="line">                 flushCheckpointMs = config.logFlushOffsetCheckpointIntervalMs, <span class="comment">//note: 更新 check-point 的频率,默认是60s</span></span><br><span class="line">                 retentionCheckMs = config.logCleanupIntervalMs, <span class="comment">//note: log-cleaner 检查 topic 是否需要删除的频率,默认是5min</span></span><br><span class="line">                 scheduler = kafkaScheduler,</span><br><span class="line">                 brokerState = brokerState,</span><br><span class="line">                 time = time)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="LogManager-初始化"><a href="#LogManager-初始化" class="headerlink" title="LogManager 初始化"></a>LogManager 初始化</h3><p>LogManager 在初始化时，首先会检查 server 端配置的日志目录信息，然后会加载日志目录下的所有分区日志，其实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogManager</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">  <span class="comment">//note: 检查点表示日志已经刷新到磁盘的位置，主要是用于数据恢复</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">RecoveryPointCheckpointFile</span> = <span class="string">"recovery-point-offset-checkpoint"</span> <span class="comment">//note: 检查点文件</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logs = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">TopicPartition</span>, <span class="type">Log</span>]() <span class="comment">//note: 分区与日志实例的对应关系</span></span><br><span class="line"></span><br><span class="line">  createAndValidateLogDirs(logDirs) <span class="comment">//note: 检查日志目录</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dirLocks = lockLogDirs(logDirs)</span><br><span class="line">  <span class="comment">//note: 每个数据目录都有一个检查点文件,存储这个数据目录下所有分区的检查点信息</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> recoveryPointCheckpoints = logDirs.map(dir =&gt; (dir, <span class="keyword">new</span> <span class="type">OffsetCheckpoint</span>(<span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">RecoveryPointCheckpointFile</span>)))).toMap</span><br><span class="line">  loadLogs()</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//note: 创建指定的数据目录,并做相应的检查:</span></span><br><span class="line">  <span class="comment">//note: 1.确保数据目录中没有重复的数据目录;</span></span><br><span class="line">  <span class="comment">//note: 2.数据不存在的话就创建相应的目录;</span></span><br><span class="line">  <span class="comment">//note: 3.检查每个目录路径是否是可读的。</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAndValidateLogDirs</span></span>(dirs: <span class="type">Seq</span>[<span class="type">File</span>]) &#123;</span><br><span class="line">    <span class="keyword">if</span>(dirs.map(_.getCanonicalPath).toSet.size &lt; dirs.size)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Duplicate log directory found: "</span> + logDirs.mkString(<span class="string">", "</span>))</span><br><span class="line">    <span class="keyword">for</span>(dir &lt;- dirs) &#123;</span><br><span class="line">      <span class="keyword">if</span>(!dir.exists) &#123;</span><br><span class="line">        info(<span class="string">"Log directory '"</span> + dir.getAbsolutePath + <span class="string">"' not found, creating it."</span>)</span><br><span class="line">        <span class="keyword">val</span> created = dir.mkdirs()</span><br><span class="line">        <span class="keyword">if</span>(!created)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Failed to create data directory "</span> + dir.getAbsolutePath)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(!dir.isDirectory || !dir.canRead)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(dir.getAbsolutePath + <span class="string">" is not a readable log directory."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 加载所有的日志,而每个日志也会调用 loadSegments() 方法加载所有的分段,过程比较慢,所有每个日志都会创建一个单独的线程</span></span><br><span class="line">  <span class="comment">//note: 日志管理器采用线程池提交任务,标识不用的任务可以同时运行</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">loadLogs</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    info(<span class="string">"Loading logs."</span>)</span><br><span class="line">    <span class="keyword">val</span> startMs = time.milliseconds</span><br><span class="line">    <span class="keyword">val</span> threadPools = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">ExecutorService</span>]</span><br><span class="line">    <span class="keyword">val</span> jobs = mutable.<span class="type">Map</span>.empty[<span class="type">File</span>, <span class="type">Seq</span>[<span class="type">Future</span>[_]]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (dir &lt;- <span class="keyword">this</span>.logDirs) &#123; <span class="comment">//note: 处理每一个日志目录</span></span><br><span class="line">      <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(ioThreads) <span class="comment">//note: 默认为 1</span></span><br><span class="line">      threadPools.append(pool) <span class="comment">//note: 每个对应的数据目录都有一个线程池</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> cleanShutdownFile = <span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">Log</span>.<span class="type">CleanShutdownFile</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (cleanShutdownFile.exists) &#123;</span><br><span class="line">        debug(</span><br><span class="line">          <span class="string">"Found clean shutdown file. "</span> +</span><br><span class="line">          <span class="string">"Skipping recovery for all logs in data directory: "</span> +</span><br><span class="line">          dir.getAbsolutePath)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// log recovery itself is being performed by `Log` class during initialization</span></span><br><span class="line">        brokerState.newState(<span class="type">RecoveringFromUncleanShutdown</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">var</span> recoveryPoints = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read <span class="comment">//note: 读取检查点文件</span></span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          warn(<span class="string">"Error occured while reading recovery-point-offset-checkpoint file of directory "</span> + dir, e)</span><br><span class="line">          warn(<span class="string">"Resetting the recovery checkpoint to 0"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> jobsForDir = <span class="keyword">for</span> &#123;</span><br><span class="line">        dirContent &lt;- <span class="type">Option</span>(dir.listFiles).toList <span class="comment">//note: 数据目录下的所有日志目录</span></span><br><span class="line">        logDir &lt;- dirContent <span class="keyword">if</span> logDir.isDirectory <span class="comment">//note: 日志目录下每个分区目录</span></span><br><span class="line">      &#125; <span class="keyword">yield</span> &#123;</span><br><span class="line">        <span class="type">CoreUtils</span>.runnable &#123; <span class="comment">//note: 每个分区的目录都对应了一个线程</span></span><br><span class="line">          debug(<span class="string">"Loading log '"</span> + logDir.getName + <span class="string">"'"</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> topicPartition = <span class="type">Log</span>.parseTopicPartitionName(logDir)</span><br><span class="line">          <span class="keyword">val</span> config = topicConfigs.getOrElse(topicPartition.topic, defaultConfig)</span><br><span class="line">          <span class="keyword">val</span> logRecoveryPoint = recoveryPoints.getOrElse(topicPartition, <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> current = <span class="keyword">new</span> <span class="type">Log</span>(logDir, config, logRecoveryPoint, scheduler, time)<span class="comment">//note: 创建 Log 对象后，初始化时会加载所有的 segment</span></span><br><span class="line">          <span class="keyword">if</span> (logDir.getName.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) &#123; <span class="comment">//note: 该目录被标记为删除</span></span><br><span class="line">            <span class="keyword">this</span>.logsToBeDeleted.add(current)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">val</span> previous = <span class="keyword">this</span>.logs.put(topicPartition, current) <span class="comment">//note: 创建日志后,加入日志管理的映射表</span></span><br><span class="line">            <span class="keyword">if</span> (previous != <span class="literal">null</span>) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span><br><span class="line">                <span class="string">"Duplicate log directories found: %s, %s!"</span>.format(</span><br><span class="line">                  current.dir.getAbsolutePath, previous.dir.getAbsolutePath))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      jobs(cleanShutdownFile) = jobsForDir.map(pool.submit).toSeq <span class="comment">//note: 提交任务</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> ((cleanShutdownFile, dirJobs) &lt;- jobs) &#123;</span><br><span class="line">        dirJobs.foreach(_.get)</span><br><span class="line">        cleanShutdownFile.delete()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">ExecutionException</span> =&gt; &#123;</span><br><span class="line">        error(<span class="string">"There was an error in one of the threads during logs loading: "</span> + e.getCause)</span><br><span class="line">        <span class="keyword">throw</span> e.getCause</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      threadPools.foreach(_.shutdown())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    info(<span class="string">s"Logs loading complete in <span class="subst">$&#123;time.milliseconds - startMs&#125;</span> ms."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始化 LogManger 代码有两个主要方法：</p>
<ol>
<li><code>createAndValidateLogDirs()</code>：创建指定的数据目录，并做相应的检查： 1.确保数据目录中没有重复的数据目录、2.数据目录不存在的话就创建相应的目录；3. 检查每个目录路径是否是可读的；</li>
<li><code>loadLogs()</code>：加载所有的日志分区，而每个日志也会调用 <code>loadSegments()</code> 方法加载该分区所有的 segment 文件，过程比较慢，所以 LogManager 使用线程池的方式，为每个日志的加载都会创建一个单独的线程。</li>
</ol>
<p>虽然使用的是线程池提交任务，并发进行 load 分区日志，但这个任务本身是阻塞式的，只有当所有的分区日志加载完成，才能调用 <code>startup()</code> 启动 LogManager 线程。</p>
<h3 id="LogManager-启动"><a href="#LogManager-启动" class="headerlink" title="LogManager 启动"></a>LogManager 启动</h3><p>在日志目录的所有分区日志都加载完成后，KafkaServer 调用 <code>startup()</code> 方法启动 LogManager 线程，LogManager 启动后，后台会运行四个定时任务，代码实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  <span class="comment">/* Schedule the cleanup task to delete old logs */</span></span><br><span class="line">  <span class="keyword">if</span>(scheduler != <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">//note: 定时清理过期的日志 segment,并维护日志的大小</span></span><br><span class="line">    info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</span><br><span class="line">    scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</span><br><span class="line">                       cleanupLogs,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = retentionCheckMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    <span class="comment">//note: 定时刷新还没有写到磁盘上日志</span></span><br><span class="line">    info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</span><br><span class="line">    scheduler.schedule(<span class="string">"kafka-log-flusher"</span>,</span><br><span class="line">                       flushDirtyLogs,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = flushCheckMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    <span class="comment">//note: 定时将所有数据目录所有日志的检查点写到检查点文件中</span></span><br><span class="line">    scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</span><br><span class="line">                       checkpointRecoveryPointOffsets,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = flushCheckpointMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    <span class="comment">//note: 定时删除标记为 delete 的日志文件</span></span><br><span class="line">    scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</span><br><span class="line">                       deleteLogs,</span><br><span class="line">                       delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">                       period = defaultConfig.fileDeleteDelayMs,</span><br><span class="line">                       <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 如果设置为 true， 自动清理 compaction 类型的 topic</span></span><br><span class="line">  <span class="keyword">if</span>(cleanerConfig.enableCleaner)</span><br><span class="line">    cleaner.startup()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>四个后台定时线程的作用：</p>
<ol>
<li><code>cleanupLogs</code>：定时清理过期的日志 segment，并维护日志的大小（默认5min）；</li>
<li><code>flushDirtyLogs</code>：定时刷新将还没有写到磁盘上日志刷新到磁盘（默认 无限大）；</li>
<li><code>checkpointRecoveryPointOffsets</code>：定时将所有数据目录所有日志的检查点写到检查点文件中（默认 60s）；</li>
<li><code>deleteLogs</code>：定时删除标记为 delete 的日志文件（默认 30s）。</li>
</ol>
<h3 id="检查点文件"><a href="#检查点文件" class="headerlink" title="检查点文件"></a>检查点文件</h3><p>在 LogManager 中有一个非常重要的文件——检查点文件：</p>
<ol>
<li>Kafka 启动时创建 LogManager，读取检查点文件，并把每个分区对应的检查点（checkPoint）作为日志的恢复点（recoveryPoint），最后创建分区对应的日志实例；</li>
<li>消息追加到分区对应的日志，在刷新日志时，将最新的偏移量作为日志的检查点（也即是刷新日志时，会更新检查点位置）；</li>
<li>LogManager 会启动一个定时任务，读取所有日志的检查点，并写入全局的检查点文件（定时将检查点的位置更新到检查点文件中）。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note：通常所有数据目录都会一起执行，不会专门操作某一个数据目录的检查点文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpointRecoveryPointOffsets</span></span>() &#123;</span><br><span class="line">  <span class="keyword">this</span>.logDirs.foreach(checkpointLogsInDir)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Make a checkpoint for all logs in provided directory.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 对数据目录下的所有日志（即所有分区），将其检查点写入检查点文件</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkpointLogsInDir</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> recoveryPoints = <span class="keyword">this</span>.logsByDir.get(dir.toString)</span><br><span class="line">  <span class="keyword">if</span> (recoveryPoints.isDefined) &#123;</span><br><span class="line">    <span class="keyword">this</span>.recoveryPointCheckpoints(dir).write(recoveryPoints.get.mapValues(_.recoveryPoint))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里留一个问题：启动时，如果发现检查点文件的 offset 比 segment 中最大的 offset 小时（最新的检查点在更新到文件前机器宕机了），应该怎么处理？答案将在下一篇文章中讲述。</p>
</blockquote>
<h2 id="日志刷新"><a href="#日志刷新" class="headerlink" title="日志刷新"></a>日志刷新</h2><p>日志管理器会定时调度 <code>flushDirtyLogs()</code> 方法，定期将页面缓存中的数据真正刷新到磁盘文件中。如果缓存中的数据（在 pagecache 中）在 flush 到磁盘之前，Broker 宕机了，那么会导致数据丢失（多副本减少了这个风险）。</p>
<p>在 Kafka 中有两种策略，将日志刷新到磁盘上：</p>
<ul>
<li>时间策略，（<code>log.flush.interval.ms</code> 中配置调度周期，默认为无限大，即选择大小策略）：</li>
<li>大小策略，（<code>log.flush.interval.messages</code> 中配置当未刷新的 msg 数超过这个值后，进行刷新）。</li>
</ul>
<p>LogManager 刷新日志的实现方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: LogManager 启动时，会启动一个周期性调度任务，调度这个方法，定时刷新日志。</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">flushDirtyLogs</span></span>() = &#123;</span><br><span class="line">  debug(<span class="string">"Checking for dirty logs to flush..."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> ((topicPartition, log) &lt;- logs) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//note: 每个日志的刷新时间并不相同</span></span><br><span class="line">      <span class="keyword">val</span> timeSinceLastFlush = time.milliseconds - log.lastFlushTime</span><br><span class="line">      debug(<span class="string">"Checking if flush is needed on "</span> + topicPartition.topic + <span class="string">" flush interval  "</span> + log.config.flushMs +</span><br><span class="line">            <span class="string">" last flushed "</span> + log.lastFlushTime + <span class="string">" time since last flush: "</span> + timeSinceLastFlush)</span><br><span class="line">      <span class="keyword">if</span>(timeSinceLastFlush &gt;= log.config.flushMs)</span><br><span class="line">        log.flush</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        error(<span class="string">"Error flushing topic "</span> + topicPartition.topic, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>LogManager 这个方法最后的结果还是调用了 <code>log.flush()</code> 进行刷新操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flush all log segments</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(): <span class="type">Unit</span> = flush(<span class="keyword">this</span>.logEndOffset)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Flush log segments for all offsets up to offset-1</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param offset The offset to flush up to (non-inclusive); the new recovery point</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(offset: <span class="type">Long</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (offset &lt;= <span class="keyword">this</span>.recoveryPoint)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  debug(<span class="string">"Flushing log '"</span> + name + <span class="string">" up to offset "</span> + offset + <span class="string">", last flushed: "</span> + lastFlushTime + <span class="string">" current time: "</span> +</span><br><span class="line">        time.milliseconds + <span class="string">" unflushed = "</span> + unflushedMessages)</span><br><span class="line">  <span class="comment">//note: 刷新检查点到最新偏移量之间的所有日志分段</span></span><br><span class="line">  <span class="keyword">for</span>(segment &lt;- logSegments(<span class="keyword">this</span>.recoveryPoint, offset))</span><br><span class="line">    segment.flush()<span class="comment">//note: 刷新数据文件和索引文件（调用操作系统的 fsync）</span></span><br><span class="line">  lock synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span>(offset &gt; <span class="keyword">this</span>.recoveryPoint) &#123;</span><br><span class="line">      <span class="keyword">this</span>.recoveryPoint = offset</span><br><span class="line">      lastflushedTime.set(time.milliseconds)<span class="comment">//note: 更新刷新时间</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的内容实际上只是按 <code>log.flush.interval.ms</code> 设置去 flush 日志到磁盘，那么 <code>log.flush.interval.messages</code> 策略是在什么地方生效的呢？用心想一下，大家应该能猜出来，是在数据追加到 Log 中的时候，这时候会判断没有 flush 的数据大小是否达到阈值，具体实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 其他部分这里暂时忽略了</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</span><br><span class="line">  <span class="comment">// now append to the log</span></span><br><span class="line">  segment.append(firstOffset = appendInfo.firstOffset,</span><br><span class="line">    largestOffset = appendInfo.lastOffset,</span><br><span class="line">    largestTimestamp = appendInfo.maxTimestamp,</span><br><span class="line">    shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</span><br><span class="line">    records = validRecords)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// increment the log end offset</span></span><br><span class="line">  updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></span><br><span class="line">    .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</span><br><span class="line">    flush()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h2><p>为了保证分区的总大小不超过阈值（<code>log.retention.bytes</code>），日志管理器会定时清理旧的数据。</p>
<blockquote>
<p>不过一般情况下，都是通过配置 <code>log.retention.hours</code> 来配置 segment 的保存时间，而不是通过单日志的总大小配置，因为不同的 topic，其 partition 大小相差很大，导致最后的保存时间可能也不一致，不利于管理。</p>
</blockquote>
<p>清理旧日志分段方法，主要有两种：</p>
<ol>
<li>删除：超过时间或大小阈值的旧 segment，直接进行删除；</li>
<li>压缩：不是直接删除日志分段，而是采用合并压缩的方式进行。</li>
</ol>
<p>这里主要讲述第一种方法，第二种将会后续文章介绍。</p>
<p>先看下 LogManager 中日志清除任务的实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Delete any eligible logs. Return the number of segments deleted.</span></span><br><span class="line"><span class="comment"> * Only consider logs that are not compacted.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 日志清除任务</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanupLogs</span></span>() &#123;</span><br><span class="line">  debug(<span class="string">"Beginning log cleanup..."</span>)</span><br><span class="line">  <span class="keyword">var</span> total = <span class="number">0</span></span><br><span class="line">  <span class="keyword">val</span> startMs = time.milliseconds</span><br><span class="line">  <span class="keyword">for</span>(log &lt;- allLogs; <span class="keyword">if</span> !log.config.compact) &#123;</span><br><span class="line">    debug(<span class="string">"Garbage collecting '"</span> + log.name + <span class="string">"'"</span>)</span><br><span class="line">    total += log.deleteOldSegments() <span class="comment">//note: 清理过期的 segment</span></span><br><span class="line">  &#125;</span><br><span class="line">  debug(<span class="string">"Log cleanup completed. "</span> + total + <span class="string">" files deleted in "</span> +</span><br><span class="line">                (time.milliseconds - startMs) / <span class="number">1000</span> + <span class="string">" seconds"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>日志清除任务的实现还是在 Log 的 <code>deleteOldSegments()</code> 中实现的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Delete any log segments that have either expired due to time based retention</span></span><br><span class="line"><span class="comment">  * or because the log size is &gt; retentionSize</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!config.delete) <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  deleteRetenionMsBreachedSegments() + deleteRetentionSizeBreachedSegments()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 清除保存时间满足条件的 segment</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetenionMsBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (config.retentionMs &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">val</span> startMs = time.milliseconds</span><br><span class="line">  deleteOldSegments(startMs - _.largestTimestamp &gt; config.retentionMs)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: 清除保存大小满足条件的 segment</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetentionSizeBreachedSegments</span></span>() : <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (config.retentionSize &lt; <span class="number">0</span> || size &lt; config.retentionSize) <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> diff = size - config.retentionSize</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shouldDelete</span></span>(segment: <span class="type">LogSegment</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (diff - segment.size &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">      diff -= segment.size</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  deleteOldSegments(shouldDelete)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>清除日志的两个方法：</p>
<ol>
<li><code>deleteRetenionMsBreachedSegments()</code>：如果 segment 保存时间超过设置的时间，那么进行删除；</li>
<li><code>deleteRetentionSizeBreachedSegments()</code>：如果当前最新的日志大小减少下一个即将删除的 segment 分段的大小超过阈值，那么就允许删除该 segment，否则就不允许。</li>
</ol>
<p>调用 <code>deleteOldSegments()</code> 方法删除日志数据文件及索引文件的具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 清除相应的 segment 及相应的索引文件</span></span><br><span class="line"><span class="comment">//note: 其中 predicate 是一个高阶函数，只有返回值为 true 该 segment 才会被删除</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(predicate: <span class="type">LogSegment</span> =&gt; <span class="type">Boolean</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  lock synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> deletable = deletableSegments(predicate)</span><br><span class="line">    <span class="keyword">val</span> numToDelete = deletable.size</span><br><span class="line">    <span class="keyword">if</span> (numToDelete &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// we must always have at least one segment, so if we are going to delete all the segments, create a new one first</span></span><br><span class="line">      <span class="keyword">if</span> (segments.size == numToDelete)</span><br><span class="line">        roll()</span><br><span class="line">      <span class="comment">// remove the segments for lookups</span></span><br><span class="line">      deletable.foreach(deleteSegment) <span class="comment">//note: 删除 segment</span></span><br><span class="line">    &#125;</span><br><span class="line">    numToDelete</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</span><br><span class="line">  info(<span class="string">"Scheduling log segment %d for log %s for deletion."</span>.format(segment.baseOffset, name))</span><br><span class="line">  lock synchronized &#123;</span><br><span class="line">    segments.remove(segment.baseOffset) <span class="comment">//note:  从映射关系表中删除数据</span></span><br><span class="line">    asyncDeleteSegment(segment) <span class="comment">//note: 异步删除日志 segment</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Perform an asynchronous delete on the given file if it exists (otherwise do nothing)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @throws KafkaStorageException if the file can't be renamed and still exists</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">asyncDeleteSegment</span></span>(segment: <span class="type">LogSegment</span>) &#123;</span><br><span class="line">  segment.changeFileSuffixes(<span class="string">""</span>, <span class="type">Log</span>.<span class="type">DeletedFileSuffix</span>) <span class="comment">//note: 先将 segment 的数据文件和索引文件后缀添加 `.deleted`</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteSeg</span></span>() &#123;</span><br><span class="line">    info(<span class="string">"Deleting segment %d from log %s."</span>.format(segment.baseOffset, name))</span><br><span class="line">    segment.delete()</span><br><span class="line">  &#125;</span><br><span class="line">  scheduler.schedule(<span class="string">"delete-file"</span>, deleteSeg, delay = config.fileDeleteDelayMs) <span class="comment">//note: 异步调度进行删除</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的讲解来看，Kafka LogManager 线程工作还是比较清晰简洁的，它的作用就是负责日志的创建、检索、清理，并不负责日志的读写等实际操作。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matt33.com/2018/03/12/kafka-log-manager/" data-id="ck3wirblg007ho7za8forlnxx" class="article-share-link">分享到</a><div class="copyright"><a href="http://matt33.com/copyright/">博客版权说明</a></div><div class="tags"><a href="/tags/kafka/">kafka</a></div><div class="post-nav"><a href="/2018/03/18/kafka-server-handle-produce-request/" class="pre">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a><a href="/2018/02/04/linux-mmap/" class="next">操作系统之共享对象学习</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button></div><script>var disqus_shortname = 'http-matt33-com';
var disqus_identifier = '2018/03/12/kafka-log-manager/';
var disqus_title = 'Kafka 源码解析之日志管理（十一）';
var disqus_url = 'http://matt33.com/2018/03/12/kafka-log-manager/';
$('.btn_click_load').click(function() {
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('.btn_click_load').css('display','none');
});
$.ajax({
  url: 'https://disqus.com/favicon.ico',
  timeout: 3000,
  type: 'GET',
  success: (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    $('.btn_click_load').css('display','none');
  })(),
  error: function() {
    $('.btn_click_load').css('display','block');
  }
});</script><script id="dsq-count-scr" src="//http-matt33-com.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-weibo"> 微博</i></div><iframe width="100%" height="90" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=100&fansRow=1&ptype=1&speed=0&skin=1&isTitle=0&noborder=1&isWeibo=0&isFans=0&uid=2650396571&verifier=f2f0e397&colors=D8D8D8,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书屋/">书屋</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影如人生/">影如人生</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/旅行/">旅行</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/转载/">转载</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tcp/" style="font-size: 15px;">tcp</a> <a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/storm/" style="font-size: 15px;">storm</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/learn/" style="font-size: 15px;">learn</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/travel/" style="font-size: 15px;">travel</a> <a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/database/" style="font-size: 15px;">database</a> <a href="/tags/电影随想/" style="font-size: 15px;">电影随想</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/转载/" style="font-size: 15px;">转载</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/jvm/" style="font-size: 15px;">jvm</a> <a href="/tags/web/" style="font-size: 15px;">web</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a> <a href="/tags/bk/" style="font-size: 15px;">bk</a> <a href="/tags/rpc/" style="font-size: 15px;">rpc</a> <a href="/tags/thrift/" style="font-size: 15px;">thrift</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/calcite/" style="font-size: 15px;">calcite</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/12/08/flink-stream-graph-2/">Flink DataStream API 概述及 StreamGraph 如何转换</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/flink-learn-start-1/">Apache Flink 初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/paper-chandy-lamport/">Paper 阅读: Distributed Snapshots: Determining Global States of Distributed Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/20/paper-flink-snapshot/">Paper 阅读: Lightweight Asynchronous Snapshots for Distributed Dataflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/19/paper-ray1/">Paper 阅读: Real-Time Machine Learning: The Missing Pieces</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/17/apache-calcite-planner/">Apache Calcite 优化器详解（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/28/bk-store-realize/">BookKeeper 原理浅谈</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/21/effective-learning/">如何高效学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/04/kafka-transaction/">Kafka Exactly-Once 之事务性实现</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://matt33.com/about/" title="个人公众号：柳年思水" target="_blank">个人公众号：柳年思水</a><ul></ul><a href="http://tech.meituan.com/" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://jm.taobao.org/" title="阿里中间件团队博客" target="_blank">阿里中间件团队博客</a><ul></ul><a href="http://www.jianshu.com/" title="简书" target="_blank">简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matt's Blog 柳年思水.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><div class="analytics"><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1256517224'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1256517224%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script></div><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-64518924-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5cf44757fa0d23bc7637935e44a9104a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>