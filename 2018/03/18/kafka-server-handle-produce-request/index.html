<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="与一群有趣的人，做一些有趣的事."><title>Kafka 源码解析之 Server 端如何处理 Produce 请求（十二） | Matt's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</h1><a id="logo" href="/.">Matt's Blog</a><p class="description">王蒙</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</h1><div class="post-meta">Mar 18, 2018<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><span> | </span><span class="post-count">6,664</span><span> 字</span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Times</span></span></div><a data-disqus-identifier="2018/03/18/kafka-server-handle-produce-request/" href="/2018/03/18/kafka-server-handle-produce-request/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#produce-请求处理整体流程"><span class="toc-number">1.</span> <span class="toc-text">produce 请求处理整体流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Server-端处理"><span class="toc-number">2.</span> <span class="toc-text">Server 端处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KafkaApis-处理-Produce-请求"><span class="toc-number">2.1.</span> <span class="toc-text">KafkaApis 处理 Produce 请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReplicaManager"><span class="toc-number">2.2.</span> <span class="toc-text">ReplicaManager</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#appendRecords-实现"><span class="toc-number">2.2.1.</span> <span class="toc-text">appendRecords() 实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#appendToLocalLog-的实现"><span class="toc-number">2.2.2.</span> <span class="toc-text">appendToLocalLog() 的实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Partition-appendRecordsToLeader-方法"><span class="toc-number">2.3.</span> <span class="toc-text">Partition.appendRecordsToLeader() 方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#存储层"><span class="toc-number">3.</span> <span class="toc-text">存储层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Log-对象"><span class="toc-number">3.1.</span> <span class="toc-text">Log 对象</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#日志写入"><span class="toc-number">3.1.1.</span> <span class="toc-text">日志写入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#日志分段"><span class="toc-number">3.1.2.</span> <span class="toc-text">日志分段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#offset-索引文件"><span class="toc-number">3.1.3.</span> <span class="toc-text">offset 索引文件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogSegment-写入"><span class="toc-number">3.2.</span> <span class="toc-text">LogSegment 写入</span></a></li></ol></li></ol></div></div><div class="post-content"><p>这部分想了很久应该怎么去写才能更容易让大家明白，本来是计划先把 Kafka 存储层 Log 这块的写操作处理流程先详细介绍一下，但是这块属于比较底层的部分，大家可能对于这部分在整个处理过程处在哪个位置并不是很清楚，所以还是准备以 Server 端如何处理 Producer Client 的 Produce 请求为入口。但是 Server 端的内容较多，本篇文章并不能全部涵盖，涉及到其他内容，在本篇文章暂时先不详细讲述，后面会再分析，本篇文章会以 Server 处理 produce 为主线，主要详细讲解 Kafka 存储层的内容。</p>
<h2 id="produce-请求处理整体流程"><a href="#produce-请求处理整体流程" class="headerlink" title="produce 请求处理整体流程"></a>produce 请求处理整体流程</h2><p>根据在这篇 <a href="http://matt33.com/2017/06/25/kafka-producer-send-module/">Kafka 源码解析之 Producer 发送模型（一）</a> 中的讲解，在 Producer Client 端，Producer 会维护一个 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches</code> 的变量，然后会根据 topic-partition 的 leader 信息，将 leader 在同一台机器上的 batch 放在一个 request 中，发送到 server，这样可以节省很多网络开销，提高发送效率。</p>
<p>Producer Client 发送请求的方法实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//<span class="doctag">NOTE:</span> 发送 produce 请求</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;RecordBatch&gt; batches)</span> </span>&#123;</span><br><span class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">for</span> (RecordBatch batch : batches) &#123;</span><br><span class="line">        TopicPartition tp = batch.topicPartition;</span><br><span class="line">        produceRecordsByPartition.put(tp, batch.records());</span><br><span class="line">        recordsByPartition.put(tp, batch);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ProduceRequest.Builder requestBuilder =</span><br><span class="line">            <span class="keyword">new</span> ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</span><br><span class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</span><br><span class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    String nodeId = Integer.toString(destination);</span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, callback);</span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">    log.trace(<span class="string">"Sent produce request to &#123;&#125;: &#123;&#125;"</span>, nodeId, requestBuilder);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在发送 Produce 的请求里，Client 是把一个 <code>Map&lt;TopicPartition, MemoryRecords&gt;</code> 类型的 <code>produceRecordsByPartition</code> 作为内容发送给了 Server 端，那么 Server 端是如何处理这个请求的呢？这就是本篇文章要讲述的内容，Server 处理这个请求的总体逻辑如下图所示：</p>
<p><img src="/images/kafka/kafka_produce_process.png" alt="Server 端处理 produce 请求的总体过程"></p>
<p>Broker 在收到 Produce 请求后，会有一个 KafkaApis 进行处理，KafkaApis 是 Server 端处理所有请求的入口，它会负责将请求的具体处理交给相应的组件进行处理，从上图可以看到 Produce 请求是交给了 ReplicaManager 对象进行处理了。</p>
<h2 id="Server-端处理"><a href="#Server-端处理" class="headerlink" title="Server 端处理"></a>Server 端处理</h2><p>Server 端的处理过程会按照上图的流程一块一块去介绍。</p>
<h3 id="KafkaApis-处理-Produce-请求"><a href="#KafkaApis-处理-Produce-请求" class="headerlink" title="KafkaApis 处理 Produce 请求"></a>KafkaApis 处理 Produce 请求</h3><p>KafkaApis 处理 produce 请求是在 <code>handleProducerRequest()</code> 方法中完成，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Handle a produce request</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleProducerRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> produceRequest = request.body.asInstanceOf[<span class="type">ProduceRequest</span>]</span><br><span class="line">  <span class="keyword">val</span> numBytesAppended = request.header.sizeOf + produceRequest.sizeOf</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 按 exist 和有 Describe 权限进行筛选</span></span><br><span class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = produceRequest.partitionRecords.asScala.partition &#123;</span><br><span class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic)) &amp;&amp; metadataCache.contains(topicPartition.topic)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 判断有没有 Write 权限</span></span><br><span class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForWriteRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</span><br><span class="line">    <span class="keyword">case</span> (topicPartition, _) =&gt; authorize(request.session, <span class="type">Write</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, topicPartition.topic))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the callback for sending a produce response</span></span><br><span class="line">  <span class="comment">//note: 回调函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mergedResponseStatus = responseStatus ++</span><br><span class="line">      unauthorizedForWriteRequestInfo.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)) ++</span><br><span class="line">      nonExistingOrUnauthorizedForDescribeTopics.mapValues(_ =&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> errorInResponse = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    mergedResponseStatus.foreach &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (status.error != <span class="type">Errors</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">        errorInResponse = <span class="literal">true</span></span><br><span class="line">        debug(<span class="string">"Produce request with correlation id %d from client %s on partition %s failed due to %s"</span>.format(</span><br><span class="line">          request.header.correlationId,</span><br><span class="line">          request.header.clientId,</span><br><span class="line">          topicPartition,</span><br><span class="line">          status.error.exceptionName))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">produceResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// no operation needed if producer request.required.acks = 0; however, if there is any error in handling</span></span><br><span class="line">        <span class="comment">// the request, since no response is expected by the producer, the server will close socket server so that</span></span><br><span class="line">        <span class="comment">// the producer client will know that some error has happened and will refresh its metadata</span></span><br><span class="line">        <span class="comment">//note: 因为设置的 ack=0, 相当于 client 会默认发送成功了,如果 server 在处理的过程出现了错误,那么就会关闭 socket 连接来间接地通知 client</span></span><br><span class="line">        <span class="comment">//note: client 会重新刷新 meta,重新建立相应的连接</span></span><br><span class="line">        <span class="keyword">if</span> (errorInResponse) &#123;</span><br><span class="line">          <span class="keyword">val</span> exceptionsSummary = mergedResponseStatus.map &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</span><br><span class="line">            topicPartition -&gt; status.error.exceptionName</span><br><span class="line">          &#125;.mkString(<span class="string">", "</span>)</span><br><span class="line">          info(</span><br><span class="line">            <span class="string">s"Closing connection due to error during produce request with correlation id <span class="subst">$&#123;request.header.correlationId&#125;</span> "</span> +</span><br><span class="line">              <span class="string">s"from client id <span class="subst">$&#123;request.header.clientId&#125;</span> with ack=0\n"</span> +</span><br><span class="line">              <span class="string">s"Topic and partition to exceptions: <span class="subst">$exceptionsSummary</span>"</span></span><br><span class="line">          )</span><br><span class="line">          requestChannel.closeConnection(request.processor, request)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          requestChannel.noOperation(request.processor, request)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> respBody = request.header.apiVersion <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="number">0</span> =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava)</span><br><span class="line">          <span class="keyword">case</span> version@(<span class="number">1</span> | <span class="number">2</span>) =&gt; <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava, delayTimeMs, version)</span><br><span class="line">          <span class="comment">// This case shouldn't happen unless a new version of ProducerRequest is added without</span></span><br><span class="line">          <span class="comment">// updating this part of the code to handle it properly.</span></span><br><span class="line">          <span class="keyword">case</span> version =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Version `<span class="subst">$version</span>` of ProduceRequest is not handled. Code must be updated."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, respBody))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></span><br><span class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</span><br><span class="line"></span><br><span class="line">    quotas.produce.recordAndMaybeThrottle(</span><br><span class="line">      request.session.sanitizedUser,</span><br><span class="line">      request.header.clientId,</span><br><span class="line">      numBytesAppended,</span><br><span class="line">      produceResponseCallback)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</span><br><span class="line">    sendResponseCallback(<span class="type">Map</span>.empty)</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> internalTopicsAllowed = request.header.clientId == <span class="type">AdminUtils</span>.<span class="type">AdminClientId</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// call the replica manager to append messages to the replicas</span></span><br><span class="line">    <span class="comment">//note: 追加 Record</span></span><br><span class="line">    replicaManager.appendRecords(</span><br><span class="line">      produceRequest.timeout.toLong,</span><br><span class="line">      produceRequest.acks,</span><br><span class="line">      internalTopicsAllowed,</span><br><span class="line">      authorizedRequestInfo,</span><br><span class="line">      sendResponseCallback)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if the request is put into the purgatory, it will have a held reference</span></span><br><span class="line">    <span class="comment">// and hence cannot be garbage collected; hence we clear its data here in</span></span><br><span class="line">    <span class="comment">// order to let GC re-claim its memory since it is already appended to log</span></span><br><span class="line">    produceRequest.clearPartitionRecords()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总体来说，处理过程是（在权限系统的情况下）：</p>
<ol>
<li>查看 topic 是否存在，以及 client 是否有相应的 Desribe 权限；</li>
<li>对于已经有 Describe 权限的 topic 查看是否有 Write 权限；</li>
<li>调用 <code>replicaManager.appendRecords()</code> 方法向有 Write 权限的 topic-partition 追加相应的 record。</li>
</ol>
<h3 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h3><p>ReplicaManager 顾名思义，它就是副本管理器，副本管理器的作用是管理这台 broker 上的所有副本（replica）。在 Kafka 中，每个副本（replica）都会跟日志实例（Log 对象）一一对应，一个副本会对应一个 Log 对象。</p>
<p>Kafka Server 在启动的时候，会创建 ReplicaManager 对象，如下所示。在 ReplicaManager 的构造方法中，它需要 LogManager 作为成员变量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//kafka.server.KafkaServer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    info(<span class="string">"starting"</span>)</span><br><span class="line">    <span class="comment">/* start replica manager */</span></span><br><span class="line">    replicaManager = <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkUtils, kafkaScheduler, logManager, isShuttingDown, quotaManagers.follower)</span><br><span class="line">    replicaManager.startup()</span><br><span class="line">  &#125;<span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">    fatal(<span class="string">"Fatal error during KafkaServer startup. Prepare to shutdown"</span>, e)</span><br><span class="line">    isStartingUp.set(<span class="literal">false</span>)</span><br><span class="line">    shutdown()</span><br><span class="line">    <span class="keyword">throw</span> e</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ReplicaManager 的<strong>并不负责具体的日志创建，它只是管理 Broker 上的所有分区</strong>（也就是图中下一步的那个 Partition 对象）。在创建 Partition 对象时，它需要 ReplicaManager 的 logManager 对象，Partition 会通过这个 logManager 对象为每个 replica 创建对应的日志。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Partition</span>(<span class="params">val topic: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                val partitionId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                time: <span class="type">Time</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                replicaManager: <span class="type">ReplicaManager</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partitionId)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> localBrokerId = replicaManager.config.brokerId</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logManager = replicaManager.logManager <span class="comment">//note: 日志管理器</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ReplicaManager 与 LogManger 对比如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>管理对象</th>
<th>组成部分</th>
</tr>
</thead>
<tbody>
<tr>
<td>日志管理器（LogManager）</td>
<td>日志（Log）</td>
<td>日志分段（LogSegment）</td>
</tr>
<tr>
<td>副本管理器（ReplicaManager）</td>
<td>分区（Partition）</td>
<td>副本（Replica）</td>
</tr>
</tbody>
</table>
<p>关于 ReplicaManager 后面还会介绍，这篇文章先不详细展开。</p>
<h4 id="appendRecords-实现"><a href="#appendRecords-实现" class="headerlink" title="appendRecords() 实现"></a><code>appendRecords()</code> 实现</h4><p>下面我们来看 <code>appendRecords()</code> 方法的具体实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 向 partition 的 leader 写入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecords</span></span>(timeout: <span class="type">Long</span>,</span><br><span class="line">                  requiredAcks: <span class="type">Short</span>,</span><br><span class="line">                  internalTopicsAllowed: <span class="type">Boolean</span>,</span><br><span class="line">                  entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</span><br><span class="line">                  responseCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>] =&gt; <span class="type">Unit</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isValidRequiredAcks(requiredAcks)) &#123; <span class="comment">//note: acks 设置有效</span></span><br><span class="line">    <span class="keyword">val</span> sTime = time.milliseconds</span><br><span class="line">    <span class="comment">//note: 向本地的副本 log 追加数据</span></span><br><span class="line">    <span class="keyword">val</span> localProduceResults = appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)</span><br><span class="line">    debug(<span class="string">"Produce to local log in %d ms"</span>.format(time.milliseconds - sTime))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> produceStatus = localProduceResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">      topicPartition -&gt;</span><br><span class="line">              <span class="type">ProducePartitionStatus</span>(</span><br><span class="line">                result.info.lastOffset + <span class="number">1</span>, <span class="comment">// required offset</span></span><br><span class="line">                <span class="keyword">new</span> <span class="type">PartitionResponse</span>(result.error, result.info.firstOffset, result.info.logAppendTime)) <span class="comment">// response status</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</span><br><span class="line">      <span class="comment">//note: 处理 ack=-1 的情况,需要等到 isr 的 follower 都写入成功的话,才能返回最后结果</span></span><br><span class="line">      <span class="comment">// create delayed produce operation</span></span><br><span class="line">      <span class="keyword">val</span> produceMetadata = <span class="type">ProduceMetadata</span>(requiredAcks, produceStatus)</span><br><span class="line">      <span class="comment">//note: 延迟 produce 请求</span></span><br><span class="line">      <span class="keyword">val</span> delayedProduce = <span class="keyword">new</span> <span class="type">DelayedProduce</span>(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed produce operation</span></span><br><span class="line">      <span class="keyword">val</span> producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(_)).toSeq</span><br><span class="line"></span><br><span class="line">      <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory</span></span><br><span class="line">      <span class="comment">// this is because while the delayed produce operation is being created, new</span></span><br><span class="line">      <span class="comment">// requests may arrive and hence make this operation completable.</span></span><br><span class="line">      delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// we can respond immediately</span></span><br><span class="line">      <span class="comment">//note: 通过回调函数直接返回结果</span></span><br><span class="line">      <span class="keyword">val</span> produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</span><br><span class="line">      responseCallback(produceResponseStatus)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// If required.acks is outside accepted range, something is wrong with the client</span></span><br><span class="line">    <span class="comment">// Just return an error and don't handle the request at all</span></span><br><span class="line">    <span class="comment">//note: 返回 INVALID_REQUIRED_ACKS 错误</span></span><br><span class="line">    <span class="keyword">val</span> responseStatus = entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, _) =&gt;</span><br><span class="line">      topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">INVALID_REQUIRED_ACKS</span>,</span><br><span class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.firstOffset, <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    responseCallback(responseStatus)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的实现来看，<code>appendRecords()</code> 的实现主要分为以下几步：</p>
<ol>
<li>首先判断 acks 设置是否有效（-1，0，1三个值有效），无效的话直接返回异常，不再处理；</li>
<li>acks 设置有效的话，调用 <code>appendToLocalLog()</code> 方法将 records 追加到本地对应的 log 对象中；</li>
<li><code>appendToLocalLog()</code> 处理完后，如果发现 clients 设置的 acks=-1，即需要 isr 的其他的副本同步完成才能返回 response，那么就会创建一个 DelayedProduce 对象，等待 isr 的其他副本进行同步，否则的话直接返回追加的结果。</li>
</ol>
<h4 id="appendToLocalLog-的实现"><a href="#appendToLocalLog-的实现" class="headerlink" title="appendToLocalLog() 的实现"></a><code>appendToLocalLog()</code> 的实现</h4><p>追加日志的实际操作是在 <code>appendToLocalLog()</code>  中完成的，这里看下它的具体实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Append the messages to the local replica logs</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 向本地的 replica 写入数据</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">appendToLocalLog</span></span>(internalTopicsAllowed: <span class="type">Boolean</span>,</span><br><span class="line">                             entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</span><br><span class="line">                             requiredAcks: <span class="type">Short</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">LogAppendResult</span>] = &#123;</span><br><span class="line">  trace(<span class="string">"Append [%s] to local log "</span>.format(entriesPerPartition))</span><br><span class="line">  entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, records) =&gt; <span class="comment">//note: 遍历要写的所有 topic-partition</span></span><br><span class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).totalProduceRequestRate.mark()</span><br><span class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalProduceRequestRate.mark()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reject appending to internal topics if it is not allowed</span></span><br><span class="line">    <span class="comment">//note: 不能向 kafka 内部使用的 topic 追加数据</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topicPartition.topic) &amp;&amp; !internalTopicsAllowed) &#123;</span><br><span class="line">      (topicPartition, <span class="type">LogAppendResult</span>(</span><br><span class="line">        <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>,</span><br><span class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">InvalidTopicException</span>(<span class="string">s"Cannot append to internal topic <span class="subst">$&#123;topicPartition.topic&#125;</span>"</span>))))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//note: 查找对应的 Partition,并向分区对应的副本写入数据文件</span></span><br><span class="line">        <span class="keyword">val</span> partitionOpt = getPartition(topicPartition) <span class="comment">//note: 获取 topic-partition 的 Partition 对象</span></span><br><span class="line">        <span class="keyword">val</span> info = partitionOpt <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</span><br><span class="line">            partition.appendRecordsToLeader(records, requiredAcks) <span class="comment">//note: 如果找到了这个对象,就开始追加日志</span></span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">"Partition %s doesn't exist on %d"</span></span><br><span class="line">            .format(topicPartition, localBrokerId)) <span class="comment">//note: 没有找到的话,返回异常</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 追加的 msg 数</span></span><br><span class="line">        <span class="keyword">val</span> numAppendedMessages =</span><br><span class="line">          <span class="keyword">if</span> (info.firstOffset == <span class="number">-1</span>L || info.lastOffset == <span class="number">-1</span>L)</span><br><span class="line">            <span class="number">0</span></span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            info.lastOffset - info.firstOffset + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// update stats for successfully appended bytes and messages as bytesInRate and messageInRate</span></span><br><span class="line">        <span class="comment">//note:  更新 metrics</span></span><br><span class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesInRate.mark(records.sizeInBytes)</span><br><span class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesInRate.mark(records.sizeInBytes)</span><br><span class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).messagesInRate.mark(numAppendedMessages)</span><br><span class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.messagesInRate.mark(numAppendedMessages)</span><br><span class="line"></span><br><span class="line">        trace(<span class="string">"%d bytes written to log %s-%d beginning at offset %d and ending at offset %d"</span></span><br><span class="line">          .format(records.sizeInBytes, topicPartition.topic, topicPartition.partition, info.firstOffset, info.lastOffset))</span><br><span class="line">        (topicPartition, <span class="type">LogAppendResult</span>(info))</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123; <span class="comment">//note: 处理追加过程中出现的异常</span></span><br><span class="line">        <span class="comment">// <span class="doctag">NOTE:</span> Failed produce requests metric is not incremented for known exceptions</span></span><br><span class="line">        <span class="comment">// it is supposed to indicate un-expected failures of a broker in handling a produce request</span></span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</span><br><span class="line">          fatal(<span class="string">"Halting due to unrecoverable I/O error while handling produce request: "</span>, e)</span><br><span class="line">          <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</span><br><span class="line">          (topicPartition, <span class="literal">null</span>)</span><br><span class="line">        <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</span><br><span class="line">                 _: <span class="type">NotLeaderForPartitionException</span> |</span><br><span class="line">                 _: <span class="type">RecordTooLargeException</span> |</span><br><span class="line">                 _: <span class="type">RecordBatchTooLargeException</span> |</span><br><span class="line">                 _: <span class="type">CorruptRecordException</span> |</span><br><span class="line">                 _: <span class="type">InvalidTimestampException</span>) =&gt;</span><br><span class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(e)))</span><br><span class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">          <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).failedProduceRequestRate.mark()</span><br><span class="line">          <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.failedProduceRequestRate.mark()</span><br><span class="line">          error(<span class="string">"Error processing append operation on partition %s"</span>.format(topicPartition), t)</span><br><span class="line">          (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(t)))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面可以看到 <code>appendToLocalLog()</code> 的实现如下：</p>
<ol>
<li>首先判断要写的 topic 是不是 Kafka 内置的 topic，内置的 topic 是不允许 Producer 写入的；</li>
<li>先查找 topic-partition 对应的 Partition 对象，如果在 <code>allPartitions</code> 中查找到了对应的 partition，那么直接调用 <code>partition.appendRecordsToLeader()</code> 方法追加相应的 records，否则会向 client 抛出异常。</li>
</ol>
<h3 id="Partition-appendRecordsToLeader-方法"><a href="#Partition-appendRecordsToLeader-方法" class="headerlink" title="Partition.appendRecordsToLeader() 方法"></a>Partition.appendRecordsToLeader() 方法</h3><p>ReplicaManager 在追加 records 时，调用的是 Partition 的 <code>appendRecordsToLeader()</code> 方法，其具体的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecordsToLeader</span></span>(records: <span class="type">MemoryRecords</span>, requiredAcks: <span class="type">Int</span> = <span class="number">0</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) &#123;</span><br><span class="line">    leaderReplicaIfLocal <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">        <span class="keyword">val</span> log = leaderReplica.log.get <span class="comment">//note: 获取对应的 Log 对象</span></span><br><span class="line">        <span class="keyword">val</span> minIsr = log.config.minInSyncReplicas</span><br><span class="line">        <span class="keyword">val</span> inSyncSize = inSyncReplicas.size</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Avoid writing to leader if there are not enough insync replicas to make it safe</span></span><br><span class="line">        <span class="comment">//note: 如果 ack 设置为-1, isr 数小于设置的 min.isr 时,就会向 producer 抛出相应的异常</span></span><br><span class="line">        <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == <span class="number">-1</span>) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotEnoughReplicasException</span>(<span class="string">"Number of insync replicas for partition %s is [%d], below required minimum [%d]"</span></span><br><span class="line">            .format(topicPartition, inSyncSize, minIsr))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//note: 向副本对应的 log 追加响应的数据</span></span><br><span class="line">        <span class="keyword">val</span> info = log.append(records, assignOffsets = <span class="literal">true</span>)</span><br><span class="line">        <span class="comment">// probably unblock some follower fetch requests since log end offset has been updated</span></span><br><span class="line">        replicaManager.tryCompleteDelayedFetch(<span class="type">TopicPartitionOperationKey</span>(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</span><br><span class="line">        <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></span><br><span class="line">        <span class="comment">//note: 判断是否需要增加 HHW（追加日志后会进行一次判断）</span></span><br><span class="line">        (info, maybeIncrementLeaderHW(leaderReplica))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">//note: leader 不在本台机器上</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">"Leader not local for partition %s on broker %d"</span></span><br><span class="line">          .format(topicPartition, localBrokerId))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// some delayed operations may be unblocked after HW changed</span></span><br><span class="line">  <span class="keyword">if</span> (leaderHWIncremented)</span><br><span class="line">    tryCompleteDelayedRequests()</span><br><span class="line"></span><br><span class="line">  info</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法里，会根据 topic 的 <code>min.isrs</code> 配置以及当前这个 partition 的 isr 情况判断是否可以写入，如果不满足条件，就会抛出 <code>NotEnoughReplicasException</code> 的异常，如果满足条件，就会调用 <code>log.append()</code> 向 replica 追加日志。</p>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><p>跟着最开始图中的流程及代码分析，走到这里，才算是到了 Kafka 的存储层部分，在这里会详细讲述在存储层 Kafka 如何写入日志。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>在上面有过一些介绍，每个 replica 会对应一个 log 对象，log 对象是管理当前分区的一个单位，它会包含这个分区的所有 segment 文件（包括对应的 offset 索引和时间戳索引文件），它会提供一些增删查的方法。</p>
<p>在 Log 对象的初始化时，有三个变量是比较重要的：</p>
<ol>
<li><code>nextOffsetMetadata</code>：可以叫做下一个偏移量元数据，它包括 activeSegment 的下一条消息的偏移量，该 activeSegment 的基准偏移量及日志分段的大小；</li>
<li><code>activeSegment</code>：指的是该 Log 管理的 segments 中那个最新的 segment（这里叫做活跃的 segment），一个 Log 中只会有一个活跃的 segment，其他的 segment 都已经被持久化到磁盘了；</li>
<li><code>logEndOffset</code>：表示下一条消息的 offset，它取自 <code>nextOffsetMetadata</code> 的 offset，实际上就是活动日志分段的下一个偏移量。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: nextOffsetMetadata 声明为 volatile，如果该值被修改，其他使用此变量的线程就可以立刻见到变化后的值，在生产和消费都会使用到这个值</span></span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> nextOffsetMetadata: <span class="type">LogOffsetMetadata</span> = _</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Calculate the offset of the next message */</span></span><br><span class="line"><span class="comment">//note: 下一个偏移量元数据</span></span><br><span class="line"><span class="comment">//note: 第一个参数：下一条消息的偏移量；第二个参数：日志分段的基准偏移量；第三个参数：日志分段大小</span></span><br><span class="line">nextOffsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(activeSegment.nextOffset(), activeSegment.baseOffset, activeSegment.size.toInt)</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* The active segment that is currently taking appends</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//note: 任何时刻，只会有一个活动的日志分段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activeSegment</span> </span>= segments.lastEntry.getValue</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*  The offset of the next message that will be appended to the log</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//note: 下一条消息的 offset，从 nextOffsetMetadata 中获取的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logEndOffset</span></span>: <span class="type">Long</span> = nextOffsetMetadata.messageOffset</span><br></pre></td></tr></table></figure>
<h4 id="日志写入"><a href="#日志写入" class="headerlink" title="日志写入"></a>日志写入</h4><p>在 Log 中一个重要的方法就是日志的写入方法，下面来看下这个方法的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Append this message set to the active segment of the log, rolling over to a fresh segment if necessary.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This method will generally be responsible for assigning offsets to the messages,</span></span><br><span class="line"><span class="comment"> * however if the assignOffsets=false flag is passed we will only check that the existing offsets are valid.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param records The log records to append</span></span><br><span class="line"><span class="comment"> * @param assignOffsets Should the log assign offsets to this message set or blindly apply what it is given</span></span><br><span class="line"><span class="comment"> * @throws KafkaStorageException If the append fails due to an I/O error.</span></span><br><span class="line"><span class="comment"> * @return Information about the appended messages including the first and last offset.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 向 active segment 追加 log,必要的情况下,滚动创建新的 segment</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> appendInfo = analyzeAndValidateRecords(records) <span class="comment">//note: 返回这批消息的该要信息,并对这批 msg 进行校验</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// if we have any valid messages, append them to the log</span></span><br><span class="line">  <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> appendInfo</span><br><span class="line"></span><br><span class="line">  <span class="comment">// trim any invalid bytes or partial messages before appending it to the on-disk log</span></span><br><span class="line">  <span class="comment">//note: 删除这批消息中无效的消息</span></span><br><span class="line">  <span class="keyword">var</span> validRecords = trimInvalidBytes(records, appendInfo)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// they are valid, insert them in the log</span></span><br><span class="line">    lock synchronized &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (assignOffsets) &#123;</span><br><span class="line">        <span class="comment">// assign offsets to the message set</span></span><br><span class="line">        <span class="comment">//note: 计算这个消息集起始 offset，对 offset 的操作是一个原子操作</span></span><br><span class="line">        <span class="keyword">val</span> offset = <span class="keyword">new</span> <span class="type">LongRef</span>(nextOffsetMetadata.messageOffset)</span><br><span class="line">        appendInfo.firstOffset = offset.value <span class="comment">//note: 作为消息集的第一个 offset</span></span><br><span class="line">        <span class="keyword">val</span> now = time.milliseconds <span class="comment">//note: 设置的时间错以 server 收到的时间戳为准</span></span><br><span class="line">        <span class="comment">//note: 验证消息,并为没条 record 设置相应的 offset 和 timestrap</span></span><br><span class="line">        <span class="keyword">val</span> validateAndOffsetAssignResult = <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(validRecords,</span><br><span class="line">                                                        offset,</span><br><span class="line">                                                        now,</span><br><span class="line">                                                        appendInfo.sourceCodec,</span><br><span class="line">                                                        appendInfo.targetCodec,</span><br><span class="line">                                                        config.compact,</span><br><span class="line">                                                        config.messageFormatVersion.messageFormatVersion,</span><br><span class="line">                                                        config.messageTimestampType,</span><br><span class="line">                                                        config.messageTimestampDifferenceMaxMs)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Error in validating messages while appending to log '%s'"</span>.format(name), e)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//note: 返回已经计算好 offset 和 timestrap 的 MemoryRecords</span></span><br><span class="line">        validRecords = validateAndOffsetAssignResult.validatedRecords</span><br><span class="line">        appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</span><br><span class="line">        appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</span><br><span class="line">        appendInfo.lastOffset = offset.value - <span class="number">1</span> <span class="comment">//note: 最后一条消息的 offset</span></span><br><span class="line">        <span class="keyword">if</span> (config.messageTimestampType == <span class="type">TimestampType</span>.<span class="type">LOG_APPEND_TIME</span>)</span><br><span class="line">          appendInfo.logAppendTime = now</span><br><span class="line"></span><br><span class="line">        <span class="comment">// re-validate message sizes if there's a possibility that they have changed (due to re-compression or message</span></span><br><span class="line">        <span class="comment">// format conversion)</span></span><br><span class="line">        <span class="comment">//note: 更新 metrics 的记录</span></span><br><span class="line">        <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) &#123;</span><br><span class="line">          <span class="keyword">for</span> (logEntry &lt;- validRecords.shallowEntries.asScala) &#123;</span><br><span class="line">            <span class="keyword">if</span> (logEntry.sizeInBytes &gt; config.maxMessageSize) &#123;</span><br><span class="line">              <span class="comment">// we record the original message set size instead of the trimmed size</span></span><br><span class="line">              <span class="comment">// to be consistent with pre-compression bytesRejectedRate recording</span></span><br><span class="line">              <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">              <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(<span class="string">"Message size is %d bytes which exceeds the maximum configured message size of %d."</span></span><br><span class="line">                .format(logEntry.sizeInBytes, config.maxMessageSize))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// we are taking the offsets we are given</span></span><br><span class="line">        <span class="keyword">if</span> (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &lt; nextOffsetMetadata.messageOffset)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Out of order offsets found in "</span> + records.deepEntries.asScala.map(_.offset))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// check messages set size may be exceed config.segmentSize</span></span><br><span class="line">      <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordBatchTooLargeException</span>(<span class="string">"Message set size is %d bytes which exceeds the maximum configured segment size of %d."</span></span><br><span class="line">          .format(validRecords.sizeInBytes, config.segmentSize))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// maybe roll the log if this segment is full</span></span><br><span class="line">      <span class="comment">//note: 如果当前 segment 满了，就需要重新新建一个 segment</span></span><br><span class="line">      <span class="keyword">val</span> segment = maybeRoll(messagesSize = validRecords.sizeInBytes,</span><br><span class="line">        maxTimestampInMessages = appendInfo.maxTimestamp,</span><br><span class="line">        maxOffsetInMessages = appendInfo.lastOffset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="comment">// now append to the log</span></span><br><span class="line">      <span class="comment">//note: 追加消息到当前 segment</span></span><br><span class="line">      segment.append(firstOffset = appendInfo.firstOffset,</span><br><span class="line">        largestOffset = appendInfo.lastOffset,</span><br><span class="line">        largestTimestamp = appendInfo.maxTimestamp,</span><br><span class="line">        shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</span><br><span class="line">        records = validRecords)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// increment the log end offset</span></span><br><span class="line">      <span class="comment">//note: 修改最新的 next_offset</span></span><br><span class="line">      updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></span><br><span class="line">        .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)<span class="comment">//note: 满足条件的话，刷新磁盘</span></span><br><span class="line">        flush()</span><br><span class="line"></span><br><span class="line">      appendInfo</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaStorageException</span>(<span class="string">"I/O exception in append to log '%s'"</span>.format(name), e)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Server 将每个分区的消息追加到日志中时，是以 segment 为单位的，当 segment 的大小到达阈值大小之后，会滚动新建一个日志分段（segment）保存新的消息，而分区的消息总是追加到最新的日志分段（也就是 activeSegment）中。每个日志分段都会有一个基准偏移量（segmentBaseOffset，或者叫做 baseOffset），这个基准偏移量就是分区级别的绝对偏移量，而且这个值在日志分段是固定的。有了这个基准偏移量，就可以计算出来每条消息在分区中的绝对偏移量，最后把数据以及对应的绝对偏移量写到日志文件中。<code>append()</code> 方法的过程可以总结如下：</p>
<ol>
<li><code>analyzeAndValidateRecords()</code>：对这批要写入的消息进行检测，主要是检查消息的大小及 crc 校验；</li>
<li><code>trimInvalidBytes()</code>：会将这批消息中无效的消息删除，返回一个都是有效消息的 MemoryRecords；</li>
<li><code>LogValidator.validateMessagesAndAssignOffsets()</code>：为每条消息设置相应的 offset（绝对偏移量） 和 timestrap；</li>
<li><code>maybeRoll()</code>：判断是否需要新建一个 segment 的，如果当前的 segment 放不下这批消息的话，需要新建一个 segment；</li>
<li><code>segment.append()</code>：向 segment 中添加消息；</li>
<li>更新 logEndOffset 和判断是否需要刷新磁盘（如果需要的话，调用 <code>flush()</code> 方法刷到磁盘）。</li>
</ol>
<p>关于 timestrap 的设置，这里也顺便介绍一下，在新版的 Kafka 中，每条 msg 都会有一个对应的时间戳记录，producer 端可以设置这个字段 <code>message.timestamp.type</code> 来选择 timestrap 的类型，默认是按照创建时间，只能选择从下面的选择中二选一：</p>
<ol>
<li><code>CreateTime</code>，默认值；</li>
<li><code>LogAppendTime</code>。</li>
</ol>
<h4 id="日志分段"><a href="#日志分段" class="headerlink" title="日志分段"></a>日志分段</h4><p>在 Log 的 <code>append()</code> 方法中，会调用 <code>maybeRoll()</code> 方法来判断是否需要进行相应日志分段操作，其具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Roll the log over to a new empty log segment if necessary.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param messagesSize The messages set size in bytes</span></span><br><span class="line"><span class="comment"> * @param maxTimestampInMessages The maximum timestamp in the messages.</span></span><br><span class="line"><span class="comment"> * logSegment will be rolled if one of the following conditions met</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt; The logSegment is full</span></span><br><span class="line"><span class="comment"> * &lt;li&gt; The maxTime has elapsed since the timestamp of first message in the segment (or since the create time if</span></span><br><span class="line"><span class="comment"> * the first message does not have a timestamp)</span></span><br><span class="line"><span class="comment"> * &lt;li&gt; The index is full</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;</span></span><br><span class="line"><span class="comment"> * @return The currently active segment after (perhaps) rolling to a new segment</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 判断是否需要创建日志分段,如果不需要返回当前分段,需要的话,返回新创建的日志分段</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeRoll</span></span>(messagesSize: <span class="type">Int</span>, maxTimestampInMessages: <span class="type">Long</span>, maxOffsetInMessages: <span class="type">Long</span>): <span class="type">LogSegment</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> segment = activeSegment <span class="comment">//note: 对活跃的日志分段进行判断,它也是最新的一个日志分段</span></span><br><span class="line">  <span class="keyword">val</span> now = time.milliseconds</span><br><span class="line">  <span class="comment">//note: 距离上次日志分段的时间是否达到了设置的阈值（log.roll.hours）</span></span><br><span class="line">  <span class="keyword">val</span> reachedRollMs = segment.timeWaitedForRoll(now, maxTimestampInMessages) &gt; config.segmentMs - segment.rollJitterMs</span><br><span class="line">  <span class="comment">//note: 这是五个条件: 1. 文件满了,不足以放心这么大的 messageSet; 2. 文件有数据,并且到分段的时间阈值; 3. 索引文件满了;</span></span><br><span class="line">  <span class="comment">//note: 4. 时间索引文件满了; 5. 最大的 offset，其相对偏移量超过了正整数的阈值</span></span><br><span class="line">  <span class="keyword">if</span> (segment.size &gt; config.segmentSize - messagesSize ||</span><br><span class="line">      (segment.size &gt; <span class="number">0</span> &amp;&amp; reachedRollMs) ||</span><br><span class="line">      segment.index.isFull || segment.timeIndex.isFull || !segment.canConvertToRelativeOffset(maxOffsetInMessages)) &#123;</span><br><span class="line">    debug(<span class="string">s"Rolling new log segment in <span class="subst">$name</span> (log_size = <span class="subst">$&#123;segment.size&#125;</span>/<span class="subst">$&#123;config.segmentSize&#125;</span>&#125;, "</span> +</span><br><span class="line">        <span class="string">s"index_size = <span class="subst">$&#123;segment.index.entries&#125;</span>/<span class="subst">$&#123;segment.index.maxEntries&#125;</span>, "</span> +</span><br><span class="line">        <span class="string">s"time_index_size = <span class="subst">$&#123;segment.timeIndex.entries&#125;</span>/<span class="subst">$&#123;segment.timeIndex.maxEntries&#125;</span>, "</span> +</span><br><span class="line">        <span class="string">s"inactive_time_ms = <span class="subst">$&#123;segment.timeWaitedForRoll(now, maxTimestampInMessages)&#125;</span>/<span class="subst">$&#123;config.segmentMs - segment.rollJitterMs&#125;</span>)."</span>)</span><br><span class="line">    roll(maxOffsetInMessages - <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>) <span class="comment">//note: 创建新的日志分段</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    segment <span class="comment">//note: 使用当前的日志分段</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从 <code>maybeRoll()</code> 的实现可以看到，是否需要创建新的日志分段，有下面几种情况：</p>
<ol>
<li>当前日志分段的大小加上消息的大小超过了日志分段的阈值（<code>log.segment.bytes</code>）；</li>
<li>距离上次创建日志分段的时间达到了一定的阈值（<code>log.roll.hours</code>），并且数据文件有数据；</li>
<li>索引文件满了；</li>
<li>时间索引文件满了；</li>
<li>最大的 offset，其相对偏移量超过了正整数的阈值。</li>
</ol>
<p>如果上面的其中一个条件，就会创建新的 segment 文件，见 <code>roll()</code> 方法实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Roll the log over to a new active segment starting with the current logEndOffset.</span></span><br><span class="line"><span class="comment"> * This will trim the index to the exact size of the number of entries it currently contains.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @return The newly rolled segment</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 滚动创建日志,并添加到日志管理的映射表中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">roll</span></span>(expectedNextOffset: <span class="type">Long</span> = <span class="number">0</span>): <span class="type">LogSegment</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> start = time.nanoseconds</span><br><span class="line">  lock synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> newOffset = <span class="type">Math</span>.max(expectedNextOffset, logEndOffset) <span class="comment">//note: 选择最新的 offset 作为基准偏移量</span></span><br><span class="line">    <span class="keyword">val</span> logFile = logFilename(dir, newOffset) <span class="comment">//note: 创建数据文件</span></span><br><span class="line">    <span class="keyword">val</span> indexFile = indexFilename(dir, newOffset) <span class="comment">//note: 创建 offset 索引文件</span></span><br><span class="line">    <span class="keyword">val</span> timeIndexFile = timeIndexFilename(dir, newOffset) <span class="comment">//note: 创建 time 索引文件</span></span><br><span class="line">    <span class="keyword">for</span>(file &lt;- <span class="type">List</span>(logFile, indexFile, timeIndexFile); <span class="keyword">if</span> file.exists) &#123;</span><br><span class="line">      warn(<span class="string">"Newly rolled segment file "</span> + file.getName + <span class="string">" already exists; deleting it first"</span>)</span><br><span class="line">      file.delete()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    segments.lastEntry() <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="literal">null</span> =&gt;</span><br><span class="line">      <span class="keyword">case</span> entry =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> seg = entry.getValue</span><br><span class="line">        seg.onBecomeInactiveSegment()</span><br><span class="line">        seg.index.trimToValidSize()</span><br><span class="line">        seg.timeIndex.trimToValidSize()</span><br><span class="line">        seg.log.trim()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 创建一个 segment 对象</span></span><br><span class="line">    <span class="keyword">val</span> segment = <span class="keyword">new</span> <span class="type">LogSegment</span>(dir,</span><br><span class="line">                                 startOffset = newOffset,</span><br><span class="line">                                 indexIntervalBytes = config.indexInterval,</span><br><span class="line">                                 maxIndexSize = config.maxIndexSize,</span><br><span class="line">                                 rollJitterMs = config.randomSegmentJitter,</span><br><span class="line">                                 time = time,</span><br><span class="line">                                 fileAlreadyExists = <span class="literal">false</span>,</span><br><span class="line">                                 initFileSize = initFileSize,</span><br><span class="line">                                 preallocate = config.preallocate)</span><br><span class="line">    <span class="keyword">val</span> prev = addSegment(segment) <span class="comment">//note: 添加到日志管理中</span></span><br><span class="line">    <span class="keyword">if</span>(prev != <span class="literal">null</span>)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Trying to roll a new log segment for topic partition %s with start offset %d while it already exists."</span>.format(name, newOffset))</span><br><span class="line">    <span class="comment">// We need to update the segment base offset and append position data of the metadata when log rolls.</span></span><br><span class="line">    <span class="comment">// The next offset should not change.</span></span><br><span class="line">    updateLogEndOffset(nextOffsetMetadata.messageOffset) <span class="comment">//note: 更新 offset</span></span><br><span class="line">    <span class="comment">// schedule an asynchronous flush of the old segment</span></span><br><span class="line">    scheduler.schedule(<span class="string">"flush-log"</span>, () =&gt; flush(newOffset), delay = <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">    info(<span class="string">"Rolled new log segment for '"</span> + name + <span class="string">"' in %.0f ms."</span>.format((<span class="type">System</span>.nanoTime - start) / (<span class="number">1000.0</span>*<span class="number">1000.0</span>)))</span><br><span class="line"></span><br><span class="line">    segment</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>创建一个 segment 对象，真正的实现是在 Log 的 <code>roll()</code> 方法中，也就是上面的方法中，创建 segment 对象，主要包括三部分：数据文件、offset 索引文件和 time 索引文件。</p>
<h4 id="offset-索引文件"><a href="#offset-索引文件" class="headerlink" title="offset 索引文件"></a>offset 索引文件</h4><p>这里顺便讲述一下 offset 索引文件，Kafka 的索引文件有下面一个特点：</p>
<ol>
<li>采用 <strong>绝对偏移量+相对偏移量</strong> 的方式进行存储的，每个 segment 最开始绝对偏移量也是其基准偏移量；</li>
<li>数据文件每隔一定的大小创建一个索引条目，而不是每条消息会创建索引条目，通过 <code>index.interval.bytes</code> 来配置，默认是 4096，也就是4KB；</li>
</ol>
<p>这样做的好处也非常明显：</p>
<ol>
<li>因为不是每条消息都创建相应的索引条目，所以索引条目是稀疏的；</li>
<li>索引的相对偏移量占据4个字节，而绝对偏移量占据8个字节，加上物理位置的4个字节，使用相对索引可以将每条索引条目的大小从12字节减少到8个字节；</li>
<li>因为偏移量有序的，再读取数据时，可以按照二分查找的方式去快速定位偏移量的位置；</li>
<li>这样的稀疏索引是可以完全放到内存中，加快偏移量的查找。</li>
</ol>
<h3 id="LogSegment-写入"><a href="#LogSegment-写入" class="headerlink" title="LogSegment 写入"></a>LogSegment 写入</h3><p>真正的日志写入，还是在 LogSegment 的 <code>append()</code> 方法中完成的，LogSegment 会跟 Kafka 最底层的文件通道、mmap 打交道。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Append the given messages starting with the given offset. Add</span></span><br><span class="line"><span class="comment"> * an entry to the index if needed.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * It is assumed this method is being called from within a lock.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param firstOffset The first offset in the message set.</span></span><br><span class="line"><span class="comment"> * @param largestTimestamp The largest timestamp in the message set.</span></span><br><span class="line"><span class="comment"> * @param shallowOffsetOfMaxTimestamp The offset of the message that has the largest timestamp in the messages to append.</span></span><br><span class="line"><span class="comment"> * @param records The log entries to append.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="comment">//note: 在指定的 offset 处追加指定的 msgs, 需要的情况下追加相应的索引</span></span><br><span class="line"><span class="meta">@nonthreadsafe</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(firstOffset: <span class="type">Long</span>, largestOffset: <span class="type">Long</span>, largestTimestamp: <span class="type">Long</span>, shallowOffsetOfMaxTimestamp: <span class="type">Long</span>, records: <span class="type">MemoryRecords</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (records.sizeInBytes &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    trace(<span class="string">"Inserting %d bytes at offset %d at position %d with largest timestamp %d at shallow offset %d"</span></span><br><span class="line">        .format(records.sizeInBytes, firstOffset, log.sizeInBytes(), largestTimestamp, shallowOffsetOfMaxTimestamp))</span><br><span class="line">    <span class="keyword">val</span> physicalPosition = log.sizeInBytes()</span><br><span class="line">    <span class="keyword">if</span> (physicalPosition == <span class="number">0</span>)</span><br><span class="line">      rollingBasedTimestamp = <span class="type">Some</span>(largestTimestamp)</span><br><span class="line">    <span class="comment">// append the messages</span></span><br><span class="line">    require(canConvertToRelativeOffset(largestOffset), <span class="string">"largest offset in message set can not be safely converted to relative offset."</span>)</span><br><span class="line">    <span class="keyword">val</span> appendedBytes = log.append(records) <span class="comment">//note: 追加到数据文件中</span></span><br><span class="line">    trace(<span class="string">s"Appended <span class="subst">$appendedBytes</span> to <span class="subst">$&#123;log.file()&#125;</span> at offset <span class="subst">$firstOffset</span>"</span>)</span><br><span class="line">    <span class="comment">// Update the in memory max timestamp and corresponding offset.</span></span><br><span class="line">    <span class="keyword">if</span> (largestTimestamp &gt; maxTimestampSoFar) &#123;</span><br><span class="line">      maxTimestampSoFar = largestTimestamp</span><br><span class="line">      offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// append an entry to the index (if needed)</span></span><br><span class="line">    <span class="comment">//note: 判断是否需要追加索引（数据每次都会添加到数据文件中,但不是每次都会添加索引的,间隔 indexIntervalBytes 大小才会写入一个索引文件）</span></span><br><span class="line">    <span class="keyword">if</span>(bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123;</span><br><span class="line">      index.append(firstOffset, physicalPosition) <span class="comment">//note: 添加索引</span></span><br><span class="line">      timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</span><br><span class="line">      bytesSinceLastIndexEntry = <span class="number">0</span> <span class="comment">//note: 重置为0</span></span><br><span class="line">    &#125;</span><br><span class="line">    bytesSinceLastIndexEntry += records.sizeInBytes</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过上面的分析，一个消息集（MemoryRecords）在 Kafka 存储层的调用情况如下图所示：</p>
<p><img src="/images/kafka/log_append.png" alt="MemoryRecords 追加过程"></p>
<p>最后还是利用底层的 Java NIO 实现。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matt33.com/2018/03/18/kafka-server-handle-produce-request/" data-id="cjv8bavn60095ox9kd63uou91" class="article-share-link">分享到</a><div class="copyright"><a href="http://matt33.com/copyright/">博客版权说明</a></div><div class="tags"><a href="/tags/kafka/">kafka</a></div><div class="post-nav"><a href="/2018/04/15/kafka-server-handle-fetch-request/" class="pre">Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</a><a href="/2018/03/12/kafka-log-manager/" class="next">Kafka 源码解析之日志管理（十一）</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button></div><script>var disqus_shortname = 'http-matt33-com';
var disqus_identifier = '2018/03/18/kafka-server-handle-produce-request/';
var disqus_title = 'Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）';
var disqus_url = 'http://matt33.com/2018/03/18/kafka-server-handle-produce-request/';
$('.btn_click_load').click(function() {
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('.btn_click_load').css('display','none');
});
$.ajax({
  url: 'https://disqus.com/favicon.ico',
  timeout: 3000,
  type: 'GET',
  success: (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    $('.btn_click_load').css('display','none');
  })(),
  error: function() {
    $('.btn_click_load').css('display','block');
  }
});</script><script id="dsq-count-scr" src="//http-matt33-com.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-weibo"> 微博</i></div><iframe width="100%" height="90" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=100&fansRow=1&ptype=1&speed=0&skin=1&isTitle=0&noborder=1&isWeibo=0&isFans=0&uid=2650396571&verifier=f2f0e397&colors=D8D8D8,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书屋/">书屋</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影如人生/">影如人生</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/旅行/">旅行</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/转载/">转载</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tcp/" style="font-size: 15px;">tcp</a> <a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/database/" style="font-size: 15px;">database</a> <a href="/tags/storm/" style="font-size: 15px;">storm</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/learn/" style="font-size: 15px;">learn</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/travel/" style="font-size: 15px;">travel</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/电影随想/" style="font-size: 15px;">电影随想</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/转载/" style="font-size: 15px;">转载</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/jvm/" style="font-size: 15px;">jvm</a> <a href="/tags/web/" style="font-size: 15px;">web</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a> <a href="/tags/bk/" style="font-size: 15px;">bk</a> <a href="/tags/rpc/" style="font-size: 15px;">rpc</a> <a href="/tags/thrift/" style="font-size: 15px;">thrift</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/calcite/" style="font-size: 15px;">calcite</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/03/17/apache-calcite-planner/">Apache Calcite 优化器详解（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/28/bk-store-realize/">BookKeeper 原理浅谈</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/21/effective-learning/">如何高效学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/04/kafka-transaction/">Kafka Exactly-Once 之事务性实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/24/kafka-idempotent/">Kafka 事务性之幂等性实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/19/bk-cluster-install-and-use/">BookKeeper 集群搭建及使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/01/yarn-architecture-learn/">YARN 架构学习总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/01/system-learn-summary/">如何学习开源项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/28/jvm-cms/">JVM 之 ParNew 和 CMS 日志分析</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://matt33.com/about/" title="个人公众号：柳年思水" target="_blank">个人公众号：柳年思水</a><ul></ul><a href="http://tech.meituan.com/" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://jm.taobao.org/" title="阿里中间件团队博客" target="_blank">阿里中间件团队博客</a><ul></ul><a href="http://www.jianshu.com/" title="简书" target="_blank">简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matt's Blog 柳年思水.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><div class="analytics"><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1256517224'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1256517224%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script></div><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-64518924-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5cf44757fa0d23bc7637935e44a9104a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>