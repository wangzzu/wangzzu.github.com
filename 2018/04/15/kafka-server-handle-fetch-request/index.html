<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="与一群有趣的人，做一些有趣的事."><title>Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三） | Matt's Blog</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</h1><a id="logo" href="/.">Matt's Blog</a><p class="description">王蒙</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）</h1><div class="post-meta">Apr 15, 2018<span> | </span><span class="category"><a href="/categories/技术/">技术</a></span><span> | </span><span class="post-count">5,612</span><span> 字</span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Times</span></span></div><a data-disqus-identifier="2018/04/15/kafka-server-handle-fetch-request/" href="/2018/04/15/kafka-server-handle-fetch-request/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Fetch-请求处理的整体流程"><span class="toc-number">1.</span> <span class="toc-text">Fetch 请求处理的整体流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Fetch-请求的来源"><span class="toc-number">1.1.</span> <span class="toc-text">Fetch 请求的来源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Consumer-Fetch-请求"><span class="toc-number">1.1.1.</span> <span class="toc-text">Consumer Fetch 请求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Replica-同步-Fetch-请求"><span class="toc-number">1.1.2.</span> <span class="toc-text">Replica 同步 Fetch 请求</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Server-端的处理"><span class="toc-number">2.</span> <span class="toc-text">Server 端的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KafkaApis-如何处理-Fetch-请求"><span class="toc-number">2.1.</span> <span class="toc-text">KafkaApis 如何处理 Fetch 请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReplicaManager-如何处理-Fetch-请求"><span class="toc-number">2.2.</span> <span class="toc-text">ReplicaManager 如何处理 Fetch 请求</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#fetchMessages"><span class="toc-number">2.2.1.</span> <span class="toc-text">fetchMessages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#readFromLocalLog"><span class="toc-number">2.2.2.</span> <span class="toc-text">readFromLocalLog</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#存储层对-Fetch-请求的处理"><span class="toc-number">3.</span> <span class="toc-text">存储层对 Fetch 请求的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Log-对象"><span class="toc-number">3.1.</span> <span class="toc-text">Log 对象</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#read"><span class="toc-number">3.1.1.</span> <span class="toc-text">read()</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogSegment"><span class="toc-number">3.2.</span> <span class="toc-text">LogSegment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#read-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">read()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#translateOffset"><span class="toc-number">3.2.2.</span> <span class="toc-text">translateOffset()</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#查找-offset-索引文件"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">查找 offset 索引文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#搜索数据文件获取准确的物理位置"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">搜索数据文件获取准确的物理位置</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="post-content"><p>上一篇讲述完 Kafka 如何处理 Produce 请求以及日志写操作之后，这篇文章开始讲述 Kafka 如何处理 Fetch 请求以及日志读操作。日志的读写操作是 Kafka 存储层最重要的内容，本文会以 Server 端处理 Fetch 请求的过程为入口，一步步深入到底层的 Log 实例部分。与 Produce 请求不一样的地方是，对于 Fetch 请求，是有两种不同的来源：consumer 和 follower，consumer 读取数据与副本同步数据都是通过向 leader 发送 Fetch 请求来实现的，在对这两种不同情况处理过程中，其底层的实现是统一的，只是实现方法的参数不同而已，在本文中会详细讲述对这两种不同情况的处理。</p>
<h2 id="Fetch-请求处理的整体流程"><a href="#Fetch-请求处理的整体流程" class="headerlink" title="Fetch 请求处理的整体流程"></a>Fetch 请求处理的整体流程</h2><p>Fetch 请求（读请求）的处理与 Produce 请求（写请求）的整体流程非常类似，读和写由最上面的抽象层做入口，最终还是在存储层的 Log 对象实例进行真正的读写操作，在这一点上，Kafka 封装的非常清晰，这样的系统设计是非常值得学习的，甚至可以作为分布式系统的模范系统来学习。</p>
<p>Fetch 请求处理的整体流程如下图所示，与 Produce 请求的处理流程非常相似。</p>
<p><img src="/images/kafka/kafka_fetch_request.png" alt="Server 端处理 Fetch 请求的总体过程"></p>
<h3 id="Fetch-请求的来源"><a href="#Fetch-请求的来源" class="headerlink" title="Fetch 请求的来源"></a>Fetch 请求的来源</h3><p>那 Server 要处理的 Fetch 请求有几种类型呢？来自于哪里呢？第一个来源肯定是 Consumer，Consumer 在消费数据时会向 Server 端发送 Fetch 请求，那么是不是还没有其他的类型，对 Kafka 比较熟悉的同学大概会猜到，还有一种就是：副本同步，follower 在从 leader 同步数据时，也是发送的 Fetch 请求，下面看下这两种情况的具体实现（代码会进行简化，并不完全与源码一致，便于理解）。</p>
<h4 id="Consumer-Fetch-请求"><a href="#Consumer-Fetch-请求" class="headerlink" title="Consumer Fetch 请求"></a>Consumer Fetch 请求</h4><p>Consumer 的 Fetch 请求是在 poll 方法中调用的，Fetcher 请求的构造过程及发送如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set-up a fetch request for any node that we have assigned partitions for which doesn't already have</span></span><br><span class="line"><span class="comment"> * an in-flight fetch or pending fetch data.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> number of fetches sent</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 向订阅的所有 partition （只要该 leader 暂时没有拉取请求）所在 leader 发送 fetch请求</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//note: 1 创建 Fetch Request</span></span><br><span class="line">    Map&lt;Node, FetchRequest.Builder&gt; fetchRequestMap = createFetchRequests();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchRequest.Builder&gt; fetchEntry : fetchRequestMap.entrySet()) &#123;</span><br><span class="line">        <span class="keyword">final</span> FetchRequest.Builder request = fetchEntry.getValue();</span><br><span class="line">        <span class="keyword">final</span> Node fetchTarget = fetchEntry.getKey();</span><br><span class="line"></span><br><span class="line">        log.debug(<span class="string">"Sending fetch for partitions &#123;&#125; to broker &#123;&#125;"</span>, request.fetchData().keySet(), fetchTarget);</span><br><span class="line">        <span class="comment">//note: 2 发送 Fetch Request</span></span><br><span class="line">        client.send(fetchTarget, request)</span><br><span class="line">                .addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>&#123;</span><br><span class="line">                        ...</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>&#123;</span><br><span class="line">                        ...</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fetchRequestMap.size();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create fetch requests for all nodes for which we have assigned partitions</span></span><br><span class="line"><span class="comment"> * that have no existing requests in flight.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 为所有 node 创建 fetch request</span></span><br><span class="line"><span class="keyword">private</span> Map&lt;Node, FetchRequest.Builder&gt; createFetchRequests() &#123;</span><br><span class="line">    <span class="comment">// create the fetch info</span></span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    Map&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; fetchable = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition partition : fetchablePartitions()) &#123;</span><br><span class="line">        Node node = cluster.leaderFor(partition);</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</span><br><span class="line">            metadata.requestUpdate();</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.client.pendingRequestCount(node) == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// if there is a leader and no in-flight requests, issue a new fetch</span></span><br><span class="line">            LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt; fetch = fetchable.get(node);</span><br><span class="line">            <span class="keyword">if</span> (fetch == <span class="keyword">null</span>) &#123;</span><br><span class="line">                fetch = <span class="keyword">new</span> LinkedHashMap&lt;&gt;();</span><br><span class="line">                fetchable.put(node, fetch);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> position = <span class="keyword">this</span>.subscriptions.position(partition);</span><br><span class="line">            <span class="comment">//note: 要 fetch 的 position 以及 fetch 的大小</span></span><br><span class="line">            fetch.put(partition, <span class="keyword">new</span> FetchRequest.PartitionData(position, <span class="keyword">this</span>.fetchSize));</span><br><span class="line">            log.trace(<span class="string">"Added fetch request for partition &#123;&#125; at offset &#123;&#125; to node &#123;&#125;"</span>, partition, position, node);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            log.trace(<span class="string">"Skipping fetch for partition &#123;&#125; because there is an in-flight request to &#123;&#125;"</span>, partition, node);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the fetches</span></span><br><span class="line">    Map&lt;Node, FetchRequest.Builder&gt; requests = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, LinkedHashMap&lt;TopicPartition, FetchRequest.PartitionData&gt;&gt; entry : fetchable.entrySet()) &#123;</span><br><span class="line">        Node node = entry.getKey();</span><br><span class="line">        <span class="comment">// 构造 Fetch 请求</span></span><br><span class="line">        FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</span><br><span class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></span><br><span class="line">        requests.put(node, fetch);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> requests;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面可以看出，Consumer 的 Fetcher 请求构造为：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FetchRequest.Builder fetch = <span class="keyword">new</span> FetchRequest.Builder(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, entry.getValue()).</span><br><span class="line">                setMaxBytes(<span class="keyword">this</span>.maxBytes);<span class="comment">//note: 构建 Fetch Request</span></span><br></pre></td></tr></table></figure>
<h4 id="Replica-同步-Fetch-请求"><a href="#Replica-同步-Fetch-请求" class="headerlink" title="Replica 同步 Fetch 请求"></a>Replica 同步 Fetch 请求</h4><p>在 Replica 同步（Replica 同步流程的讲解将会在下篇文章中详细展开）的 Fetch 请求中，其 Fetch 请求的构造如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 构造 Fetch 请求</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">buildFetchRequest</span></span>(partitionMap: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionFetchState</span>)]): <span class="type">FetchRequest</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> requestMap = <span class="keyword">new</span> util.<span class="type">LinkedHashMap</span>[<span class="type">TopicPartition</span>, <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>]</span><br><span class="line"></span><br><span class="line">  partitionMap.foreach &#123; <span class="keyword">case</span> (topicPartition, partitionFetchState) =&gt;</span><br><span class="line">    <span class="comment">// We will not include a replica in the fetch request if it should be throttled.</span></span><br><span class="line">    <span class="keyword">if</span> (partitionFetchState.isActive &amp;&amp; !shouldFollowerThrottle(quota, topicPartition))</span><br><span class="line">      requestMap.put(topicPartition, <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">PartitionData</span>(partitionFetchState.offset, fetchSize))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 关键在于 setReplicaId 方法,设置了 replicaId, consumer 的该值为 CONSUMER_REPLICA_ID（-1）</span></span><br><span class="line">  <span class="keyword">val</span> requestBuilder = <span class="keyword">new</span> <span class="type">JFetchRequest</span>.<span class="type">Builder</span>(maxWait, minBytes, requestMap).</span><br><span class="line">      setReplicaId(replicaId).setMaxBytes(maxBytes)</span><br><span class="line">  requestBuilder.setVersion(fetchRequestVersion)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">FetchRequest</span>(requestBuilder)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>与 Consumer Fetch 请求进行对比，这里区别仅在于在构造 FetchRequest 时，调用了 <code>setReplicaId()</code> 方法设置了对应的 replicaId，而 Consumer 在构造时则没有进行设置，该值默认为 <code>CONSUMER_REPLICA_ID</code>，即 <strong>-1</strong>，这个值是作为 Consumer 的 Fetch 请求与 Replica 同步的 Fetch 请求的区分。</p>
<h2 id="Server-端的处理"><a href="#Server-端的处理" class="headerlink" title="Server 端的处理"></a>Server 端的处理</h2><p>这里开始真正讲解 Fetch 请求的处理过程，会按照前面图中的处理流程开始讲解，本节主要是 Server 端抽象层的内容。</p>
<h3 id="KafkaApis-如何处理-Fetch-请求"><a href="#KafkaApis-如何处理-Fetch-请求" class="headerlink" title="KafkaApis 如何处理 Fetch 请求"></a>KafkaApis 如何处理 Fetch 请求</h3><p>关于 Fetch 请求的处理，如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Handle a fetch request</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> fetchRequest = request.body.asInstanceOf[<span class="type">FetchRequest</span>]</span><br><span class="line">  <span class="keyword">val</span> versionId = request.header.apiVersion</span><br><span class="line">  <span class="keyword">val</span> clientId = request.header.clientId</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 判断 tp 是否存在以及是否有 Describe 权限</span></span><br><span class="line">  <span class="keyword">val</span> (existingAndAuthorizedForDescribeTopics, nonExistingOrUnauthorizedForDescribeTopics) = fetchRequest.fetchData.asScala.toSeq.partition &#123;</span><br><span class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Describe</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic)) &amp;&amp; metadataCache.contains(tp.topic)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 判断 tp 是否有 Read 权限</span></span><br><span class="line">  <span class="keyword">val</span> (authorizedRequestInfo, unauthorizedForReadRequestInfo) = existingAndAuthorizedForDescribeTopics.partition &#123;</span><br><span class="line">    <span class="keyword">case</span> (tp, _) =&gt; authorize(request.session, <span class="type">Read</span>, <span class="keyword">new</span> <span class="type">Resource</span>(auth.<span class="type">Topic</span>, tp.topic))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 不存在或没有 Describe 权限的 topic 返回 UNKNOWN_TOPIC_OR_PARTITION 错误</span></span><br><span class="line">  <span class="keyword">val</span> nonExistingOrUnauthorizedForDescribePartitionData = nonExistingOrUnauthorizedForDescribeTopics.map &#123;</span><br><span class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 没有 Read 权限的 topic 返回 TOPIC_AUTHORIZATION_FAILED 错误</span></span><br><span class="line">  <span class="keyword">val</span> unauthorizedForReadPartitionData = unauthorizedForReadRequestInfo.map &#123;</span><br><span class="line">    <span class="keyword">case</span> (tp, _) =&gt; (tp, <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>.code, <span class="number">-1</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the callback for sending a fetch response</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responsePartitionData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)]) &#123;</span><br><span class="line">    ....</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetchResponseCallback</span></span>(delayTimeMs: <span class="type">Int</span>) &#123;</span><br><span class="line">      trace(<span class="string">s"Sending fetch response to client <span class="subst">$clientId</span> of "</span> +</span><br><span class="line">        <span class="string">s"<span class="subst">$&#123;convertedPartitionData.map &#123; case (_, v) =&gt; v.records.sizeInBytes &#125;</span>.sum&#125; bytes"</span>)</span><br><span class="line">      <span class="keyword">val</span> fetchResponse = <span class="keyword">if</span> (delayTimeMs &gt; <span class="number">0</span>) <span class="keyword">new</span> <span class="type">FetchResponse</span>(versionId, fetchedPartitionData, delayTimeMs) <span class="keyword">else</span> response</span><br><span class="line">      requestChannel.sendResponse(<span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">Response</span>(request, fetchResponse))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// When this callback is triggered, the remote API call has completed</span></span><br><span class="line">    request.apiRemoteCompleteTimeMs = time.milliseconds</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 配额情况的处理</span></span><br><span class="line">    <span class="keyword">if</span> (fetchRequest.isFromFollower) &#123;</span><br><span class="line">      <span class="comment">// We've already evaluated against the quota and are good to go. Just need to record it now.</span></span><br><span class="line">      <span class="keyword">val</span> responseSize = sizeOfThrottledPartitions(versionId, fetchRequest, mergedPartitionData, quotas.leader)</span><br><span class="line">      quotas.leader.record(responseSize)</span><br><span class="line">      fetchResponseCallback(<span class="number">0</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      quotas.fetch.recordAndMaybeThrottle(request.session.sanitizedUser, clientId, response.sizeOf, fetchResponseCallback)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</span><br><span class="line">    sendResponseCallback(<span class="type">Seq</span>.empty)</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// call the replica manager to fetch messages from the local replica</span></span><br><span class="line">    <span class="comment">//note: 从 replica 上拉取数据,满足条件后调用回调函数进行返回</span></span><br><span class="line">    replicaManager.fetchMessages(</span><br><span class="line">      fetchRequest.maxWait.toLong, <span class="comment">//note: 拉取请求最长的等待时间</span></span><br><span class="line">      fetchRequest.replicaId, <span class="comment">//note: Replica 编号，Consumer 的为 -1</span></span><br><span class="line">      fetchRequest.minBytes, <span class="comment">//note: 拉取请求设置的最小拉取字节</span></span><br><span class="line">      fetchRequest.maxBytes, <span class="comment">//note: 拉取请求设置的最大拉取字节</span></span><br><span class="line">      versionId &lt;= <span class="number">2</span>,</span><br><span class="line">      authorizedRequestInfo,</span><br><span class="line">      replicationQuota(fetchRequest),</span><br><span class="line">      sendResponseCallback)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Fetch 请求处理的真正实现是在 replicaManager 的 <code>fetchMessages()</code> 方法中，在这里，可以看出，无论是 Fetch 请求还是 Produce 请求，都是通过副本管理器来实现的，副本管理器（ReplicaManager）管理的对象是分区实例（Partition），而每个分区都会与相应的副本实例对应（Replica），在这个节点上的副本又会与唯一的 Log 实例对应，正如流程图的上半部分一样，Server 就是通过这几部分抽象概念来管理真正存储层的内容。</p>
<h3 id="ReplicaManager-如何处理-Fetch-请求"><a href="#ReplicaManager-如何处理-Fetch-请求" class="headerlink" title="ReplicaManager 如何处理 Fetch 请求"></a>ReplicaManager 如何处理 Fetch 请求</h3><p>ReplicaManger 处理 Fetch 请求的入口在 <code>fetchMessages()</code> 方法。</p>
<h4 id="fetchMessages"><a href="#fetchMessages" class="headerlink" title="fetchMessages"></a>fetchMessages</h4><p><code>fetchMessages()</code> 方法的具体如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Fetch messages from the leader replica, and wait until enough data can be fetched and return;</span></span><br><span class="line"><span class="comment"> * the callback function will be triggered either when timeout or required fetch info is satisfied</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 从 leader 拉取数据,等待拉取到足够的数据或者达到 timeout 时间后返回拉取的结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetchMessages</span></span>(timeout: <span class="type">Long</span>,</span><br><span class="line">                  replicaId: <span class="type">Int</span>,</span><br><span class="line">                  fetchMinBytes: <span class="type">Int</span>,</span><br><span class="line">                  fetchMaxBytes: <span class="type">Int</span>,</span><br><span class="line">                  hardMaxBytesLimit: <span class="type">Boolean</span>,</span><br><span class="line">                  fetchInfos: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</span><br><span class="line">                  quota: <span class="type">ReplicaQuota</span> = <span class="type">UnboundedQuota</span>,</span><br><span class="line">                  responseCallback: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)] =&gt; <span class="type">Unit</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> isFromFollower = replicaId &gt;= <span class="number">0</span> <span class="comment">//note: 判断请求是来自 consumer （这个值为 -1）还是副本同步</span></span><br><span class="line">  <span class="comment">//note: 默认都是从 leader 拉取，推测这个值只是为了后续能从 follower 消费数据而设置的</span></span><br><span class="line">  <span class="keyword">val</span> fetchOnlyFromLeader: <span class="type">Boolean</span> = replicaId != <span class="type">Request</span>.<span class="type">DebuggingConsumerId</span></span><br><span class="line">  <span class="comment">//note: 如果拉取请求来自 consumer（true）,只拉取 HW 以内的数据,如果是来自 Replica 同步,则没有该限制（false）。</span></span><br><span class="line">  <span class="keyword">val</span> fetchOnlyCommitted: <span class="type">Boolean</span> = ! <span class="type">Request</span>.isValidBrokerId(replicaId)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// read from local logs</span></span><br><span class="line">  <span class="comment">//note：获取本地日志</span></span><br><span class="line">  <span class="keyword">val</span> logReadResults = readFromLocalLog(</span><br><span class="line">    replicaId = replicaId,</span><br><span class="line">    fetchOnlyFromLeader = fetchOnlyFromLeader,</span><br><span class="line">    readOnlyCommitted = fetchOnlyCommitted,</span><br><span class="line">    fetchMaxBytes = fetchMaxBytes,</span><br><span class="line">    hardMaxBytesLimit = hardMaxBytesLimit,</span><br><span class="line">    readPartitionInfo = fetchInfos,</span><br><span class="line">    quota = quota)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// if the fetch comes from the follower,</span></span><br><span class="line">  <span class="comment">// update its corresponding log end offset</span></span><br><span class="line">  <span class="comment">//note: 如果 fetch 来自 broker 的副本同步,那么就更新相关的 log end offset</span></span><br><span class="line">  <span class="keyword">if</span>(<span class="type">Request</span>.isValidBrokerId(replicaId))</span><br><span class="line">    updateFollowerLogReadResults(replicaId, logReadResults)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// check if this fetch request can be satisfied right away</span></span><br><span class="line">  <span class="keyword">val</span> logReadResultValues = logReadResults.map &#123; <span class="keyword">case</span> (_, v) =&gt; v &#125;</span><br><span class="line">  <span class="keyword">val</span> bytesReadable = logReadResultValues.map(_.info.records.sizeInBytes).sum</span><br><span class="line">  <span class="keyword">val</span> errorReadingData = logReadResultValues.foldLeft(<span class="literal">false</span>) ((errorIncurred, readResult) =&gt;</span><br><span class="line">    errorIncurred || (readResult.error != <span class="type">Errors</span>.<span class="type">NONE</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// respond immediately if 1) fetch request does not want to wait</span></span><br><span class="line">  <span class="comment">//                        2) fetch request does not require any data</span></span><br><span class="line">  <span class="comment">//                        3) has enough data to respond</span></span><br><span class="line">  <span class="comment">//                        4) some error happens while reading data</span></span><br><span class="line">  <span class="comment">//note: 如果满足以下条件的其中一个,将会立马返回结果:</span></span><br><span class="line">  <span class="comment">//note: 1. timeout 达到; 2. 拉取结果为空; 3. 拉取到足够的数据; 4. 拉取是遇到 error</span></span><br><span class="line">  <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span> || fetchInfos.isEmpty || bytesReadable &gt;= fetchMinBytes || errorReadingData) &#123;</span><br><span class="line">    <span class="keyword">val</span> fetchPartitionData = logReadResults.map &#123; <span class="keyword">case</span> (tp, result) =&gt;</span><br><span class="line">      tp -&gt; <span class="type">FetchPartitionData</span>(result.error, result.hw, result.info.records)</span><br><span class="line">    &#125;</span><br><span class="line">    responseCallback(fetchPartitionData)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">//note： 其他情况下,延迟发送结果</span></span><br><span class="line">    <span class="comment">// construct the fetch results from the read results</span></span><br><span class="line">    <span class="keyword">val</span> fetchPartitionStatus = logReadResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">      <span class="keyword">val</span> fetchInfo = fetchInfos.collectFirst &#123;</span><br><span class="line">        <span class="keyword">case</span> (tp, v) <span class="keyword">if</span> tp == topicPartition =&gt; v</span><br><span class="line">      &#125;.getOrElse(sys.error(<span class="string">s"Partition <span class="subst">$topicPartition</span> not found in fetchInfos"</span>))</span><br><span class="line">      (topicPartition, <span class="type">FetchPartitionStatus</span>(result.info.fetchOffsetMetadata, fetchInfo))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> fetchMetadata = <span class="type">FetchMetadata</span>(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, fetchOnlyFromLeader,</span><br><span class="line">      fetchOnlyCommitted, isFromFollower, replicaId, fetchPartitionStatus)</span><br><span class="line">    <span class="keyword">val</span> delayedFetch = <span class="keyword">new</span> <span class="type">DelayedFetch</span>(timeout, fetchMetadata, <span class="keyword">this</span>, quota, responseCallback)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed fetch operation</span></span><br><span class="line">    <span class="keyword">val</span> delayedFetchKeys = fetchPartitionStatus.map &#123; <span class="keyword">case</span> (tp, _) =&gt; <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(tp) &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory;</span></span><br><span class="line">    <span class="comment">// this is because while the delayed fetch operation is being created, new requests</span></span><br><span class="line">    <span class="comment">// may arrive and hence make this operation completable.</span></span><br><span class="line">    delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整体来说，分为以下几步：</p>
<ol>
<li><code>readFromLocalLog()</code>：调用该方法，从本地日志拉取相应的数据；</li>
<li>判断 Fetch 请求来源，如果来自副本同步，那么更新该副本的 the end offset 记录，如果该副本不在 isr 中，并判断是否需要更新 isr；</li>
<li>返回结果，满足条件的话立马返回，否则的话，通过延迟操作，延迟返回结果。</li>
</ol>
<h4 id="readFromLocalLog"><a href="#readFromLocalLog" class="headerlink" title="readFromLocalLog"></a>readFromLocalLog</h4><p><code>readFromLocalLog()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Read from multiple topic partitions at the given offset up to maxSize bytes</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//note: 按 offset 从 tp 列表中读取相应的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readFromLocalLog</span></span>(replicaId: <span class="type">Int</span>,</span><br><span class="line">                     fetchOnlyFromLeader: <span class="type">Boolean</span>,</span><br><span class="line">                     readOnlyCommitted: <span class="type">Boolean</span>,</span><br><span class="line">                     fetchMaxBytes: <span class="type">Int</span>,</span><br><span class="line">                     hardMaxBytesLimit: <span class="type">Boolean</span>,</span><br><span class="line">                     readPartitionInfo: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</span><br><span class="line">                     quota: <span class="type">ReplicaQuota</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(tp: <span class="type">TopicPartition</span>, fetchInfo: <span class="type">PartitionData</span>, limitBytes: <span class="type">Int</span>, minOneMessage: <span class="type">Boolean</span>): <span class="type">LogReadResult</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> offset = fetchInfo.offset</span><br><span class="line">    <span class="keyword">val</span> partitionFetchSize = fetchInfo.maxBytes</span><br><span class="line"></span><br><span class="line">    <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).totalFetchRequestRate.mark()</span><br><span class="line">    <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().totalFetchRequestRate.mark()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      trace(<span class="string">s"Fetching log segment for partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>, partition fetch size <span class="subst">$partitionFetchSize</span>, "</span> +</span><br><span class="line">        <span class="string">s"remaining response limit <span class="subst">$limitBytes</span>"</span> +</span><br><span class="line">        (<span class="keyword">if</span> (minOneMessage) <span class="string">s", ignoring response/partition size limits"</span> <span class="keyword">else</span> <span class="string">""</span>))</span><br><span class="line"></span><br><span class="line">      <span class="comment">// decide whether to only fetch from leader</span></span><br><span class="line">      <span class="comment">//note: 根据决定 [是否只从 leader 读取数据] 来获取相应的副本</span></span><br><span class="line">      <span class="comment">//note: 根据 tp 获取 Partition 对象, 在获取相应的 Replica 对象</span></span><br><span class="line">      <span class="keyword">val</span> localReplica = <span class="keyword">if</span> (fetchOnlyFromLeader)</span><br><span class="line">        getLeaderReplicaIfLocal(tp)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        getReplicaOrException(tp)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// decide whether to only fetch committed data (i.e. messages below high watermark)</span></span><br><span class="line">      <span class="comment">//note: 获取 hw 位置，副本同步不设置这个值</span></span><br><span class="line">      <span class="keyword">val</span> maxOffsetOpt = <span class="keyword">if</span> (readOnlyCommitted)</span><br><span class="line">        <span class="type">Some</span>(localReplica.highWatermark.messageOffset)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="type">None</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">/* Read the LogOffsetMetadata prior to performing the read from the log.</span></span><br><span class="line"><span class="comment">       * We use the LogOffsetMetadata to determine if a particular replica is in-sync or not.</span></span><br><span class="line"><span class="comment">       * Using the log end offset after performing the read can lead to a race condition</span></span><br><span class="line"><span class="comment">       * where data gets appended to the log immediately after the replica has consumed from it</span></span><br><span class="line"><span class="comment">       * This can cause a replica to always be out of sync.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">val</span> initialLogEndOffset = localReplica.logEndOffset.messageOffset <span class="comment">//note: the end offset</span></span><br><span class="line">      <span class="keyword">val</span> initialHighWatermark = localReplica.highWatermark.messageOffset <span class="comment">//note: hw</span></span><br><span class="line">      <span class="keyword">val</span> fetchTimeMs = time.milliseconds</span><br><span class="line">      <span class="keyword">val</span> logReadInfo = localReplica.log <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</span><br><span class="line">          <span class="keyword">val</span> adjustedFetchSize = math.min(partitionFetchSize, limitBytes)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Try the read first, this tells us whether we need all of adjustedFetchSize for this partition</span></span><br><span class="line">          <span class="comment">//note: 从指定的 offset 位置开始读取数据，副本同步不需要 maxOffsetOpt</span></span><br><span class="line">          <span class="keyword">val</span> fetch = log.read(offset, adjustedFetchSize, maxOffsetOpt, minOneMessage)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// If the partition is being throttled, simply return an empty set.</span></span><br><span class="line">          <span class="keyword">if</span> (shouldLeaderThrottle(quota, tp, replicaId)) <span class="comment">//note: 如果被限速了,那么返回 空 集合</span></span><br><span class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">          <span class="comment">// For FetchRequest version 3, we replace incomplete message sets with an empty one as consumers can make</span></span><br><span class="line">          <span class="comment">// progress in such cases and don't need to report a `RecordTooLargeException`</span></span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">if</span> (!hardMaxBytesLimit &amp;&amp; fetch.firstEntryIncomplete)</span><br><span class="line">            <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">          <span class="keyword">else</span> fetch</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          error(<span class="string">s"Leader for partition <span class="subst">$tp</span> does not have a local log"</span>)</span><br><span class="line">          <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//note: 返回最后的结果,返回的都是 LogReadResult 对象</span></span><br><span class="line">      <span class="type">LogReadResult</span>(info = logReadInfo,</span><br><span class="line">                    hw = initialHighWatermark,</span><br><span class="line">                    leaderLogEndOffset = initialLogEndOffset,</span><br><span class="line">                    fetchTimeMs = fetchTimeMs,</span><br><span class="line">                    readSize = partitionFetchSize,</span><br><span class="line">                    exception = <span class="type">None</span>)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// <span class="doctag">NOTE:</span> Failed fetch requests metric is not incremented for known exceptions since it</span></span><br><span class="line">      <span class="comment">// is supposed to indicate un-expected failure of a broker in handling a fetch request</span></span><br><span class="line">      <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</span><br><span class="line">               _: <span class="type">NotLeaderForPartitionException</span> |</span><br><span class="line">               _: <span class="type">ReplicaNotAvailableException</span> |</span><br><span class="line">               _: <span class="type">OffsetOutOfRangeException</span>) =&gt;</span><br><span class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</span><br><span class="line">                      hw = <span class="number">-1</span>L,</span><br><span class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</span><br><span class="line">                      fetchTimeMs = <span class="number">-1</span>L,</span><br><span class="line">                      readSize = partitionFetchSize,</span><br><span class="line">                      exception = <span class="type">Some</span>(e))</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(tp.topic).failedFetchRequestRate.mark()</span><br><span class="line">        <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats().failedFetchRequestRate.mark()</span><br><span class="line">        error(<span class="string">s"Error processing fetch operation on partition <span class="subst">$tp</span>, offset <span class="subst">$offset</span>"</span>, e)</span><br><span class="line">        <span class="type">LogReadResult</span>(info = <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>),</span><br><span class="line">                      hw = <span class="number">-1</span>L,</span><br><span class="line">                      leaderLogEndOffset = <span class="number">-1</span>L,</span><br><span class="line">                      fetchTimeMs = <span class="number">-1</span>L,</span><br><span class="line">                      readSize = partitionFetchSize,</span><br><span class="line">                      exception = <span class="type">Some</span>(e))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> limitBytes = fetchMaxBytes</span><br><span class="line">  <span class="keyword">val</span> result = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]</span><br><span class="line">  <span class="keyword">var</span> minOneMessage = !hardMaxBytesLimit</span><br><span class="line">  readPartitionInfo.foreach &#123; <span class="keyword">case</span> (tp, fetchInfo) =&gt;</span><br><span class="line">    <span class="keyword">val</span> readResult = read(tp, fetchInfo, limitBytes, minOneMessage) <span class="comment">//note: 读取该 tp 的数据</span></span><br><span class="line">    <span class="keyword">val</span> messageSetSize = readResult.info.records.sizeInBytes</span><br><span class="line">    <span class="comment">// Once we read from a non-empty partition, we stop ignoring request and partition level size limits</span></span><br><span class="line">    <span class="keyword">if</span> (messageSetSize &gt; <span class="number">0</span>)</span><br><span class="line">      minOneMessage = <span class="literal">false</span></span><br><span class="line">    limitBytes = math.max(<span class="number">0</span>, limitBytes - messageSetSize)</span><br><span class="line">    result += (tp -&gt; readResult)</span><br><span class="line">  &#125;</span><br><span class="line">  result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>readFromLocalLog()</code> 方法的处理过程：</p>
<ol>
<li>先根据要拉取的 topic-partition 获取对应的 Partition 对象，根据 Partition 对象获取对应的 Replica 对象；</li>
<li>根据 Replica 对象找到对应的 Log 对象，然后调用其 <code>read()</code> 方法从指定的位置读取数据。</li>
</ol>
<h2 id="存储层对-Fetch-请求的处理"><a href="#存储层对-Fetch-请求的处理" class="headerlink" title="存储层对 Fetch 请求的处理"></a>存储层对 Fetch 请求的处理</h2><p>接着前面的流程开始往下走。</p>
<h3 id="Log-对象"><a href="#Log-对象" class="headerlink" title="Log 对象"></a>Log 对象</h3><p>每个 Replica 会对应一个 log 对象，而每个 log 对象会管理相应的 LogSegment 实例。</p>
<h4 id="read"><a href="#read" class="headerlink" title="read()"></a>read()</h4><p>Log 对象的 <code>read()</code> 方法的实现如下所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 从指定 offset 开始读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxLength: <span class="type">Int</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>] = <span class="type">None</span>, minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</span><br><span class="line">  trace(<span class="string">"Reading %d bytes from offset %d in log %s of length %d bytes"</span>.format(maxLength, startOffset, name, size))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Because we don't use lock for reading, the synchronization is a little bit tricky.</span></span><br><span class="line">  <span class="comment">// We create the local variables to avoid race conditions with updates to the log.</span></span><br><span class="line">  <span class="keyword">val</span> currentNextOffsetMetadata = nextOffsetMetadata</span><br><span class="line">  <span class="keyword">val</span> next = currentNextOffsetMetadata.messageOffset</span><br><span class="line">  <span class="keyword">if</span>(startOffset == next)</span><br><span class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(currentNextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 先查找对应的日志分段（segment）</span></span><br><span class="line">  <span class="keyword">var</span> entry = segments.floorEntry(startOffset)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// attempt to read beyond the log end offset is an error</span></span><br><span class="line">  <span class="keyword">if</span>(startOffset &gt; next || entry == <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(<span class="string">"Request for offset %d but we only have log segments in the range %d to %d."</span>.format(startOffset, segments.firstKey, next))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Do the read on the segment with a base offset less than the target offset</span></span><br><span class="line">  <span class="comment">// but if that segment doesn't contain any messages with an offset greater than that</span></span><br><span class="line">  <span class="comment">// continue to read from successive segments until we get some messages or we reach the end of the log</span></span><br><span class="line">  <span class="keyword">while</span>(entry != <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">// If the fetch occurs on the active segment, there might be a race condition where two fetch requests occur after</span></span><br><span class="line">    <span class="comment">// the message is appended but before the nextOffsetMetadata is updated. In that case the second fetch may</span></span><br><span class="line">    <span class="comment">// cause OffsetOutOfRangeException. To solve that, we cap the reading up to exposed position instead of the log</span></span><br><span class="line">    <span class="comment">// end of the active segment.</span></span><br><span class="line">    <span class="comment">//note: 如果 Fetch 请求刚好发生在 the active segment 上,当多个 Fetch 请求同时处理,如果 nextOffsetMetadata 更新不及时,可能会导致</span></span><br><span class="line">    <span class="comment">//note: 发送 OffsetOutOfRangeException 异常; 为了解决这个问题, 这里能读取的最大位置是对应的物理位置（exposedPos）</span></span><br><span class="line">    <span class="comment">//note: 而不是 the log end of the active segment.</span></span><br><span class="line">    <span class="keyword">val</span> maxPosition = &#123;</span><br><span class="line">      <span class="keyword">if</span> (entry == segments.lastEntry) &#123;</span><br><span class="line">        <span class="comment">//note: nextOffsetMetadata 对应的实际物理位置</span></span><br><span class="line">        <span class="keyword">val</span> exposedPos = nextOffsetMetadata.relativePositionInSegment.toLong</span><br><span class="line">        <span class="comment">// Check the segment again in case a new segment has just rolled out.</span></span><br><span class="line">        <span class="keyword">if</span> (entry != segments.lastEntry) <span class="comment">//note: 可能会有新的 segment 产生,所以需要再次判断</span></span><br><span class="line">          <span class="comment">// New log segment has rolled out, we can read up to the file end.</span></span><br><span class="line">          entry.getValue.size</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">          exposedPos</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        entry.getValue.size</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//note: 从 segment 中读取相应的数据</span></span><br><span class="line">    <span class="keyword">val</span> fetchInfo = entry.getValue.read(startOffset, maxOffset, maxLength, maxPosition, minOneMessage)</span><br><span class="line">    <span class="keyword">if</span>(fetchInfo == <span class="literal">null</span>) &#123; <span class="comment">//note: 如果该日志分段没有读取到数据,则读取更高的日志分段</span></span><br><span class="line">      entry = segments.higherEntry(entry.getKey)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> fetchInfo</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// okay we are beyond the end of the last segment with no data fetched although the start offset is in range,</span></span><br><span class="line">  <span class="comment">// this can happen when all messages with offset larger than start offsets have been deleted.</span></span><br><span class="line">  <span class="comment">// In this case, we will return the empty set with log end offset metadata</span></span><br><span class="line">  <span class="type">FetchDataInfo</span>(nextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从实现可以看出，该方法会先查找对应的 Segment 对象（日志分段），然后循环直到读取到数据结束，如果当前的日志分段没有读取到相应的数据，那么会更新日志分段及对应的最大位置。</p>
<p>日志分段实际上是逻辑概念，它管理了物理概念的一个数据文件、一个时间索引文件和一个 offset 索引文件，读取日志分段时，会先读取 offset 索引文件再读取数据文件，具体步骤如下：</p>
<ol>
<li>根据要读取的起始偏移量（startOffset）读取 offset 索引文件中对应的物理位置；</li>
<li>查找 offset 索引文件最后返回：起始偏移量对应的最近物理位置（startPosition）；</li>
<li>根据 startPosition 直接定位到数据文件，然后读取数据文件内容；</li>
<li>最多能读到数据文件的结束位置（maxPosition）。</li>
</ol>
<h3 id="LogSegment"><a href="#LogSegment" class="headerlink" title="LogSegment"></a>LogSegment</h3><p>关乎 数据文件、offset 索引文件和时间索引文件真正的操作都是在 LogSegment 对象中的，日志读取也与这个方法息息相关。</p>
<h4 id="read-1"><a href="#read-1" class="headerlink" title="read()"></a>read()</h4><p><code>read()</code> 方法的实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 读取日志分段（副本同步不会设置 maxSize）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>], maxSize: <span class="type">Int</span>, maxPosition: <span class="type">Long</span> = size,</span><br><span class="line">         minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (maxSize &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Invalid max size for log read (%d)"</span>.format(maxSize))</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: log 文件物理长度</span></span><br><span class="line">  <span class="keyword">val</span> logSize = log.sizeInBytes <span class="comment">// this may change, need to save a consistent copy</span></span><br><span class="line">  <span class="comment">//note: 将起始的 offset 转换为起始的实际物理位置</span></span><br><span class="line">  <span class="keyword">val</span> startOffsetAndSize = translateOffset(startOffset)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// if the start position is already off the end of the log, return null</span></span><br><span class="line">  <span class="keyword">if</span> (startOffsetAndSize == <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> startPosition = startOffsetAndSize.position.toInt</span><br><span class="line">  <span class="keyword">val</span> offsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(startOffset, <span class="keyword">this</span>.baseOffset, startPosition)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> adjustedMaxSize =</span><br><span class="line">    <span class="keyword">if</span> (minOneMessage) math.max(maxSize, startOffsetAndSize.size)</span><br><span class="line">    <span class="keyword">else</span> maxSize</span><br><span class="line"></span><br><span class="line">  <span class="comment">// return a log segment but with zero size in the case below</span></span><br><span class="line">  <span class="keyword">if</span> (adjustedMaxSize == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// calculate the length of the message set to read based on whether or not they gave us a maxOffset</span></span><br><span class="line">  <span class="comment">//note: 计算读取的长度</span></span><br><span class="line">  <span class="keyword">val</span> length = maxOffset <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="comment">//note: 副本同步时的计算方式</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      <span class="comment">// no max offset, just read until the max position</span></span><br><span class="line">      min((maxPosition - startPosition).toInt, adjustedMaxSize) <span class="comment">//note: 直接读取到最大的位置</span></span><br><span class="line">    <span class="comment">//note: consumer 拉取时,计算方式</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt;</span><br><span class="line">      <span class="comment">// there is a max offset, translate it to a file position and use that to calculate the max read size;</span></span><br><span class="line">      <span class="comment">// when the leader of a partition changes, it's possible for the new leader's high watermark to be less than the</span></span><br><span class="line">      <span class="comment">// true high watermark in the previous leader for a short window. In this window, if a consumer fetches on an</span></span><br><span class="line">      <span class="comment">// offset between new leader's high watermark and the log end offset, we want to return an empty response.</span></span><br><span class="line">      <span class="keyword">if</span> (offset &lt; startOffset)</span><br><span class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>, firstEntryIncomplete = <span class="literal">false</span>)</span><br><span class="line">      <span class="keyword">val</span> mapping = translateOffset(offset, startPosition)</span><br><span class="line">      <span class="keyword">val</span> endPosition =</span><br><span class="line">        <span class="keyword">if</span> (mapping == <span class="literal">null</span>)</span><br><span class="line">          logSize <span class="comment">// the max offset is off the end of the log, use the end of the file</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">          mapping.position</span><br><span class="line">      min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//note: 根据起始的物理位置和读取长度读取数据文件</span></span><br><span class="line">  <span class="type">FetchDataInfo</span>(offsetMetadata, log.read(startPosition, length),</span><br><span class="line">    firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的实现来看，上述过程分为以下三部分：</p>
<ol>
<li>根据 startOffset 得到实际的物理位置（<code>translateOffset()</code>）；</li>
<li>计算要读取的实际物理长度；</li>
<li>根据实际起始物理位置和要读取实际物理长度读取数据文件。</li>
</ol>
<h4 id="translateOffset"><a href="#translateOffset" class="headerlink" title="translateOffset()"></a>translateOffset()</h4><p><code>translateOffset()</code> 方法的实现过程主要分为两部分：</p>
<ol>
<li>查找 offset 索引文件：调用 offset 索引文件的 <code>lookup()</code> 查找方法，获取离 startOffset 最接近的物理位置；</li>
<li>调用数据文件的 <code>searchFor()</code> 方法，从指定的物理位置开始读取每条数据，知道找到对应 offset 的物理位置。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">translateOffset</span></span>(offset: <span class="type">Long</span>, startingFilePosition: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogEntryPosition</span> = &#123;</span><br><span class="line">  <span class="comment">//note: 获取离 offset 最新的物理位置,返回包括 offset 和物理位置（不是准确值）</span></span><br><span class="line">  <span class="keyword">val</span> mapping = index.lookup(offset)</span><br><span class="line">  <span class="comment">//note: 从指定的位置开始消费,直到找到 offset 对应的实际物理位置,返回包括 offset 和物理位置（准确值）</span></span><br><span class="line">  log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="查找-offset-索引文件"><a href="#查找-offset-索引文件" class="headerlink" title="查找 offset 索引文件"></a>查找 offset 索引文件</h5><p>offset 索引文件是使用内存映射（不了解的，可以阅读 <a href="http://matt33.com/2018/02/04/linux-mmap/">操作系统之共享对象学习</a>）的方式加载到内存中的，在查询的过程中，内存映射是会发生变化，所以在 <code>lookup()</code> 中先拷贝出来了一个（idx），然后再进行查询，具体实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 查找小于等于指定 offset 的最大 offset,并且返回对应的 offset 和实际物理位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(targetOffset: <span class="type">Long</span>): <span class="type">OffsetPosition</span> = &#123;</span><br><span class="line">  maybeLock(lock) &#123;</span><br><span class="line">    <span class="keyword">val</span> idx = mmap.duplicate <span class="comment">//note: 查询时,mmap 会发生变化,先复制出来一个</span></span><br><span class="line">    <span class="keyword">val</span> slot = indexSlotFor(idx, targetOffset, <span class="type">IndexSearchType</span>.<span class="type">KEY</span>) <span class="comment">//note: 二分查找</span></span><br><span class="line">    <span class="keyword">if</span>(slot == <span class="number">-1</span>)</span><br><span class="line">      <span class="type">OffsetPosition</span>(baseOffset, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="comment">//note: 先计算绝对偏移量,再计算物理位置</span></span><br><span class="line">      parseEntry(idx, slot).asInstanceOf[<span class="type">OffsetPosition</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">parseEntry</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">IndexEntry</span> = &#123;</span><br><span class="line">    <span class="type">OffsetPosition</span>(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">relativeOffset</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">physical</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize + <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>关于 relativeOffset 和 physical 的计算方法，可以参考下面这张图（来自《Kafka 计算内幕》）：</p>
<p><img src="/images/kafka/offset-physical.png" alt="根据索引条目编号查找偏移量的值和物理位置的值"></p>
<h5 id="搜索数据文件获取准确的物理位置"><a href="#搜索数据文件获取准确的物理位置" class="headerlink" title="搜索数据文件获取准确的物理位置"></a>搜索数据文件获取准确的物理位置</h5><p>前面通过 offset 索引文件获取的物理位置是一个接近值，下面通过实际读取数据文件将会得到一个真正的准确值，它是通过遍历数据文件实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Search forward for the file position of the last offset that is greater than or equal to the target offset</span></span><br><span class="line"><span class="comment"> * and return its physical position and the size of the message (including log overhead) at the returned offset. If</span></span><br><span class="line"><span class="comment"> * no such offsets are found, return null.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param targetOffset The offset to search for.</span></span><br><span class="line"><span class="comment"> * @param startingPosition The starting position in the file to begin searching from.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">public <span class="type">LogEntryPosition</span> searchForOffsetWithSize(long targetOffset, int startingPosition) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileChannelLogEntry</span> entry : shallowEntriesFrom(startingPosition)) &#123;</span><br><span class="line">        long offset = entry.offset();</span><br><span class="line">        <span class="keyword">if</span> (offset &gt;= targetOffset)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">LogEntryPosition</span>(offset, entry.position(), entry.sizeInBytes());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>到这里，一个 Fetch 请求的处理过程算是完成了。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/" data-id="ck4imvxqq00a05zzaxl2dfep7" class="article-share-link">分享到</a><div class="copyright"><a href="http://matt33.com/copyright/">博客版权说明</a></div><div class="tags"><a href="/tags/kafka/">kafka</a></div><div class="post-nav"><a href="/2018/04/29/kafka-replica-fetcher-thread/" class="pre">Kafka 源码解析之副本同步机制实现（十四）</a><a href="/2018/03/18/kafka-server-handle-produce-request/" class="next">Kafka 源码解析之 Server 端如何处理 Produce 请求（十二）</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论 「请确保 disqus.com 可以正常加载」</button></div><script>var disqus_shortname = 'http-matt33-com';
var disqus_identifier = '2018/04/15/kafka-server-handle-fetch-request/';
var disqus_title = 'Kafka 源码解析之 Server 端如何处理 Fetch 请求（十三）';
var disqus_url = 'http://matt33.com/2018/04/15/kafka-server-handle-fetch-request/';
$('.btn_click_load').click(function() {
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('.btn_click_load').css('display','none');
});
$.ajax({
  url: 'https://disqus.com/favicon.ico',
  timeout: 3000,
  type: 'GET',
  success: (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    $('.btn_click_load').css('display','none');
  })(),
  error: function() {
    $('.btn_click_load').css('display','block');
  }
});</script><script id="dsq-count-scr" src="//http-matt33-com.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-weibo"> 微博</i></div><iframe width="100%" height="90" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=100&fansRow=1&ptype=1&speed=0&skin=1&isTitle=0&noborder=1&isWeibo=0&isFans=0&uid=2650396571&verifier=f2f0e397&colors=D8D8D8,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/书屋/">书屋</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/影如人生/">影如人生</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术/">技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/旅行/">旅行</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/转载/">转载</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tcp/" style="font-size: 15px;">tcp</a> <a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/storm/" style="font-size: 15px;">storm</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/learn/" style="font-size: 15px;">learn</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/bug/" style="font-size: 15px;">bug</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/travel/" style="font-size: 15px;">travel</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/database/" style="font-size: 15px;">database</a> <a href="/tags/电影随想/" style="font-size: 15px;">电影随想</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/转载/" style="font-size: 15px;">转载</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/jvm/" style="font-size: 15px;">jvm</a> <a href="/tags/web/" style="font-size: 15px;">web</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a> <a href="/tags/bk/" style="font-size: 15px;">bk</a> <a href="/tags/rpc/" style="font-size: 15px;">rpc</a> <a href="/tags/thrift/" style="font-size: 15px;">thrift</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/calcite/" style="font-size: 15px;">calcite</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/12/23/flink-master-5/">Flink Master 详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/19/flink-execution-graph-4/">Flink 如何生成 ExecutionGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/09/flink-job-graph-3/">Flink Streaming 作业如何转化为 JobGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/08/flink-stream-graph-2/">Flink DataStream API 概述及作业如何转换为 StreamGraph</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/11/23/flink-learn-start-1/">Apache Flink 初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/27/paper-chandy-lamport/">Paper 阅读: Distributed Snapshots: Determining Global States of Distributed Systems</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/20/paper-flink-snapshot/">Paper 阅读: Lightweight Asynchronous Snapshots for Distributed Dataflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/19/paper-ray1/">Paper 阅读: Real-Time Machine Learning: The Missing Pieces</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/17/apache-calcite-planner/">Apache Calcite 优化器详解（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/07/apache-calcite-process-flow/">Apache Calcite 处理流程详解（一）</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://matt33.com/about/" title="个人公众号：柳年思水" target="_blank">个人公众号：柳年思水</a><ul></ul><a href="http://tech.meituan.com/" title="美团点评技术团队" target="_blank">美团点评技术团队</a><ul></ul><a href="http://jm.taobao.org/" title="阿里中间件团队博客" target="_blank">阿里中间件团队博客</a><ul></ul><a href="http://www.jianshu.com/" title="简书" target="_blank">简书</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Matt's Blog 柳年思水.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><div class="analytics"><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1256517224'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1256517224%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script></div><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-64518924-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?5cf44757fa0d23bc7637935e44a9104a";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>